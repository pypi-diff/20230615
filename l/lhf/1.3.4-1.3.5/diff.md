# Comparing `tmp/lhf-1.3.4-py3-none-manylinux_2_28_x86_64.whl.zip` & `tmp/lhf-1.3.5-py3-none-manylinux_2_28_x86_64.whl.zip`

## zipinfo {}

```diff
@@ -1,79 +1,81 @@
-Zip file size: 5605504 bytes, number of entries: 77
-drwxr-xr-x  2.0 unx        0 b- stor 23-May-25 20:02 lhf-1.3.4.dist-info/
-drwxr-xr-x  2.0 unx        0 b- stor 23-May-25 20:02 pyLHF/
-drwxr-xr-x  2.0 unx        0 b- stor 23-May-25 20:02 lhf.libs/
-drwxr-xr-x  2.0 unx        0 b- stor 23-May-25 20:02 LHF/
--rw-rw-r--  2.0 unx     6465 b- defN 23-May-25 20:02 lhf-1.3.4.dist-info/RECORD
--rw-r--r--  2.0 unx       10 b- defN 23-May-25 20:02 lhf-1.3.4.dist-info/top_level.txt
--rw-r--r--  2.0 unx     3773 b- defN 23-May-25 20:02 lhf-1.3.4.dist-info/METADATA
--rw-r--r--  2.0 unx      133 b- defN 23-May-25 20:02 lhf-1.3.4.dist-info/WHEEL
--rw-r--r--  2.0 unx        0 b- defN 23-May-25 20:02 pyLHF/__init__.py
--rw-r--r--  2.0 unx       98 b- defN 23-May-25 20:02 pyLHF/__main__.py
--rw-r--r--  2.0 unx      581 b- defN 23-May-25 20:02 pyLHF/pyLHF.py
--rwxr-xr-x  2.0 unx   533337 b- defN 23-May-25 20:02 lhf.libs/libmpfr-b7fafdf8.so.4.1.6
--rwxr-xr-x  2.0 unx  1570473 b- defN 23-May-25 20:02 lhf.libs/libgmp-452f15e0.so.10.3.2
--rwxr-xr-x  2.0 unx   150777 b- defN 23-May-25 20:02 lhf.libs/libbetaSubSkeletonComplex-8505afc6.so
--rwxr-xr-x  2.0 unx   775729 b- defN 23-May-25 20:02 lhf.libs/libopen-pal-d1f167ef.so.40.30.1
--rwxr-xr-x  2.0 unx   251561 b- defN 23-May-25 20:02 lhf.libs/libreadInput-09604ee0.so
--rwxr-xr-x  2.0 unx   152769 b- defN 23-May-25 20:02 lhf.libs/libnaiveWindow-e81362bf.so
--rwxr-xr-x  2.0 unx    16937 b- defN 23-May-25 20:02 lhf.libs/libstreamingKmeans-1d5a99ac.so
--rwxr-xr-x  2.0 unx   138633 b- defN 23-May-25 20:02 lhf.libs/libupscalePipe-026df4c1.so
--rwxr-xr-x  2.0 unx   343225 b- defN 23-May-25 20:02 lhf.libs/libhwloc-a765a097.so.15.2.0
--rwxr-xr-x  2.0 unx  1335505 b- defN 23-May-25 20:02 lhf.libs/libmpi-2fa556dc.so.40.30.1
--rwxr-xr-x  2.0 unx   272617 b- defN 23-May-25 20:02 lhf.libs/libevent_core-2-e6cf51bb.1.so.6.0.2
--rwxr-xr-x  2.0 unx   155449 b- defN 23-May-25 20:02 lhf.libs/libbasePipe-666b0f21.so
--rwxr-xr-x  2.0 unx   190113 b- defN 23-May-25 20:02 lhf.libs/libbetaSkeletonBasedComplex-3d0aa4d3.so
--rwxr-xr-x  2.0 unx    82113 b- defN 23-May-25 20:02 lhf.libs/libneighGraphPipe-9a204ef5.so
--rwxr-xr-x  2.0 unx    58865 b- defN 23-May-25 20:02 lhf.libs/libkMeansPlusPlus-6a0e4583.so
--rwxr-xr-x  2.0 unx   296369 b- defN 23-May-25 20:02 lhf.libs/libfastPersistence-28e8389e.so
--rwxr-xr-x  2.0 unx    94569 b- defN 23-May-25 20:02 lhf.libs/libdistMatrixPipe-6e32877f.so
--rwxr-xr-x  2.0 unx   243857 b- defN 23-May-25 20:02 lhf.libs/libsimplexTree-05a4764e.so
--rwxr-xr-x  2.0 unx   779985 b- defN 23-May-25 20:02 lhf.libs/libdelaunayPipe-96aabe07.so
--rwxr-xr-x  2.0 unx    76241 b- defN 23-May-25 20:02 lhf.libs/libgmpxx-6bc3ea01.so.4.5.2
--rwxr-xr-x  2.0 unx    16937 b- defN 23-May-25 20:02 lhf.libs/libstreamingUtils-33aec4bf.so
--rwxr-xr-x  2.0 unx    48937 b- defN 23-May-25 20:02 lhf.libs/libwriteOutput-6d79e888.so
--rwxr-xr-x  2.0 unx    54137 b- defN 23-May-25 20:02 lhf.libs/libwitnessComplex-4477f1c7.so
--rwxr-xr-x  2.0 unx  3207745 b- defN 23-May-25 20:02 lhf.libs/libcrypto-d9af65bd.so.1.1.1k
--rwxr-xr-x  2.0 unx   277153 b- defN 23-May-25 20:02 lhf.libs/libutils-fe381086.so
--rwxr-xr-x  2.0 unx   619425 b- defN 23-May-25 20:02 lhf.libs/libqhull_r-c601f68f.so.8.1-alpha3
--rwxr-xr-x  2.0 unx   176801 b- defN 23-May-25 20:02 lhf.libs/libmpi_cxx-c0da7930.so.40.30.0
--rwxr-xr-x  2.0 unx    80921 b- defN 23-May-25 20:02 lhf.libs/libripsPipe-551e5c47.so
--rwxr-xr-x  2.0 unx   333625 b- defN 23-May-25 20:02 lhf.libs/libqhullPipe-4752bd44.so
--rwxr-xr-x  2.0 unx   186977 b- defN 23-May-25 20:02 lhf.libs/libslidingWindow-7bc560f7.so
--rwxr-xr-x  2.0 unx    30777 b- defN 23-May-25 20:02 lhf.libs/libcluster-6eab6735.so
--rwxr-xr-x  2.0 unx   179385 b- defN 23-May-25 20:02 lhf.libs/libalphaComplex-6dc790b1.so
--rwxr-xr-x  2.0 unx    54809 b- defN 23-May-25 20:02 lhf.libs/libpipePacket-4067a5e9.so
--rwxr-xr-x  2.0 unx   116241 b- defN 23-May-25 20:02 lhf.libs/libbetaComplex-ec9e55cb.so
--rwxr-xr-x  2.0 unx   322609 b- defN 23-May-25 20:02 lhf.libs/libincrementalPersistence-9375ebef.so
--rwxr-xr-x  2.0 unx    93969 b- defN 23-May-25 20:02 lhf.libs/libdenStream-7f09c880.so
--rwxr-xr-x  2.0 unx    64049 b- defN 23-May-25 20:02 lhf.libs/libkdTree-17ab0886.so
--rwxr-xr-x  2.0 unx    22729 b- defN 23-May-25 20:02 lhf.libs/libdbscan-7d5374b7.so
--rwxr-xr-x  2.0 unx    12985 b- defN 23-May-25 20:02 lhf.libs/libopen-orted-mpir-06a2f0b2.so
--rwxr-xr-x  2.0 unx   220025 b- defN 23-May-25 20:02 lhf.libs/libsimplexBase-6b25e600.so
--rwxr-xr-x  2.0 unx   854009 b- defN 23-May-25 20:02 lhf.libs/libopen-rte-1aa20eaf.so.40.30.1
--rwxr-xr-x  2.0 unx    88417 b- defN 23-May-25 20:02 lhf.libs/libpreprocessor-73ae98ae.so
--rwxr-xr-x  2.0 unx    17129 b- defN 23-May-25 20:02 lhf.libs/libevent_pthreads-2-9991b589.1.so.6.0.2
--rwxr-xr-x  2.0 unx   253289 b- defN 23-May-25 20:02 lhf.libs/libgomp-2c51bfab.so.1.0.0
--rwxr-xr-x  2.0 unx    71961 b- defN 23-May-25 20:02 lhf.libs/libargParser-e71dc2de.so
--rwxr-xr-x  2.0 unx   247281 b- defN 23-May-25 20:02 lhf.libs/libsimplexArrayList-4cbe4342.so
-drwxr-xr-x  2.0 unx        0 b- stor 23-May-25 20:02 LHF/DataGeneration/
-drwxr-xr-x  2.0 unx        0 b- stor 23-May-25 20:02 LHF/OutputAnalysis/
--rw-r--r--  2.0 unx     9210 b- defN 23-May-25 20:02 LHF/utilities.py
--rw-r--r--  2.0 unx     5461 b- defN 23-May-25 20:02 LHF/LHF.py
--rw-r--r--  2.0 unx     3361 b- defN 23-May-25 20:02 LHF/triangulation.py
--rw-r--r--  2.0 unx      149 b- defN 23-May-25 20:02 LHF/__init__.py
--rwxr-xr-x  2.0 unx  3265945 b- defN 23-May-25 20:02 LHF/libLHFlib.so
--rw-r--r--  2.0 unx     2020 b- defN 23-May-25 20:02 LHF/curves.py
--rw-r--r--  2.0 unx     2209 b- defN 23-May-25 20:02 LHF/DataGeneration/embeddings.py
--rw-r--r--  2.0 unx     2258 b- defN 23-May-25 20:02 LHF/DataGeneration/projectivePlane.py
--rw-r--r--  2.0 unx    14879 b- defN 23-May-25 20:02 LHF/DataGeneration/torus.py
--rw-r--r--  2.0 unx      101 b- defN 23-May-25 20:02 LHF/DataGeneration/__init__.py
--rw-r--r--  2.0 unx     9822 b- defN 23-May-25 20:02 LHF/DataGeneration/dSphere.py
--rw-r--r--  2.0 unx     6852 b- defN 23-May-25 20:02 LHF/OutputAnalysis/persistenceDiagrams.py
--rw-r--r--  2.0 unx     8760 b- defN 23-May-25 20:02 LHF/OutputAnalysis/piFilters.py
--rw-r--r--  2.0 unx      140 b- defN 23-May-25 20:02 LHF/OutputAnalysis/__init__.py
--rw-r--r--  2.0 unx     3306 b- defN 23-May-25 20:02 LHF/OutputAnalysis/barcodeDiagrams.py
--rw-r--r--  2.0 unx     4904 b- defN 23-May-25 20:02 LHF/OutputAnalysis/bettiCurve.py
--rw-r--r--  2.0 unx     2593 b- defN 23-May-25 20:02 LHF/OutputAnalysis/heatmap.py
--rw-r--r--  2.0 unx     5904 b- defN 23-May-25 20:02 LHF/OutputAnalysis/filtrationImage.py
-77 files, 18530380 bytes uncompressed, 5594812 bytes compressed:  69.8%
+Zip file size: 5610196 bytes, number of entries: 79
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-15 14:37 pyLHF/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-15 14:37 lhf-1.3.5.dist-info/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-15 14:37 lhf.libs/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-15 14:37 LHF/
+-rw-r--r--  2.0 unx        0 b- defN 23-Jun-15 14:37 pyLHF/__init__.py
+-rw-r--r--  2.0 unx       98 b- defN 23-Jun-15 14:37 pyLHF/__main__.py
+-rw-r--r--  2.0 unx     1663 b- defN 23-Jun-15 14:37 pyLHF/pyLHF.py
+-rw-rw-r--  2.0 unx     6641 b- defN 23-Jun-15 14:37 lhf-1.3.5.dist-info/RECORD
+-rw-r--r--  2.0 unx       10 b- defN 23-Jun-15 14:37 lhf-1.3.5.dist-info/top_level.txt
+-rw-r--r--  2.0 unx     3773 b- defN 23-Jun-15 14:37 lhf-1.3.5.dist-info/METADATA
+-rw-r--r--  2.0 unx      133 b- defN 23-Jun-15 14:37 lhf-1.3.5.dist-info/WHEEL
+-rwxr-xr-x  2.0 unx   533337 b- defN 23-Jun-15 14:37 lhf.libs/libmpfr-b7fafdf8.so.4.1.6
+-rwxr-xr-x  2.0 unx  1570473 b- defN 23-Jun-15 14:37 lhf.libs/libgmp-452f15e0.so.10.3.2
+-rwxr-xr-x  2.0 unx   150777 b- defN 23-Jun-15 14:37 lhf.libs/libbetaSubSkeletonComplex-8505afc6.so
+-rwxr-xr-x  2.0 unx   775729 b- defN 23-Jun-15 14:37 lhf.libs/libopen-pal-d1f167ef.so.40.30.1
+-rwxr-xr-x  2.0 unx   251561 b- defN 23-Jun-15 14:37 lhf.libs/libreadInput-09604ee0.so
+-rwxr-xr-x  2.0 unx   152769 b- defN 23-Jun-15 14:37 lhf.libs/libnaiveWindow-e81362bf.so
+-rwxr-xr-x  2.0 unx    16937 b- defN 23-Jun-15 14:37 lhf.libs/libstreamingKmeans-1d5a99ac.so
+-rwxr-xr-x  2.0 unx   138633 b- defN 23-Jun-15 14:37 lhf.libs/libupscalePipe-026df4c1.so
+-rwxr-xr-x  2.0 unx   343225 b- defN 23-Jun-15 14:37 lhf.libs/libhwloc-a765a097.so.15.2.0
+-rwxr-xr-x  2.0 unx  1335505 b- defN 23-Jun-15 14:37 lhf.libs/libmpi-2fa556dc.so.40.30.1
+-rwxr-xr-x  2.0 unx   272617 b- defN 23-Jun-15 14:37 lhf.libs/libevent_core-2-e6cf51bb.1.so.6.0.2
+-rwxr-xr-x  2.0 unx   155449 b- defN 23-Jun-15 14:37 lhf.libs/libbasePipe-666b0f21.so
+-rwxr-xr-x  2.0 unx   190113 b- defN 23-Jun-15 14:37 lhf.libs/libbetaSkeletonBasedComplex-3d0aa4d3.so
+-rwxr-xr-x  2.0 unx    82113 b- defN 23-Jun-15 14:37 lhf.libs/libneighGraphPipe-9a204ef5.so
+-rwxr-xr-x  2.0 unx    58865 b- defN 23-Jun-15 14:37 lhf.libs/libkMeansPlusPlus-6a0e4583.so
+-rwxr-xr-x  2.0 unx   296369 b- defN 23-Jun-15 14:37 lhf.libs/libfastPersistence-28e8389e.so
+-rwxr-xr-x  2.0 unx    94569 b- defN 23-Jun-15 14:37 lhf.libs/libdistMatrixPipe-6e32877f.so
+-rwxr-xr-x  2.0 unx   243857 b- defN 23-Jun-15 14:37 lhf.libs/libsimplexTree-05a4764e.so
+-rwxr-xr-x  2.0 unx   779985 b- defN 23-Jun-15 14:37 lhf.libs/libdelaunayPipe-96aabe07.so
+-rwxr-xr-x  2.0 unx    76241 b- defN 23-Jun-15 14:37 lhf.libs/libgmpxx-6bc3ea01.so.4.5.2
+-rwxr-xr-x  2.0 unx    16937 b- defN 23-Jun-15 14:37 lhf.libs/libstreamingUtils-33aec4bf.so
+-rwxr-xr-x  2.0 unx    48937 b- defN 23-Jun-15 14:37 lhf.libs/libwriteOutput-6d79e888.so
+-rwxr-xr-x  2.0 unx    54137 b- defN 23-Jun-15 14:37 lhf.libs/libwitnessComplex-4477f1c7.so
+-rwxr-xr-x  2.0 unx  3207745 b- defN 23-Jun-15 14:37 lhf.libs/libcrypto-d9af65bd.so.1.1.1k
+-rwxr-xr-x  2.0 unx   277153 b- defN 23-Jun-15 14:37 lhf.libs/libutils-fe381086.so
+-rwxr-xr-x  2.0 unx   619425 b- defN 23-Jun-15 14:37 lhf.libs/libqhull_r-c601f68f.so.8.1-alpha3
+-rwxr-xr-x  2.0 unx   176801 b- defN 23-Jun-15 14:37 lhf.libs/libmpi_cxx-c0da7930.so.40.30.0
+-rwxr-xr-x  2.0 unx    80921 b- defN 23-Jun-15 14:37 lhf.libs/libripsPipe-551e5c47.so
+-rwxr-xr-x  2.0 unx   333625 b- defN 23-Jun-15 14:37 lhf.libs/libqhullPipe-4752bd44.so
+-rwxr-xr-x  2.0 unx   186977 b- defN 23-Jun-15 14:37 lhf.libs/libslidingWindow-7bc560f7.so
+-rwxr-xr-x  2.0 unx    30777 b- defN 23-Jun-15 14:37 lhf.libs/libcluster-6eab6735.so
+-rwxr-xr-x  2.0 unx   179385 b- defN 23-Jun-15 14:37 lhf.libs/libalphaComplex-6dc790b1.so
+-rwxr-xr-x  2.0 unx    54809 b- defN 23-Jun-15 14:37 lhf.libs/libpipePacket-4067a5e9.so
+-rwxr-xr-x  2.0 unx   116241 b- defN 23-Jun-15 14:37 lhf.libs/libbetaComplex-ec9e55cb.so
+-rwxr-xr-x  2.0 unx   322609 b- defN 23-Jun-15 14:37 lhf.libs/libincrementalPersistence-9375ebef.so
+-rwxr-xr-x  2.0 unx    93969 b- defN 23-Jun-15 14:37 lhf.libs/libdenStream-7f09c880.so
+-rwxr-xr-x  2.0 unx    64049 b- defN 23-Jun-15 14:37 lhf.libs/libkdTree-17ab0886.so
+-rwxr-xr-x  2.0 unx    22729 b- defN 23-Jun-15 14:37 lhf.libs/libdbscan-7d5374b7.so
+-rwxr-xr-x  2.0 unx    12985 b- defN 23-Jun-15 14:37 lhf.libs/libopen-orted-mpir-06a2f0b2.so
+-rwxr-xr-x  2.0 unx   220025 b- defN 23-Jun-15 14:37 lhf.libs/libsimplexBase-6b25e600.so
+-rwxr-xr-x  2.0 unx   854009 b- defN 23-Jun-15 14:37 lhf.libs/libopen-rte-1aa20eaf.so.40.30.1
+-rwxr-xr-x  2.0 unx    88417 b- defN 23-Jun-15 14:37 lhf.libs/libpreprocessor-73ae98ae.so
+-rwxr-xr-x  2.0 unx    17129 b- defN 23-Jun-15 14:37 lhf.libs/libevent_pthreads-2-9991b589.1.so.6.0.2
+-rwxr-xr-x  2.0 unx   253289 b- defN 23-Jun-15 14:37 lhf.libs/libgomp-2c51bfab.so.1.0.0
+-rwxr-xr-x  2.0 unx    71961 b- defN 23-Jun-15 14:37 lhf.libs/libargParser-e71dc2de.so
+-rwxr-xr-x  2.0 unx   247281 b- defN 23-Jun-15 14:37 lhf.libs/libsimplexArrayList-4cbe4342.so
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-15 14:37 LHF/DataGeneration/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-15 14:37 LHF/OutputAnalysis/
+-rw-r--r--  2.0 unx    11163 b- defN 23-Jun-15 14:37 LHF/utilities.py
+-rw-r--r--  2.0 unx     5461 b- defN 23-Jun-15 14:37 LHF/LHF.py
+-rw-r--r--  2.0 unx     3361 b- defN 23-Jun-15 14:37 LHF/triangulation.py
+-rw-r--r--  2.0 unx      149 b- defN 23-Jun-15 14:37 LHF/__init__.py
+-rwxr-xr-x  2.0 unx  3265945 b- defN 23-Jun-15 14:37 LHF/libLHFlib.so
+-rw-r--r--  2.0 unx     2020 b- defN 23-Jun-15 14:37 LHF/curves.py
+-rw-r--r--  2.0 unx     2209 b- defN 23-Jun-15 14:37 LHF/DataGeneration/embeddings.py
+-rw-r--r--  2.0 unx     1723 b- defN 23-Jun-15 14:37 LHF/DataGeneration/projectivePlane.py
+-rw-r--r--  2.0 unx    10395 b- defN 23-Jun-15 14:37 LHF/DataGeneration/dataGen.py
+-rw-r--r--  2.0 unx     5310 b- defN 23-Jun-15 14:37 LHF/DataGeneration/objGen.py
+-rw-r--r--  2.0 unx    13189 b- defN 23-Jun-15 14:37 LHF/DataGeneration/torus.py
+-rw-r--r--  2.0 unx      146 b- defN 23-Jun-15 14:37 LHF/DataGeneration/__init__.py
+-rw-r--r--  2.0 unx     8421 b- defN 23-Jun-15 14:37 LHF/DataGeneration/dSphere.py
+-rw-r--r--  2.0 unx     6852 b- defN 23-Jun-15 14:37 LHF/OutputAnalysis/persistenceDiagrams.py
+-rw-r--r--  2.0 unx     8760 b- defN 23-Jun-15 14:37 LHF/OutputAnalysis/piFilters.py
+-rw-r--r--  2.0 unx      140 b- defN 23-Jun-15 14:37 LHF/OutputAnalysis/__init__.py
+-rw-r--r--  2.0 unx     3306 b- defN 23-Jun-15 14:37 LHF/OutputAnalysis/barcodeDiagrams.py
+-rw-r--r--  2.0 unx     4904 b- defN 23-Jun-15 14:37 LHF/OutputAnalysis/bettiCurve.py
+-rw-r--r--  2.0 unx     2593 b- defN 23-Jun-15 14:37 LHF/OutputAnalysis/heatmap.py
+-rw-r--r--  2.0 unx     5904 b- defN 23-Jun-15 14:37 LHF/OutputAnalysis/filtrationImage.py
+79 files, 18545715 bytes uncompressed, 5599238 bytes compressed:  69.8%
```

## zipnote {}

```diff
@@ -1,38 +1,38 @@
-Filename: lhf-1.3.4.dist-info/
+Filename: pyLHF/
 Comment: 
 
-Filename: pyLHF/
+Filename: lhf-1.3.5.dist-info/
 Comment: 
 
 Filename: lhf.libs/
 Comment: 
 
 Filename: LHF/
 Comment: 
 
-Filename: lhf-1.3.4.dist-info/RECORD
+Filename: pyLHF/__init__.py
 Comment: 
 
-Filename: lhf-1.3.4.dist-info/top_level.txt
+Filename: pyLHF/__main__.py
 Comment: 
 
-Filename: lhf-1.3.4.dist-info/METADATA
+Filename: pyLHF/pyLHF.py
 Comment: 
 
-Filename: lhf-1.3.4.dist-info/WHEEL
+Filename: lhf-1.3.5.dist-info/RECORD
 Comment: 
 
-Filename: pyLHF/__init__.py
+Filename: lhf-1.3.5.dist-info/top_level.txt
 Comment: 
 
-Filename: pyLHF/__main__.py
+Filename: lhf-1.3.5.dist-info/METADATA
 Comment: 
 
-Filename: pyLHF/pyLHF.py
+Filename: lhf-1.3.5.dist-info/WHEEL
 Comment: 
 
 Filename: lhf.libs/libmpfr-b7fafdf8.so.4.1.6
 Comment: 
 
 Filename: lhf.libs/libgmp-452f15e0.so.10.3.2
 Comment: 
@@ -195,14 +195,20 @@
 
 Filename: LHF/DataGeneration/embeddings.py
 Comment: 
 
 Filename: LHF/DataGeneration/projectivePlane.py
 Comment: 
 
+Filename: LHF/DataGeneration/dataGen.py
+Comment: 
+
+Filename: LHF/DataGeneration/objGen.py
+Comment: 
+
 Filename: LHF/DataGeneration/torus.py
 Comment: 
 
 Filename: LHF/DataGeneration/__init__.py
 Comment: 
 
 Filename: LHF/DataGeneration/dSphere.py
```

## pyLHF/pyLHF.py

```diff
@@ -1,35 +1,69 @@
 import sys
 sys.path.append('../')
 import tadasets
 import persim
 import numpy as np
+import matplotlib.pyplot as plt
 
 from LHF import LHF
 from LHF.OutputAnalysis import persistenceDiagram, heatmap, barcodeDiagram, bettiCurve
 
-
-
-
-#Initialize the LHF Library  
-pyLHF = LHF.LHF()
-
-#Set debug mode to true, configure other arguments (optional)
-pyLHF.args["debug"] = "0"
-pyLHF.args["epsilon"]= 1.0
+from LHF.DataGeneration import dataGen as dg
+from LHF.DataGeneration import objGen as og
 
 pis = []
 
-for i in range(0, 10):
-	
-	#Load data from file/generate data
-	data = tadasets.dsphere(n=100+(i*15), d=3, r=1, noise=0.1)
-
-	pis = pyLHF.runPH(data)
 
-	print(len(pis))
 
+#def fish(param):
+#	a= 1
+#	return (param[0]**2+param[1]**2)**2 - a*param[0]*(param[0]**2 - param[1]**2) 
+
+def plot(pointCloud) :
+
+    plt.title("Point Cloud, Total Points: %s" % pointCloud.shape[0])
+    plt.scatter(pointCloud[:,0],pointCloud[:,1], marker=".", s=1)
+    plt.axis('square')
+    plt.show()
+
+    return
+
+
+for i in range(0, 1):
+
+
+    #Initialize the LHF Library  
+    pyLHF = LHF.LHF()
+
+    #Set debug mode to true, configure other arguments (optional)
+    pyLHF.args["debug"] = "1"
+    pyLHF.args["mode"] = "reduced"
+    pyLHF.args["clusters"] = 200
+    pyLHF.args["dimensions"] = 2
+    
+    #Generate a square of points and make cuts using new dataGen library
+    #square = dg.genFilledCube(dim=2)
+    
+    #d = dg.buildObj('(_x[0]**2) + _x[1]**2 <= 0.8', square)
+    #d = dg.buildObj('\
+    #_x[0] < -0.25 or \
+    #(_x[0]+0.25)**2 + (_x[1]-0.25)**2 > 0.6**2\
+    #', d)
+    
+    
+    #Fish example
+    d = og.butterflyCurve(3000, 2)
+    np.savetxt('./pointCloud.csv',d,delimiter=',')
+    plot(d)
+    
+    #Run PH and get the full bettiTable, pipePacket object
+    boundpis, ppkt, elapsed = pyLHF.runPH(d)
+       
+    ##Extract the raw pis from the bettiTable (i.e. remove last column, boundary generators)
+    pis = np.array([[z[0],z[1],z[2]] for z in boundpis])
+        
 
 
-plt = persistenceDiagram(pis, show=True)
+plt = bettiCurve(pis, dimension=1, show=True)
```

## LHF/utilities.py

```diff
@@ -6,263 +6,280 @@
 import sys
 import os
 import numpy as np
 from sklearn.random_projection import GaussianRandomProjection 
 from sklearn.random_projection import SparseRandomProjection 
 from sklearn.preprocessing import StandardScaler
 from sklearn.preprocessing import MinMaxScaler
-from scipy.spatial import distance_matrix
+from sklearn.neighbors import NearestNeighbors
 import pylab
 from mpl_toolkits.mplot3d import Axes3D
 
-
 def cliProgressBar(iteration, totalIterations):
+        
 	"""
 	A quick progress bar for some indicator while running
 	
 	Parameters
-    ----------
-    iteration : int
+        ----------
+        iteration : int
         Current iteration number
-    totalIterations: int
+        totalIterations: int
         Total number of iterations
-
-    """
+        
+        """
     
 	percent = ("{0:.1f}").format(100 * (iteration / float(totalIterations)))
 	filled = int(100 * iteration // totalIterations)
 	bar = '█' * filled + '-' * (100 - filled)
 	print(f'\rProgress: |{bar}| {percent}% Complete', end='' )
 	
 	if iteration == totalIterations:
 		print('\n')
 	
 	return
 	
 
 def embedPointCloud(ptCld, dim, rot='yes', fillVals=(0.0)) :
-    """
-    Parameters
-    ----------
-    ptCld : numpy.array(n, d)
+        """
+        Parameters
+        ----------
+        ptCld : numpy.array(n, d)
         point cloud in dimension d
-    dim: int
+        dim: int
         dimension of the point cloud to embed the original point cloud within
-    rotate : str
+        rotate : str
         'yes' (default), perform a random rotation of the high dimensional space (this will rotate the embedded point cloud in the
         higher dimensional space (the d-dimensional shape will be there, but its orientation will be randomly positioned);
         !='yes', no rotation is performed on the final point cloud 
-    fillVals : list(int)
+        fillVals : list(int)
         list of fill values to use for the additional dimensions (default: (0.0))
-
-    Return
-    ------
-    numpy.array(n, dim)
-
-    """
-    size = ptCld.shape
-
-    if size[1] < dim :
-        print ('Embedding dimension (' + str(dim) + ') is less than the dimension of the point cloud (' + str(size[1]) + ').  Aborting....')
-        sys.exit(-1)
-
-    if len(fillVals) == 1 :
-        fillVals = np.full((size[0], dim - size[1]), fillVals)
-
-    newPC = np.concatenate(ptCld, fillVals)
-    if rot == 'yes' :
-        rotate, _ = np.linalg.qr(np.random.random((dim, dim)))
-        return np.dot(newPC, rotate)
-    else :
-        return newPC
+        
+        Return
+        ------
+        numpy.array(n, dim)
+        
+        """
+        size = ptCld.shape
+
+        if size[1] < dim :
+                print ('Embedding dimension (' + str(dim) + ') is less than the dimension of the point cloud (' + str(size[1]) + ').  Aborting....')
+                sys.exit(-1)
+
+        if len(fillVals) == 1 :
+                fillVals = np.full((size[0], dim - size[1]), fillVals)
+
+        newPC = np.concatenate(ptCld, fillVals)
+        if rot == 'yes' :
+                rotate, _ = np.linalg.qr(np.random.random((dim, dim)))
+                return np.dot(newPC, rotate)
+        else :
+                return newPC
     
     
 def projectDataTo3D(ptCld, projectionType='gaussian') :
-    """ 
-    Project high dimensional point cloud to a 3D point cloud.
-
-    Perform random projection to achieve data reduction of a high dimensional point cloud to a point cloud in 3D.  The function
-    uses either DB Friendly (SparseRandomProjection, with modifications from Ping-06-KDD) or JL (GaussianRandomProjection) methods
-    for the data reduction step.
-
-    Parameters
-    ----------
-    ptCld : numpy.array(n, d)
+        """ 
+        Project high dimensional point cloud to a 3D point cloud.
+        
+        Perform random projection to achieve data reduction of a high dimensional point cloud to a point cloud in 3D.  The function
+        uses either DB Friendly (SparseRandomProjection, with modifications from Ping-06-KDD) or JL (GaussianRandomProjection) methods
+        for the data reduction step.
+        
+        Parameters
+        ----------
+        ptCld : numpy.array(n, d)
         Cartisian coordinates in R^d to project 
-    projectionType : str
+        projectionType : str
         type of projection to perform: 'gaussian' (default), following the JL lemma; or 'sparse', DB Friendly
-
-    Returns
-    -------
-    numpy.array(n, 3)
+        
+        Returns
+        -------
+        numpy.array(n, 3)
         Cartisian coordinates of reduced data in R^3
-
-    """
-    if projectionType == 'gaussian' :
-        projector = GaussianRandomProjection(n_components=3)
-    elif projectionType == 'sparse' :
-        projector = SparseRandomProjection(n_components=3)
-    else :
-        print ('testingUtilities.projectDataTo3D: illegal projection projection type requsted; only gaussian or sparse supported')
-        sys.exit(1)
-    return projector.fit_transform(ptCldList)
-
+        
+        """
+        if projectionType == 'gaussian' :
+                projector = GaussianRandomProjection(n_components=3)
+        elif projectionType == 'sparse' :
+                projector = SparseRandomProjection(n_components=3)
+        else :
+                print ('testingUtilities.projectDataTo3D: illegal projection projection type requsted; only gaussian or sparse supported')
+                sys.exit(1)
+        return projector.fit_transform(ptCldList)
 
 
 ## not sure this method is necessary, but i'm leaving it in here since its here....we may want to remove it in the figure.
 def normalize(ptCld, method='MinMaxScalar') :
-    """
-    Normalize a point cloud.
-
-    Normalize a point cloud using the sklearn functions MinMaxScalar[-1.0,1.0] or StandardScalar.
-
-    Parameters
-    ----------
-    ptCld : numpy.array(n, d) 
+        """
+        Normalize a point cloud.
+        
+        Normalize a point cloud using the sklearn functions MinMaxScalar[-1.0,1.0] or StandardScalar.
+        
+        Parameters
+        ----------
+        ptCld : numpy.array(n, d) 
         Point cloud in R^d to normalize
-    method : string
+        method : string
         The scaling method from sklearn to use: 'MinMaxScalar' (the default) or StandardScalar
-
-    Returns
-    -------
-    numpy.array(n, d)
+        
+        Returns
+        -------
+        numpy.array(n, d)
         Normalized point cloud in R^d
 
-
-    """
-    if method == 'StandardScalar' :
-        standardizer = StandardScaler().fit(ptCld)
-        return standardizer.transform(ptCld)
-    elif method == 'MinMaxScalar' :
-        scaler = MinMaxScaler(feature_range=(-1.0,1.0))
-        return scaler.fit_transform(ptCld)
-    else :
-        print ('testingUtilities.normalize: illegal normalization type requsted; only StandardScalar or MinMaxScalar supported')
-        sys.exit(1)
+        """
+        if method == 'StandardScalar' :
+                standardizer = StandardScaler().fit(ptCld)
+                return standardizer.transform(ptCld)
+        elif method == 'MinMaxScalar' :
+                scaler = MinMaxScaler(feature_range=(-1.0,1.0))
+                return scaler.fit_transform(ptCld)
+        else :
+                print ('testingUtilities.normalize: illegal normalization type requsted; only StandardScalar or MinMaxScalar supported')
+                sys.exit(1)
         return ptCld
 
 
-
 def plot3DPointCloud(ptCld, title='', setAxis=True) :
-    """
-    Generate a 3D plot of a point cloud.
-
-    Parameters
-    ----------
-    ptCld : numpy.array(n, 3) 
+        """
+        Generate a 3D plot of a point cloud.
+        
+        Parameters
+        ----------
+        ptCld : numpy.array(n, 3) 
         Cartisian coordinates in R^3 to plot
-    title : string
+        title : string
         String to place in figure title, if blank no title generated
-    setAxis : boolean
+        setAxis : boolean
 		If true, scales all axis to min/max of PC data; otherwise default
-    """
-    figure = pylab.figure()
-    fig = figure.add_subplot(111, projection='3d')
-    if title != '' :
-        pylab.title(title)
-    #fig.set(aspect='equal')
+        """
+        figure = pylab.figure()
+        fig = figure.add_subplot(111, projection='3d')
+        if title != '' :
+                pylab.title(title)
+        #fig.set(aspect='equal')
 	
 	#NOTE: Currently set this behind a flag but for PCs not normalized the stretch needs to be updated to take the 
 	#       maximum difference of any dimension as the bounding box (this will be tighter and keep aspect ratio)
-    if(setAxis):
-        max = np.max(ptCld)
-        min = np.min(ptCld)
-        # i prefer the same boundary limits on the axes
-        if (min < 0) :
-            if -min > max :
-                max = -min
-            else :
-                min = -max
-        fig.set_xlim3d(min, max)
-        fig.set_ylim3d(min, max)
-        fig.set_zlim3d(min, max)
-        #fig.set_axis_off()
-        fig.set_xticks([min, 0, max])
-        fig.set_yticks([min, 0, max])
-        fig.set_zticks([min, 0, max])
-
-    fig.scatter(ptCld[:,0], ptCld[:,1], ptCld[:,2], marker='.', s=3)
-    #pylab.show()
-    return figure
+        if(setAxis):
+                max = np.max(ptCld)
+                min = np.min(ptCld)
+                # i prefer the same boundary limits on the axes
+                if (min < 0) :
+                        if -min > max :
+                                max = -min
+                        else :
+                                min = -max
+                fig.set_xlim3d(min, max)
+                fig.set_ylim3d(min, max)
+                fig.set_zlim3d(min, max)
+                #fig.set_axis_off()
+                fig.set_xticks([min, 0, max])
+                fig.set_yticks([min, 0, max])
+                fig.set_zticks([min, 0, max])
+
+        fig.scatter(ptCld[:,0], ptCld[:,1], ptCld[:,2], marker='.', s=3)
+        #pylab.show()
+        return figure
     
 # this next function is planned for removal
 
 ## read a list of csv files containing point cloud data.  each file is read into a numpy matrix and stored in a standard
 ## python list.  the rows of the csv files each represent a unique n-dimensional point in the n-space.  each csv file
 ## can contain points for a a unique n-space. 
 def inputDataFiles(fileNameList) :
-    ptCld = []
-
-    for i in fileNameList :
-        # add .csv suffix if missing
-        if not(i.endswith('.csv')) :
-            i += '.csv'
-        ptCld.append(np.loadtxt(i, delimiter=',', comments='#', dtype=np.float))
-
-    return ptCld
-
-
+        ptCld = []
+        
+        for i in fileNameList :
+                # add .csv suffix if missing
+                if not(i.endswith('.csv')) :
+                        i += '.csv'
+                ptCld.append(np.loadtxt(i, delimiter=',', comments='#', dtype=np.float))
+                        
+        return ptCld
 
 
 # Referenced from https://stackoverflow.com/a/50107963
 def rejection_sample(n, sample_func, accept_func):
-    '''Perform rejection sampling to obtain n valid points.
-    
-    Parameters
-    ----------
-    n: int
+        '''Perform rejection sampling to obtain n valid points.
+        
+        Parameters
+        ----------
+        n: int
         The number of points to sample
-    sample_func: f(n) -> points
+        sample_func: f(n) -> points
         A function with one parameter, n, which returns n points randomly
         sampled from a surface
-    accept_func: f(points) -> mask
+        accept_func: f(points) -> mask
         A function with one parameter, points, which is an array of points,
         which returns a boolean array (true if the point is valid and false otherwise) 
-
-    Return
-    ------
-    points: numpy.array(n, dim)
-    '''
-    oversample = 5 * n  # Arbitrary oversample by 5 to limit the number of calls to sample_func
-    points = sample_func(oversample)
-    mask = accept_func(points)
-    reject, = np.where(~mask)
-    while oversample - reject.size < n:
-        fill = sample_func(reject.size)
-        mask = accept_func(fill)
-        points[reject[mask]] = fill[mask]
-        reject = reject[~mask]
-    return np.delete(points, reject, axis = 0)[:n]
-
-def run_trials(sample_func, trials, selection):
-    '''
-    Sample a point cloud multiple times and return the best sample according to a selection criteria
-
-    Parameters
-    ----------
-    sample_func: f() -> np.array(numPoints, dim)
+        
+        Return
+        ------
+        points: numpy.array(n, dim)
+        '''
+        oversample = 5 * n  # Arbitrary oversample by 5 to limit the number of calls to sample_func
+        points = sample_func(oversample)
+        mask = accept_func(points)
+        reject, = np.where(~mask)
+        while oversample - reject.size < n:
+                fill = sample_func(reject.size)
+                mask = accept_func(fill)
+                points[reject[mask]] = fill[mask]
+                reject = reject[~mask]
+        return np.delete(points, reject, axis = 0)[:n]
+
+def run_trials(sample_func, trials=8, selection=min):
+        '''
+        Sample a point cloud multiple times and return the best sample according to a selection criteria
+        
+        Parameters
+        ----------
+        sample_func: f() -> np.array(numPoints, dim)
         Returns points sampled from a surface
-    trials: int 
+        trials: int 
         The number of trials used to optimize the distribution of points
-    selection: str (min, max, mean, var)
+        selection: str (min, max, mean, var)
         The selection criteria to select between two candidate point clouds:
-            'min' : maximize the minimum nearest neighbor distance of the points
-            'max' (default): minmimize the maximum nearest neighbor distance of the points
-            'mean' : maximize the mean of the nearest neighbor distances.
-            'stdev' : minimize the standard deviation of the nearest neighbors'''
-    points = sample_func()
-    pointsNN = np.sort(np.sort(distance_matrix(points, points))[:,1])
-
-    for i in range(trials) :
-        pointsTrial = sample_func()
-        pointsTrialNN = np.sort(np.sort(distance_matrix(pointsTrial, pointsTrial))[:,1])
-        selectTrial = False
-
-        if selection == 'mean' and np.mean(pointsTrialNN) > np.mean(pointsNN) : selectTrial = True
-        if selection == 'stdev' and np.std(pointsTrialNN) < np.std(pointsNN) : selectTrial = True
-        if selection == 'min' and pointsTrialNN[0] > pointsNN[0] : selectTrial = True
-        if selection == 'max' and pointsTrialNN[-1] < pointsNN[-1] : selectTrial = True
-        if selectTrial == True :
-            points = pointsTrial
-            pointsNN = pointsTrialNN
- 
-    return points
+            'min' : (default) maximize the minimum nearest neighbor distance of the points
+            'mean' : maximize the mean of the pairwise distances
+            'stdev' : minimize the standard deviation of the pairwise distances
+'''
+        
+        ## compute the minimum distance to the nearest neighbor for each point in our data
+        def minNNDists(data) :
+                # can't affort the space for a full distance matrix, so let's get the measures one by one
+                minNN = []
+                for i in range(data.shape[0]) :
+                        dist = np.sort(distance_matrix(np.array([data[i]]),data)).flatten()
+                        # nearest neighbor distance to this point
+                        minNN.append(dist[1])
+                        
+                minDists = np.sort(np.array(minNN))
+
+                # min, max, mean, stdev
+                return [minDists[0], np.mean(minDists), np.std(minDists)]
+
+        points = sample_func()
+
+        nnPoints = NearestNeighbors(n_neighbors = 2, n_jobs=-1).fit(points)
+        nnDists, _ = nnPoints.kneighbors(points)
+        nnDists = np.sort(nnDists[:,1])
+        nnStats = [nnDists[0], np.mean(nnDists), np.std(nnDists)]
+
+        # run the trials to see if we can improve the distribution of the data
+        for i in range(trials) :
+                pointsTrial = sample_func()
+                nnPointsTrial = NearestNeighbors(n_neighbors = 2, n_jobs=-1).fit(pointsTrial)
+                nnDistsTrial, _ = nnPoints.kneighbors(pointsTrial)
+                nnDistsTrial = np.sort(nnDistsTrial[:,1])
+                nnStatsTrial = [nnDistsTrial[0], np.mean(nnDistsTrial), np.std(nnDistsTrial)]
+
+                selectTrial = False
+                if selection == 'min' and nnStatsTrial[0] > nnStats[0] : selectTrial = True
+                if selection == 'mean' and nnStatsTrial[1] > nnStats[1] : selectTrial = True
+                if selection == 'stdev' and nnStatsTrial[2] < nnStats[2] : selectTrial = True
+                if selectTrial == True :
+                        points = pointsTrial
+                        nnStats = nnStatsTrial
+                        
+        return points
```

## LHF/DataGeneration/projectivePlane.py

```diff
@@ -3,55 +3,41 @@
 ## This is a simple generator to build real projective planes for testing TDA tools/techniques. 
 
 import numpy as np
 from .dSphere import dSphere_uniformRandom
 from ..utilities import run_trials
 import datetime
 
-def real_projective_plane(numPoints = 1000, trials = 16, selection = 'max'):
+def real_projective_plane(numPoints = 20000, trials = 8, selection = 'min'):
     '''
     There is an embedding of RP2 in R4 defined by sampling points from a sphere and mapping (x, y, z) 
     # to (xy, xz, y^2 − z^2, 2yz). This embedding is defined in https://en.wikipedia.org/wiki/Real_projective_plane. 
     # However, this sampling is not guaranteed to be uniform.
 
     Parameters
     ----------
     numPoints: int 
-        The number of points desired in the output point cloud (default 1000)
+        The number of points desired in the output point cloud (default 20000)
     trials: int 
-        The number of trials used to optimize the distribution of points in the dSphere (default: 16)
-    selection: str (min, max, mean, var)
+        The number of trials used to optimize the distribution of points in the dSphere (default: 8)
+    selection: str (min, mean, var)
         The selection criteria to select between two candidate point clouds:
             'min' : maximize the minimum nearest neighbor distance of the points
-            'max' (default): minmimize the maximum nearest neighbor distance of the points
             'mean' : maximize the mean of the nearest neighbor distances.
             'stdev' : minimize the standard deviation of the nearest neighbors
 
     Returns
     -------
-    plane: dict
-
-        plane['ptCldObject'] = 'Real Projective Plane'
-        plane['date'] = datetime.datetime.now()
-        plane['origin'] = (0, 0, 0, 0) : record the expected origin of this point cloud
-        plane['points'] = points : the point cloud
+    plane: the point cloud
     '''
 
     def getPlane(numPoints):
-        sphere = dSphere_uniformRandom(3, numPoints, r1=1.0, r2=1.0, trials=1)['points']
+        sphere = dSphere_uniformRandom(3, numPoints, r1=1.0, r2=1.0, trials=1)
         plane = np.zeros((numPoints, 4))
         plane[:,0] = sphere[:,0] * sphere[:,1]
         plane[:,1] = sphere[:,0] * sphere[:,2]
         plane[:,2] = sphere[:,1]**2 - sphere[:,2]**2
         plane[:,3] = sphere[:,1] * sphere[:,2]
         return plane
 
     sample_func = lambda: getPlane(numPoints)
-    points = run_trials(sample_func, trials, selection)
-
-    plane = dict()
-    plane['ptCldObject'] = 'Real Projective Plane'
-    plane['date'] = datetime.datetime.now()
-    plane['origin'] = np.zeros(4)
-    plane['points'] = points
-
-    return plane
+    return run_trials(sample_func, trials, selection)
```

## LHF/DataGeneration/torus.py

```diff
@@ -7,15 +7,15 @@
 from sklearn.metrics.pairwise import euclidean_distances
 import math
 import random
 from ..utilities import rejection_sample, run_trials
 from .dSphere import dSphere_uniformRandom
 import datetime
 
-def torus_noisy_surface_uniformRandom(dimension=2, numPoints=1000, r1=1.0, r2=2.0, trials=16, selection='max') :
+def torus_noisy_surface_uniformRandom(dimension=2, numPoints=20000, r1=1.0, r2=2.0, trials=8, selection='min') :
     """
     Generate a point cloud for the boundary of a torus populated with randomly distributed points.
 
     This function generates a torus with intrinsic dimension {dimension} embedded in R^{2 * dimension}. For example, a
     2-torus will be embedded in R^4, and this is known as a Clifford Torus: see https://en.wikipedia.org/wiki/Clifford_torus.
     This can be generalized to higher dimensions, as described in the wikipedia pages, and each such torus is "flat" in the sense
     that is has zero Gaussian curvature: see https://en.wikipedia.org/wiki/Torus#Flat_torus.
@@ -36,39 +36,33 @@
     embedded in R^4 instead of R^3
 
     Parameters
     ----------
     dimension : int 
         The number of dimensions for the desired torus (default 2)
     numPoints : int 
-        The number of points desired in the output point cloud (default 1000)
+        The number of points desired in the output point cloud (default 20000)
     r1 : either a float (default) or list of floats of length dimension
         [float]: The ith entry is the inner radius of the ith circle
         float: The radius of the point cloud (default 1.0). Every circle will have the same radius
     r2 : either a float (default) or list of floats of length dimension
         [float]: The ith entry is the outer radius of the ith circle
         float: The radius of the point cloud (default 2.0). Every circle will have the same radius
 
     trials: int 
-        The number of trials used to optimize the distribution of points in the dSphere (default: 16)
-    selection: str (min, max, mean, var)
+        The number of trials used to optimize the distribution of points in the dSphere (default: 8)
+    selection: str (min, mean, var)
         The selection criteria to select between two candidate point clouds:
             'min' : maximize the minimum nearest neighbor distance of the points
-            'max' (default): minmimize the maximum nearest neighbor distance of the points
             'mean' : maximize the mean of the nearest neighbor distances.
             'stdev' : minimize the standard deviation of the nearest neighbors
 
     Returns
     -------
-    torus: dict
-
-        torus['ptCldObject'] = 'Clifford Torus'
-        torus['origin'] = (0, 0, ...) : 
-        torus['radius'] = (r1, r2) : the radius of the point cloud boundary
-        torus['points'] = randomPoints : the point cloud
+    torus: the torus point cloud
 
     """
 
     #### --------------------------------------------------------------------------------
     #### helper functions
     #### --------------------------------------------------------------------------------
     
@@ -111,35 +105,26 @@
         randomPoints = np.array([uniform_torus_point(dimension, r1, r2) for _ in range(numPoints)])
         
         return randomPoints
     
     #### --------------------------------------------------------------------------------
 
     sample_func = lambda: get_torus(dimension, numPoints, r1, r2)
-    points = run_trials(sample_func, trials, selection)
-
-    torus = dict()
-    torus['ptCldObject'] = 'Clifford Torus'
-    torus['date'] = datetime.datetime.now()
-    torus['origin'] = np.zeros(2*dimension)
-    torus['radius'] = (r1, r2)
-    torus['points'] = points
-
-    return torus
+    return run_trials(sample_func, trials, selection)
 
-def torus_boundary_uniformRandom(dimension=2, numPoints=1000, r=1.0, trials=16, selection='max') :
+def torus_boundary_uniformRandom(dimension=2, numPoints=20000, r=1.0, trials=8, selection='min') :
     """
     Generate a point cloud for the boundary of a torus populated with uniformly distributed points.
     
     The parameters are the same as the torus_noisy_surface_uniformRandom function, but both r1 and r2 are set to r to 
     guarantee that the points are selected uniformly randomly.
     """
     return torus_noisy_surface_uniformRandom(dimension, numPoints, r, r, trials, selection)
 
-def box_genus_with_boundary_uniformRandom(numPoints=1000, genus=1, trials=16, selection='max'):
+def box_genus_with_boundary_uniformRandom(numPoints=20000, genus=1, trials=8, selection='min'):
     '''
     Generate a point cloud for the boundary of a surface with genus populated with uniformly distributed points. The
     surface is generated by sampling from the sides of a rectangular prism, and then inserting tunnels for each genus.
     The result is a box-like looking surface which is homeomorphic to the torus with corresponding genus.
 
     The points are in R3. The box has dimensions (2*genus + 1) x 3 x 1, and each tunnel has dimensions 1 x 1 x 1.
     A horizontal cross section of the double torus looks like the shape below.
@@ -147,34 +132,28 @@
     |   __     __   |
     |  |__|   |__|  |
     |_______________|
 
     Parameters
     ----------
     numPoints: int 
-        The number of points desired in the output point cloud (default 1000)
+        The number of points desired in the output point cloud (default 20000)
     genus: int
         The desired genus of the surface (default: 1)
     trials: int 
-        The number of trials used to optimize the distribution of points in the dSphere (default: 16)
-    selection: str (min, max, mean, var)
+        The number of trials used to optimize the distribution of points in the dSphere (default: 8)
+    selection: str (min, mean, var)
         The selection criteria to select between two candidate point clouds:
             'min' : maximize the minimum nearest neighbor distance of the points
-            'max' (default): minmimize the maximum nearest neighbor distance of the points
             'mean' : maximize the mean of the nearest neighbor distances.
             'stdev' : minimize the standard deviation of the nearest neighbors
 
     Returns
     ----------
-    torus: dict
-
-        torus['ptCldObject'] = 'Box Torus'
-        torus['origin'] = (0, 0, 0) : 
-        torus['genus'] = genus
-        torus['points'] = randomPoints : the point cloud
+    torus: the torus point cloud
     '''
 
     def sampleTorus(n, g):
         # Sample points from the interior of the box 
         pts = np.column_stack((np.random.uniform(0, 2*g+1, n), np.random.uniform(0, 3, n), np.random.uniform(0, 1, n)))
         rand = np.random.randint(0, 20*g+14, n)
         for i in range(n):
@@ -199,95 +178,71 @@
         return pts
 
     def acceptTorus(points):
         # Remove points inside the tunnels
         return (points[:,0] % 2 <= 1) | (points[:,1] <= 1) | (points[:,1] >= 2)
 
     sample_func = lambda: rejection_sample(numPoints, lambda numPoints: sampleTorus(numPoints, genus), acceptTorus)
-    points = run_trials(sample_func, trials, selection)
-
-    torus = dict()
-    torus['ptCldObject'] = 'Box Torus'
-    torus['genus'] = genus
-    torus['date'] = datetime.datetime.now()
-    torus['origin'] = np.zeros(3)
-    torus['points'] = points
-
-    return torus
+    return run_trials(sample_func, trials, selection)
 
-def box_genus_interior_uniformRandom(numPoints=1000, genus=1, trials=16, selection='max'):
+def box_genus_interior_uniformRandom(numPoints=20000, genus=1, trials=8, selection='min'):
     '''This function is exactly analogous to box_genus_with_boundary_uniformRandom, but it returns points sampled
     uniformly randomly from the interior of a surface with genus g instead of the boundary
     '''
     def sampleSolidTorus(n, g):
         return np.column_stack((np.random.uniform(0, 2*g+1, n), np.random.uniform(0, 3, n), np.random.uniform(0, 1, n)))
 
     def acceptSolidTorus(points):
         return (points[:,0] % 2 < 1) | (points[:,1] < 1) | (points[:,1] > 2)
 
     sample_func = lambda: rejection_sample(numPoints, lambda numPoints: sampleSolidTorus(numPoints, genus), acceptSolidTorus)
-    points = run_trials(sample_func, trials, selection)
+    return run_trials(sample_func, trials, selection)
 
-    torus = dict()
-    torus['ptCldObject'] = 'Box Torus Interior'
-    torus['genus'] = genus
-    torus['date'] = datetime.datetime.now()
-    torus['origin'] = np.zeros(3)
-    torus['points'] = points
-
-    return torus
-
-def spherical_genus_boundary_random(numPoints = 1000, genus = 3, radius = .3, trials = 16, selection = 'max'):
+def spherical_genus_boundary_random(numPoints = 20000, genus = 3, radius = .3, trials = 8, selection = 'min'):
     """
     Generate a point cloud for the boundary of a torus with genus 1, 3, or 5. These points will NOT be distributed 
     uniformly randomly along the boundary. 
 
     This function first samples points uniformly random from the surface of a sphere with radius 1. The function then
     adds tunnels to add genus. For genus 1, a single tunnel is added by sampling points from a cylinder.
     This resembles the standard torus point cloud. For genus 3, a second cross-tunnel is added
     (reference Figure 3 of "Estimating Betti Numbers Using Deep Learning"). For genus 5, a third
     cross-tunnel is added (reference https://mathoverflow.net/questions/98925/building-a-genus-n-torus-from-cubes)
 
     Parameters
     ----------
     numPoints: int 
-        The number of points desired in the output point cloud (default 1000)
+        The number of points desired in the output point cloud (default 20000)
     genus: (1, 3, or 5)
         The desired genus of the surface (default 3)
     radius:
         The radius of each tunnel (default .3)
     trials: int 
-        The number of trials used to optimize the distribution of points in the dSphere (default: 16)
-    selection: str (min, max, mean, var)
+        The number of trials used to optimize the distribution of points in the dSphere (default: 8)
+    selection: str (min, mean, var)
         The selection criteria to select between two candidate point clouds:
-            'min' : maximize the minimum nearest neighbor distance of the points
-            'max' (default): minmimize the maximum nearest neighbor distance of the points
+            'min' : (default) maximize the minimum nearest neighbor distance of the points
             'mean' : maximize the mean of the nearest neighbor distances.
             'stdev' : minimize the standard deviation of the nearest neighbors
 
     Returns
     ----------
-    torus: dict
-
-        torus['ptCldObject'] = 'Spherical Torus'
-        torus['origin'] = (0, 0, 0)
-        torus['genus'] = genus
-        torus['points'] = randomPoints : the point cloud
+    torus: the torus point cloud
     """
     def sampleCylinder(n, r, h):
         """Sample n points from a cylinder of radius r and heigh h (z-values stretching from -h/2 to h/2)"""
         angles = np.random.uniform(0, 2*math.pi, n)
         return np.column_stack((r*np.cos(angles), r*np.sin(angles), np.random.uniform(-h/2, h/2, n)))
 
     def sampleSphericalTorus(n, g, r):
         if g not in [1, 3, 5]:
             raise "Invalid genus: only 1, 3, or 5 supported"
         cylinderN = n // 8
         sphereN = n - cylinderN*((g+1)//2)
-        sphere = dSphere_uniformRandom(dimension=3, numPoints=sphereN, r1=1, r2=1, trials=1)['points']
+        sphere = dSphere_uniformRandom(dimension=3, numPoints=sphereN, r1=1, r2=1, trials=1)
         cylinder = sampleCylinder(cylinderN, r, 2)
         # Add the first tunnel in the z direction
         points = np.concatenate((sphere, cylinder))
         if g == 3 or g == 5:
             # Add a cross tunnel in the x direction
             cylinder = sampleCylinder(cylinderN, r, 2)[:,[2,0,1]]
             points = np.concatenate((points, cylinder))
@@ -304,17 +259,8 @@
             mask &= (points[:,1]**2 + points[:,2]**2) >= r ** 2
         if g == 5:
             mask &= (points[:,0]**2 + points[:,2]**2) >= r ** 2
         return mask
         
     sample_func = lambda: rejection_sample(numPoints, lambda numPoints: sampleSphericalTorus(numPoints, genus, radius), 
                                         lambda points: acceptSphericalTorus(points, genus, radius))
-    points = run_trials(sample_func, trials, selection)
-
-    torus = dict()
-    torus['ptCldObject'] = 'Spherical Torus'
-    torus['genus'] = genus
-    torus['date'] = datetime.datetime.now()
-    torus['origin'] = np.zeros(3)
-    torus['points'] = points
-
-    return torus
+    return run_trials(sample_func, trials, selection)
```

## LHF/DataGeneration/__init__.py

```diff
@@ -1,4 +1,6 @@
 from .dSphere import *
 from .torus import *
 from .embeddings import *
 from .projectivePlane import *
+from .dataGen import *
+from .objGen import *
```

## LHF/DataGeneration/dSphere.py

```diff
@@ -17,68 +17,61 @@
 import math
 from functools import reduce
 from scipy.spatial import distance_matrix
 import datetime
 from ..utilities import run_trials
 
 
-def dSphere_uniformRandom(dimension=3, numPoints=1000, r1=1.0, r2=1.0, trials=16, selection='max') :
+def dSphere_uniformRandom(dimension=3, numPoints=20000, r1=1.0, r2=1.0, trials=8, selection='min') :
     """
     Generate an optimized point cloud for the boundary of a dSphere that is populated with uniformly distributed points between
     two radii (r1 < r2). 
 
     Generate an optimized point cloud representation of a dSphere by selecting from multiple trials.  Each trial point cloud is
     generated by the nested function named get_dSphere.  get_dSphere uses Muller's method [Muller-59] to generate a uniform random
     set of points on a boundary of a d-sphere.  The boundary of points follows a uniform random distribution between radii r1 <
     r2.  The candidate point clouds represent d-spheres as a point cloud boundary of uniform random points in a boundary between
     r1 < r2 of a d-sphere.  This is function takes multiple trials for a dSphere and selects the trial that optimizes one of three
     values of the generated point cloud.  The optimization method is based on nearest neighbor distances for the points.  There
     are three options for the optimization method, namely: 'mean' (default), maximize the mean of the nearest neighbor distances;
-    'var', minimize the variance of the mean nearest neighbor distance; and 'min', maximize the minimum nearest neighbor distance.
+    'stdev', minimize the variance of the mean nearest neighbor distance; and 'min', maximize the minimum nearest neighbor distance.
 
     While a d-sphere is technically an n-dimensional manifold that can be embedded in euclidean (d+1)-space, we will use the term
     dimension here to refer to the euclidean space as that is the more commonly used interpretation outside of the math-head
     community.  Thus, if this program will generate a circle at dimension 2, a sphere at dimension 3, and so on....
 
     [Muller-59] Mervin E. Muller, "A note on a method for generating points uniformly on n-dimensional spheres." Communications of
                 the ACM, 19-20, vol2, no 4, April 1959. DOI=http://dx.doi.org/10.1145/377939.377946
 
     Parameters
     ----------
     dimension : int 
         The number of dimensions for the desired d-sphere (default 3)
     numPoints : int 
-        The number of points desired in the output point cloud (default 1000)
+        The number of points desired in the output point cloud (default 20000)
     r1 : float 
         The inner radius bounds of the point cloud (default 1.0)
     r2 : float 
         The outer radius bounds of the point cloud (default 1.0)
     trials: int 
-        The number of trials used to optimize the distribution of points in the dSphere (default: 16)
+        The number of trials used to optimize the distribution of points in the dSphere (default: 8)
     selection: str (min, max, mean, var)
         The selection criteria to select between two candidate point clouds:
             'min' : maximize the minimum nearest neighbor distance of the points
-            'max' (default): minmimize the maximum nearest neighbor distance of the points
             'mean' : maximize the mean of the nearest neighbor distances.
             'stdev' : minimize the standard deviation of the nearest neighbors
 
     Returns
     -------
-    dShere: dict
-
-        dsphere['ptCldObject'] = 'd-Sphere'
-        dsphere['date'] = datetime.datetime.now()
-        dsphere['origin'] = (0, 0, ...) : record the expected origin of this point cloud
-        dsphere['radius'] = (r1, r2) : the inner and outer radii of the point cloud boundary
-        dsphere['points'] = randomPoints : the point cloud
+    dShere: [numPoints x dimension] random points on the dSphere
 
     Notes:
     ------
     1. While it may seem that optimizing the stdev or min 1-NN distance would be the best approach, from the standpoint of getting
-       the earliest birth time, selecting the minimum max 1-NN distance will actually produce the best result.
+       the earliest birth time, maximizing the 1-NN distance will actually produce the best result.
     2. Experimental testing shows that the sobol sequence generator provides the best results to minimize the max 1-NN distance
        between points. 
     """
 
     #### --------------------------------------------------------------------------------
     #### helper function that builds a trial point cloud 
     #### --------------------------------------------------------------------------------
@@ -109,72 +102,54 @@
             r = np.sqrt(np.sum(np.power(randomPoints[i], 2)))
             randomPoints[i] = (randomPoints[i] / r) * float(radii[i])
         
         return randomPoints
     #### --------------------------------------------------------------------------------
 
     sample_func = lambda: get_dSphere(dimension, numPoints, r1, r2)
-    dsphere = run_trials(sample_func, trials, selection)
-
-    retVal = dict()
-    retVal['ptCldObject'] = 'd-Sphere'
-    # in isolation, these next four definitions appear redundant, however, when embedding objects in another point cloud, they become important
-    retVal['date'] = datetime.datetime.now()
-    retVal['origin'] = np.zeros(dimension)
-    retVal['radius'] = (r1, r2)
-    retVal['points'] = dsphere
-
-    # other than the trial of the selected point cloud captured, this data is already captured in other fields
-    #dsphere['profile'] = dsphereProfile
-    return retVal
+    return run_trials(sample_func, trials, selection)
 
-def dSphere_product(dimensions=[2,2], numPoints=1000, r1=1.0, r2=1.0, trials=16, selection='max') :
-    """
-    Generates a point cloud for the Cartesian product of spheres of specified dimension. This creates
-    point clouds with interesting features in multiple dimensions, as described by https://topospaces.subwiki.org/wiki/Homology_of_product_of_spheres.
+def dSphere_product(dimensions=[2,2], numPoints=20000, r1=1.0, r2=1.0, trials=8, selection='min') :
+    '''
+    Generates a point cloud for the Cartesian product of spheres of specified dimension. This creates point clouds with
+    interesting features in multiple dimensions, as described by
+    https://topospaces.subwiki.org/wiki/Homology_of_product_of_spheres.  
     The true Betti numbers are returned by the function and are calculated using the Poincare polynomial of the
     product of the spheres. This generalizes the flat torus. The torus with intrinsic dimension n embedded in 
     Rn can be created using dSphere_product([2 for _ in range(n)])
 
     Parameters
     ----------
     dimensions : list of ints 
         The number of dimensions for the spheres in the product (default [2, 2])
     numPoints : int 
-        The number of points desired in the output point cloud (default 1000)
+        The number of points desired in the output point cloud (default 20000)
     r1 : float 
         The inner radius bounds of the point cloud (default 1.0)
     r2 : float 
         The outer radius bounds of the point cloud (default 1.0)
     trials: int 
-        The number of trials used to optimize the distribution of points in the dSphere (default: 16)
+        The number of trials used to optimize the distribution of points in the dSphere (default: 8)
     selection: str (min, max, mean, var)
         The selection criteria to select between two candidate point clouds:
             'min' : maximize the minimum nearest neighbor distance of the points
-            'max' (default): minmimize the maximum nearest neighbor distance of the points
             'mean' : maximize the mean of the nearest neighbor distances.
             'stdev' : minimize the standard deviation of the nearest neighbors
 
     Returns
     -------
-    product: dict
+    product: [numPoints x dimensions points
+    '''
 
-        product['ptCldObject'] = 'd-Sphere Product'
-        product['date'] = datetime.datetime.now()
-        product['origin'] = (0, 0, ...) : record the expected origin of this point cloud
-        product['radius'] = (r1, r2) : the inner and outer radii of the point cloud boundary
-        product['points'] = points : the point cloud
-        product['trueBettis'] = {0: b0, 1: b1, ...} : the true Betti numbers for the point cloud in each dimension
-        product['points'] = points : the point cloud
-        product['dimensions'] = dimensions
-    """
     generateSphere = lambda dimension: dSphere_uniformRandom(dimension, numPoints, r1, r2, trials, selection)
-    dSpheres = tuple(generateSphere(dimension)['points'] for dimension in dimensions)
-    points = np.concatenate(dSpheres, axis = 1)
+    dSpheres = tuple(generateSphere(dimension) for dimension in dimensions)
+    return np.concatenate(dSpheres, axis = 1)
 
+    #### I am preserving this code as it computes the true Betti's; keeping in case we end up wanting this value for some of our testing
+    '''  
     polynomials = []
     for dimension in dimensions:
         polynomials.append([0 for _ in range(dimension)])
         polynomials[0] = 1
         polynomials[-1] = 1
     poincarePolynomial = reduce(np.convolve, polynomials)
     trueBettis = dict(enumerate(poincarePolynomial))    
@@ -183,10 +158,8 @@
     product['ptCldObject'] = 'd-Sphere Product'
     product['date'] = datetime.datetime.now()
     product['origin'] = np.zeros(sum(dimensions))
     product['radius'] = (r1, r2)
     product['points'] = points
     product['trueBettis'] = trueBettis
     product['dimensions'] = dimensions
-
-
-    return product
+    '''
```

## Comparing `lhf-1.3.4.dist-info/RECORD` & `lhf-1.3.5.dist-info/RECORD`

 * *Files 14% similar despite different names*

```diff
@@ -1,14 +1,14 @@
-lhf-1.3.4.dist-info/RECORD,,
-lhf-1.3.4.dist-info/top_level.txt,sha256=fB2BmDkD0d4vgvbRkZhFc7SJJ_6qi-D_oiiuQnsrrCk,10
-lhf-1.3.4.dist-info/METADATA,sha256=gDtdnbBqag_VEvmnyRjW0NErY3ZRKvuW0NNBIPt8050,3773
-lhf-1.3.4.dist-info/WHEEL,sha256=FCQIbLYz_6oJJwbVuwkBeTLpFef3kEGVTXXi4swVx20,133
 pyLHF/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 pyLHF/__main__.py,sha256=B05_aOVsf8o--VOMHWVwYA_IcZjVZ0r5ZtJQOnct4PY,98
-pyLHF/pyLHF.py,sha256=oLM6rLdV2zeokbe5iktZdVf1cRU7CprHsQ6y1IE3hHU,581
+pyLHF/pyLHF.py,sha256=6Z5jk5RWz9CvHnLewKOExoqHaqJtO7YZtrb7NcANE5o,1663
+lhf-1.3.5.dist-info/RECORD,,
+lhf-1.3.5.dist-info/top_level.txt,sha256=fB2BmDkD0d4vgvbRkZhFc7SJJ_6qi-D_oiiuQnsrrCk,10
+lhf-1.3.5.dist-info/METADATA,sha256=7nB6L-ORBHR2ZGYzGMXddMYwnA1I_h5dtsTJ-n02j-E,3773
+lhf-1.3.5.dist-info/WHEEL,sha256=FCQIbLYz_6oJJwbVuwkBeTLpFef3kEGVTXXi4swVx20,133
 lhf.libs/libmpfr-b7fafdf8.so.4.1.6,sha256=sZJLd-t9PdU8HtWqk9UiG61yH6KS5si0xkPLQHHj8iU,533337
 lhf.libs/libgmp-452f15e0.so.10.3.2,sha256=kHA0du_vPKRKVapg5JkvgYJRF9ZP6QuttSZDFlE38a8,1570473
 lhf.libs/libbetaSubSkeletonComplex-8505afc6.so,sha256=5WEsa-NvY9McEDafLAbbf6dQ21jbHhXZABt0Pdw7x48,150777
 lhf.libs/libopen-pal-d1f167ef.so.40.30.1,sha256=FMUZrCrzyxDPZmHZL6SjYz7bRD06dliy1FlOrTZTCg4,775729
 lhf.libs/libreadInput-09604ee0.so,sha256=MIgGWsAb40ddFxpivqpvFZ3lq05I-DlyKL5VN7WkBY4,251561
 lhf.libs/libnaiveWindow-e81362bf.so,sha256=Y14eESEWJahilLJNPD6EjN8uwBPrBqkWReYpzvnoE9I,152769
 lhf.libs/libstreamingKmeans-1d5a99ac.so,sha256=_uFDRtdBESvkHtq1wIg_GirXkHFdH8eUKeUI1uOMb5U,16937
@@ -47,25 +47,27 @@
 lhf.libs/libsimplexBase-6b25e600.so,sha256=lT7hBzasWy0MwwGQKW-vfrOiMuh_8dUnDsVmPh9BPlQ,220025
 lhf.libs/libopen-rte-1aa20eaf.so.40.30.1,sha256=rkstjvz_u0R0U6enkqMhxNkEQftCNRFmTXVADvLDwPg,854009
 lhf.libs/libpreprocessor-73ae98ae.so,sha256=QAi4d2Zvp8rqoC2Ur9gITEbYDVK1fF3y3PJ2QiFrKKc,88417
 lhf.libs/libevent_pthreads-2-9991b589.1.so.6.0.2,sha256=0TVnNfvg4aTLy9xZ4K2rc5GfsuCPYNJ3C0O-03EaLTc,17129
 lhf.libs/libgomp-2c51bfab.so.1.0.0,sha256=imIpWFhj6TDsKK3vSE1pMgyytX7howKopJy2HWrBk2U,253289
 lhf.libs/libargParser-e71dc2de.so,sha256=M3BnZtjNClC4My5eOnXSuL3HvZ9ZJcTENZxQ_q-7Xzc,71961
 lhf.libs/libsimplexArrayList-4cbe4342.so,sha256=dYDlzpT2oD6S0XiYGhi6GgKIsEAb01gY1ZBY4XLQJqU,247281
-LHF/utilities.py,sha256=879DxNudbP-kzRQ9_treTjLc7NCH15TpdhnMAj63UbY,9210
+LHF/utilities.py,sha256=N8TPvB1TM14c-7PqKr8Y2zZ_05wCdCMRlG2hjB1HTEM,11163
 LHF/LHF.py,sha256=CNG-que_TRpcsLfkDli-gdqkLiyRjhV-kT1P6DK4m7s,5461
 LHF/triangulation.py,sha256=GwvqBQt4fHaDq2d4KPC9OwrNUXDFNeUX4npVXpgIylE,3361
 LHF/__init__.py,sha256=yQ3BHV6vo-jDwwsDrgJArIQ2N-d3cjtbmcMPiTMULd0,149
 LHF/libLHFlib.so,sha256=E0M_YAZIJCtnP98WToAjx1Rt7da9ypgMTxCSfwouIek,3265945
 LHF/curves.py,sha256=cWep5ZqBMgpJQvb2ua7U9V4fXQrE9av7vpu_6PNaWBc,2020
 LHF/DataGeneration/embeddings.py,sha256=MK6RKmtUte7OtV40n6hjDarOED5RzZ5_rA1k4NXFna0,2209
-LHF/DataGeneration/projectivePlane.py,sha256=spm5O1ln6HObHg91URCLpLg4KVkmS-P7s3wDwLaVWLc,2258
-LHF/DataGeneration/torus.py,sha256=Hv9jL6uAvdtRVMvFQvAGuKnk8KDuo6xNhAo5bQ9NZ1g,14879
-LHF/DataGeneration/__init__.py,sha256=nQF-7FGJwj2NMjL5ImUg0oD-LhfZQ65MMOtot4PaAPs,101
-LHF/DataGeneration/dSphere.py,sha256=_at_d_jnZK4FSphpBlCczVMM-ktw90MCbKjzEO6XnKE,9822
+LHF/DataGeneration/projectivePlane.py,sha256=R8Gm168SVNDolcpbNpzbECcKS5gmxVRxh9gpB-AXs90,1723
+LHF/DataGeneration/dataGen.py,sha256=mWvRSbuiu7w5CqaiDaSgtZCG4v0Q4rzwxG6mSQ-36us,10395
+LHF/DataGeneration/objGen.py,sha256=krmEolt_8elrr-30M5jtsgYlWirqh_DRs-ixk7v7laI,5310
+LHF/DataGeneration/torus.py,sha256=1aOx8vl11eYf4ZbNHahHFzg5I-a-WcG9NfvcaUjeXfk,13189
+LHF/DataGeneration/__init__.py,sha256=8bWpf8Rycqje2-gEnP2X1T8BudoYZR_0Z_-6j5qzFe0,146
+LHF/DataGeneration/dSphere.py,sha256=CsAYGC1SmDrEN-gfrRsIaw-bOFoNM5Z49KQHv05er7g,8421
 LHF/OutputAnalysis/persistenceDiagrams.py,sha256=bY72AYNdkPfxaBxtkYT9GA5p9W8ljHoc98OrKma_rA8,6852
 LHF/OutputAnalysis/piFilters.py,sha256=_2BmWDTC7JgTBIEXXmq16t2rFUWqqMiYJfwRK-K0KcQ,8760
 LHF/OutputAnalysis/__init__.py,sha256=_viTVH5m5TaHvYn_TGuQZZB6XHL3raTm0JwKgmNINPQ,140
 LHF/OutputAnalysis/barcodeDiagrams.py,sha256=eQNfAhZkEpwZfW5hjB6_iF-0fh9NRPFJJ_qyn7WoiYw,3306
 LHF/OutputAnalysis/bettiCurve.py,sha256=ShYY07_4Y5Dvx8AiqBzPzYS17RvvVC9Zn0ySYYco_0o,4904
 LHF/OutputAnalysis/heatmap.py,sha256=pMPqBTwkccNcZbBt7QlbUFSugDiRzaC9vCCoOQxOJxQ,2593
 LHF/OutputAnalysis/filtrationImage.py,sha256=FXI9tUQ7PkVXmxd-O4h3rBgwtyIduKedgraatjh9Y_Q,5904
```

## Comparing `lhf-1.3.4.dist-info/METADATA` & `lhf-1.3.5.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: lhf
-Version: 1.3.4
+Version: 1.3.5
 Summary: Light Weight Homology Framework
 Home-page: UNKNOWN
 License: UNKNOWN
 Project-URL: LHF, https://github.com/wilseypa/lhf
 Project-URL: pyLHF, https://pypi.org/project/lhf
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
```

