# Comparing `tmp/parsons-1.0.0.tar.gz` & `tmp/parsons-1.1.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "parsons-1.0.0.tar", last modified: Fri Mar  3 16:14:21 2023, max compression
+gzip compressed data, was "parsons-1.1.0.tar", last modified: Thu Jun 15 17:42:30 2023, max compression
```

## Comparing `parsons-1.0.0.tar` & `parsons-1.1.0.tar`

### file list

```diff
@@ -1,332 +1,332 @@
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.910335 parsons-1.0.0/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    11493 2023-03-03 16:13:25.000000 parsons-1.0.0/LICENSE.md
--rw-rw-r--   0 shauna    (1000) shauna    (1000)      575 2023-03-03 16:14:21.910335 parsons-1.0.0/PKG-INFO
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     4229 2023-03-03 16:13:25.000000 parsons-1.0.0/README.md
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.878334 parsons-1.0.0/parsons/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     3972 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/__init__.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.878334 parsons-1.0.0/parsons/actblue/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       73 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/actblue/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     6709 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/actblue/actblue.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.882334 parsons-1.0.0/parsons/action_kit/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       83 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/action_kit/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    40411 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/action_kit/action_kit.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.882334 parsons-1.0.0/parsons/action_network/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       99 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/action_network/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    15803 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/action_network/action_network.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.882334 parsons-1.0.0/parsons/airtable/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       77 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/airtable/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     5794 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/airtable/airtable.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.882334 parsons-1.0.0/parsons/alchemer/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       99 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/alchemer/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     3550 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/alchemer/alchemer.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.882334 parsons-1.0.0/parsons/auth0/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       65 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/auth0/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     1947 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/auth0/auth0.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.882334 parsons-1.0.0/parsons/aws/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)      204 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/aws/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     4564 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/aws/aws_async.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     9154 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/aws/lambda_distribute.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    14516 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/aws/s3.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.882334 parsons-1.0.0/parsons/azure/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)      100 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/azure/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    14730 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/azure/azure_blob_storage.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.882334 parsons-1.0.0/parsons/bill_com/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       75 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/bill_com/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    11956 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/bill_com/bill_com.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.882334 parsons-1.0.0/parsons/bloomerang/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       85 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/bloomerang/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    12716 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/bloomerang/bloomerang.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.882334 parsons-1.0.0/parsons/bluelink/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)      362 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/bluelink/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     3168 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/bluelink/bluelink.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     7886 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/bluelink/person.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.882334 parsons-1.0.0/parsons/box/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       57 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/box/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    15090 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/box/box.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.882334 parsons-1.0.0/parsons/braintree/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       81 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/braintree/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    21427 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/braintree/braintree.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.882334 parsons-1.0.0/parsons/capitol_canary/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       99 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/capitol_canary/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    15294 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/capitol_canary/capitol_canary.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.882334 parsons-1.0.0/parsons/civis/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       83 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/civis/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     5224 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/civis/civisclient.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.886334 parsons-1.0.0/parsons/controlshift/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       93 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/controlshift/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     2439 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/controlshift/controlshift.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.886334 parsons-1.0.0/parsons/copper/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       69 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/copper/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    13399 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/copper/copper.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.886334 parsons-1.0.0/parsons/crowdtangle/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       89 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/crowdtangle/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     9176 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/crowdtangle/crowdtangle.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.886334 parsons-1.0.0/parsons/databases/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/databases/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     2293 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/databases/alchemy.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.886334 parsons-1.0.0/parsons/databases/database/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)      117 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/databases/database/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     2377 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/databases/database/constants.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     9841 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/databases/database/database.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    13363 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/databases/db_sync.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.886334 parsons-1.0.0/parsons/databases/mysql/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       75 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/databases/mysql/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)      639 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/databases/mysql/constants.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     3393 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/databases/mysql/create_table.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    11744 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/databases/mysql/mysql.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.886334 parsons-1.0.0/parsons/databases/postgres/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       87 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/databases/postgres/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)      306 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/databases/postgres/constants.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     3645 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/databases/postgres/postgres.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     8948 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/databases/postgres/postgres_core.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     6727 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/databases/postgres/postgres_create_statement.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.890334 parsons-1.0.0/parsons/databases/redshift/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       87 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/databases/redshift/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)      303 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/databases/redshift/constants.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    49174 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/databases/redshift/redshift.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     5571 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/databases/redshift/rs_copy_table.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     7830 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/databases/redshift/rs_create_table.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     1794 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/databases/redshift/rs_schema.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    26820 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/databases/redshift/rs_table_utilities.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     3820 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/databases/table.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.890334 parsons-1.0.0/parsons/donorbox/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/donorbox/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     8971 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/donorbox/donorbox.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.890334 parsons-1.0.0/parsons/etl/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)      158 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/etl/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    40994 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/etl/etl.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     7123 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/etl/table.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    30172 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/etl/tofrom.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.890334 parsons-1.0.0/parsons/facebook_ads/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       91 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/facebook_ads/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    15331 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/facebook_ads/facebook_ads.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.890334 parsons-1.0.0/parsons/freshdesk/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       81 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/freshdesk/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     6835 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/freshdesk/freshdesk.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.890334 parsons-1.0.0/parsons/geocode/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       95 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/geocode/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     4545 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/geocode/census_geocoder.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.890334 parsons-1.0.0/parsons/github/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       69 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/github/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    14517 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/github/github.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.890334 parsons-1.0.0/parsons/google/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/google/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     3834 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/google/google_admin.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    11610 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/google/google_bigquery.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     3497 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/google/google_civic.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     9577 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/google/google_cloud_storage.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    15329 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/google/google_sheets.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)      829 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/google/utitities.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.894334 parsons-1.0.0/parsons/hustle/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       69 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/hustle/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)      352 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/hustle/column_map.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    14854 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/hustle/hustle.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.894334 parsons-1.0.0/parsons/mailchimp/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       81 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/mailchimp/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    17757 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/mailchimp/mailchimp.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.894334 parsons-1.0.0/parsons/mobilize_america/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       93 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/mobilize_america/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    14174 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/mobilize_america/ma.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.894334 parsons-1.0.0/parsons/newmode/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       73 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/newmode/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     9748 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/newmode/newmode.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.898334 parsons-1.0.0/parsons/ngpvan/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       60 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/ngpvan/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     3471 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/ngpvan/activist_codes.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    11551 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/ngpvan/bulk_import.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     1454 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/ngpvan/canvass_responses.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     3510 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/ngpvan/changed_entities.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     6493 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/ngpvan/codes.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     1753 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/ngpvan/contact_notes.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     2270 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/ngpvan/custom_fields.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     8727 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/ngpvan/events.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     3101 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/ngpvan/locations.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    23450 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/ngpvan/people.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     1556 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/ngpvan/printed_lists.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    12918 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/ngpvan/saved_lists.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    13696 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/ngpvan/scores.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     5956 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/ngpvan/signups.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     3128 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/ngpvan/supporter_groups.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     3813 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/ngpvan/survey_questions.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     2357 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/ngpvan/targets.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)      560 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/ngpvan/utilities.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     2098 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/ngpvan/van.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     3210 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/ngpvan/van_connector.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.898334 parsons-1.0.0/parsons/notifications/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/notifications/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     2731 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/notifications/gmail.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     8950 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/notifications/sendmail.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     8537 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/notifications/slack.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     2498 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/notifications/smtp.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.898334 parsons-1.0.0/parsons/pdi/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       57 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/pdi/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     4132 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/pdi/acquisition_types.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     1843 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/pdi/activities.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     7885 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/pdi/contacts.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    18805 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/pdi/events.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     3628 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/pdi/flag_ids.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     2299 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/pdi/flags.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     1592 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/pdi/locations.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     5543 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/pdi/pdi.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     1957 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/pdi/questions.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)      898 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/pdi/universes.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.898334 parsons-1.0.0/parsons/phone2action/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       84 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/phone2action/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     9541 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/phone2action/p2a.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.898334 parsons-1.0.0/parsons/quickbase/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       81 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/quickbase/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     2931 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/quickbase/quickbase.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.898334 parsons-1.0.0/parsons/redash/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       69 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/redash/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     7982 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/redash/redash.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.898334 parsons-1.0.0/parsons/rockthevote/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/rockthevote/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    12180 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/rockthevote/rtv.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.902335 parsons-1.0.0/parsons/salesforce/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       85 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/salesforce/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     8532 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/salesforce/salesforce.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.902335 parsons-1.0.0/parsons/scytl/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       65 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/scytl/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    24803 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/scytl/scytl.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.902335 parsons-1.0.0/parsons/sftp/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       61 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/sftp/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    14526 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/sftp/sftp.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)      626 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/sftp/utilities.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.902335 parsons-1.0.0/parsons/shopify/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       73 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/shopify/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     7634 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/shopify/shopify.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.902335 parsons-1.0.0/parsons/sisense/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       73 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/sisense/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     2951 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/sisense/sisense.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.902335 parsons-1.0.0/parsons/targetsmart/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)      205 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/targetsmart/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    13464 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/targetsmart/targetsmart_api.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    10285 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/targetsmart/targetsmart_automation.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    12797 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/targetsmart/targetsmart_smartmatch.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.902335 parsons-1.0.0/parsons/tools/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/tools/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     5828 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/tools/credential_tools.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.902335 parsons-1.0.0/parsons/turbovote/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       81 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/turbovote/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     1979 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/turbovote/turbovote.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.902335 parsons-1.0.0/parsons/twilio/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       69 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/twilio/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     5760 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/twilio/twilio.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.902335 parsons-1.0.0/parsons/utilities/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/utilities/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     9917 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/utilities/api_connector.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)      503 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/utilities/check_env.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     1559 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/utilities/cloud_storage.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     1646 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/utilities/datetime.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    13086 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/utilities/files.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     1342 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/utilities/json_format.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     3511 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/utilities/oauth_api_connector.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)      343 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/utilities/sql_helpers.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     1483 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/utilities/zip_archive.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.902335 parsons-1.0.0/parsons/zoom/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       61 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/zoom/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     7064 2023-03-03 16:13:25.000000 parsons-1.0.0/parsons/zoom/zoom.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.878334 parsons-1.0.0/parsons.egg-info/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)      575 2023-03-03 16:14:21.000000 parsons-1.0.0/parsons.egg-info/PKG-INFO
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     7876 2023-03-03 16:14:21.000000 parsons-1.0.0/parsons.egg-info/SOURCES.txt
--rw-rw-r--   0 shauna    (1000) shauna    (1000)        1 2023-03-03 16:14:21.000000 parsons-1.0.0/parsons.egg-info/dependency_links.txt
--rw-rw-r--   0 shauna    (1000) shauna    (1000)      942 2023-03-03 16:14:21.000000 parsons-1.0.0/parsons.egg-info/requires.txt
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       13 2023-03-03 16:14:21.000000 parsons-1.0.0/parsons.egg-info/top_level.txt
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       38 2023-03-03 16:14:21.910335 parsons-1.0.0/setup.cfg
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     2925 2023-03-03 16:13:25.000000 parsons-1.0.0/setup.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.906335 parsons-1.0.0/test/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:13:25.000000 parsons-1.0.0/test/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)      104 2023-03-03 16:13:25.000000 parsons-1.0.0/test/conftest.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)      619 2023-03-03 16:13:25.000000 parsons-1.0.0/test/fixtures.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    22525 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_action_kit.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)      910 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_auth0.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     3766 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_aws_async.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.906335 parsons-1.0.0/test/test_bloomerang/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_bloomerang/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     6458 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_bloomerang/test_bloomerang.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    28953 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_bloomerang/test_data.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.906335 parsons-1.0.0/test/test_braintree/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_braintree/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     5927 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_braintree/test_braintree.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    10663 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_capitol_canary.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     1439 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_civis.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     3448 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_credential_tools.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.906335 parsons-1.0.0/test/test_databases/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_databases/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     2988 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_databases/fakes.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     4022 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_databases/test_database.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    12085 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_databases/test_dbsync.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     6001 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_databases/test_mysql.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     9775 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_databases/test_postgres.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    28109 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_etl.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     4539 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_facebook_ads.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.906335 parsons-1.0.0/test/test_freshdesk/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_freshdesk/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     2295 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_freshdesk/expected_json.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     1137 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_freshdesk/test_freshdesk.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.906335 parsons-1.0.0/test/test_github/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_github/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     2488 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_github/test_github.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.906335 parsons-1.0.0/test/test_gmail/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_gmail/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    20996 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_gmail/test_gmail.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.906335 parsons-1.0.0/test/test_mailchimp/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_mailchimp/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    12671 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_mailchimp/expected_json.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     1402 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_mailchimp/test_mailchimp.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.906335 parsons-1.0.0/test/test_mobilize/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_mobilize/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     3688 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_mobilize/test_mobilize_america.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     9119 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_mobilize/test_mobilize_json.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    10380 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_p2a.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.910335 parsons-1.0.0/test/test_pdi/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_pdi/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)      787 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_pdi/conftest.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       49 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_pdi/test_acquisitiontypes.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     3558 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_pdi/test_flag_ids.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       38 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_pdi/test_flags.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     1163 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_pdi/test_pdi.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       42 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_pdi/test_questions.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)       42 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_pdi/test_universes.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     4065 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_redash.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    36477 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_redshift.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     6113 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_s3.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     7712 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_sendmail.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    10322 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_sftp.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     4711 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_sftp_ssh.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     5261 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_shopify.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.910335 parsons-1.0.0/test/test_sisense/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_sisense/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)      664 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_sisense/test_data.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     1663 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_sisense/test_sisense.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.910335 parsons-1.0.0/test/test_slack/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_slack/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     7192 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_slack/test_slack.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     4490 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_smtp.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.910335 parsons-1.0.0/test/test_targetsmart/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_targetsmart/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     6715 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_targetsmart/test_targetsmart_api.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     2640 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_targetsmart/test_targetsmart_automation.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     2408 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_targetsmart/test_targetsmart_smartmatch.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     4981 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_utilities.py
-drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:14:21.910335 parsons-1.0.0/test/test_van/
--rw-rw-r--   0 shauna    (1000) shauna    (1000)        0 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_van/__init__.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     4517 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_van/responses_people.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     1492 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_van/responses_printed_lists.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     2959 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_van/test_activist_codes.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     7421 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_van/test_bulkimport.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     2989 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_van/test_changed_entities.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     3575 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_van/test_codes.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)      820 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_van/test_contact_notes.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     1990 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_van/test_custom_fields.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     6301 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_van/test_events.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     2878 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_van/test_locations.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     5846 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_van/test_ngpvan.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     8025 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_van/test_people.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)      865 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_van/test_printed_lists.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     7492 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_van/test_saved_lists.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     6993 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_van/test_scores.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     4509 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_van/test_signups.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     4367 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_van/test_targets.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)    14532 2023-03-03 16:13:25.000000 parsons-1.0.0/test/test_zoom.py
--rw-rw-r--   0 shauna    (1000) shauna    (1000)     1289 2023-03-03 16:13:25.000000 parsons-1.0.0/test/utils.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:30.021605 parsons-1.1.0/
+-rw-r--r--   0 shauna    (1000) shauna    (1000)    11493 2022-02-09 16:51:28.000000 parsons-1.1.0/LICENSE.md
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)      572 2023-06-15 17:42:30.021605 parsons-1.1.0/PKG-INFO
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     4229 2023-05-19 18:49:32.000000 parsons-1.1.0/README.md
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.957603 parsons-1.1.0/parsons/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     3959 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/__init__.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.957603 parsons-1.1.0/parsons/actblue/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       67 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/actblue/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     6803 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/actblue/actblue.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.961603 parsons-1.1.0/parsons/action_kit/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       77 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/action_kit/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    42777 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/action_kit/action_kit.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.961603 parsons-1.1.0/parsons/action_network/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       93 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/action_network/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    16034 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/action_network/action_network.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.961603 parsons-1.1.0/parsons/airtable/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       71 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/airtable/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     5896 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/airtable/airtable.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.961603 parsons-1.1.0/parsons/alchemer/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       99 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/alchemer/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     3618 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/alchemer/alchemer.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.961603 parsons-1.1.0/parsons/auth0/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       59 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/auth0/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     2196 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/auth0/auth0.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.961603 parsons-1.1.0/parsons/aws/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)      190 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/aws/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     4596 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/aws/aws_async.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     8966 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/aws/lambda_distribute.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    15194 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/aws/s3.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.961603 parsons-1.1.0/parsons/azure/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       94 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/azure/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    14981 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/azure/azure_blob_storage.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.965603 parsons-1.1.0/parsons/bill_com/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       69 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/bill_com/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    11744 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/bill_com/bill_com.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.965603 parsons-1.1.0/parsons/bloomerang/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       79 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/bloomerang/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    12703 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/bloomerang/bloomerang.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.965603 parsons-1.1.0/parsons/bluelink/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)      410 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/bluelink/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     3165 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/bluelink/bluelink.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     8086 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/bluelink/person.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.965603 parsons-1.1.0/parsons/box/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       51 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/box/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    15017 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/box/box.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.965603 parsons-1.1.0/parsons/braintree/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       75 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/braintree/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    21822 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/braintree/braintree.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.965603 parsons-1.1.0/parsons/capitol_canary/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       93 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/capitol_canary/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    14982 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/capitol_canary/capitol_canary.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.965603 parsons-1.1.0/parsons/civis/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       77 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/civis/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     5258 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/civis/civisclient.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.969604 parsons-1.1.0/parsons/controlshift/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       87 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/controlshift/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     2453 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/controlshift/controlshift.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.969604 parsons-1.1.0/parsons/copper/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       63 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/copper/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    13467 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/copper/copper.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.969604 parsons-1.1.0/parsons/crowdtangle/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       83 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/crowdtangle/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     9235 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/crowdtangle/crowdtangle.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.969604 parsons-1.1.0/parsons/databases/
+-rw-r--r--   0 shauna    (1000) shauna    (1000)        0 2022-02-09 16:51:28.000000 parsons-1.1.0/parsons/databases/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     2291 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/databases/alchemy.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.969604 parsons-1.1.0/parsons/databases/database/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)      111 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/databases/database/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     2885 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/databases/database/constants.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     9876 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/databases/database/database.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    13977 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/databases/db_sync.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.969604 parsons-1.1.0/parsons/databases/mysql/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       69 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/databases/mysql/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)      640 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/databases/mysql/constants.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     3392 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/databases/mysql/create_table.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    11638 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/databases/mysql/mysql.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.973604 parsons-1.1.0/parsons/databases/postgres/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       81 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/databases/postgres/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)      306 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/databases/postgres/constants.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     3715 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/databases/postgres/postgres.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     8966 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/databases/postgres/postgres_core.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     6542 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/databases/postgres/postgres_create_statement.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.973604 parsons-1.1.0/parsons/databases/redshift/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       81 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/databases/redshift/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)      303 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/databases/redshift/constants.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    49826 2023-06-15 17:05:03.000000 parsons-1.1.0/parsons/databases/redshift/redshift.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     5766 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/databases/redshift/rs_copy_table.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     7813 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/databases/redshift/rs_create_table.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     1795 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/databases/redshift/rs_schema.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    26964 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/databases/redshift/rs_table_utilities.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     3842 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/databases/table.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.973604 parsons-1.1.0/parsons/donorbox/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)        0 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/donorbox/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     9010 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/donorbox/donorbox.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.977604 parsons-1.1.0/parsons/etl/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)      144 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/etl/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    41579 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/etl/etl.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     7206 2023-06-15 17:05:03.000000 parsons-1.1.0/parsons/etl/table.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    30978 2023-06-15 17:05:03.000000 parsons-1.1.0/parsons/etl/tofrom.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.977604 parsons-1.1.0/parsons/facebook_ads/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       85 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/facebook_ads/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    15512 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/facebook_ads/facebook_ads.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.977604 parsons-1.1.0/parsons/freshdesk/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       75 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/freshdesk/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     6866 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/freshdesk/freshdesk.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.977604 parsons-1.1.0/parsons/geocode/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       89 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/geocode/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     4869 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/geocode/census_geocoder.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.977604 parsons-1.1.0/parsons/github/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       63 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/github/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    14763 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/github/github.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.977604 parsons-1.1.0/parsons/google/
+-rw-r--r--   0 shauna    (1000) shauna    (1000)        0 2022-02-09 16:51:28.000000 parsons-1.1.0/parsons/google/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     4008 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/google/google_admin.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    11762 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/google/google_bigquery.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     3497 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/google/google_civic.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     9653 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/google/google_cloud_storage.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    15527 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/google/google_sheets.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)      823 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/google/utitities.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.981604 parsons-1.1.0/parsons/hustle/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       63 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/hustle/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)      279 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/hustle/column_map.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    15152 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/hustle/hustle.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.981604 parsons-1.1.0/parsons/mailchimp/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       75 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/mailchimp/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    17601 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/mailchimp/mailchimp.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.981604 parsons-1.1.0/parsons/mobilize_america/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       87 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/mobilize_america/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    14286 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/mobilize_america/ma.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.981604 parsons-1.1.0/parsons/newmode/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       67 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/newmode/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     9747 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/newmode/newmode.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.989604 parsons-1.1.0/parsons/ngpvan/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       54 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/ngpvan/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     3313 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/ngpvan/activist_codes.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    12495 2023-06-15 17:05:03.000000 parsons-1.1.0/parsons/ngpvan/bulk_import.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     1453 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/ngpvan/canvass_responses.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     3596 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/ngpvan/changed_entities.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     6731 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/ngpvan/codes.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     1766 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/ngpvan/contact_notes.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     2373 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/ngpvan/custom_fields.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     8890 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/ngpvan/events.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     3106 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/ngpvan/locations.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    24191 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/ngpvan/people.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     1637 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/ngpvan/printed_lists.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    13192 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/ngpvan/saved_lists.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    13693 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/ngpvan/scores.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     5984 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/ngpvan/signups.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     3193 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/ngpvan/supporter_groups.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     3855 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/ngpvan/survey_questions.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     2336 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/ngpvan/targets.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)      558 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/ngpvan/utilities.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     2175 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/ngpvan/van.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     3344 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/ngpvan/van_connector.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.989604 parsons-1.1.0/parsons/notifications/
+-rw-r--r--   0 shauna    (1000) shauna    (1000)        0 2022-02-09 16:51:28.000000 parsons-1.1.0/parsons/notifications/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     2754 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/notifications/gmail.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     8901 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/notifications/sendmail.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     8879 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/notifications/slack.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     2573 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/notifications/smtp.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.993604 parsons-1.1.0/parsons/pdi/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       51 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/pdi/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     4194 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/pdi/acquisition_types.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     1797 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/pdi/activities.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     8110 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/pdi/contacts.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    18413 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/pdi/events.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     3592 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/pdi/flag_ids.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     2316 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/pdi/flags.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     1542 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/pdi/locations.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     5509 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/pdi/pdi.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     1997 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/pdi/questions.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)      897 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/pdi/universes.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.993604 parsons-1.1.0/parsons/phone2action/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       78 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/phone2action/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     9292 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/phone2action/p2a.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.993604 parsons-1.1.0/parsons/quickbase/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       75 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/quickbase/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     2902 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/quickbase/quickbase.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.997604 parsons-1.1.0/parsons/redash/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       63 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/redash/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    10209 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/redash/redash.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.997604 parsons-1.1.0/parsons/rockthevote/
+-rw-r--r--   0 shauna    (1000) shauna    (1000)        0 2022-02-09 16:51:28.000000 parsons-1.1.0/parsons/rockthevote/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    12263 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/rockthevote/rtv.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.997604 parsons-1.1.0/parsons/salesforce/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       79 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/salesforce/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     8575 2023-06-15 17:05:03.000000 parsons-1.1.0/parsons/salesforce/salesforce.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.997604 parsons-1.1.0/parsons/scytl/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       59 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/scytl/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    25333 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/scytl/scytl.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.997604 parsons-1.1.0/parsons/sftp/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       55 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/sftp/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    14988 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/sftp/sftp.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)      626 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/sftp/utilities.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.997604 parsons-1.1.0/parsons/shopify/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       67 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/shopify/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     8582 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/shopify/shopify.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:30.001604 parsons-1.1.0/parsons/sisense/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       67 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/sisense/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     2995 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/sisense/sisense.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:30.001604 parsons-1.1.0/parsons/targetsmart/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)      195 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/targetsmart/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    13464 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/targetsmart/targetsmart_api.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    10285 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/targetsmart/targetsmart_automation.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    12797 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/targetsmart/targetsmart_smartmatch.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:30.001604 parsons-1.1.0/parsons/tools/
+-rw-r--r--   0 shauna    (1000) shauna    (1000)        0 2022-02-09 16:51:28.000000 parsons-1.1.0/parsons/tools/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     5838 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/tools/credential_tools.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:30.001604 parsons-1.1.0/parsons/turbovote/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       75 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/turbovote/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     1960 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/turbovote/turbovote.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:30.001604 parsons-1.1.0/parsons/twilio/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       63 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/twilio/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     5795 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/twilio/twilio.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:30.005604 parsons-1.1.0/parsons/utilities/
+-rw-r--r--   0 shauna    (1000) shauna    (1000)        0 2022-02-09 16:51:28.000000 parsons-1.1.0/parsons/utilities/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    10023 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/utilities/api_connector.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)      530 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/utilities/check_env.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     1579 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/utilities/cloud_storage.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     1655 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/utilities/datetime.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    13049 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/utilities/files.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     1341 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/utilities/json_format.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     3694 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/utilities/oauth_api_connector.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)      343 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/utilities/sql_helpers.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     1483 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/utilities/zip_archive.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:30.005604 parsons-1.1.0/parsons/zoom/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       55 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/zoom/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     7129 2023-05-19 18:49:32.000000 parsons-1.1.0/parsons/zoom/zoom.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:29.957603 parsons-1.1.0/parsons.egg-info/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)      572 2023-06-15 17:42:29.000000 parsons-1.1.0/parsons.egg-info/PKG-INFO
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     7876 2023-06-15 17:42:29.000000 parsons-1.1.0/parsons.egg-info/SOURCES.txt
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)        1 2023-06-15 17:42:29.000000 parsons-1.1.0/parsons.egg-info/dependency_links.txt
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)      957 2023-06-15 17:42:29.000000 parsons-1.1.0/parsons.egg-info/requires.txt
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       13 2023-06-15 17:42:29.000000 parsons-1.1.0/parsons.egg-info/top_level.txt
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)       38 2023-06-15 17:42:30.021605 parsons-1.1.0/setup.cfg
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     2900 2023-06-15 17:38:50.000000 parsons-1.1.0/setup.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:30.009604 parsons-1.1.0/test/
+-rw-r--r--   0 shauna    (1000) shauna    (1000)        0 2022-02-09 16:51:28.000000 parsons-1.1.0/test/__init__.py
+-rw-r--r--   0 shauna    (1000) shauna    (1000)      104 2022-02-09 16:51:28.000000 parsons-1.1.0/test/conftest.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)      619 2023-05-19 18:49:32.000000 parsons-1.1.0/test/fixtures.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    24140 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_action_kit.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)      967 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_auth0.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     3724 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_aws_async.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:30.009604 parsons-1.1.0/test/test_bloomerang/
+-rw-r--r--   0 shauna    (1000) shauna    (1000)        0 2022-02-09 16:51:28.000000 parsons-1.1.0/test/test_bloomerang/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     6762 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_bloomerang/test_bloomerang.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    22406 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_bloomerang/test_data.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:30.009604 parsons-1.1.0/test/test_braintree/
+-rw-r--r--   0 shauna    (1000) shauna    (1000)        0 2022-02-09 16:51:28.000000 parsons-1.1.0/test/test_braintree/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     5989 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_braintree/test_braintree.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    10507 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_capitol_canary.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     1444 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_civis.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     3360 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_credential_tools.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:30.013604 parsons-1.1.0/test/test_databases/
+-rw-r--r--   0 shauna    (1000) shauna    (1000)        0 2022-02-09 16:51:28.000000 parsons-1.1.0/test/test_databases/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     3031 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_databases/fakes.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     4404 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_databases/test_database.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    12263 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_databases/test_dbsync.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     6072 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_databases/test_mysql.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    10055 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_databases/test_postgres.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    29013 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_etl.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     4898 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_facebook_ads.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:30.013604 parsons-1.1.0/test/test_freshdesk/
+-rw-r--r--   0 shauna    (1000) shauna    (1000)        0 2022-02-09 16:51:28.000000 parsons-1.1.0/test/test_freshdesk/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     2841 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_freshdesk/expected_json.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     1136 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_freshdesk/test_freshdesk.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:30.013604 parsons-1.1.0/test/test_github/
+-rw-r--r--   0 shauna    (1000) shauna    (1000)        0 2022-02-09 16:51:28.000000 parsons-1.1.0/test/test_github/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     2579 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_github/test_github.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:30.013604 parsons-1.1.0/test/test_gmail/
+-rw-r--r--   0 shauna    (1000) shauna    (1000)        0 2022-02-09 16:51:28.000000 parsons-1.1.0/test/test_gmail/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    22115 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_gmail/test_gmail.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:30.013604 parsons-1.1.0/test/test_mailchimp/
+-rw-r--r--   0 shauna    (1000) shauna    (1000)        0 2022-02-09 16:51:28.000000 parsons-1.1.0/test/test_mailchimp/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    13126 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_mailchimp/expected_json.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     1436 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_mailchimp/test_mailchimp.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:30.013604 parsons-1.1.0/test/test_mobilize/
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)        0 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_mobilize/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     4200 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_mobilize/test_mobilize_america.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     8972 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_mobilize/test_mobilize_json.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    10227 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_p2a.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:30.017605 parsons-1.1.0/test/test_pdi/
+-rw-r--r--   0 shauna    (1000) shauna    (1000)        0 2022-02-09 16:51:28.000000 parsons-1.1.0/test/test_pdi/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)      801 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_pdi/conftest.py
+-rw-r--r--   0 shauna    (1000) shauna    (1000)       49 2022-02-09 16:51:28.000000 parsons-1.1.0/test/test_pdi/test_acquisitiontypes.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     3612 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_pdi/test_flag_ids.py
+-rw-r--r--   0 shauna    (1000) shauna    (1000)       38 2022-02-09 16:51:28.000000 parsons-1.1.0/test/test_pdi/test_flags.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     1225 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_pdi/test_pdi.py
+-rw-r--r--   0 shauna    (1000) shauna    (1000)       42 2022-02-09 16:51:28.000000 parsons-1.1.0/test/test_pdi/test_questions.py
+-rw-r--r--   0 shauna    (1000) shauna    (1000)       42 2022-02-09 16:51:28.000000 parsons-1.1.0/test/test_pdi/test_universes.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     5134 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_redash.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    38442 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_redshift.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     6224 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_s3.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     7646 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_sendmail.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    10460 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_sftp.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     4888 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_sftp_ssh.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     5515 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_shopify.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:30.017605 parsons-1.1.0/test/test_sisense/
+-rw-r--r--   0 shauna    (1000) shauna    (1000)        0 2022-02-09 16:51:28.000000 parsons-1.1.0/test/test_sisense/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)      668 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_sisense/test_data.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     1897 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_sisense/test_sisense.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:30.017605 parsons-1.1.0/test/test_slack/
+-rw-r--r--   0 shauna    (1000) shauna    (1000)        0 2022-02-09 16:51:28.000000 parsons-1.1.0/test/test_slack/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     8291 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_slack/test_slack.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     4986 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_smtp.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:30.017605 parsons-1.1.0/test/test_targetsmart/
+-rw-r--r--   0 shauna    (1000) shauna    (1000)        0 2022-02-09 16:51:28.000000 parsons-1.1.0/test/test_targetsmart/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     7111 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_targetsmart/test_targetsmart_api.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     2607 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_targetsmart/test_targetsmart_automation.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     2408 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_targetsmart/test_targetsmart_smartmatch.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     5013 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_utilities.py
+drwxrwxr-x   0 shauna    (1000) shauna    (1000)        0 2023-06-15 17:42:30.021605 parsons-1.1.0/test/test_van/
+-rw-r--r--   0 shauna    (1000) shauna    (1000)        0 2022-02-09 16:51:28.000000 parsons-1.1.0/test/test_van/__init__.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     4626 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_van/responses_people.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     2034 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_van/responses_printed_lists.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     3210 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_van/test_activist_codes.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     7361 2023-06-15 17:05:03.000000 parsons-1.1.0/test/test_van/test_bulkimport.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     3135 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_van/test_changed_entities.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     3518 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_van/test_codes.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)      821 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_van/test_contact_notes.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     2164 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_van/test_custom_fields.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     6251 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_van/test_events.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     2670 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_van/test_locations.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     6276 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_van/test_ngpvan.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     8950 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_van/test_people.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)      864 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_van/test_printed_lists.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     7500 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_van/test_saved_lists.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     7695 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_van/test_scores.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     4459 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_van/test_signups.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     5023 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_van/test_targets.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)    16391 2023-05-19 18:49:32.000000 parsons-1.1.0/test/test_zoom.py
+-rw-rw-r--   0 shauna    (1000) shauna    (1000)     1290 2023-05-19 18:49:32.000000 parsons-1.1.0/test/utils.py
```

### Comparing `parsons-1.0.0/LICENSE.md` & `parsons-1.1.0/LICENSE.md`

 * *Files identical despite different names*

### Comparing `parsons-1.0.0/README.md` & `parsons-1.1.0/README.md`

 * *Files identical despite different names*

### Comparing `parsons-1.0.0/parsons/__init__.py` & `parsons-1.1.0/parsons/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -7,25 +7,25 @@
 
 # Define the default logging config for Parsons and its submodules. For now the
 # logger gets a StreamHandler by default. At some point a NullHandler may be more
 # appropriate, so the end user must decide on logging behavior.
 
 logger = logging.getLogger(__name__)
 _handler = logging.StreamHandler()
-_formatter = logging.Formatter('%(module)s %(levelname)s %(message)s')
+_formatter = logging.Formatter("%(module)s %(levelname)s %(message)s")
 _handler.setFormatter(_formatter)
 logger.addHandler(_handler)
 
-if os.environ.get('TESTING'):
+if os.environ.get("TESTING"):
     # Log less stuff in automated tests
-    logger.setLevel('WARNING')
-elif os.environ.get('DEBUG'):
-    logger.setLevel('DEBUG')
+    logger.setLevel("WARNING")
+elif os.environ.get("DEBUG"):
+    logger.setLevel("DEBUG")
 else:
-    logger.setLevel('INFO')
+    logger.setLevel("INFO")
 
 # Table is referenced by many connectors, so we add it immediately to limit the damage
 # of circular dependencies
 __all__ = ["Table"]
 for module_path, connector_name in (
     ("parsons.actblue.actblue", "ActBlue"),
     ("parsons.action_kit.action_kit", "ActionKit"),
@@ -79,17 +79,15 @@
     ("parsons.shopify.shopify", "Shopify"),
     ("parsons.sisense.sisense", "Sisense"),
     ("parsons.targetsmart.targetsmart_api", "TargetSmartAPI"),
     ("parsons.targetsmart.targetsmart_automation", "TargetSmartAutomation"),
     ("parsons.turbovote.turbovote", "TurboVote"),
     ("parsons.twilio.twilio", "Twilio"),
     ("parsons.zoom.zoom", "Zoom"),
-
 ):
     try:
         globals()[connector_name] = getattr(
-            importlib.import_module(module_path),
-            connector_name
+            importlib.import_module(module_path), connector_name
         )
         __all__.append(connector_name)
     except ImportError:
         logger.debug(f"Could not import {module_path}.{connector_name}; skipping")
```

### Comparing `parsons-1.0.0/parsons/actblue/actblue.py` & `parsons-1.1.0/parsons/actblue/actblue.py`

 * *Files 8% similar despite different names*

```diff
@@ -27,26 +27,35 @@
                 use this URI parameter if a different endpoint is necessary - for example, when
                 running this code in a test environment where you don't want to hit the actual API.
 
         For instructions on how to generate a Client UUID and Client Secret set,
         visit https://secure.actblue.com/docs/csv_api#authentication.
     """
 
-    def __init__(self, actblue_client_uuid=None, actblue_client_secret=None, actblue_uri=None):
-        self.actblue_client_uuid = check_env.check('ACTBLUE_CLIENT_UUID', actblue_client_uuid)
-        self.actblue_client_secret = check_env.check('ACTBLUE_CLIENT_SECRET', actblue_client_secret)
-        self.uri = check_env.check(
-            'ACTBLUE_URI', actblue_uri, optional=True
-        ) or ACTBLUE_API_ENDPOINT
+    def __init__(
+        self, actblue_client_uuid=None, actblue_client_secret=None, actblue_uri=None
+    ):
+        self.actblue_client_uuid = check_env.check(
+            "ACTBLUE_CLIENT_UUID", actblue_client_uuid
+        )
+        self.actblue_client_secret = check_env.check(
+            "ACTBLUE_CLIENT_SECRET", actblue_client_secret
+        )
+        self.uri = (
+            check_env.check("ACTBLUE_URI", actblue_uri, optional=True)
+            or ACTBLUE_API_ENDPOINT
+        )
         self.headers = {
             "accept": "application/json",
         }
-        self.client = APIConnector(self.uri,
-                                   auth=(self.actblue_client_uuid, self.actblue_client_secret),
-                                   headers=self.headers)
+        self.client = APIConnector(
+            self.uri,
+            auth=(self.actblue_client_uuid, self.actblue_client_secret),
+            headers=self.headers,
+        )
 
     def post_request(self, csv_type=None, date_range_start=None, date_range_end=None):
         """
         POST request to ActBlue API to begin generating the CSV.
 
         `Args:`
             csv_type: str
@@ -71,17 +80,19 @@
             Response of POST request; a successful response includes 'id', a unique identifier for
             the CSV being generated.
         """
 
         body = {
             "csv_type": csv_type,
             "date_range_start": date_range_start,
-            "date_range_end": date_range_end
+            "date_range_end": date_range_end,
         }
-        logger.info(f'Requesting {csv_type} from {date_range_start} up to {date_range_end}.')
+        logger.info(
+            f"Requesting {csv_type} from {date_range_start} up to {date_range_end}."
+        )
         response = self.client.post_request(url="csvs", json=body)
         return response
 
     def get_download_url(self, csv_id=None):
         """
         GET request to retrieve download_url for generated CSV.
 
@@ -91,15 +102,15 @@
 
         `Returns:`
             While CSV is being generated, 'None' is returned. When CSV is ready, the method returns
             the download_url.
         """
         response = self.client.get_request(url=f"csvs/{csv_id}")
 
-        return response['download_url']
+        return response["download_url"]
 
     def poll_for_download_url(self, csv_id):
         """
         Poll the GET request method to check whether CSV generation has finished, signified by the
         presence of a download_url.
 
         `Args:`
@@ -108,22 +119,22 @@
 
         `Returns:`
             Download URL from which you can download the generated CSV, valid for 10 minutes after
             retrieval. Null until CSV has finished generating. Keep this URL secure because until
             it expires, it could be used by anyone to download the CSV.
         """
 
-        logger.info('Request received. Please wait while ActBlue generates this data.')
+        logger.info("Request received. Please wait while ActBlue generates this data.")
         download_url = None
         while download_url is None:
             download_url = self.get_download_url(csv_id)
             time.sleep(POLLING_DELAY)
 
-        logger.info('Completed data generation.')
-        logger.info('Beginning conversion to Parsons Table.')
+        logger.info("Completed data generation.")
+        logger.info("Beginning conversion to Parsons Table.")
         return download_url
 
     def get_contributions(self, csv_type, date_range_start, date_range_end):
         """
         Get specified contribution data from CSV API as Parsons table.
 
         `Args:`
@@ -145,13 +156,15 @@
             date_range_end: str
                 End of date range to withdraw contribution data (exclusive). Ex: '2020-02-01'
 
         `Returns:`
             Contents of the generated contribution CSV as a Parsons table.
         """
 
-        post_request_response = self.post_request(csv_type, date_range_start, date_range_end)
+        post_request_response = self.post_request(
+            csv_type, date_range_start, date_range_end
+        )
         csv_id = post_request_response["id"]
         download_url = self.poll_for_download_url(csv_id)
         table = Table.from_csv(download_url)
-        logger.info('Completed conversion to Parsons Table.')
+        logger.info("Completed conversion to Parsons Table.")
         return table
```

### Comparing `parsons-1.0.0/parsons/action_kit/action_kit.py` & `parsons-1.1.0/parsons/action_kit/action_kit.py`

 * *Files 9% similar despite different names*

```diff
@@ -21,38 +21,38 @@
             The authorized ActionKit username. Not required if ``ACTION_KIT_USERNAME`` env
             variable set.
         password: str
             The authorized ActionKit user password. Not required if ``ACTION_KIT_PASSWORD``
             env variable set.
     """
 
-    _default_headers = {'content-type': 'application/json',
-                        'accepts': 'application/json'}
+    _default_headers = {
+        "content-type": "application/json",
+        "accepts": "application/json",
+    }
 
     def __init__(self, domain=None, username=None, password=None):
-
-        self.domain = check_env.check('ACTION_KIT_DOMAIN', domain)
-        self.username = check_env.check('ACTION_KIT_USERNAME', username)
-        self.password = check_env.check('ACTION_KIT_PASSWORD', password)
+        self.domain = check_env.check("ACTION_KIT_DOMAIN", domain)
+        self.username = check_env.check("ACTION_KIT_USERNAME", username)
+        self.password = check_env.check("ACTION_KIT_PASSWORD", password)
         self.conn = self._conn()
 
     def _conn(self, default_headers=_default_headers):
-
         client = requests.Session()
         client.auth = (self.username, self.password)
         client.headers.update(default_headers)
         return client
 
     def _base_endpoint(self, endpoint, entity_id=None):
         # Create the base endpoint URL
 
-        url = f'https://{self.domain}/rest/v1/{endpoint}/'
+        url = f"https://{self.domain}/rest/v1/{endpoint}/"
 
         if entity_id:
-            return url + f'{entity_id}/'
+            return url + f"{entity_id}/"
         return url
 
     def _base_get(self, endpoint, entity_id=None, exception_message=None, params=None):
         # Make a general get request to ActionKit
 
         resp = self.conn.get(self._base_endpoint(endpoint, entity_id), params=params)
         if exception_message and resp.status_code == 404:
@@ -65,63 +65,64 @@
         resp = self.conn.post(self._base_endpoint(endpoint), data=json.dumps(kwargs))
 
         if resp.status_code != 201:
             raise Exception(self.parse_error(resp, exception_message))
 
         # Some of the methods should just return pointer to location of created
         # object.
-        if 'headers' in resp.__dict__ and not return_full_json:
-            return resp.__dict__['headers']['Location']
+        if "headers" in resp.__dict__ and not return_full_json:
+            return resp.__dict__["headers"]["Location"]
 
         # Not all responses return a json
         try:
             return resp.json()
 
         except ValueError:
             return None
 
     def parse_error(self, resp, exception_message):
         # AK provides some pretty robust/helpful error reporting. We should surface them with
         # our exceptions.
 
-        if 'errors' in resp.json().keys():
-            if isinstance(resp.json()['errors'], list):
-                exception_message += '\n' + ','.join(resp.json()['errors'])
+        if "errors" in resp.json().keys():
+            if isinstance(resp.json()["errors"], list):
+                exception_message += "\n" + ",".join(resp.json()["errors"])
             else:
-                for k, v in resp.json()['errors'].items():
-                    exception_message += str('\n' + k + ': ' + ','.join(v))
+                for k, v in resp.json()["errors"].items():
+                    exception_message += str("\n" + k + ": " + ",".join(v))
 
         return exception_message
 
     def get_user(self, user_id):
         """
         Get a user.
 
         `Args:`
             user_id: int
                 The user id of the record to get.
         `Returns`:
             User json object
         """
 
-        return self._base_get(endpoint='user', entity_id=user_id,
-                              exception_message='User not found')
+        return self._base_get(
+            endpoint="user", entity_id=user_id, exception_message="User not found"
+        )
 
     def get_user_fields(self):
         """
         Get list of valid user fields that can be passed with the
         :meth:`ActionKit.create_user` method.
 
         `Returns`:
             List of user fields
         """
 
-        resp = self._base_get(endpoint='user/schema')
+        resp = self._base_get(endpoint="user/schema")
 
-        return list(resp['fields'].keys())
+        return list(resp["fields"].keys())
 
     def create_user(self, email, **kwargs):
         """
         Create a user.
 
         `Args:`
             email: str
@@ -130,16 +131,20 @@
                 Optional arguments and fields to pass to the client. A full list can be found
                 in the `ActionKit API Documentation <https://roboticdogs.actionkit.com/docs/\
                 manual/api/rest/actionprocessing.html>`_.
         `Returns:`
             User json object
         """
 
-        return self._base_post(endpoint='user', exception_message='Could not create user',
-                               email=email, **kwargs)
+        return self._base_post(
+            endpoint="user",
+            exception_message="Could not create user",
+            email=email,
+            **kwargs,
+        )
 
     def update_user(self, user_id, **kwargs):
         """
         Update a user.
 
         `Args:`
             user_id: int
@@ -148,16 +153,18 @@
                 Optional arguments and fields to pass to the client. A full list can be found
                 in the `ActionKit API Documentation <https://roboticdogs.actionkit.com/docs/\
                 manual/api/rest/actionprocessing.html>`_.
         `Returns:`
             ``None``
         """
 
-        resp = self.conn.patch(self._base_endpoint('user', user_id), data=json.dumps(kwargs))
-        logger.info(f'{resp.status_code}: {user_id}')
+        resp = self.conn.patch(
+            self._base_endpoint("user", user_id), data=json.dumps(kwargs)
+        )
+        logger.info(f"{resp.status_code}: {user_id}")
 
     def get_event(self, event_id):
         """Get an event.
 
         `Args:`
             event_id: int
                 The id for the event.
@@ -186,15 +193,15 @@
                 .. code-block:: python
 
                     ak.get_events(name__contains="FirstName")
         `Returns:`
             Parsons.Table
                 The events data.
         """
-        return self.paginated_get('event', limit=limit, **kwargs)
+        return self.paginated_get("event", limit=limit, **kwargs)
 
     def update_event(self, event_id, **kwargs):
         """
         Update an event.
 
         `Args:`
             event_id: int
@@ -203,56 +210,61 @@
                 Optional arguments and fields to pass to the client. A full list can be found
                 in the `ActionKit API Documentation <https://roboticdogs.actionkit.com/docs/\
                 manual/api/rest/actionprocessing.html>`_.
         `Returns:`
             ``None``
         """
 
-        resp = self.conn.patch(self._base_endpoint('event', event_id), data=json.dumps(kwargs))
-        logger.info(f'{resp.status_code}: {event_id}')
+        resp = self.conn.patch(
+            self._base_endpoint("event", event_id), data=json.dumps(kwargs)
+        )
+        logger.info(f"{resp.status_code}: {event_id}")
 
     def delete_user(self, user_id):
         """
         Delete a user.
 
         `Args:`
             user_id: int
                 The user id of the person to delete
         `Returns:`
             ``None``
         """
 
-        resp = self.conn.delete(self._base_endpoint('user', user_id))
-        logger.info(f'{resp.status_code}: {user_id}')
+        resp = self.conn.delete(self._base_endpoint("user", user_id))
+        logger.info(f"{resp.status_code}: {user_id}")
 
     def get_campaign(self, campaign_id):
         """
         Get a campaign.
 
         `Args:`
             campaign_id: int
                 The campaign id of the record.
         `Returns`:
             Campaign json object
         """
 
-        return self._base_get(endpoint='campaign', entity_id=campaign_id,
-                              exception_message='Campaign not found')
+        return self._base_get(
+            endpoint="campaign",
+            entity_id=campaign_id,
+            exception_message="Campaign not found",
+        )
 
     def get_campaign_fields(self):
         """
         Get list of valid campaign fields that can be passed with the
         :meth:`ActionKit.create_campaign` and :meth:`ActionKit.update_campaign` methods.
 
         `Returns`:
             List of campaign fields
         """
 
-        resp = self._base_get(endpoint='campaign/schema')
-        return list(resp['fields'].keys())
+        resp = self._base_get(endpoint="campaign/schema")
+        return list(resp["fields"].keys())
 
     def create_campaign(self, name, **kwargs):
         """
         Create a campaign.
 
         `Args:`
             name: str
@@ -261,42 +273,49 @@
                 Optional arguments and fields to pass to the client. A full list can be found
                 in the `ActionKit API Documentation <https://roboticdogs.actionkit.com/docs/\
                 manual/api/rest/actionprocessing.html>`_.
         `Returns`:
             API location of new resource
         """
 
-        return self._base_post(endpoint='campaign', exception_message='Could not create campaign',
-                               name=name, **kwargs)
+        return self._base_post(
+            endpoint="campaign",
+            exception_message="Could not create campaign",
+            name=name,
+            **kwargs,
+        )
 
     def get_event_create_page(self, event_create_page_id):
         """
         Get a event create page.
 
         `Args:`
             event_create_page_id: int
                 The event create page id of the record to get.
         `Returns`:
             Event create page json object
         """
 
-        return self._base_get(endpoint='eventcreatepage', entity_id=event_create_page_id,
-                              exception_message='Event create page not found')
+        return self._base_get(
+            endpoint="eventcreatepage",
+            entity_id=event_create_page_id,
+            exception_message="Event create page not found",
+        )
 
     def get_event_create_page_fields(self):
         """
         Get list of event create page fields that can be passed with the
         :meth:`ActionKit.create_event_create_page`.
 
         `Returns`:
             List of event create page fields
         """
 
-        resp = self._base_get(endpoint='eventcreatepage/schema')
-        return list(resp['fields'].keys())
+        resp = self._base_get(endpoint="eventcreatepage/schema")
+        return list(resp["fields"].keys())
 
     def create_event_create_page(self, name, campaign_id, title, **kwargs):
         """
         Add an event page to a campaign.
 
         `Args:`
             campaign_id: int
@@ -309,46 +328,51 @@
                 Optional arguments and fields to pass to the client. A full list can be found
                 in the `ActionKit API Documentation <https://roboticdogs.actionkit.com/docs/\
                 manual/api/rest/actionprocessing.html>`_.
         `Returns`:
             API location of new resource
         """
 
-        return self._base_post(endpoint='eventcreatepage',
-                               exception_message='Could not create event create page',
-                               campaign=f'/rest/v1/campaign/{campaign_id}/',
-                               name=name,
-                               title=title,
-                               **kwargs)
+        return self._base_post(
+            endpoint="eventcreatepage",
+            exception_message="Could not create event create page",
+            campaign=f"/rest/v1/campaign/{campaign_id}/",
+            name=name,
+            title=title,
+            **kwargs,
+        )
 
     def get_event_create_form(self, event_create_form_id):
         """
         Get a event create form.
 
         `Args:`
             event_create_form_id: int
                 The event create form id of the record to get.
         `Returns`:
             Event create form json object
         """
 
-        return self._base_get(endpoint='eventcreateform', entity_id=event_create_form_id,
-                              exception_message='Event create page not found')
+        return self._base_get(
+            endpoint="eventcreateform",
+            entity_id=event_create_form_id,
+            exception_message="Event create page not found",
+        )
 
     def get_event_create_form_fields(self):
         """
         Get list of valid event create form fields that can be passed with the
         :meth:`ActionKit.create_event_create_form` method.
 
         `Returns`:
             List of event create form fields
         """
 
-        resp = self._base_get(endpoint='eventcreateform/schema')
-        return list(resp['fields'].keys())
+        resp = self._base_get(endpoint="eventcreateform/schema")
+        return list(resp["fields"].keys())
 
     def create_event_create_form(self, page_id, thank_you_text, **kwargs):
         """
         Create a event create form.
 
         `Args:`
             page_id: int
@@ -359,45 +383,50 @@
                 Optional arguments and fields to pass to the client. A full list can be found
                 in the `ActionKit API Documentation <https://roboticdogs.actionkit.com/docs/\
                 manual/api/rest/actionprocessing.html>`_.
         `Returns:`
             API location of new resource
         """
 
-        return self._base_post(endpoint='eventcreateform',
-                               exception_message='Could not event create form',
-                               page=f'/rest/v1/eventcreatepage/{page_id}/',
-                               thank_you_text=thank_you_text,
-                               **kwargs)
+        return self._base_post(
+            endpoint="eventcreateform",
+            exception_message="Could not event create form",
+            page=f"/rest/v1/eventcreatepage/{page_id}/",
+            thank_you_text=thank_you_text,
+            **kwargs,
+        )
 
     def get_event_signup_page(self, event_signup_page_id):
         """
         Get event signup page.
 
         `Args:`
             event_signup_page_id: int
                 The event signup page id of the record to get.
         `Returns`:
             Event signup page json object
         """
 
-        return self._base_get(endpoint='eventsignuppage', entity_id=event_signup_page_id,
-                              exception_message='User page signup page not found')
+        return self._base_get(
+            endpoint="eventsignuppage",
+            entity_id=event_signup_page_id,
+            exception_message="User page signup page not found",
+        )
 
     def get_event_signup_page_fields(self):
         """
         Get list of valid event signup page fields that can be passed with the
         :meth:`ActionKit.create_event_signup_page` method.
 
         `Returns`:
             List of event signup page fields
         """
 
-        resp = self._base_get(endpoint='eventsignuppage/schema')
-        return list(resp['fields'].keys())
+        resp = self._base_get(endpoint="eventsignuppage/schema")
+        return list(resp["fields"].keys())
 
     def create_event_signup_page(self, name, campaign_id, title, **kwargs):
         """
         Add an event signup page to a campaign.
 
         `Args:`
             campaign_id: int
@@ -410,46 +439,51 @@
                 Optional arguments and fields to pass to the client. A full list can be found
                 in the `ActionKit API Documentation <https://roboticdogs.actionkit.com/docs/\
                 manual/api/rest/actionprocessing.html>`_.
         `Returns`:
             API location of new resource
         """
 
-        return self._base_post(endpoint='eventsignuppage',
-                               exception_message='Could not create signup page',
-                               campaign=f'/rest/v1/campaign/{campaign_id}/',
-                               name=name,
-                               title=title,
-                               **kwargs)
+        return self._base_post(
+            endpoint="eventsignuppage",
+            exception_message="Could not create signup page",
+            campaign=f"/rest/v1/campaign/{campaign_id}/",
+            name=name,
+            title=title,
+            **kwargs,
+        )
 
     def get_event_signup_form(self, event_signup_form_id):
         """
         Get a user.
 
         `Args:`
             event_signup_form_id: str
                 The event signup form id of the record to get.
         `Returns`:
             Event signup form json object
         """
 
-        return self._base_get(endpoint='eventsignupform', entity_id=event_signup_form_id,
-                              exception_message='User page signup form not found')
+        return self._base_get(
+            endpoint="eventsignupform",
+            entity_id=event_signup_form_id,
+            exception_message="User page signup form not found",
+        )
 
     def get_event_signup_form_fields(self):
         """
         Get list of valid event signup form fields that can be passed with the
         :meth:`ActionKit.create_event_signup_form` method.
 
         `Returns`:
             List of event signup form fields
         """
 
-        resp = self._base_get(endpoint='eventsignupform/schema')
-        return list(resp['fields'].keys())
+        resp = self._base_get(endpoint="eventsignupform/schema")
+        return list(resp["fields"].keys())
 
     def create_event_signup_form(self, page_id, thank_you_text, **kwargs):
         """
         Create a event signup form.
 
         `Args:`
             page_id: int
@@ -460,19 +494,21 @@
                 Optional arguments and fields to pass to the client. A full list can be found
                 in the `ActionKit API Documentation <https://roboticdogs.actionkit.com/docs/\
                 manual/api/rest/actionprocessing.html>`_.
         `Returns:`
             API location of new resource
         """
 
-        return self._base_post(endpoint='eventsignupform',
-                               exception_message='Could not event create signup form',
-                               page=f'/rest/v1/page/{page_id}/',
-                               thank_you_text=thank_you_text,
-                               **kwargs)
+        return self._base_post(
+            endpoint="eventsignupform",
+            exception_message="Could not event create signup form",
+            page=f"/rest/v1/page/{page_id}/",
+            thank_you_text=thank_you_text,
+            **kwargs,
+        )
 
     def update_event_signup(self, event_signup_id, **kwargs):
         """
         Update an event signup.
 
         `Args:`
             event_signup_id: int
@@ -483,54 +519,58 @@
                 Optional arguments and fields to pass to the client. A full list can be found
                 in the `ActionKit API Documentation <https://roboticdogs.actionkit.com/docs/\
                 manual/api/rest/actionprocessing.html>`_.
         `Returns:`
             ``None``
         """
 
-        resp = self.conn.patch(self._base_endpoint('eventsignup', event_signup_id),
-                               data=json.dumps(kwargs))
-        logger.info(f'{resp.status_code}: {event_signup_id}')
+        resp = self.conn.patch(
+            self._base_endpoint("eventsignup", event_signup_id), data=json.dumps(kwargs)
+        )
+        logger.info(f"{resp.status_code}: {event_signup_id}")
 
     def get_mailer(self, entity_id):
         """
         Get a mailer.
 
         `Args:`
             entity_id: int
                 The entity id of the record to get.
         `Returns`:
             Mailer json object
         """
 
-        return self._base_get(endpoint='mailer', entity_id=entity_id)
+        return self._base_get(endpoint="mailer", entity_id=entity_id)
 
     def create_mailer(self, **kwargs):
         """
         Create a mailer.
 
         `Args:`
             **kwargs:
                 Arguments and fields to pass to the client. A full list can be found in the
                 `ActionKit API Documentation <https://roboticdogs.actionkit.com/docs/manual/api/\
                 rest/mailer.html>`_.
         `Returns:`
             URI of new mailer
         """
 
-        return self._base_post(endpoint='mailer', exception_message='Could not create mailer',
-                               **kwargs)
+        return self._base_post(
+            endpoint="mailer", exception_message="Could not create mailer", **kwargs
+        )
 
     def copy_mailer(self, mailer_id):
         """
         copy a mailer
         returns new copy of mailer which should be updatable.
         """
-        resp = self.conn.post(self._base_endpoint('mailer', entity_id=mailer_id) + '/copy')
-        return(resp)
+        resp = self.conn.post(
+            self._base_endpoint("mailer", entity_id=mailer_id) + "/copy"
+        )
+        return resp
 
     def update_mailing(self, mailer_id, **kwargs):
         """
         Update a mailing.
 
         `Args:`
             mailing_id: int
@@ -539,44 +579,50 @@
                 Optional arguments and fields to pass to the client. A full list can be found
                 in the `ActionKit API Documentation <https://roboticdogs.actionkit.com/docs/\
                 manual/api/rest/actionprocessing.html>`_.
         `Returns:`
             ``None``
         """
 
-        resp = self.conn.patch(self._base_endpoint('mailer', mailer_id), data=json.dumps(kwargs))
-        logger.info(f'{resp.status_code}: {mailer_id}')
+        resp = self.conn.patch(
+            self._base_endpoint("mailer", mailer_id), data=json.dumps(kwargs)
+        )
+        logger.info(f"{resp.status_code}: {mailer_id}")
 
     def rebuild_mailer(self, mailing_id):
         """
         Rebuild a mailer.
 
         `Args:`
             mailing_id: int
                 Id of the mailer.
         `Returns:`
             URI to poll for progress
         """
 
-        return self._base_post(endpoint='mailer/' + str(mailing_id) + '/rebuild',
-                               exception_message='Could not rebuild mailer')
+        return self._base_post(
+            endpoint="mailer/" + str(mailing_id) + "/rebuild",
+            exception_message="Could not rebuild mailer",
+        )
 
     def queue_mailer(self, mailing_id):
         """
         Queue a mailer.
 
         `Args:`
             mailing_id: int
                 Id of the mailer.
         `Returns:`
             URI to poll for progress
         """
 
-        return self._base_post(endpoint='mailer/' + str(mailing_id) + '/queue',
-                               exception_message='Could not queue mailer')
+        return self._base_post(
+            endpoint="mailer/" + str(mailing_id) + "/queue",
+            exception_message="Could not queue mailer",
+        )
 
     def paginated_get(self, object_type, limit=None, **kwargs):
         """Get multiple objects of a given type.
 
         `Args:`
             object_type: string
                 The type of object to search for.
@@ -604,25 +650,31 @@
         # get only `limit` objects if it's below 100, otherwise get 100 at a time
         kwargs["_limit"] = min(100, limit or 1_000_000_000)
         json_data = self._base_get(object_type, params=kwargs)
         data = json_data["objects"]
 
         next_url = json_data.get("meta", {}).get("next")
         while next_url:
-            resp = self.conn.get(f'https://{self.domain}{next_url}')
+            resp = self.conn.get(f"https://{self.domain}{next_url}")
             data.extend(resp.json().get("objects", []))
             next_url = resp.json().get("meta", {}).get("next")
             if limit and len(data) >= limit:
                 break
 
         return Table(data[:limit])
 
-    def paginated_get_custom_limit(self, object_type, limit=None,
-                                   threshold_field=None, threshold_value=None,
-                                   ascdesc='asc', **kwargs):
+    def paginated_get_custom_limit(
+        self,
+        object_type,
+        limit=None,
+        threshold_field=None,
+        threshold_value=None,
+        ascdesc="asc",
+        **kwargs,
+    ):
         """Get multiple objects of a given type, stopping based on the value of a field.
 
         `Args:`
             object_type: string
                 The type of object to search for.
             limit: int
                 The maximum number of objects to return. Even if the threshold
@@ -661,15 +713,15 @@
         next_url = json_data.get("meta", {}).get("next")
         while next_url:
             last = data[-1].get(threshold_field)
             if ascdesc == "asc" and last > threshold_value:
                 break
             if ascdesc == "desc" and last < threshold_value:
                 break
-            resp = self.conn.get(f'https://{self.domain}{next_url}')
+            resp = self.conn.get(f"https://{self.domain}{next_url}")
             data += resp.json().get("objects", [])
             next_url = resp.json().get("meta", {}).get("next")
             if limit and len(data) >= limit:
                 break
         # This could be more efficient but it's still O(n) so no big deal
         i = len(data) - 1  # start at the end; 0-indexed means the end is length - 1
         if ascdesc == "asc":
@@ -677,14 +729,29 @@
                 i = i - 1
         else:
             while data[i].get(threshold_field) < threshold_value:
                 i = i - 1
         data = data[:i]
         return Table(data[:limit])
 
+    def get_order(self, order_id):
+        """
+        Get an order.
+
+        `Args:`
+            order_id: int
+                The order id of the record to get.
+        `Returns`:
+            User json object
+        """
+
+        return self._base_get(
+            endpoint="order", entity_id=order_id, exception_message="Order not found"
+        )
+
     def update_order(self, order_id, **kwargs):
         """
         Update an order.
 
         `Args:`
             order_id: int
                 The id of the order to update
@@ -692,33 +759,60 @@
                 Optional arguments and fields to pass to the client. A full list can be found
                 in the `ActionKit API Documentation <https://roboticdogs.actionkit.com/docs/\
                 manual/api/rest/actionprocessing.html>`_.
         `Returns:`
             ``None``
         """
 
-        resp = self.conn.patch(self._base_endpoint('order', order_id),
-                               data=json.dumps(kwargs))
-        logger.info(f'{resp.status_code}: {order_id}')
+        resp = self.conn.patch(
+            self._base_endpoint("order", order_id), data=json.dumps(kwargs)
+        )
+        logger.info(f"{resp.status_code}: {order_id}")
 
     def cancel_orderrecurring(self, recurring_id):
         """
         Cancel a recurring order.
 
         `Args:`
             recurring_id: int
                 The id of the recurring order to update (NOT the order_id)
         `Returns:`
             ``None``
         """
 
-        resp = self.conn.post(self._base_endpoint('orderrecurring', str(recurring_id)+'/cancel'))
-        logger.info(f'{resp.status_code}: {recurring_id}')
+        resp = self.conn.post(
+            self._base_endpoint("orderrecurring", str(recurring_id) + "/cancel")
+        )
+        logger.info(f"{resp.status_code}: {recurring_id}")
         return resp
 
+    def get_orders(self, limit=None, **kwargs):
+        """Get multiple orders.
+
+        `Args:`
+            limit: int
+                The number of orders to return. If omitted, all orders are returned.
+            **kwargs:
+                Optional arguments to pass to the client. A full list can be found
+                in the `ActionKit API Documentation <https://roboticdogs.actionkit.com/docs/\
+                manual/api/rest/actionprocessing.html>`_.
+
+                Additionally, expressions to filter the data can also be provided. For addition
+                info, visit `Django's docs on field lookups <https://docs.djangoproject.com/\
+                en/3.1/topics/db/queries/#field-lookups>`_.
+
+                .. code-block:: python
+
+                    ak.get_orders(import_id="my-import-123")
+        `Returns:`
+            Parsons.Table
+                The events data.
+        """
+        return self.paginated_get("order", limit=limit, **kwargs)
+
     def update_paymenttoken(self, paymenttoken_id, **kwargs):
         """
         Update a saved payment token.
 
         `Args:`
             paymenttoken_id: int
                 The id of the payment token to update
@@ -726,44 +820,49 @@
                 Optional arguments and fields to pass to the client. A full list can be found
                 in the `ActionKit API Documentation <https://roboticdogs.actionkit.com/docs/\
                 manual/api/rest/actionprocessing.html>`_.
         `Returns:`
             ``HTTP response``
         """
 
-        resp = self.conn.patch(self._base_endpoint('paymenttoken', paymenttoken_id),
-                               data=json.dumps(kwargs))
-        logger.info(f'{resp.status_code}: {paymenttoken_id}')
+        resp = self.conn.patch(
+            self._base_endpoint("paymenttoken", paymenttoken_id),
+            data=json.dumps(kwargs),
+        )
+        logger.info(f"{resp.status_code}: {paymenttoken_id}")
         return resp
 
     def get_page_followup(self, page_followup_id):
         """
         Get a page followup.
 
         `Args:`
             page_followup_id: int
                 The user id of the record to get.
         `Returns`:
             Page followup json object
         """
 
-        return self._base_get(endpoint='pagefollowup', entity_id=page_followup_id,
-                              exception_message='Page followup not found')
+        return self._base_get(
+            endpoint="pagefollowup",
+            entity_id=page_followup_id,
+            exception_message="Page followup not found",
+        )
 
     def get_page_followup_fields(self):
         """
         Get list of valid page followup fields that can be passed with the
         :meth:`ActionKit.create_page_followup` method.
 
         `Returns`:
             List of page followup fields
         """
 
-        resp = self._base_get(endpoint='pagefollowup/schema')
-        return list(resp['fields'].keys())
+        resp = self._base_get(endpoint="pagefollowup/schema")
+        return list(resp["fields"].keys())
 
     def create_page_followup(self, signup_page_id, url, **kwargs):
         """
         Add a page followup.
 
         `Args:`
             signup_page_id: int
@@ -774,33 +873,38 @@
                 Optional arguments and fields to pass to the client. A full list can be found
                 in the `ActionKit API Documentation <https://roboticdogs.actionkit.com/docs/\
                 manual/api/rest/actionprocessing.html>`_.
         `Returns`:
             API location of new resource
         """
 
-        return self._base_post(endpoint='pagefollowup',
-                               exception_message='Could not create page followup',
-                               page=f'/rest/v1/eventsignuppage/{signup_page_id}/',
-                               url=url,
-                               **kwargs)
+        return self._base_post(
+            endpoint="pagefollowup",
+            exception_message="Could not create page followup",
+            page=f"/rest/v1/eventsignuppage/{signup_page_id}/",
+            url=url,
+            **kwargs,
+        )
 
     def get_survey_question(self, survey_question_id):
         """
         Get a survey question.
 
         `Args:`
             survey_question_id: int
                 The survey question id of the record to get.
         `Returns`:
             Survey question json object
         """
 
-        return self._base_get(endpoint='surveyquestion', entity_id=survey_question_id,
-                              exception_message='Survey question not found')
+        return self._base_get(
+            endpoint="surveyquestion",
+            entity_id=survey_question_id,
+            exception_message="Survey question not found",
+        )
 
     def update_survey_question(self, survey_question_id, **kwargs):
         """
         Update a survey question.
 
         `Args:`
             survey_question_id: int
@@ -811,31 +915,35 @@
                 Optional arguments and fields to pass to the client. A full list can be found
                 in the `ActionKit API Documentation <https://roboticdogs.actionkit.com/docs/\
                 manual/api/rest/actionprocessing.html>`_.
         `Returns:`
             ``None``
         """
 
-        resp = self.conn.patch(self._base_endpoint('surveyquestion', survey_question_id),
-                               data=json.dumps(kwargs))
-        logger.info(f'{resp.status_code}: {survey_question_id}')
+        resp = self.conn.patch(
+            self._base_endpoint("surveyquestion", survey_question_id),
+            data=json.dumps(kwargs),
+        )
+        logger.info(f"{resp.status_code}: {survey_question_id}")
 
     def create_transaction(self, **kwargs):
         """
         Create a transaction.
 
         `Args:`
             **kwargs:
                 Optional arguments and fields to pass to the client.
         `Returns:`
             Transaction json object
         """
 
         return self._base_post(
-            endpoint='transaction', exception_message='Could not create transaction', **kwargs
+            endpoint="transaction",
+            exception_message="Could not create transaction",
+            **kwargs,
         )
 
     def update_transaction(self, transaction_id, **kwargs):
         """
         Update a transaction.
 
         `Args:`
@@ -845,17 +953,42 @@
                 Optional arguments and fields to pass to the client. A full list can be found
                 in the `ActionKit API Documentation <https://roboticdogs.actionkit.com/docs/\
                 manual/api/rest/actionprocessing.html>`_.
         `Returns:`
             ``None``
         """
 
-        resp = self.conn.patch(self._base_endpoint('transaction', transaction_id),
-                               data=json.dumps(kwargs))
-        logger.info(f'{resp.status_code}: {transaction_id}')
+        resp = self.conn.patch(
+            self._base_endpoint("transaction", transaction_id), data=json.dumps(kwargs)
+        )
+        logger.info(f"{resp.status_code}: {transaction_id}")
+
+    def get_transactions(self, limit=None, **kwargs):
+        """Get multiple transactions.
+
+        `Args:`
+            limit: int
+                The number of transactions to return. If omitted, all transactions are returned.
+            **kwargs:
+                Optional arguments to pass to the client. A full list can be found
+                in the `ActionKit API Documentation <https://roboticdogs.actionkit.com/docs/\
+                manual/api/rest/actionprocessing.html>`_.
+
+                Additionally, expressions to filter the data can also be provided. For addition
+                info, visit `Django's docs on field lookups <https://docs.djangoproject.com/\
+                en/3.1/topics/db/queries/#field-lookups>`_.
+
+                .. code-block:: python
+
+                    ak.get_transactions(order="order-1")
+        `Returns:`
+            Parsons.Table
+                The events data.
+        """
+        return self.paginated_get("transaction", limit=limit, **kwargs)
 
     def create_generic_action(self, page, email=None, ak_id=None, **kwargs):
         """
         Post a generic action. One of ``ak_id`` or ``email`` is a required argument.
 
         `Args:`
             page:
@@ -870,25 +1003,32 @@
                 manual/api/rest/actionprocessing.html>`_.
         `Returns`:
             dict
                 The response json
         """  # noqa: E501,E261
 
         if not email or ak_id:
-            raise ValueError('One of email or ak_id is required.')
+            raise ValueError("One of email or ak_id is required.")
 
-        return self._base_post(endpoint='action',
-                               exception_message='Could not create action.',
-                               email=email,
-                               page=page,
-                               return_full_json=True,
-                               **kwargs)
+        return self._base_post(
+            endpoint="action",
+            exception_message="Could not create action.",
+            email=email,
+            page=page,
+            return_full_json=True,
+            **kwargs,
+        )
 
-    def bulk_upload_csv(self, csv_file, import_page,
-                        autocreate_user_fields=False, user_fields_only=False):
+    def bulk_upload_csv(
+        self,
+        csv_file,
+        import_page,
+        autocreate_user_fields=False,
+        user_fields_only=False,
+    ):
         """
         Bulk upload a csv file of new users or user updates.
         If you are uploading a table object, use bulk_upload_table instead.
         See `ActionKit User Upload Documentation <https://roboticdogs.actionkit.com/docs/manual/api/rest/uploads.html>`_
         Be careful that blank values in columns will overwrite existing data.
 
         If you get a 500 error, try sending a much smaller file (say, one row),
@@ -913,37 +1053,43 @@
             dict
                 success: whether upload was successful
                 progress_url: an API URL to get progress on upload processing
                 res: requests http response object
         """  # noqa: E501,E261
 
         # self.conn defaults to JSON, but this has to be form/multi-part....
-        upload_client = self._conn({'accepts': 'application/json'})
+        upload_client = self._conn({"accepts": "application/json"})
         if isinstance(csv_file, str):
-            csv_file = open(csv_file, 'rb')
+            csv_file = open(csv_file, "rb")
 
-        url = self._base_endpoint('upload')
-        files = {'upload': csv_file}
+        url = self._base_endpoint("upload")
+        files = {"upload": csv_file}
         data = {
-            'page': import_page,
-            'autocreate_user_fields': int(autocreate_user_fields),
-            'user_fields_only': int(user_fields_only),
+            "page": import_page,
+            "autocreate_user_fields": int(autocreate_user_fields),
+            "user_fields_only": int(user_fields_only),
         }
         with upload_client.post(url, files=files, data=data) as res:
-            progress_url = res.headers.get('Location')
+            progress_url = res.headers.get("Location")
             rv = {
-                'res': res,
-                'success': res.status_code == 201,
-                'id': progress_url.split('/')[-2] if progress_url else None,
-                'progress_url': progress_url
+                "res": res,
+                "success": res.status_code == 201,
+                "id": progress_url.split("/")[-2] if progress_url else None,
+                "progress_url": progress_url,
             }
             return rv
 
-    def bulk_upload_table(self, table, import_page, autocreate_user_fields=0,
-                          no_overwrite_on_empty=False, set_only_columns=None):
+    def bulk_upload_table(
+        self,
+        table,
+        import_page,
+        autocreate_user_fields=0,
+        no_overwrite_on_empty=False,
+        set_only_columns=None,
+    ):
         """
         Bulk upload a table of new users or user updates.
         See `ActionKit User Upload Documentation <https://roboticdogs.actionkit.com/docs/manual/api/rest/uploads.html>`_
         Be careful that blank values in columns will overwrite existing data.
 
         Tables with only an identifying column (user_id/email) and user_ user fields
         will be fast-processed -- this is useful for setting/updating user fields.
@@ -975,54 +1121,62 @@
         `Returns`:
             dict
                 success: bool -- whether upload was successful (individual rows may not have been)
                 results: [dict] -- This is a list of the full results.
                          progress_url and res for any results
         """  # noqa: E501,E261
 
-        import_page = check_env.check('ACTION_KIT_IMPORTPAGE', import_page)
+        import_page = check_env.check("ACTION_KIT_IMPORTPAGE", import_page)
         upload_tables = self._split_tables_no_empties(
-            table, no_overwrite_on_empty, set_only_columns)
+            table, no_overwrite_on_empty, set_only_columns
+        )
         results = []
         for tbl in upload_tables:
-            user_fields_only = int(not any([
-                h for h in tbl.columns
-                if h != 'email' and not h.startswith('user_')]))
-            results.append(self.bulk_upload_csv(tbl.to_csv(),
-                                                import_page,
-                                                autocreate_user_fields=autocreate_user_fields,
-                                                user_fields_only=user_fields_only))
-        return {
-            'success': all([r['success'] for r in results]),
-            'results': results
-        }
+            user_fields_only = int(
+                not any(
+                    [
+                        h
+                        for h in tbl.columns
+                        if h != "email" and not h.startswith("user_")
+                    ]
+                )
+            )
+            results.append(
+                self.bulk_upload_csv(
+                    tbl.to_csv(),
+                    import_page,
+                    autocreate_user_fields=autocreate_user_fields,
+                    user_fields_only=user_fields_only,
+                )
+            )
+        return {"success": all([r["success"] for r in results]), "results": results}
 
     def _split_tables_no_empties(self, table, no_overwrite_on_empty, set_only_columns):
         table_groups = {}
         # uploading combo of user_id and email column should be mutually exclusive
         blank_columns_test = table.columns
         if not no_overwrite_on_empty:
-            blank_columns_test = (set(['user_id', 'email'] + (set_only_columns or []))
-                                  .intersection(table.columns))
+            blank_columns_test = set(
+                ["user_id", "email"] + (set_only_columns or [])
+            ).intersection(table.columns)
         for row in table:
-            blanks = tuple(k for k in blank_columns_test
-                           if row.get(k) in (None, ''))
+            blanks = tuple(k for k in blank_columns_test if row.get(k) in (None, ""))
             grp = table_groups.setdefault(blanks, [])
             grp.append(row)
         results = []
         for blanks, subset in table_groups.items():
             subset_table = Table(subset)
             if blanks:
                 subset_table.table = subset_table.table.cutout(*blanks)
-            logger.debug(f'Column Upload Blanks: {blanks}')
-            logger.debug(f'Column Upload Columns: {subset_table.columns}')
-            if not set(['user_id', 'email']).intersection(subset_table.columns):
+            logger.debug(f"Column Upload Blanks: {blanks}")
+            logger.debug(f"Column Upload Columns: {subset_table.columns}")
+            if not set(["user_id", "email"]).intersection(subset_table.columns):
                 logger.warning(
-                    f'Upload will fail without user_id or email. '
-                    f'Rows: {subset_table.num_rows}, Columns: {subset_table.columns}'
+                    f"Upload will fail without user_id or email. "
+                    f"Rows: {subset_table.num_rows}, Columns: {subset_table.columns}"
                 )
             results.append(subset_table)
         return results
 
     def collect_upload_errors(self, result_array):
         """
         Collect any upload errors as a list of objects from bulk_upload_table 'results' key value.
@@ -1037,19 +1191,21 @@
             [dict]
                 message: str -- error message
                 upload: str -- upload progress API path e.g. "/rest/v1/upload/123456/"
                 id: int -- upload error record id (different than upload id)
         """
         errors = []
         for res in result_array:
-            upload_id = res.get('id')
+            upload_id = res.get("id")
             if upload_id:
                 while True:
-                    upload = self._base_get(endpoint='upload', entity_id=upload_id)
-                    if not upload or upload.get('status') != 'new':
+                    upload = self._base_get(endpoint="upload", entity_id=upload_id)
+                    if not upload or upload.get("status") != "new":
                         break
                     else:
                         time.sleep(1)
-                error_data = self._base_get(endpoint='uploaderror', params={'upload': upload_id})
-                logger.debug(f'error collect result: {error_data}')
-                errors.extend(error_data.get('objects') or [])
+                error_data = self._base_get(
+                    endpoint="uploaderror", params={"upload": upload_id}
+                )
+                logger.debug(f"error collect result: {error_data}")
+                errors.extend(error_data.get("objects") or [])
         return errors
```

### Comparing `parsons-1.0.0/parsons/action_network/action_network.py` & `parsons-1.1.0/parsons/action_network/action_network.py`

 * *Files 5% similar despite different names*

```diff
@@ -3,59 +3,57 @@
 import re
 from parsons.utilities import check_env
 from parsons.utilities.api_connector import APIConnector
 import logging
 
 logger = logging.getLogger(__name__)
 
-API_URL = 'https://actionnetwork.org/api/v2'
+API_URL = "https://actionnetwork.org/api/v2"
 
 
 class ActionNetwork(object):
     """
     `Args:`
         api_token: str
             The OSDI API token
     """
 
     def __init__(self, api_token=None):
-        self.api_token = check_env.check('AN_API_TOKEN', api_token)
+        self.api_token = check_env.check("AN_API_TOKEN", api_token)
         self.headers = {
             "Content-Type": "application/json",
-            "OSDI-API-Token": self.api_token
+            "OSDI-API-Token": self.api_token,
         }
         self.api_url = API_URL
         self.api = APIConnector(self.api_url, headers=self.headers)
 
     def _get_page(self, object_name, page, per_page=25, filter=None):
         # returns data from one page of results
         if per_page > 25:
             per_page = 25
-            logger.info("Action Network's API will not return more than 25 entries per page. \
-            Changing per_page parameter to 25.")
-        params = {
-            "page": page,
-            "per_page": per_page,
-            "filter": filter
-        }
+            logger.info(
+                "Action Network's API will not return more than 25 entries per page. \
+            Changing per_page parameter to 25."
+            )
+        params = {"page": page, "per_page": per_page, "filter": filter}
         return self.api.get_request(url=object_name, params=params)
 
     def _get_entry_list(self, object_name, limit=None, per_page=25, filter=None):
         # returns a list of entries for a given object, such as people, tags, or actions
         # Filter can only be applied to people, petitions, events, forms, fundraising_pages,
         # event_campaigns, campaigns, advocacy_campaigns, signatures, attendances, submissions,
         # donations and outreaches.
         # See Action Network API docs for more info: https://actionnetwork.org/docs/v2/
         count = 0
         page = 1
         return_list = []
         while True:
             response = self._get_page(object_name, page, per_page, filter=filter)
             page = page + 1
-            response_list = response['_embedded'][f"osdi:{object_name}"]
+            response_list = response["_embedded"][f"osdi:{object_name}"]
             if not response_list:
                 return Table(return_list)
             return_list.extend(response_list)
             count = count + len(response_list)
             if limit:
                 if count >= limit:
                     return Table(return_list[0:limit])
@@ -86,17 +84,26 @@
                 Id of the person.
         `Returns:`
             A  JSON of the entry. If the entry doesn't exist, Action Network returns
             ``{'error': 'Couldn't find person with id = <id>'}``.
         """
         return self.api.get_request(url=f"people/{person_id}")
 
-    def upsert_person(self, email_address=None, given_name=None, family_name=None, tags=None,
-                      languages_spoken=None, postal_addresses=None, mobile_number=None,
-                      mobile_status='subscribed', **kwargs):
+    def upsert_person(
+        self,
+        email_address=None,
+        given_name=None,
+        family_name=None,
+        tags=None,
+        languages_spoken=None,
+        postal_addresses=None,
+        mobile_number=None,
+        mobile_status="subscribed",
+        **kwargs,
+    ):
         """
         Creates or updates a person record. In order to update an existing record instead of
         creating a new one, you must supply an email or mobile number which matches a record
         in the database.
 
         `Args:`
             email_address:
@@ -146,43 +153,50 @@
         """
         email_addresses_field = None
         if type(email_address) == str:
             email_addresses_field = [{"address": email_address}]
         elif type(email_address) == list:
             if type(email_address[0]) == str:
                 email_addresses_field = [{"address": email} for email in email_address]
-                email_addresses_field[0]['primary'] = True
+                email_addresses_field[0]["primary"] = True
             if type(email_address[0]) == dict:
                 email_addresses_field = email_address
 
         mobile_numbers_field = None
         if type(mobile_number) == str:
-            mobile_numbers_field = [{"number": re.sub('[^0-9]', "", mobile_number),
-                                     "status": mobile_status}]
+            mobile_numbers_field = [
+                {"number": re.sub("[^0-9]", "", mobile_number), "status": mobile_status}
+            ]
         elif type(mobile_number) == int:
-            mobile_numbers_field = [{"number": str(mobile_number), "status": mobile_status}]
+            mobile_numbers_field = [
+                {"number": str(mobile_number), "status": mobile_status}
+            ]
         elif type(mobile_number) == list:
             if len(mobile_number) > 1:
-                raise('Action Network allows only 1 phone number per activist')
+                raise ("Action Network allows only 1 phone number per activist")
             if type(mobile_number[0]) == str:
-                mobile_numbers_field = [{"number": re.sub('[^0-9]', "", cell),
-                                        "status": mobile_status}
-                                        for cell in mobile_number]
-                mobile_numbers_field[0]['primary'] = True
+                mobile_numbers_field = [
+                    {"number": re.sub("[^0-9]", "", cell), "status": mobile_status}
+                    for cell in mobile_number
+                ]
+                mobile_numbers_field[0]["primary"] = True
             if type(mobile_number[0]) == int:
-                mobile_numbers_field = [{"number": cell, "status": mobile_status}
-                                        for cell in mobile_number]
-                mobile_numbers_field[0]['primary'] = True
+                mobile_numbers_field = [
+                    {"number": cell, "status": mobile_status} for cell in mobile_number
+                ]
+                mobile_numbers_field[0]["primary"] = True
             if type(mobile_number[0]) == dict:
                 mobile_numbers_field = mobile_number
 
         if not email_addresses_field and not mobile_numbers_field:
-            raise("Either email_address or mobile_number is required and can be formatted "
-                  "as a string, list of strings, a dictionary, a list of dictionaries, or "
-                  "(for mobile_number only) an integer or list of integers")
+            raise (
+                "Either email_address or mobile_number is required and can be formatted "
+                "as a string, list of strings, a dictionary, a list of dictionaries, or "
+                "(for mobile_number only) an integer or list of integers"
+            )
 
         data = {"person": {}}
 
         if email_addresses_field is not None:
             data["person"]["email_addresses"] = email_addresses_field
         if mobile_numbers_field is not None:
             data["person"]["phone_numbers"] = mobile_numbers_field
@@ -193,37 +207,59 @@
         if languages_spoken is not None:
             data["person"]["languages_spoken"] = languages_spoken
         if postal_addresses is not None:
             data["person"]["postal_addresses"] = postal_addresses
         if tags is not None:
             data["add_tags"] = tags
         data["person"]["custom_fields"] = {**kwargs}
-        response = self.api.post_request(url=f"{self.api_url}/people", data=json.dumps(data))
-        identifiers = response['identifiers']
-        person_id = [entry_id.split(':')[1]
-                     for entry_id in identifiers if 'action_network:' in entry_id][0]
-        if response['created_date'] == response['modified_date']:
+        response = self.api.post_request(
+            url=f"{self.api_url}/people", data=json.dumps(data)
+        )
+        identifiers = response["identifiers"]
+        person_id = [
+            entry_id.split(":")[1]
+            for entry_id in identifiers
+            if "action_network:" in entry_id
+        ][0]
+        if response["created_date"] == response["modified_date"]:
             logger.info(f"Entry {person_id} successfully added.")
         else:
             logger.info(f"Entry {person_id} successfully updated.")
         return response
 
-    def add_person(self, email_address=None, given_name=None, family_name=None, tags=None,
-                   languages_spoken=None, postal_addresses=None, mobile_number=None,
-                   mobile_status='subscribed', **kwargs):
+    def add_person(
+        self,
+        email_address=None,
+        given_name=None,
+        family_name=None,
+        tags=None,
+        languages_spoken=None,
+        postal_addresses=None,
+        mobile_number=None,
+        mobile_status="subscribed",
+        **kwargs,
+    ):
         """
         Creates a person in the database. WARNING: this endpoint has been deprecated in favor of
         upsert_person.
         """
-        logger.warning("Method 'add_person' has been deprecated. Please use 'upsert_person'.")
+        logger.warning(
+            "Method 'add_person' has been deprecated. Please use 'upsert_person'."
+        )
         # Pass inputs to preferred method:
-        self.upsert_person(email_address=email_address, given_name=given_name,
-                           family_name=family_name, languages_spoken=languages_spoken,
-                           postal_addresses=postal_addresses, mobile_number=mobile_number,
-                           mobile_status=mobile_status, **kwargs)
+        self.upsert_person(
+            email_address=email_address,
+            given_name=given_name,
+            family_name=family_name,
+            languages_spoken=languages_spoken,
+            postal_addresses=postal_addresses,
+            mobile_number=mobile_number,
+            mobile_status=mobile_status,
+            **kwargs,
+        )
 
     def update_person(self, entry_id, **kwargs):
         """
         Updates a person's data in Action Network, given their Action Network ID. Note that you
         can't alter a person's tags with this method. Instead, use upsert_person.
 
         `Args:`
@@ -255,16 +291,19 @@
                         Optional field. A list of dictionaries.
                         For details, see Action Network's documentation:
                         https://actionnetwork.org/docs/v2/people#put
                     custom_fields:
                         A dictionary of any other fields to store about the person.
         """
         data = {**kwargs}
-        response = self.api.put_request(url=f"{self.api_url}/people/{entry_id}",
-                                        data=json.dumps(data), success_codes=[204, 201, 200])
+        response = self.api.put_request(
+            url=f"{self.api_url}/people/{entry_id}",
+            data=json.dumps(data),
+            success_codes=[204, 201, 200],
+        )
         logger.info(f"Person {entry_id} successfully updated")
         return response
 
     def get_tags(self, limit=None, per_page=25, page=None):
         """
         `Args:`
             limit:
@@ -294,21 +333,24 @@
     def add_tag(self, name):
         """
         `Args:`
             name:
                 The tag's name. This is the ONLY editable field
         Adds a tag to Action Network. Once created, tags CANNOT be edited or deleted.
         """
-        data = {
-            "name": name
-        }
-        response = self.api.post_request(url=f"{self.api_url}/tags", data=json.dumps(data))
-        identifiers = response['identifiers']
-        person_id = [entry_id.split(':')[1]
-                     for entry_id in identifiers if 'action_network:' in entry_id][0]
+        data = {"name": name}
+        response = self.api.post_request(
+            url=f"{self.api_url}/tags", data=json.dumps(data)
+        )
+        identifiers = response["identifiers"]
+        person_id = [
+            entry_id.split(":")[1]
+            for entry_id in identifiers
+            if "action_network:" in entry_id
+        ][0]
         logger.info(f"Tag {person_id} successfully added to tags.")
         return response
 
     def create_event(self, title, start_date=None, location=None):
         """
         Create an event in Action Network
 
@@ -334,24 +376,24 @@
                         "country": "US"
                     }
 
         `Returns:`
             Dict of Action Network Event data.
         """
 
-        data = {
-            "title": title
-        }
+        data = {"title": title}
 
         if start_date:
             start_date = str(start_date)
             data["start_date"] = start_date
 
         if isinstance(location, dict):
             data["location"] = location
 
-        event_dict = self.api.post_request(url=f"{self.api_url}/events", data=json.dumps(data))
+        event_dict = self.api.post_request(
+            url=f"{self.api_url}/events", data=json.dumps(data)
+        )
 
-        an_event_id = event_dict["_links"]["self"]["href"].split('/')[-1]
+        an_event_id = event_dict["_links"]["self"]["href"].split("/")[-1]
         event_dict["event_id"] = an_event_id
 
         return event_dict
```

### Comparing `parsons-1.0.0/parsons/airtable/airtable.py` & `parsons-1.1.0/parsons/airtable/airtable.py`

 * *Files 4% similar despite different names*

```diff
@@ -17,15 +17,15 @@
             in Excel or GoogleDocs.
         api_key: str
             The Airtable provided api key. Not required if ``AIRTABLE_API_KEY`` env variable set.
     """
 
     def __init__(self, base_key, table_name, api_key=None):
 
-        self.api_key = check_env.check('AIRTABLE_API_KEY', api_key)
+        self.api_key = check_env.check("AIRTABLE_API_KEY", api_key)
         self.client = client(base_key, table_name, self.api_key)
 
     def get_record(self, record_id):
         """
         Returns a single record.
 
         `Args:`
@@ -33,16 +33,23 @@
                 The Airtable record id
         `Returns:`
             A dictionary of the record
         """
 
         return self.client.get(record_id)
 
-    def get_records(self, fields=None, max_records=None, view=None,
-                    formula=None, sort=None, sample_size=None):
+    def get_records(
+        self,
+        fields=None,
+        max_records=None,
+        view=None,
+        formula=None,
+        sort=None,
+        sample_size=None,
+    ):
         """
         `Args:`
             fields: str or lst
                 Only return specified column or list of columns. The column name is
                 case sensitive
             max_records: int
                 The maximum total number of records that will be returned.
@@ -84,33 +91,38 @@
 
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
         # Raises an error if sort is None type. Thus, only adding if populated.
-        kwargs = {'fields': fields, 'max_records': max_records, 'view': view, 'formula': formula}
+        kwargs = {
+            "fields": fields,
+            "max_records": max_records,
+            "view": view,
+            "formula": formula,
+        }
         if sort:
-            kwargs['sort'] = sort
+            kwargs["sort"] = sort
 
         tbl = Table(self.client.get_all(**kwargs))
 
         # If the results are empty, then return an empty table.
-        if 'fields' not in tbl.columns:
+        if "fields" not in tbl.columns:
             return Table([[]])
 
         unpack_dicts_kwargs = {
-            'column': 'fields',
-            'prepend': False,
+            "column": "fields",
+            "prepend": False,
         }
         if fields:
-            unpack_dicts_kwargs['keys'] = fields
+            unpack_dicts_kwargs["keys"] = fields
 
         if sample_size:
-            unpack_dicts_kwargs['sample_size'] = sample_size
+            unpack_dicts_kwargs["sample_size"] = sample_size
 
         return tbl.unpack_dict(**unpack_dicts_kwargs)
 
     def insert_record(self, row):
         """
         Insert a single record into an Airtable.
 
@@ -121,15 +133,15 @@
                 Automatic data conversion from string values.
         `Returns:`
             Dictionary of inserted row
 
         """
 
         resp = self.client.insert(row)
-        logger.info('Record inserted')
+        logger.info("Record inserted")
         return resp
 
     def insert_records(self, table, typecast=False):
         """
         Insert multiple records into an Airtable. The columns in your Parsons table must
         exist in the Airtable. The method will attempt to map based on column name, so the
         order of the columns is irrelevant.
@@ -140,15 +152,15 @@
             typecast: boolean
                 Automatic data conversion from string values.
         `Returns:`
             List of dictionaries of inserted rows
         """
 
         resp = self.client.batch_insert(table, typecast=typecast)
-        logger.info(f'{table.num_rows} records inserted.')
+        logger.info(f"{table.num_rows} records inserted.")
         return resp
 
     def update_record(self, record_id, fields, typecast=False):
         """
         Updates a record by its record id. Only Fields passed are updated, the rest are left as
         is.
 
@@ -160,9 +172,9 @@
             typecast: boolean
                 Automatic data conversion from string values.
         `Returns:`
             ``None``
         """
 
         resp = self.client.update(record_id, fields, typecast=typecast)
-        logger.info(f'{record_id} updated')
+        logger.info(f"{record_id} updated")
         return resp
```

### Comparing `parsons-1.0.0/parsons/alchemer/alchemer.py` & `parsons-1.1.0/parsons/alchemer/alchemer.py`

 * *Files 14% similar despite different names*

```diff
@@ -6,22 +6,25 @@
 logger = logging.getLogger(__name__)
 
 
 def sg_compatibility():
     # Create backwards compatibility with SurveyGizmo class
 
     import os
-    if os.getenv('SURVEYGIZMO_API_TOKEN'):
-        os.environ['ALCHEMER_API_TOKEN'] = os.getenv('SURVEYGIZMO_API_TOKEN')
 
-    if os.getenv('SURVEYGIZMO_API_TOKEN_SECRET'):
-        os.environ['ALCHEMER_API_TOKEN_SECRET'] = os.getenv('SURVEYGIZMO_API_TOKEN_SECRET')
+    if os.getenv("SURVEYGIZMO_API_TOKEN"):
+        os.environ["ALCHEMER_API_TOKEN"] = os.getenv("SURVEYGIZMO_API_TOKEN")
 
-    if os.getenv('SURVEYGIZMO_API_VERSION'):
-        os.environ['ALCHEMER_API_VERSION'] = os.getenv('SURVEYGIZMO_API_VERSION')
+    if os.getenv("SURVEYGIZMO_API_TOKEN_SECRET"):
+        os.environ["ALCHEMER_API_TOKEN_SECRET"] = os.getenv(
+            "SURVEYGIZMO_API_TOKEN_SECRET"
+        )
+
+    if os.getenv("SURVEYGIZMO_API_VERSION"):
+        os.environ["ALCHEMER_API_VERSION"] = os.getenv("SURVEYGIZMO_API_VERSION")
 
 
 class Alchemer(object):
     """
     Instantiate Alchemer Class
 
     `Args:`
@@ -38,27 +41,29 @@
             ``ALCHEMER_API_VERSION`` env variable set.
             Default v5
 
     `Returns:`
         Alchemer Class
     """
 
-    def __init__(self, api_token=None, api_token_secret=None, api_version='v5'):
+    def __init__(self, api_token=None, api_token_secret=None, api_version="v5"):
 
         sg_compatibility()
 
-        self.api_token = check_env.check('ALCHEMER_API_TOKEN', api_token)
-        self.api_token_secret = check_env.check('ALCHEMER_API_TOKEN_SECRET', api_token_secret)
-        self.api_version = check_env.check('ALCHEMER_API_VERSION', api_version)
+        self.api_token = check_env.check("ALCHEMER_API_TOKEN", api_token)
+        self.api_token_secret = check_env.check(
+            "ALCHEMER_API_TOKEN_SECRET", api_token_secret
+        )
+        self.api_version = check_env.check("ALCHEMER_API_VERSION", api_version)
 
         self._client = surveygizmo.SurveyGizmo(
-                api_version=self.api_version,
-                api_token=self.api_token,
-                api_token_secret=self.api_token_secret
-                )
+            api_version=self.api_version,
+            api_token=self.api_token,
+            api_token_secret=self.api_token_secret,
+        )
 
     def get_surveys(self, page=None):
         """
         Get a table of lists under the account.
 
         `Args:`
             page : int
@@ -66,23 +71,23 @@
                 then all pages are retrieved.
 
         `Returns:`
             Table Class
         """
 
         r = self._client.api.survey.list(page)
-        data = r['data']
+        data = r["data"]
 
         if not page:
-            while r['page'] < r['total_pages']:
-                r = self._client.api.survey.list(page=(r['page']+1))
-                data.extend(r['data'])
+            while r["page"] < r["total_pages"]:
+                r = self._client.api.survey.list(page=(r["page"] + 1))
+                data.extend(r["data"])
 
-        tbl = Table(data).remove_column('links')
-        tbl.unpack_dict('statistics', prepend=False)
+        tbl = Table(data).remove_column("links")
+        tbl.unpack_dict("statistics", prepend=False)
 
         logger.info(f"Found {tbl.num_rows} surveys.")
 
         return tbl
 
     def get_survey_responses(self, survey_id, page=None):
         """
@@ -98,22 +103,24 @@
 
         `Returns:`
             Table Class
         """
 
         r = self._client.api.surveyresponse.list(survey_id, page)
         logger.info(f"{survey_id}: {r['total_count']} responses.")
-        data = r['data']
+        data = r["data"]
 
         if not page:
-            while r['page'] < r['total_pages']:
-                r = self._client.api.surveyresponse.list(survey_id, page=(r['page']+1))
-                data.extend(r['data'])
+            while r["page"] < r["total_pages"]:
+                r = self._client.api.surveyresponse.list(
+                    survey_id, page=(r["page"] + 1)
+                )
+                data.extend(r["data"])
 
-        tbl = Table(data).add_column('survey_id', survey_id, index=1)
+        tbl = Table(data).add_column("survey_id", survey_id, index=1)
 
         logger.info(f"Found #{tbl.num_rows} responses.")
 
         return tbl
 
 
 # Backwards compatibility for old SurveyGizmo class.
```

### Comparing `parsons-1.0.0/parsons/auth0/auth0.py` & `parsons-1.1.0/parsons/auth0/auth0.py`

 * *Files 12% similar despite different names*

```diff
@@ -13,47 +13,61 @@
         client_secret: str
             The Auth0 client secret. Not required if ``AUTH0_CLIENT_SECRET`` env variable set.
         domain: str
             The Auth0 domain. Not required if ``AUTH0_DOMAIN`` env variable set.
     `Returns:`
         Auth0 Class
     """
+
     def __init__(self, client_id=None, client_secret=None, domain=None):
         self.base_url = f"https://{check_env.check('AUTH0_DOMAIN', domain)}"
-        access_token = requests.post(f'{self.base_url}/oauth/token', data={
-            'grant_type': 'client_credentials',  # OAuth 2.0 flow to use
-            'client_id': check_env.check('AUTH0_CLIENT_ID', client_id),
-            'client_secret': check_env.check('AUTH0_CLIENT_SECRET', client_secret),
-            'audience': f'{self.base_url}/api/v2/'
-        }).json().get('access_token')
+        access_token = (
+            requests.post(
+                f"{self.base_url}/oauth/token",
+                data={
+                    "grant_type": "client_credentials",  # OAuth 2.0 flow to use
+                    "client_id": check_env.check("AUTH0_CLIENT_ID", client_id),
+                    "client_secret": check_env.check(
+                        "AUTH0_CLIENT_SECRET", client_secret
+                    ),
+                    "audience": f"{self.base_url}/api/v2/",
+                },
+            )
+            .json()
+            .get("access_token")
+        )
         self.headers = {
-            'Authorization': f'Bearer {access_token}',
-            'Content-Type': 'application/json'
+            "Authorization": f"Bearer {access_token}",
+            "Content-Type": "application/json",
         }
 
     def delete_user(self, id):
         """
         Delete Auth0 user.
 
         `Args:`
             id: str
                 The user ID of the record to delete.
         `Returns:`
             int
         """
         return requests.delete(
-            f'{self.base_url}/api/v2/users/{id}', headers=self.headers
+            f"{self.base_url}/api/v2/users/{id}", headers=self.headers
         ).status_code
 
     def get_users_by_email(self, email):
         """
         Get Auth0 users by email.
 
         `Args:`
             email: str
                 The user email of the record to get.
         `Returns:`
             Table Class
         """
-        return Table(requests.get(
-            f'{self.base_url}/api/v2/users-by-email', headers=self.headers, params={'email': email}
-        ).json())
+        return Table(
+            requests.get(
+                f"{self.base_url}/api/v2/users-by-email",
+                headers=self.headers,
+                params={"email": email},
+            ).json()
+        )
```

### Comparing `parsons-1.0.0/parsons/aws/aws_async.py` & `parsons-1.1.0/parsons/aws/aws_async.py`

 * *Files 9% similar despite different names*

```diff
@@ -30,83 +30,104 @@
     Minimal `shim <https://en.wikipedia.org/wiki/Shim_(computing)>`_
     to add to the top lambda handler function to enable distributed tasks
 
     The rest of this library is compatible with zappa.async library.
     If you have deployed your app with `Zappa <https://github.com/Miserlou/Zappa>`_,
     then you do NOT need to add this shim.
     """
-    if not set(event).intersection({'task_path', 'args', 'kwargs'}):
+    if not set(event).intersection({"task_path", "args", "kwargs"}):
         return False  # did not match an event command
-    func = import_and_get_task(event['task_path'], event.get('func_class_init_kwargs'))
+    func = import_and_get_task(event["task_path"], event.get("func_class_init_kwargs"))
     # if the func was decorated with zappa.async.task then run the real function
-    func = getattr(func, 'sync', func)
+    func = getattr(func, "sync", func)
 
     # DID match an event command
     # -- so probably don't do the usual thing the Lambda handler does
-    return (func(*event['args'], **event['kwargs'])
-            or True)
+    return func(*event["args"], **event["kwargs"]) or True
 
 
-def run(func, args=[], kwargs={}, service='lambda', capture_response=False,
-        remote_aws_lambda_function_name=None, remote_aws_region=None,
-        func_class=None, func_class_init_kwargs=None, **task_kwargs):
-    lambda_function_name = (remote_aws_lambda_function_name
-                            or os.environ.get('AWS_LAMBDA_FUNCTION_NAME'))
-    if not lambda_function_name or lambda_function_name == 'FORCE_LOCAL':
+def run(
+    func,
+    args=[],
+    kwargs={},
+    service="lambda",
+    capture_response=False,
+    remote_aws_lambda_function_name=None,
+    remote_aws_region=None,
+    func_class=None,
+    func_class_init_kwargs=None,
+    **task_kwargs,
+):
+    lambda_function_name = remote_aws_lambda_function_name or os.environ.get(
+        "AWS_LAMBDA_FUNCTION_NAME"
+    )
+    if not lambda_function_name or lambda_function_name == "FORCE_LOCAL":
         # We are neither running in Lambda environment, nor given one to invoke
         # so let's run it synchronously -- so code can be compatible both in-and-out of Lambda
         func(*args, **kwargs)
         return True
     # zappa has more robust and allows more configs -- but is not compatible with func_class
     if zappa_run and not func_class:
-        return zappa_run(func, args, kwargs, service,
-                         capture_response, remote_aws_lambda_function_name, remote_aws_region,
-                         **task_kwargs)
+        return zappa_run(
+            func,
+            args,
+            kwargs,
+            service,
+            capture_response,
+            remote_aws_lambda_function_name,
+            remote_aws_region,
+            **task_kwargs,
+        )
 
     task_path = get_func_task_path(func, func_class)
-    payload = (json.dumps({'task_path': task_path,
-                           'args': args,
-                           'kwargs': kwargs,
-                           'func_class_init_kwargs': func_class_init_kwargs})
-               .encode('utf-8'))
+    payload = json.dumps(
+        {
+            "task_path": task_path,
+            "args": args,
+            "kwargs": kwargs,
+            "func_class_init_kwargs": func_class_init_kwargs,
+        }
+    ).encode("utf-8")
     if len(payload) > 128000:  # pragma: no cover
         raise AsyncException("Payload too large for async Lambda call")
-    lambda_client = boto3.Session().client('lambda')
+    lambda_client = boto3.Session().client("lambda")
     response = lambda_client.invoke(
         FunctionName=lambda_function_name,
-        InvocationType='Event',  # makes the call async
-        Payload=payload
+        InvocationType="Event",  # makes the call async
+        Payload=payload,
     )
-    return response.get('StatusCode', 0) == 202
+    return response.get("StatusCode", 0) == 202
 
 
 ##
 # Utility Functions
 ##
 
+
 def import_and_get_task(task_path, instance_init_kwargs=None):
     """
     Given a modular path to a function, import that module
     and return the function.
     """
-    module, function = task_path.rsplit('.', 1)
+    module, function = task_path.rsplit(".", 1)
     app_module = importlib.import_module(module)
-    class_func = function.split('|')
+    class_func = function.split("|")
     app_function = getattr(app_module, class_func[0])
     if len(class_func) == 1:
         return app_function
 
     def init_and_run(*args, **kwargs):
-        print('INITRUN', args, kwargs)
+        print("INITRUN", args, kwargs)
         if len(class_func) == 3:  # instance
             instance = app_function  # actually the class
         else:
             instance = app_function(**(instance_init_kwargs or {}))
         method = getattr(instance, class_func[1])
         return method(*args, **kwargs)
+
     return init_and_run
 
 
 def get_func_task_path(func, method_class=None):
     """
     Format the modular task path for a function via inspection.
     """
@@ -114,18 +135,18 @@
     func_name = func.__name__
 
     # To support class methods, we need to see if it IS a method on a class
     # and then also determine if it is an instance method or a classmethod
     # Then we record that info with |'s to be decoded in import_and_get_task
     # classmethod format: "Foo|method|"
     # instance method format: "Foo|method"
-    task_path = '{}.{}{}{}'.format(
+    task_path = "{}.{}{}{}".format(
         module_path,
-        f'{method_class.__name__}|' if method_class else '',
+        f"{method_class.__name__}|" if method_class else "",
         func_name,
-        '|' if method_class and 'of <class' in repr(func) else ''
+        "|" if method_class and "of <class" in repr(func) else "",
     )
     return task_path
 
 
 class AsyncException(Exception):
     pass
```

### Comparing `parsons-1.0.0/parsons/aws/lambda_distribute.py` & `parsons-1.1.0/parsons/aws/lambda_distribute.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,28 +1,31 @@
 import csv
 from io import TextIOWrapper, BytesIO, StringIO
 import logging
 import sys
 import traceback
 import time
 
-from parsons.aws.aws_async import get_func_task_path, import_and_get_task, run as maybe_async_run
+from parsons.aws.aws_async import (
+    get_func_task_path,
+    import_and_get_task,
+    run as maybe_async_run,
+)
 from parsons.aws.s3 import S3
 from parsons.etl.table import Table
 from parsons.utilities.check_env import check
 
 logger = logging.getLogger(__name__)
 
 
 class DistributeTaskException(Exception):
     pass
 
 
 class TestStorage:
-
     def __init__(self):
         self.data = {}
 
     def put_object(self, bucket, key, object_bytes):
         self.data[key] = object_bytes
 
     def get_range(self, bucket, key, rangestart, rangeend):
@@ -35,50 +38,56 @@
     inside this file rather than s3.py
     """
 
     def __init__(self, use_env_token=True):
         self.s3 = S3(use_env_token=use_env_token)
 
     def put_object(self, bucket, key, object_bytes, **kwargs):
-        return self.s3.client.put_object(Bucket=bucket, Key=key, Body=object_bytes, **kwargs)
+        return self.s3.client.put_object(
+            Bucket=bucket, Key=key, Body=object_bytes, **kwargs
+        )
 
     def get_range(self, bucket, key, rangestart, rangeend):
         """
         Gets an explicit byte-range of an S3 file
         """
         # bytes is INCLUSIVE for the rangeend parameter, unlike python
         # so e.g. while python returns 2 bytes for data[2:4]
         # Range: bytes=2-4 will return 3!! So we subtract 1
         response = self.s3.client.get_object(
-            Bucket=bucket, Key=key,
-            Range='bytes={}-{}'.format(rangestart, rangeend - 1))
-        return response['Body'].read()
+            Bucket=bucket, Key=key, Range="bytes={}-{}".format(rangestart, rangeend - 1)
+        )
+        return response["Body"].read()
 
 
 FAKE_STORAGE = TestStorage()
 S3_TEMP_KEY_PREFIX = "Parsons_DistributeTask"
 
 
-def distribute_task_csv(csv_bytes_utf8, func_to_run, bucket,
-                        header=None,
-                        func_kwargs=None,
-                        func_class=None,
-                        func_class_kwargs=None,
-                        catch=False,
-                        group_count=100,
-                        storage='s3',
-                        use_s3_env_token=True):
+def distribute_task_csv(
+    csv_bytes_utf8,
+    func_to_run,
+    bucket,
+    header=None,
+    func_kwargs=None,
+    func_class=None,
+    func_class_kwargs=None,
+    catch=False,
+    group_count=100,
+    storage="s3",
+    use_s3_env_token=True,
+):
     """
     The same as distribute_task, but instead of a table, the
     first argument is bytes of a csv encoded into utf8.
     This function is used by distribute_task() which you should use instead.
     """
     global FAKE_STORAGE
     func_name = get_func_task_path(func_to_run, func_class)
-    row_chunks = csv_bytes_utf8.split(b'\n')
+    row_chunks = csv_bytes_utf8.split(b"\n")
     cursor = 0
     row_ranges = []
     # gather start/end bytes for each row
     for rowc in row_chunks:
         rng = [cursor]
         cursor = cursor + len(rowc) + 1  # +1 is the \n character
         rng.append(cursor)
@@ -91,49 +100,68 @@
         end = min(len(row_ranges) - 1, grpstep + group_count - 1)
         group_ranges.append((row_ranges[grpstep][0], row_ranges[end][1]))
 
     # upload data
     filename = hash(time.time())
     storagekey = f"{S3_TEMP_KEY_PREFIX}/{filename}.csv"
     groupcount = len(group_ranges)
-    logger.debug(f'distribute_task_csv storagekey {storagekey} w/ {groupcount} groups')
+    logger.debug(f"distribute_task_csv storagekey {storagekey} w/ {groupcount} groups")
 
     response = None
-    if storage == 's3':
+    if storage == "s3":
         response = S3Storage(use_env_token=use_s3_env_token).put_object(
             bucket, storagekey, csv_bytes_utf8
         )
     else:
         response = FAKE_STORAGE.put_object(bucket, storagekey, csv_bytes_utf8)
 
     # start processes
     results = [
         maybe_async_run(
             process_task_portion,
-            [bucket, storagekey, grp[0], grp[1], func_name, header,
-             storage, func_kwargs, catch, func_class_kwargs, use_s3_env_token],
+            [
+                bucket,
+                storagekey,
+                grp[0],
+                grp[1],
+                func_name,
+                header,
+                storage,
+                func_kwargs,
+                catch,
+                func_class_kwargs,
+                use_s3_env_token,
+            ],
             # if we are using local storage, then it must be run locally, as well
             # (good for testing/debugging)
-            remote_aws_lambda_function_name='FORCE_LOCAL' if storage == 'local' else None
+            remote_aws_lambda_function_name="FORCE_LOCAL"
+            if storage == "local"
+            else None,
         )
-        for grp in group_ranges]
-    return {'DEBUG_ONLY': 'results may vary depending on context/platform',
-            'results': results,
-            'put_response': response}
-
-
-def distribute_task(table, func_to_run,
-                    bucket=None,
-                    func_kwargs=None,
-                    func_class=None,
-                    func_class_kwargs=None,
-                    catch=False,
-                    group_count=100,
-                    storage='s3',
-                    use_s3_env_token=True):
+        for grp in group_ranges
+    ]
+    return {
+        "DEBUG_ONLY": "results may vary depending on context/platform",
+        "results": results,
+        "put_response": response,
+    }
+
+
+def distribute_task(
+    table,
+    func_to_run,
+    bucket=None,
+    func_kwargs=None,
+    func_class=None,
+    func_class_kwargs=None,
+    catch=False,
+    group_count=100,
+    storage="s3",
+    use_s3_env_token=True,
+):
     """
     Distribute processing rows in a table across multiple AWS Lambda invocations.
 
     `Args:`
         table: Parsons Table
            Table of data you wish to distribute processing across Lambda invocations
            of `func_to_run` argument.
@@ -171,59 +199,77 @@
            set to "local".
         use_s3_env_token: str
            If storage is set to "s3", sets the use_env_token parameter on the S3 storage.
     `Returns:`
         Debug information -- do not rely on the output, as it will change
         depending on how this method is invoked.
     """
-    if storage not in ('s3', 'local'):
-        raise DistributeTaskException('storage argument must be s3 or local')
-    bucket = check('S3_TEMP_BUCKET', bucket)
+    if storage not in ("s3", "local"):
+        raise DistributeTaskException("storage argument must be s3 or local")
+    bucket = check("S3_TEMP_BUCKET", bucket)
     csvdata = StringIO()
     outcsv = csv.writer(csvdata)
     outcsv.writerows(table.table.data())
-    return distribute_task_csv(csvdata.getvalue().encode('utf-8-sig'),
-                               func_to_run,
-                               bucket,
-                               header=table.columns,
-                               func_kwargs=func_kwargs,
-                               func_class=func_class,
-                               func_class_kwargs=func_class_kwargs,
-                               catch=catch,
-                               group_count=group_count,
-                               storage=storage,
-                               use_s3_env_token=use_s3_env_token)
-
-
-def process_task_portion(bucket, storagekey, rangestart, rangeend, func_name, header,
-                         storage='s3', func_kwargs=None, catch=False,
-                         func_class_kwargs=None, use_s3_env_token=True):
+    return distribute_task_csv(
+        csvdata.getvalue().encode("utf-8-sig"),
+        func_to_run,
+        bucket,
+        header=table.columns,
+        func_kwargs=func_kwargs,
+        func_class=func_class,
+        func_class_kwargs=func_class_kwargs,
+        catch=catch,
+        group_count=group_count,
+        storage=storage,
+        use_s3_env_token=use_s3_env_token,
+    )
+
+
+def process_task_portion(
+    bucket,
+    storagekey,
+    rangestart,
+    rangeend,
+    func_name,
+    header,
+    storage="s3",
+    func_kwargs=None,
+    catch=False,
+    func_class_kwargs=None,
+    use_s3_env_token=True,
+):
     global FAKE_STORAGE
 
-    logger.debug(f'process_task_portion func_name {func_name}, '
-                 f'storagekey {storagekey}, byterange {rangestart}-{rangeend}')
+    logger.debug(
+        f"process_task_portion func_name {func_name}, "
+        f"storagekey {storagekey}, byterange {rangestart}-{rangeend}"
+    )
     func = import_and_get_task(func_name, func_class_kwargs)
-    if storage == 's3':
+    if storage == "s3":
         filedata = S3Storage(use_env_token=use_s3_env_token).get_range(
             bucket, storagekey, rangestart, rangeend
         )
     else:
         filedata = FAKE_STORAGE.get_range(bucket, storagekey, rangestart, rangeend)
 
-    lines = list(csv.reader(TextIOWrapper(BytesIO(filedata), encoding='utf-8-sig')))
+    lines = list(csv.reader(TextIOWrapper(BytesIO(filedata), encoding="utf-8-sig")))
     table = Table([header] + lines)
     if catch:
         try:
             func(table, **func_kwargs)
         except Exception:
             # In Lambda you can search for '"Distribute Error"' in the logs
             type_, value_, traceback_ = sys.exc_info()
-            err_traceback_str = '\n'.join(traceback.format_exception(type_, value_, traceback_))
-            return {'Exception': 'Distribute Error',
-                    'error': err_traceback_str,
-                    'rangestart': rangestart,
-                    'rangeend': rangeend,
-                    'func_name': func_name,
-                    'bucket': bucket,
-                    'storagekey': storagekey}
+            err_traceback_str = "\n".join(
+                traceback.format_exception(type_, value_, traceback_)
+            )
+            return {
+                "Exception": "Distribute Error",
+                "error": err_traceback_str,
+                "rangestart": rangestart,
+                "rangeend": rangeend,
+                "func_name": func_name,
+                "bucket": bucket,
+                "storagekey": storagekey,
+            }
     else:
         func(table, **func_kwargs)
```

### Comparing `parsons-1.0.0/parsons/aws/s3.py` & `parsons-1.1.0/parsons/aws/s3.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,39 +1,45 @@
 import re
 import boto3
+from botocore.client import ClientError
 from parsons.utilities import files
 import logging
 import os
 
 logger = logging.getLogger(__name__)
 
 
 class AWSConnection(object):
-
-    def __init__(self, aws_access_key_id=None, aws_secret_access_key=None, aws_session_token=None,
-                 use_env_token=True):
-
+    def __init__(
+        self,
+        aws_access_key_id=None,
+        aws_secret_access_key=None,
+        aws_session_token=None,
+        use_env_token=True,
+    ):
         # Order of operations for searching for keys:
         #   1. Look for keys passed as kwargs
         #   2. Look for env variables
         #   3. Look for aws config file
         # Boto3 handles 2 & 3, but should handle 1 on it's own. Not sure
         # why that's not working.
 
         if aws_access_key_id and aws_secret_access_key:
             # The AWS session token isn't needed most of the time, so we'll check
             # for the env variable here instead of requiring it to be passed
             # whenever the aws_access_key_id and aws_secret_access_key are passed.
 
             if aws_session_token is None and use_env_token:
-                aws_session_token = os.getenv('AWS_SESSION_TOKEN')
+                aws_session_token = os.getenv("AWS_SESSION_TOKEN")
 
-            self.session = boto3.Session(aws_access_key_id=aws_access_key_id,
-                                         aws_secret_access_key=aws_secret_access_key,
-                                         aws_session_token=aws_session_token)
+            self.session = boto3.Session(
+                aws_access_key_id=aws_access_key_id,
+                aws_secret_access_key=aws_secret_access_key,
+                aws_session_token=aws_session_token,
+            )
 
         else:
             self.session = boto3.Session()
 
 
 class S3(object):
     """
@@ -54,26 +60,29 @@
             to ``True``. Set to ``False`` in order to ignore the ``AWS_SESSION_TOKEN`` environment
             variable even if the ``aws_session_token`` argument was not passed in.
 
     `Returns:`
         S3 class.
     """
 
-    def __init__(self,
-                 aws_access_key_id=None,
-                 aws_secret_access_key=None,
-                 aws_session_token=None,
-                 use_env_token=True):
-
-        self.aws = AWSConnection(aws_access_key_id=aws_access_key_id,
-                                 aws_secret_access_key=aws_secret_access_key,
-                                 aws_session_token=aws_session_token,
-                                 use_env_token=use_env_token)
+    def __init__(
+        self,
+        aws_access_key_id=None,
+        aws_secret_access_key=None,
+        aws_session_token=None,
+        use_env_token=True,
+    ):
+        self.aws = AWSConnection(
+            aws_access_key_id=aws_access_key_id,
+            aws_secret_access_key=aws_secret_access_key,
+            aws_session_token=aws_session_token,
+            use_env_token=use_env_token,
+        )
 
-        self.s3 = self.aws.session.resource('s3')
+        self.s3 = self.aws.session.resource("s3")
         """Boto3 API Session Resource object. Use for more advanced boto3 features."""
 
         self.client = self.s3.meta.client
         """Boto3 API Session client object. Use for more advanced boto3 features."""
 
     def list_buckets(self):
         """
@@ -104,17 +113,24 @@
             self.list_keys(bucket)
             return True
         except Exception:
             pass
 
         return bucket in self.list_buckets()
 
-    def list_keys(self, bucket, prefix=None, suffix=None, regex=None,
-                  date_modified_before=None, date_modified_after=None,
-                  **kwargs):
+    def list_keys(
+        self,
+        bucket,
+        prefix=None,
+        suffix=None,
+        regex=None,
+        date_modified_before=None,
+        date_modified_after=None,
+        **kwargs,
+    ):
         """
         List the keys in a bucket, along with extra info about each one.
 
         `Args:`
             bucket: str
                 The bucket name
             prefix: str
@@ -134,58 +150,81 @@
         `Returns:`
             dict
                 Dict mapping the keys to info about each key. The info includes 'LastModified',
                 'Size', and 'Owner'.
         """
 
         keys_dict = dict()
-        logger.debug(f'Fetching keys in {bucket} bucket')
+        logger.debug(f"Fetching keys in {bucket} bucket")
 
         continuation_token = None
 
         while True:
-            args = {'Bucket': bucket}
+            args = {"Bucket": bucket}
+
             if prefix:
-                args['Prefix'] = prefix
+                args["Prefix"] = prefix
+
             if continuation_token:
-                args['ContinuationToken'] = continuation_token
+                args["ContinuationToken"] = continuation_token
+
             args.update(kwargs)
 
-            resp = self.client.list_objects_v2(**args)
+            try:
+                resp = self.client.list_objects_v2(**args)
 
-            for key in resp.get('Contents', []):
+            except ClientError as e:
+                error_message = """Unable to list bucket objects!
+                This may be due to a lack of permission on the requested
+                bucket. Double-check that you have sufficient READ permissions
+                on the bucket you've requested. If you only have permissions for
+                keys within a specific prefix, make sure you include a trailing '/' in
+                in prefix."""
 
+                logger.error(error_message)
+
+                raise e
+
+            for key in resp.get("Contents", []):
                 # Match suffix
-                if suffix and not key['Key'].endswith(suffix):
+                if suffix and not key["Key"].endswith(suffix):
                     continue
 
                 # Regex matching
-                if regex and not bool(re.search(regex, key['Key'])):
+                if regex and not bool(re.search(regex, key["Key"])):
                     continue
 
                 # Match timestamp parsing
-                if date_modified_before and not key['LastModified'] < date_modified_before:
+                if (
+                    date_modified_before
+                    and not key["LastModified"] < date_modified_before
+                ):
                     continue
 
-                if date_modified_after and not key['LastModified'] > date_modified_after:
+                if (
+                    date_modified_after
+                    and not key["LastModified"] > date_modified_after
+                ):
                     continue
 
                 # Convert date to iso string
-                key['LastModified'] = key['LastModified'].isoformat()
+                key["LastModified"] = key["LastModified"].isoformat()
 
                 # Add to output dict
-                keys_dict[key.get('Key')] = key
+                keys_dict[key.get("Key")] = key
 
             # If more than 1000 results, continue with token
-            if resp.get('NextContinuationToken'):
-                continuation_token = resp['NextContinuationToken']
+            if resp.get("NextContinuationToken"):
+                continuation_token = resp["NextContinuationToken"]
+
             else:
                 break
 
-        logger.debug(f'Retrieved {len(keys_dict)} keys')
+        logger.debug(f"Retrieved {len(keys_dict)} keys")
+
         return keys_dict
 
     def key_exists(self, bucket, key):
         """
         Determine if a key exists in a bucket.
 
         `Args:`
@@ -197,18 +236,18 @@
             boolean
                 ``True`` if key exists and ``False`` if not.
         """
 
         key_count = len(self.list_keys(bucket, prefix=key))
 
         if key_count > 0:
-            logger.debug(f'Found {key} in {bucket}.')
+            logger.debug(f"Found {key} in {bucket}.")
             return True
         else:
-            logger.debug(f'Did not find {key} in {bucket}.')
+            logger.debug(f"Did not find {key} in {bucket}.")
             return False
 
     def create_bucket(self, bucket):
         """
         Create an s3 bucket.
 
         .. warning::
@@ -231,15 +270,17 @@
                 The name of the bucket to create
         `Returns:`
             ``None``
         """
 
         self.client.create_bucket(Bucket=bucket)
 
-    def put_file(self, bucket, key, local_path, acl='bucket-owner-full-control', **kwargs):
+    def put_file(
+        self, bucket, key, local_path, acl="bucket-owner-full-control", **kwargs
+    ):
         """
         Uploads an object to an S3 bucket
 
         `Args:`
             bucket: str
                 The bucket name
             key: str
@@ -250,15 +291,17 @@
                 The S3 permissions on the file
             kwargs:
                 Additional arguments for the S3 API call. See `AWS Put Object documentation
                 <https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html>`_ for more
                 info.
         """
 
-        self.client.upload_file(local_path, bucket, key, ExtraArgs={'ACL': acl, **kwargs})
+        self.client.upload_file(
+            local_path, bucket, key, ExtraArgs={"ACL": acl, **kwargs}
+        )
 
     def remove_file(self, bucket, key):
         """
         Deletes an object from an S3 bucket
 
         `Args:`
             bucket: str
@@ -313,23 +356,34 @@
             expires_in: int
                 The time, in seconds, until the url expires
         `Returns:`
             Url:
                 A link to download the object
         """
 
-        return self.client.generate_presigned_url(ClientMethod='get_object',
-                                                  Params={'Bucket': bucket,
-                                                          'Key': key},
-                                                  ExpiresIn=expires_in)
-
-    def transfer_bucket(self, origin_bucket, origin_key, destination_bucket,
-                        destination_key=None, suffix=None, regex=None,
-                        date_modified_before=None, date_modified_after=None,
-                        public_read=False, remove_original=False, **kwargs):
+        return self.client.generate_presigned_url(
+            ClientMethod="get_object",
+            Params={"Bucket": bucket, "Key": key},
+            ExpiresIn=expires_in,
+        )
+
+    def transfer_bucket(
+        self,
+        origin_bucket,
+        origin_key,
+        destination_bucket,
+        destination_key=None,
+        suffix=None,
+        regex=None,
+        date_modified_before=None,
+        date_modified_after=None,
+        public_read=False,
+        remove_original=False,
+        **kwargs,
+    ):
         """
         Transfer files between s3 buckets
 
         `Args:`
             origin_bucket: str
                 The origin bucket
             origin_key: str
@@ -356,46 +410,48 @@
                 <https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.copy>`_
                 for more info.
         `Returns:`
             ``None``
         """
 
         # If prefix, get all files for the prefix
-        if origin_key.endswith('/'):
+        if origin_key.endswith("/"):
             resp = self.list_keys(
                 origin_bucket,
                 prefix=origin_key,
                 suffix=suffix,
                 regex=regex,
                 date_modified_before=date_modified_before,
-                date_modified_after=date_modified_after
+                date_modified_after=date_modified_after,
             )
-            key_list = [value['Key'] for value in resp.values()]
+            key_list = [value["Key"] for value in resp.values()]
         else:
             key_list = [origin_key]
 
         for key in key_list:
             # If destination_key is prefix, replace
-            if destination_key and destination_key.endswith('/'):
+            if destination_key and destination_key.endswith("/"):
                 dest_key = key.replace(origin_key, destination_key)
 
             # If single destination, use destination key
             elif destination_key:
                 dest_key = destination_key
 
             # Else use key from original source
             else:
                 dest_key = key
 
-            copy_source = {'Bucket': origin_bucket, 'Key': key}
-            self.client.copy(copy_source, destination_bucket, dest_key, ExtraArgs=kwargs)
+            copy_source = {"Bucket": origin_bucket, "Key": key}
+            self.client.copy(
+                copy_source, destination_bucket, dest_key, ExtraArgs=kwargs
+            )
             if remove_original:
                 try:
                     self.remove_file(origin_bucket, origin_key)
                 except Exception as e:
-                    logger.error('Failed to delete original key: ' + str(e))
+                    logger.error("Failed to delete original key: " + str(e))
 
             if public_read:
                 object_acl = self.s3.ObjectAcl(destination_bucket, destination_key)
-                object_acl.put(ACL='public-read')
+                object_acl.put(ACL="public-read")
 
-        logger.info(f'Finished syncing {len(key_list)} keys')
+        logger.info(f"Finished syncing {len(key_list)} keys")
```

### Comparing `parsons-1.0.0/parsons/azure/azure_blob_storage.py` & `parsons-1.1.0/parsons/azure/azure_blob_storage.py`

 * *Files 4% similar despite different names*

```diff
@@ -29,42 +29,53 @@
         account_url: str
             The account URL for the Azure storage account including the account name and domain.
             Not required if ``AZURE_ACCOUNT_URL`` environment variable is set.
     `Returns:`
         `AzureBlobStorage`
     """
 
-    def __init__(self, account_name=None, credential=None, account_domain='blob.core.windows.net',
-                 account_url=None):
-        self.account_url = os.getenv('AZURE_ACCOUNT_URL', account_url)
-        self.credential = check_env.check('AZURE_CREDENTIAL', credential)
+    def __init__(
+        self,
+        account_name=None,
+        credential=None,
+        account_domain="blob.core.windows.net",
+        account_url=None,
+    ):
+        self.account_url = os.getenv("AZURE_ACCOUNT_URL", account_url)
+        self.credential = check_env.check("AZURE_CREDENTIAL", credential)
         if not self.account_url:
-            self.account_name = check_env.check('AZURE_ACCOUNT_NAME', account_name)
-            self.account_domain = check_env.check('AZURE_ACCOUNT_DOMAIN', account_domain)
-            self.account_url = f'https://{self.account_name}.{self.account_domain}/'
+            self.account_name = check_env.check("AZURE_ACCOUNT_NAME", account_name)
+            self.account_domain = check_env.check(
+                "AZURE_ACCOUNT_DOMAIN", account_domain
+            )
+            self.account_url = f"https://{self.account_name}.{self.account_domain}/"
         else:
-            if not self.account_url.startswith('http'):
-                self.account_url = f'https://{self.account_url}'
+            if not self.account_url.startswith("http"):
+                self.account_url = f"https://{self.account_url}"
             # Update the account name and domain if a URL is supplied
             parsed_url = urlparse(self.account_url)
             self.account_name = parsed_url.netloc.split(".")[0]
             self.account_domain = ".".join(parsed_url.netloc.split(".")[1:])
-        self.client = BlobServiceClient(account_url=self.account_url, credential=self.credential)
+        self.client = BlobServiceClient(
+            account_url=self.account_url, credential=self.credential
+        )
 
     def list_containers(self):
         """
         Returns a list of container names for the storage account
 
         `Returns:`
             list[str]
                 List of container names
         """
 
-        container_names = [container.name for container in self.client.list_containers()]
-        logger.info(f'Found {len(container_names)} containers.')
+        container_names = [
+            container.name for container in self.client.list_containers()
+        ]
+        logger.info(f"Found {len(container_names)} containers.")
         return container_names
 
     def container_exists(self, container_name):
         """
         Verify that a container exists within the storage account
 
         `Args:`
@@ -73,35 +84,37 @@
         `Returns:`
             bool
         """
 
         container_client = self.get_container(container_name)
         try:
             container_client.get_container_properties()
-            logger.info(f'{container_name} exists.')
+            logger.info(f"{container_name} exists.")
             return True
         except ResourceNotFoundError:
-            logger.info(f'{container_name} does not exist.')
+            logger.info(f"{container_name} does not exist.")
             return False
 
     def get_container(self, container_name):
         """
         Returns a container client
 
         `Args:`
             container_name: str
                 The name of the container
         `Returns:`
             `ContainerClient`
         """
 
-        logger.info(f'Returning {container_name} container client')
+        logger.info(f"Returning {container_name} container client")
         return self.client.get_container_client(container_name)
 
-    def create_container(self, container_name, metadata=None, public_access=None, **kwargs):
+    def create_container(
+        self, container_name, metadata=None, public_access=None, **kwargs
+    ):
         """
         Create a container
 
         `Args:`
             container_name: str
                 The name of the container
             metadata: Optional[dict[str, str]]
@@ -116,30 +129,30 @@
         `Returns:`
             `ContainerClient`
         """  # noqa
 
         container_client = self.client.create_container(
             container_name, metadata=metadata, public_access=public_access, **kwargs
         )
-        logger.info(f'Created {container_name} container.')
+        logger.info(f"Created {container_name} container.")
         return container_client
 
     def delete_container(self, container_name):
         """
         Delete a container.
 
         `Args:`
             container_name: str
                 The name of the container
         `Returns:`
             ``None``
         """
 
         self.client.delete_container(container_name)
-        logger.info(f'{container_name} container deleted.')
+        logger.info(f"{container_name} container deleted.")
 
     def list_blobs(self, container_name, name_starts_with=None):
         """
         List all of the names of blobs in a container
 
         `Args:`
             container_name: str
@@ -149,17 +162,18 @@
         `Returns:`
             list[str]
                 A list of blob names
         """
 
         container_client = self.get_container(container_name)
         blobs = [
-            blob for blob in container_client.list_blobs(name_starts_with=name_starts_with)
+            blob
+            for blob in container_client.list_blobs(name_starts_with=name_starts_with)
         ]
-        logger.info(f'Found {len(blobs)} blobs in {container_name} container.')
+        logger.info(f"Found {len(blobs)} blobs in {container_name} container.")
         return blobs
 
     def blob_exists(self, container_name, blob_name):
         """
         Verify that a blob exists in the specified container
 
         `Args:`
@@ -170,18 +184,18 @@
         `Returns:`
             bool
         """
 
         blob_client = self.get_blob(container_name, blob_name)
         try:
             blob_client.get_blob_properties()
-            logger.info(f'{blob_name} exists in {container_name} container.')
+            logger.info(f"{blob_name} exists in {container_name} container.")
             return True
         except ResourceNotFoundError:
-            logger.info(f'{blob_name} does not exist in {container_name} container.')
+            logger.info(f"{blob_name} does not exist in {container_name} container.")
             return False
 
     def get_blob(self, container_name, blob_name):
         """
         Get a blob object
 
         `Args:`
@@ -190,19 +204,26 @@
             blob_name: str
                 The blob name
         `Returns:`
             `BlobClient`
         """
 
         blob_client = self.client.get_blob_client(container_name, blob_name)
-        logger.info(f'Got {blob_name} blob from {container_name} container.')
+        logger.info(f"Got {blob_name} blob from {container_name} container.")
         return blob_client
 
-    def get_blob_url(self, container_name, blob_name, account_key=None, permission=None,
-                     expiry=None, start=None):
+    def get_blob_url(
+        self,
+        container_name,
+        blob_name,
+        account_key=None,
+        permission=None,
+        expiry=None,
+        start=None,
+    ):
         """
         Get a URL with a shared access signature for a blob
 
         `Args:`
             container_name: str
                 The container name
             blob_name: str
@@ -223,28 +244,28 @@
             str
                 URL with shared access signature for blob
         """
 
         if not account_key:
             if not self.credential:
                 raise ValueError(
-                    'An account shared access key must be provided if it was not on initialization'
+                    "An account shared access key must be provided if it was not on initialization"
                 )
             account_key = self.credential
 
         sas = generate_blob_sas(
             self.account_name,
             container_name,
             blob_name,
             account_key=account_key,
             permission=permission,
             expiry=expiry,
             start=start,
         )
-        return f'{self.account_url}/{container_name}/{blob_name}?sas={sas}'
+        return f"{self.account_url}/{container_name}/{blob_name}?sas={sas}"
 
     def _get_content_settings_from_dict(self, kwargs_dict):
         """
         Removes any keys for ``ContentSettings`` from a dict and returns a tuple of the generated
         settings or ``None`` and a dict with the settings keys removed.
 
         `Args:`
@@ -255,16 +276,20 @@
                 Any created settings or ``None`` and the dict with settings keys remvoed
         """
 
         kwargs_copy = {**kwargs_dict}
         content_settings = None
         content_settings_dict = {}
         content_settings_keys = [
-            'content_type', 'content_encoding', 'content_language', 'content_disposition',
-            'cache_control', 'content_md5'
+            "content_type",
+            "content_encoding",
+            "content_language",
+            "content_disposition",
+            "cache_control",
+            "content_md5",
         ]
         kwarg_keys = list(kwargs_copy.keys())
         for key in kwarg_keys:
             if key in content_settings_keys:
                 content_settings_dict[key] = kwargs_copy.pop(key)
         if content_settings_dict:
             content_settings = ContentSettings(**content_settings_dict)
@@ -292,24 +317,24 @@
         """  # noqa
 
         blob_client = self.get_blob(container_name, blob_name)
 
         # Move all content_settings keys into a ContentSettings object
         content_settings, kwargs_dict = self._get_content_settings_from_dict(kwargs)
 
-        with open(local_path, 'rb') as f:
+        with open(local_path, "rb") as f:
             data = f.read()
 
         blob_client = blob_client.upload_blob(
             data,
             overwrite=True,
             content_settings=content_settings,
             **kwargs_dict,
         )
-        logger.info(f'{blob_name} blob put in {container_name} container')
+        logger.info(f"{blob_name} blob put in {container_name} container")
 
         # Return refreshed BlobClient object
         return self.get_blob(container_name, blob_name)
 
     def download_blob(self, container_name, blob_name, local_path=None):
         """
         Downloads a blob from a container into the specified file path or a temporary file path
@@ -325,22 +350,22 @@
                 when the script is done running.
         `Returns:`
             str
                 The path of the downloaded file
         """
 
         if not local_path:
-            local_path = files.create_temp_file_for_path('TEMPFILEAZURE')
+            local_path = files.create_temp_file_for_path("TEMPFILEAZURE")
 
         blob_client = self.get_blob(container_name, blob_name)
 
-        logger.info(f'Downloading {blob_name} blob from {container_name} container.')
-        with open(local_path, 'wb') as f:
+        logger.info(f"Downloading {blob_name} blob from {container_name} container.")
+        with open(local_path, "wb") as f:
             blob_client.download_blob().readinto(f)
-        logger.info(f'{blob_name} blob saved to {local_path}.')
+        logger.info(f"{blob_name} blob saved to {local_path}.")
 
         return local_path
 
     def delete_blob(self, container_name, blob_name):
         """
         Delete a blob in a specified container.
 
@@ -351,17 +376,17 @@
                 The blob name
         `Returns:`
             ``None``
         """
 
         blob_client = self.get_blob(container_name, blob_name)
         blob_client.delete_blob()
-        logger.info(f'{blob_name} blob in {container_name} container deleted.')
+        logger.info(f"{blob_name} blob in {container_name} container deleted.")
 
-    def upload_table(self, table, container_name, blob_name, data_type='csv', **kwargs):
+    def upload_table(self, table, container_name, blob_name, data_type="csv", **kwargs):
         """
         Load the data from a Parsons table into a blob.
 
         `Args:`
             table: obj
                 A :ref:`parsons-table`
             container_name: str
@@ -372,19 +397,21 @@
                 The file format to use when writing the data. One of: `csv` or `json`
             kwargs:
                 Additional keyword arguments to supply to ``put_blob``
         `Returns:`
             `BlobClient`
         """
 
-        if data_type == 'csv':
+        if data_type == "csv":
             local_path = table.to_csv()
-            content_type = 'text/csv'
-        elif data_type == 'json':
+            content_type = "text/csv"
+        elif data_type == "json":
             local_path = table.to_json()
-            content_type = 'application/json'
+            content_type = "application/json"
         else:
-            raise ValueError(f'Unknown data_type value ({data_type}): must be one of: csv or json')
+            raise ValueError(
+                f"Unknown data_type value ({data_type}): must be one of: csv or json"
+            )
 
         return self.put_blob(
             container_name, blob_name, local_path, content_type=content_type, **kwargs
         )
```

### Comparing `parsons-1.0.0/parsons/bill_com/bill_com.py` & `parsons-1.1.0/parsons/bill_com/bill_com.py`

 * *Files 9% similar despite different names*

```diff
@@ -15,45 +15,45 @@
         dev_key: str
             The Bill.com dev key
         api_url:
             The Bill.com end point url
     """
 
     def __init__(self, user_name, password, org_id, dev_key, api_url):
-        self.headers = {
-            "Content-Type": "application/x-www-form-urlencoded"
-        }
+        self.headers = {"Content-Type": "application/x-www-form-urlencoded"}
         params = {
             "userName": user_name,
             "password": password,
             "orgId": org_id,
-            "devKey": dev_key
+            "devKey": dev_key,
         }
-        response = requests.post(url="%sLogin.json" % api_url,
-                                 data=params,
-                                 headers=self.headers)
+        response = requests.post(
+            url="%sLogin.json" % api_url, data=params, headers=self.headers
+        )
         self.dev_key = dev_key
         self.api_url = api_url
-        self.session_id = response.json()['response_data']['sessionId']
+        self.session_id = response.json()["response_data"]["sessionId"]
 
     def _get_payload(self, data):
         """
         `Args:`
             data: dict
                 A dictionary containing the payload to be sent in the request.
                 The dev_key and sessionId should not be included as they are
                 dealt with separately.
 
         `Returns:`
             A dictionary of the payload to be sent in the request with the
             dev_key and sessionId added.
         """
-        return {"devKey": self.dev_key,
-                "sessionId": self.session_id,
-                "data": json.dumps(data)}
+        return {
+            "devKey": self.dev_key,
+            "sessionId": self.session_id,
+            "data": json.dumps(data),
+        }
 
     def _post_request(self, data, action, object_name):
         """
         `Args:`
             data: dict
                 A dictionary containing the payload to be sent in the request.
                 The dev_key and sessionId should not be included as they are
@@ -64,18 +64,18 @@
             object_name: str
                 The id of the object. Possible values are "User", "Customer", and "Invoice".
 
         `Returns:`
             A dictionary containing the JSON response from the post request.
         """
 
-        if action == 'Read':
+        if action == "Read":
             url = "%sCrud/%s/%s.json" % (self.api_url, action, object_name)
-        elif action == 'Create':
-            data['obj']['entity'] = object_name
+        elif action == "Create":
+            data["obj"]["entity"] = object_name
             url = "%sCrud/%s/%s.json" % (self.api_url, action, object_name)
         elif action == "Send":
             url = "%s%s%s.json" % (self.api_url, action, object_name)
         else:
             url = "%s%s/%s.json" % (self.api_url, action, object_name)
         payload = self._get_payload(data)
         response = requests.post(url=url, data=payload, headers=self.headers)
@@ -115,18 +115,18 @@
             data: dict
                 Start, max, and kwargs from initial list call
             object_name: str
                 Name of the object being listed
         """
 
         r_table = Table(response)
-        max_ct = data['max']
+        max_ct = data["max"]
 
         while len(response) == max_ct:
-            data['start'] += max_ct
+            data["start"] += max_ct
             response = self._post_request(data, "List", object_name)[field]
             r_table.concat(Table(response))
 
         return r_table
 
     def get_user_list(self, start_user=0, max_user=999, **kwargs):
         """
@@ -137,19 +137,15 @@
                 The index of the max user to return
             **kwargs:
                 Any other fields to pass
 
         `Returns:`
             A Parsons Table of user information for every user from start_user to max_user.
         """
-        data = {
-           "start": start_user,
-           "max": max_user,
-           **kwargs
-        }
+        data = {"start": start_user, "max": max_user, **kwargs}
 
         return self._get_request_response(data, "List", "User")
 
     def get_customer_list(self, start_customer=0, max_customer=999, **kwargs):
         """
         `Args:`
             start_customer: int
@@ -159,19 +155,15 @@
             **kwargs:
                 Any other fields to pass
 
         `Returns:`
             A Parsons Table of customer information for every user from start_customer
             to max_customer.
         """
-        data = {
-           "start": start_customer,
-           "max": max_customer,
-           **kwargs
-        }
+        data = {"start": start_customer, "max": max_customer, **kwargs}
 
         return self._get_request_response(data, "List", "Customer")
 
     def get_invoice_list(self, start_invoice=0, max_invoice=999, **kwargs):
         """
         `Args:`
             start_invoice: int
@@ -181,48 +173,40 @@
             **kwargs:
                 Any other fields to pass
 
         `Returns:`
             A list of dictionaries of invoice information for every invoice from start_invoice
             to max_invoice.
         """
-        data = {
-           "start": start_invoice,
-           "max": max_invoice,
-           **kwargs
-        }
+        data = {"start": start_invoice, "max": max_invoice, **kwargs}
 
         return self._get_request_response(data, "List", "Invoice")
 
     def read_customer(self, customer_id):
         """
         `Args:`
             customer_id: str
                 The id of the customer to query
 
         `Returns:`
             A dictionary of the customer's information.
         """
-        data = {
-            'id': customer_id
-        }
+        data = {"id": customer_id}
         return self._get_request_response(data, "Read", "Customer")
 
     def read_invoice(self, invoice_id):
         """
         `Args:`
             invoice_id: str
                 The id of the invoice to query
 
         `Returns:`
             A dictionary of the invoice information.
         """
-        data = {
-           "id": invoice_id
-        }
+        data = {"id": invoice_id}
         return self._get_request_response(data, "Read", "Invoice")
 
     def check_customer(self, customer1, customer2):
         """
         `Args:`
             customer1: dict
                 A dictionary of data on customer1
@@ -236,16 +220,16 @@
                 2. customer1 has no id and customer1 customer2 have the same email address
             False otherwise
 
         """
         if "id" in customer1.keys():
             if customer1["id"] == customer2["id"]:
                 return True
-        if "id" not in customer1.keys() and customer2['email']:
-            if customer1['email'].lower() == customer2['email'].lower():
+        if "id" not in customer1.keys() and customer2["email"]:
+            if customer1["email"].lower() == customer2["email"].lower():
                 return True
         return False
 
     def get_or_create_customer(self, customer_name, customer_email, **kwargs):
         """
         `Args:`
             customer_name: str
@@ -257,31 +241,34 @@
                 Any other fields to store about the customer.
 
         `Returns:`
             A dictionary of the customer's information including an id.
             If the customer already exists, this function will not
             create a new id and instead use the existing id.
         """
-        customer = {"name": customer_name,
-                    "email": customer_email,
-                    **kwargs}
+        customer = {"name": customer_name, "email": customer_email, **kwargs}
 
         # check if customer already exists
         customer_list = self.get_customer_list()
         for existing_customer in customer_list:
             if self.check_customer(customer, existing_customer):
                 return existing_customer
         # customer doesn't exist, create
-        data = {
-            "obj": customer
-        }
+        data = {"obj": customer}
         return self._get_request_response(data, "Create", "Customer")
 
-    def create_invoice(self, customer_id, invoice_number, invoice_date,
-                       due_date, invoice_line_items, **kwargs):
+    def create_invoice(
+        self,
+        customer_id,
+        invoice_number,
+        invoice_date,
+        due_date,
+        invoice_line_items,
+        **kwargs
+    ):
         """
         `Args:`
             customer_id: str
                 The customer's id
             invoice_number: str
                 The invoice number. Every invoice must have a distinct
                 invoice number.
@@ -297,28 +284,36 @@
                 Any other invoice details to pass.
 
         `Returns:`
             A dictionary of the invoice's information including an id.
         """
         for invoice_line_item in invoice_line_items:
             if "entity" not in invoice_line_item:
-                invoice_line_item['entity'] = 'InvoiceLineItem'
+                invoice_line_item["entity"] = "InvoiceLineItem"
         data = {
-            "obj": {"customerId": customer_id,
-                    "invoiceNumber": invoice_number,
-                    "invoiceDate": invoice_date,
-                    "dueDate": due_date,
-                    "invoiceLineItems": invoice_line_items,
-                    **kwargs
-                    }
+            "obj": {
+                "customerId": customer_id,
+                "invoiceNumber": invoice_number,
+                "invoiceDate": invoice_date,
+                "dueDate": due_date,
+                "invoiceLineItems": invoice_line_items,
+                **kwargs,
+            }
         }
         return self._get_request_response(data, "Create", "Invoice")
 
-    def send_invoice(self, invoice_id, from_user_id, to_email_addresses,
-                     message_subject, message_body, **kwargs):
+    def send_invoice(
+        self,
+        invoice_id,
+        from_user_id,
+        to_email_addresses,
+        message_subject,
+        message_body,
+        **kwargs
+    ):
         """
         `Args:`
             invoice_id: str
                 The id of the invoice to send
             from_user_id: str
                 The id of the Bill.com user from whom to send the email
             to_email_addresses:
@@ -330,19 +325,17 @@
             **kwargs:
                 Any other details for sending the invoice
 
         `Returns:`
             A dictionary of the sent invoice.
         """
         data = {
-          "invoiceId": invoice_id,
-          "headers": {
-            "fromUserId": from_user_id,
-            "toEmailAddresses": to_email_addresses,
-            "subject": message_subject,
-            **kwargs
-          },
-          "content": {
-            "body": message_body
-          }
+            "invoiceId": invoice_id,
+            "headers": {
+                "fromUserId": from_user_id,
+                "toEmailAddresses": to_email_addresses,
+                "subject": message_subject,
+                **kwargs,
+            },
+            "content": {"body": message_body},
         }
         return self._get_request_response(data, "Send", "Invoice")
```

### Comparing `parsons-1.0.0/parsons/bloomerang/bloomerang.py` & `parsons-1.1.0/parsons/bloomerang/bloomerang.py`

 * *Files 7% similar despite different names*

```diff
@@ -30,130 +30,143 @@
             The Bloomerang client secret for OAuth2 authetication. Not required if
             the ``BLOOMERANG_CLIENT_SECRET`` env variable is set or if the ``api_key``
             parameter is set. Note that the ``client_id`` parameter must
             also be set in order to use OAuth2 authentication.
     """
 
     def __init__(self, api_key=None, client_id=None, client_secret=None):
-        self.api_key = check_env.check('BLOOMERANG_API_KEY', api_key, optional=True)
-        self.client_id = check_env.check('BLOOMERANG_CLIENT_ID', client_id, optional=True)
-        self.client_secret = check_env.check('BLOOMERANG_CLIENT_SECRET', client_secret,
-                                             optional=True)
+        self.api_key = check_env.check("BLOOMERANG_API_KEY", api_key, optional=True)
+        self.client_id = check_env.check(
+            "BLOOMERANG_CLIENT_ID", client_id, optional=True
+        )
+        self.client_secret = check_env.check(
+            "BLOOMERANG_CLIENT_SECRET", client_secret, optional=True
+        )
         self.uri = URI
         self.uri_auth = URI_AUTH
         self.conn = self._conn()
 
     def _conn(self):
         # Instantiate APIConnector with authentication credentials
-        headers = {"accept": "application/json",
-                   "Content-Type": "application/json"}
+        headers = {"accept": "application/json", "Content-Type": "application/json"}
         if self.api_key is not None:
             logger.info("Using API key authentication.")
-            headers['X-API-KEY'] = f"{self.api_key}"
+            headers["X-API-KEY"] = f"{self.api_key}"
         elif (self.client_id is not None) & (self.client_secret is not None):
-            logger.info('Using OAuth2 authentication.')
+            logger.info("Using OAuth2 authentication.")
             self._generate_authorization_code()
             self._generate_access_token()
-            headers['Authorization'] = f"Bearer {self.access_token}"
+            headers["Authorization"] = f"Bearer {self.access_token}"
         else:
-            raise Exception('Missing authorization credentials.')
+            raise Exception("Missing authorization credentials.")
         return APIConnector(uri=self.uri, headers=headers)
 
     def _generate_authorization_code(self):
-        data = {'client_id': self.client_id,
-                'response_type': 'code'}
+        data = {"client_id": self.client_id, "response_type": "code"}
         r = requests.post(url=self.uri_auth, json=data)
-        self.authorization_code = r.json().get('code', None)
+        self.authorization_code = r.json().get("code", None)
 
     def _generate_access_token(self):
-        data = {'client_id': self.client_id,
-                'client_secret': self.client_secret,
-                'grant_type': 'authorization_code',
-                'code': self.authorization_code}
-        r = requests.post(url=self.uri + 'oauth/token', json=data)
-        self.access_token = r.json().get('access_token', None)
+        data = {
+            "client_id": self.client_id,
+            "client_secret": self.client_secret,
+            "grant_type": "authorization_code",
+            "code": self.authorization_code,
+        }
+        r = requests.post(url=self.uri + "oauth/token", json=data)
+        self.access_token = r.json().get("access_token", None)
 
     def _base_endpoint(self, endpoint, entity_id=None):
-        url = f'{self.uri}{endpoint}/'
+        url = f"{self.uri}{endpoint}/"
         if entity_id:
-            url = url + f'{entity_id}/'
+            url = url + f"{entity_id}/"
         return url
 
     @staticmethod
     def _base_pagination_params(page_number=1, page_size=50):
-        return {'skip': page_size * (page_number - 1), 'take': min(page_size, 50)}
+        return {"skip": page_size * (page_number - 1), "take": min(page_size, 50)}
 
     @staticmethod
     def _base_ordering_params(order_by=None, order_direction=None):
         params = {}
 
         if order_by:
             params["orderBy"] = order_by
 
         if order_direction:
             params["orderDirection"] = order_direction
 
         return params
 
     def _base_create(self, endpoint, entity_id=None, **kwargs):
-        return self.conn.post_request(url=self._base_endpoint(endpoint, entity_id),
-                                      json=json.dumps({**kwargs}))
+        return self.conn.post_request(
+            url=self._base_endpoint(endpoint, entity_id), json=json.dumps({**kwargs})
+        )
 
     def _base_update(self, endpoint, entity_id=None, **kwargs):
-        return self.conn.put_request(url=self._base_endpoint(endpoint, entity_id),
-                                     json=json.dumps({**kwargs}))
+        return self.conn.put_request(
+            url=self._base_endpoint(endpoint, entity_id), json=json.dumps({**kwargs})
+        )
 
     def _base_get(self, endpoint, entity_id=None, params=None):
-        return self.conn.get_request(url=self._base_endpoint(endpoint, entity_id), params=params)
+        return self.conn.get_request(
+            url=self._base_endpoint(endpoint, entity_id), params=params
+        )
 
     def _base_delete(self, endpoint, entity_id=None):
         return self.conn.delete_request(url=self._base_endpoint(endpoint, entity_id))
 
     def create_constituent(self, **kwargs):
         """
         `Args:`
             **kwargs:`
                 Fields to include, e.g., FirstName = 'Rachel'.
 
                 See the Bloomerang API docs for a full list of `fields <https://bloomerang.co/features/integrations/api/rest-api#/Constituents/post_constituent>`_. # noqa
         """
-        return self._base_create('constituent', **kwargs)
+        return self._base_create("constituent", **kwargs)
 
     def update_constituent(self, constituent_id, **kwargs):
         """
         `Args:`
             constituent_id: str or int
                 Constituent ID to update
             **kwargs:`
                 Fields to update, e.g., FirstName = 'RJ'.
 
                 See the Bloomerang API docs for a full list of `fields <https://bloomerang.co/features/integrations/api/rest-api#/Constituents/put_constituent__id_>`_. # noqa
         """
-        return self._base_update('constituent', entity_id=constituent_id, **kwargs)
+        return self._base_update("constituent", entity_id=constituent_id, **kwargs)
 
     def get_constituent(self, constituent_id):
         """
         `Args:`
             constituent_id: str or int
                 Constituent ID to get fields for
         `Returns:`
             A  JSON of the entry or an error.
         """
-        return self._base_get('constituent', entity_id=constituent_id)
+        return self._base_get("constituent", entity_id=constituent_id)
 
     def delete_constituent(self, constituent_id):
         """
         `Args:`
             constituent_id: str or int
                 Constituent ID to delete
         """
-        return self._base_delete('constituent', entity_id=constituent_id)
+        return self._base_delete("constituent", entity_id=constituent_id)
 
-    def get_constituents(self, page_number=1, page_size=50, order_by=None, order_direction=None,
-                         last_modified=None):
+    def get_constituents(
+        self,
+        page_number=1,
+        page_size=50,
+        order_by=None,
+        order_direction=None,
+        last_modified=None,
+    ):
         """
         `Args:`
             page_number: int
                 Number of the page to fetch
             page_size: int
                 Number of records per page (maximum allowed is 50)
             order_by: str
@@ -167,58 +180,60 @@
         """
         params = self._base_pagination_params(page_number, page_size)
         params.update(self._base_ordering_params(order_by, order_direction))
 
         if last_modified:
             params["lastModified"] = last_modified
 
-        response = self._base_get('constituents', params=params)
-        return Table(response['Results'])
+        response = self._base_get("constituents", params=params)
+        return Table(response["Results"])
 
     def create_transaction(self, **kwargs):
         """
         `Args:`
             **kwargs:`
                 Fields to include, e.g., CreditCardType = 'Visa'.
 
                 See the Bloomerang API docs for a full list of `fields <https://bloomerang.co/features/integrations/api/rest-api#/Transactions/post_transaction>`_. # noqa
         """
-        return self._base_create('transaction', **kwargs)
+        return self._base_create("transaction", **kwargs)
 
     def update_transaction(self, transaction_id, **kwargs):
         """
         `Args:`
             transaction_id: str or int
                 Transaction ID to update
             **kwargs:`
                 Fields to update, e.g., CreditCardType = 'Visa'.
 
                 See the Bloomerang API docs for a full list of `fields <https://bloomerang.co/features/integrations/api/rest-api#/Transactions/put_transaction__id_>`_. # noqa
         """
-        return self._base_update('transaction', entity_id=transaction_id, **kwargs)
+        return self._base_update("transaction", entity_id=transaction_id, **kwargs)
 
     def get_transaction(self, transaction_id):
         """
         `Args:`
             transaction_id: str or int
                 Transaction ID to get fields for
         `Returns:`
             A  JSON of the entry or an error.
         """
-        return self._base_get('transaction', entity_id=transaction_id)
+        return self._base_get("transaction", entity_id=transaction_id)
 
     def delete_transaction(self, transaction_id):
         """
         `Args:`
             transaction_id: str or int
                 Transaction ID to delete
         """
-        return self._base_delete('transaction', entity_id=transaction_id)
+        return self._base_delete("transaction", entity_id=transaction_id)
 
-    def get_transactions(self, page_number=1, page_size=50, order_by=None, order_direction=None):
+    def get_transactions(
+        self, page_number=1, page_size=50, order_by=None, order_direction=None
+    ):
         """
         `Args:`
             page_number: int
                 Number of the page to fetch
             page_size: int
                 Number of records per page (maximum allowed is 50)
             order_by: str
@@ -227,29 +242,30 @@
                 Sorts the order_by in ``Asc`` or ``Desc`` order (default ``Desc``).
         `Returns:`
             A  JSON of the entry or an error.
         """
         params = self._base_pagination_params(page_number, page_size)
         params.update(self._base_ordering_params(order_by, order_direction))
 
-        response = self._base_get('transactions', params=params)
-        return Table(response['Results'])
+        response = self._base_get("transactions", params=params)
+        return Table(response["Results"])
 
     def get_transaction_designation(self, designation_id):
         """
         `Args:`
             designation_id: str or int
                 Transaction Designation ID to get fields for
         `Returns:`
             A  JSON of the entry or an error.
         """
-        return self._base_get('transaction/designation', entity_id=designation_id)
+        return self._base_get("transaction/designation", entity_id=designation_id)
 
-    def get_transaction_designations(self, page_number=1, page_size=50, order_by=None,
-                                     order_direction=None):
+    def get_transaction_designations(
+        self, page_number=1, page_size=50, order_by=None, order_direction=None
+    ):
         """
         `Args:`
             page_number: int
                 Number of the page to fetch
             page_size: int
                 Number of records per page (maximum allowed is 50)
             order_by: str
@@ -258,63 +274,63 @@
                 Sorts the order_by in ``Asc`` or ``Desc`` order (default ``Desc``).
         `Returns:`
             A  JSON of the entry or an error.
         """
         params = self._base_pagination_params(page_number, page_size)
         params.update(self._base_ordering_params(order_by, order_direction))
 
-        response = self._base_get('transactions/designations', params=params)
-        return Table(response['Results'])
+        response = self._base_get("transactions/designations", params=params)
+        return Table(response["Results"])
 
     def create_interaction(self, **kwargs):
         """
         `Args:`
             **kwargs:`
                 Fields to include, e.g., Channel = "Email".
 
                 See the Bloomerang API docs for a full list of `fields <https://bloomerang.co/features/integrations/api/rest-api#/Interactions/post_interaction>`_. # noqa
         """
-        return self._base_create('interaction', **kwargs)
+        return self._base_create("interaction", **kwargs)
 
     def update_interaction(self, interaction_id, **kwargs):
         """
         `Args:`
             interaction_id: str or int
                 Interaction ID to update
             **kwargs:`
                 Fields to update, e.g., EmailAddress = "user@example.com".
 
                 See the Bloomerang API docs for a full list of `fields <https://bloomerang.co/features/integrations/api/rest-api#/Interactions/put_interaction__id_>`_. # noqa
         """
-        return self._base_update('interaction', entity_id=interaction_id, **kwargs)
+        return self._base_update("interaction", entity_id=interaction_id, **kwargs)
 
     def get_interaction(self, interaction_id):
         """
         `Args:`
             interaction_id: str or int
                 Interaction ID to get fields for
         `Returns:`
             A  JSON of the entry or an error.
         """
-        return self._base_get('interaction', entity_id=interaction_id)
+        return self._base_get("interaction", entity_id=interaction_id)
 
     def delete_interaction(self, interaction_id):
         """
         `Args:`
             interaction_id: str or int
                 Interaction ID to delete
         """
-        return self._base_delete('interaction', entity_id=interaction_id)
+        return self._base_delete("interaction", entity_id=interaction_id)
 
     def get_interactions(self, page_number=1, page_size=50):
         """
         `Args:`
             page_number: int
                 Number of the page to fetch
             page_size: int
                 Number of records per page (maximum allowed is 50)
         `Returns:`
             A  JSON of the entry or an error.
         """
         params = self._base_pagination_params(page_number, page_size)
-        response = self._base_get('interactions', params=params)
-        return Table(response['Results'])
+        response = self._base_get("interactions", params=params)
+        return Table(response["Results"])
```

### Comparing `parsons-1.0.0/parsons/bluelink/bluelink.py` & `parsons-1.1.0/parsons/bluelink/bluelink.py`

 * *Files 10% similar despite different names*

```diff
@@ -2,37 +2,40 @@
 from parsons.utilities import check_env
 from parsons.bluelink.person import BluelinkPerson
 import logging
 import json
 
 logger = logging.getLogger(__name__)
 
-API_URL = 'https://api.bluelink.org/webhooks/'
+API_URL = "https://api.bluelink.org/webhooks/"
 
 
 class Bluelink:
     """
     Instantiate a Bluelink connector.
     Allows for a simple method of inserting person data to Bluelink via a webhook.
     # see: https://bluelinkdata.github.io/docs/BluelinkApiGuide#webhook
 
     `Args:`:
         user: str
             Bluelink webhook user name.
         password: str
             Bluelink webhook password.
     """
+
     def __init__(self, user=None, password=None):
-        self.user = check_env.check('BLUELINK_WEBHOOK_USER', user)
-        self.password = check_env.check('BLUELINK_WEBHOOK_PASSWORD', password)
+        self.user = check_env.check("BLUELINK_WEBHOOK_USER", user)
+        self.password = check_env.check("BLUELINK_WEBHOOK_PASSWORD", password)
         self.headers = {
             "Content-Type": "application/json",
         }
         self.api_url = API_URL
-        self.api = APIConnector(self.api_url, auth=(self.user, self.password), headers=self.headers)
+        self.api = APIConnector(
+            self.api_url, auth=(self.user, self.password), headers=self.headers
+        )
 
     def upsert_person(self, source, person=None):
         """
         Upsert a BluelinkPerson object into Bluelink.
         Rows will update, as opposed to being inserted, if an existing person record in
         Bluelink has a matching BluelinkIdentifier (same source and id) as the BluelinkPerson object
         passed into this function.
@@ -44,20 +47,19 @@
             person: BluelinkPerson
                 A BluelinkPerson object.
                 Will be inserted to Bluelink, or updated if a matching record is found.
         `Returns:`
             int
             An http status code from the http post request to the Bluelink webhook.
         """
-        data = {
-            'source': source,
-            'person': person
-        }
-        jdata = json.dumps(data,
-                           default=lambda o: {k: v for k, v in o.__dict__.items() if v is not None})
+        data = {"source": source, "person": person}
+        jdata = json.dumps(
+            data,
+            default=lambda o: {k: v for k, v in o.__dict__.items() if v is not None},
+        )
         resp = self.api.post_request(url=self.api_url, data=jdata)
         return resp
 
     def bulk_upsert_person(self, source, tbl, row_to_person):
         """
         Upsert all rows into Bluelink, using the row_to_person function to
         transform rows to BluelinkPerson objects.
```

### Comparing `parsons-1.0.0/parsons/bluelink/person.py` & `parsons-1.1.0/parsons/bluelink/person.py`

 * *Files 3% similar despite different names*

```diff
@@ -35,21 +35,37 @@
         scores: list[BluelinkScore]
             List of BluelinkScore objects. Scores are numeric scores, ie partisanship model.
         birthdate: str
             ISO 8601 formatted birth date: 'YYYY-MM-DD'
         details: dict
             additional custom data. must be json serializable.
     """
-    def __init__(self, identifiers, given_name=None, family_name=None, phones=None, emails=None,
-                 addresses=None, tags=None, employer=None, employer_address=None,
-                 occupation=None, scores=None, birthdate=None, details=None):
+
+    def __init__(
+        self,
+        identifiers,
+        given_name=None,
+        family_name=None,
+        phones=None,
+        emails=None,
+        addresses=None,
+        tags=None,
+        employer=None,
+        employer_address=None,
+        occupation=None,
+        scores=None,
+        birthdate=None,
+        details=None,
+    ):
 
         if not identifiers:
-            raise Exception("BluelinkPerson requires list of BluelinkIdentifiers with "
-                            "at least 1 BluelinkIdentifier")
+            raise Exception(
+                "BluelinkPerson requires list of BluelinkIdentifiers with "
+                "at least 1 BluelinkIdentifier"
+            )
 
         self.identifiers = identifiers
         self.addresses = addresses
         self.emails = emails
         self.phones = phones
         self.tags = tags
         self.scores = scores
@@ -107,14 +123,15 @@
             source (unlike identifier) is case insensitive.
             examples: BLUELINK, PDI, SALESFORCE, VAN:myCampaign, VAN:myVoters
         identifier: str
             Case-sensitive ID in the external system.
         details: dict
             dictionary of custom fields. must be serializable to json.
     """
+
     def __init__(self, source, identifier, details=None):
         self.source = source
         self.identifier = identifier
         self.details = details
 
 
 class BluelinkEmail(object):
@@ -127,14 +144,15 @@
         primary: bool
             True if this is known to be the primary email.
         type: str
             Type, eg: "personal", "work"
         status: str
             One of "Potential", "Subscribed", "Unsubscribed", "Bouncing", or "Spam Complaints"
     """
+
     def __init__(self, address, primary=None, type=None, status=None):
         self.address = address
         self.primary = primary
         self.type = type
         self.status = status
 
 
@@ -156,18 +174,26 @@
         type: str
             The type. ie: "home", "mailing".
         venue: str
             The venue name, if relevant.
         status: str
             A value representing the status of the address. "Potential", "Verified" or "Bad"
     """
-    def __init__(self,
-                 address_lines=None,
-                 city=None, state=None, postal_code=None, country=None,
-                 type=None, venue=None, status=None):
+
+    def __init__(
+        self,
+        address_lines=None,
+        city=None,
+        state=None,
+        postal_code=None,
+        country=None,
+        type=None,
+        venue=None,
+        status=None,
+    ):
 
         self.address_lines = address_lines or []
         self.city = city
         self.state = state
         self.postal_code = postal_code
         self.country = country
 
@@ -194,16 +220,26 @@
         sms_capable: bool
             True if this number can accept SMS.
         do_not_call: bool
             True if this number is on the US FCC Do Not Call Registry.
         details: dict
             Additional data dictionary. Must be json serializable.
     """
-    def __init__(self, number, primary=None, description=None, type=None, country=None,
-                 sms_capable=None, do_not_call=None, details=None):
+
+    def __init__(
+        self,
+        number,
+        primary=None,
+        description=None,
+        type=None,
+        country=None,
+        sms_capable=None,
+        do_not_call=None,
+        details=None,
+    ):
         self.number = number
         self.primary = primary
         self.description = description
         self.type = type
         self.country = country
         self.sms_capable = sms_capable
         self.do_not_call = do_not_call
@@ -215,14 +251,15 @@
     Instantiate a BluelinkTag object.
 
     `Args:`
         tag: str
             A tag string; convention is either a simple string
             or a string with a prefix separated by a colon, e.g., DONOR:GRASSROOTS
     """
+
     def __init__(self, tag):
         self.tag = tag
 
 
 class BluelinkScore(object):
     """
     Instantiate a score object.
@@ -232,11 +269,12 @@
         score: float
             Numeric score.
         score_type: str
             Type, eg: "Partisanship model".
         source: str
             Original source of this score.
     """
+
     def __init__(self, score, score_type, source):
         self.score = score
         self.score_type = score_type
         self.source = source
```

### Comparing `parsons-1.0.0/parsons/box/box.py` & `parsons-1.1.0/parsons/box/box.py`

 * *Files 5% similar despite different names*

```diff
@@ -25,15 +25,15 @@
 from parsons.utilities.check_env import check as check_env
 from parsons.utilities.files import create_temp_file, create_temp_file_for_path
 
 import tempfile
 
 logger = logging.getLogger(__name__)
 
-DEFAULT_FOLDER_ID = '0'
+DEFAULT_FOLDER_ID = "0"
 
 
 class Box(object):
     """Box is a file storage provider.
 
     `Args:`
         client_id: str
@@ -52,50 +52,50 @@
 
     *NOTE*: All path-based methods in this class use an intermediate
     method that looks up the relevant folder/file id by successive API
     calls. This can be slow for long paths or intermediate folders
     that contain many items. If performance is an issue, please use the
     corresponding folder_id/file_id methods for each function.
     """
+
     # In what formats can we upload/save Tables to Box? For now csv and JSON.
-    ALLOWED_FILE_FORMATS = ['csv', 'json']
+    ALLOWED_FILE_FORMATS = ["csv", "json"]
 
     def __init__(self, client_id=None, client_secret=None, access_token=None):
-        client_id = check_env('BOX_CLIENT_ID', client_id)
-        client_secret = check_env('BOX_CLIENT_SECRET', client_secret)
-        access_token = check_env('BOX_ACCESS_TOKEN', access_token)
+        client_id = check_env("BOX_CLIENT_ID", client_id)
+        client_secret = check_env("BOX_CLIENT_SECRET", client_secret)
+        access_token = check_env("BOX_ACCESS_TOKEN", access_token)
 
         oauth = boxsdk.OAuth2(
-            client_id=client_id,
-            client_secret=client_secret,
-            access_token=access_token
+            client_id=client_id, client_secret=client_secret, access_token=access_token
         )
         self.client = boxsdk.Client(oauth)
 
     def create_folder(self, path) -> str:
         """Create a Box folder.
 
         `Args`:
             path: str
                Path to the folder to be created. If no slashes are present,
                path will be name of folder created in the default folder.
 
         `Returns`:
             str: The Box id of the newly-created folder.
         """
-        if '/' in path:
-            parent_folder_path, folder_name = path.rsplit(sep='/', maxsplit=1)
+        if "/" in path:
+            parent_folder_path, folder_name = path.rsplit(sep="/", maxsplit=1)
             parent_folder_id = self.get_item_id(path=parent_folder_path)
         else:
             folder_name = path
             parent_folder_id = DEFAULT_FOLDER_ID
         return self.create_folder_by_id(folder_name, parent_folder_id=parent_folder_id)
 
-    def create_folder_by_id(self, folder_name,
-                            parent_folder_id=DEFAULT_FOLDER_ID) -> str:
+    def create_folder_by_id(
+        self, folder_name, parent_folder_id=DEFAULT_FOLDER_ID
+    ) -> str:
         """Create a Box folder.
 
         `Args`:
             folder_name: str
                The name to give to the new folder.
             parent_folder_id: str
                Folder id of the parent folder in which to create the new folder. If
@@ -141,15 +141,15 @@
 
         `Args`:
             file_id: str
               The Box id of the file to delete.
         """
         self.client.file(file_id=file_id).delete()
 
-    def list(self, path='', item_type=None) -> Table:
+    def list(self, path="", item_type=None) -> Table:
         """Return a Table of Box files and/or folders found at a path.
 
         `Args`:
             path:str
                If specified, the slash-separated path of the folder to be listed.
                If omitted, the default folder will be used.
             item_type: str
@@ -162,72 +162,74 @@
         if path:
             folder_id = self.get_item_id(path)
         else:
             folder_id = DEFAULT_FOLDER_ID
         return self.list_items_by_id(folder_id=folder_id, item_type=item_type)
 
     def list_items_by_id(self, folder_id=DEFAULT_FOLDER_ID, item_type=None) -> Table:
-        url = 'https://api.box.com/2.0/folders/' + folder_id
-        json_response = self.client.make_request('GET', url)
+        url = "https://api.box.com/2.0/folders/" + folder_id
+        json_response = self.client.make_request("GET", url)
 
-        items = Table(json_response.json()['item_collection']['entries'])
+        items = Table(json_response.json()["item_collection"]["entries"])
         if item_type:
             items = items.select_rows(lambda row: row.type == item_type)
         return items
 
     def list_files_by_id(self, folder_id=DEFAULT_FOLDER_ID) -> Table:
         """List all Box files in a folder.
 
         `Args`:
             folder_id: str
                The Box id of the folder in which to search. If omitted,
                search in the default folder.
         `Returns`: Table
             A Parsons table of files and their attributes.
         """
-        return self.list_items_by_id(folder_id=folder_id, item_type='file')
+        return self.list_items_by_id(folder_id=folder_id, item_type="file")
 
     def list_folders_by_id(self, folder_id=DEFAULT_FOLDER_ID) -> Table:
         """List all Box folders.
 
         `Args`:
             folder_id: str
                The Box id of the folder in which to search. If omitted,
                search in the default folder.
         `Returns`: Table
             A Parsons table of folders and their attributes.
         """
-        return self.list_items_by_id(folder_id=folder_id, item_type='folder')
+        return self.list_items_by_id(folder_id=folder_id, item_type="folder")
 
-    def upload_table(self, table, path='', format='csv') -> boxsdk.object.file.File:
+    def upload_table(self, table, path="", format="csv") -> boxsdk.object.file.File:
         """Save the passed table to Box.
 
         `Args`:
             table:Table
                The Parsons table to be saved.
             path: str
                Optionally, file path to filename where table should be saved.
             format: str
                For now, only 'csv' and 'json'; format in which to save table.
 
         `Returns`: BoxFile
             A Box File object
         """
-        if '/' in path:
-            folder_path, file_name = path.rsplit(sep='/', maxsplit=1)
+        if "/" in path:
+            folder_path, file_name = path.rsplit(sep="/", maxsplit=1)
             folder_id = self.get_item_id(path=folder_path)
         else:  # pragma: no cover
             file_name = path
             folder_id = DEFAULT_FOLDER_ID
 
-        return self.upload_table_to_folder_id(table=table, file_name=file_name,
-                                              folder_id=folder_id, format=format)
+        return self.upload_table_to_folder_id(
+            table=table, file_name=file_name, folder_id=folder_id, format=format
+        )
 
-    def upload_table_to_folder_id(self, table, file_name, folder_id=DEFAULT_FOLDER_ID,
-                                  format='csv') -> boxsdk.object.file.File:
+    def upload_table_to_folder_id(
+        self, table, file_name, folder_id=DEFAULT_FOLDER_ID, format="csv"
+    ) -> boxsdk.object.file.File:
         """Save the passed table to Box.
 
         `Args`:
             table:Table
                The Parsons table to be saved.
             file_name: str
                The filename under which it should be saved in Box.
@@ -237,31 +239,35 @@
                For now, only 'csv' and 'json'; format in which to save table.
 
         `Returns`: BoxFile
             A Box File object
         """
 
         if format not in self.ALLOWED_FILE_FORMATS:
-            raise ValueError(f'Format argument to upload_table() must be in one '
-                             f'of {self.ALLOWED_FILE_FORMATS}; found "{format}"')
+            raise ValueError(
+                f"Format argument to upload_table() must be in one "
+                f'of {self.ALLOWED_FILE_FORMATS}; found "{format}"'
+            )
 
         # Create a temp directory in which we will let Parsons create a
         # file. Both will go away automatically when we leave scope.
         with tempfile.TemporaryDirectory() as temp_dir_name:
-            temp_file_path = temp_dir_name + '/table.tmp'
-            if format == 'csv':
+            temp_file_path = temp_dir_name + "/table.tmp"
+            if format == "csv":
                 table.to_csv(local_path=temp_file_path)
-            elif format == 'json':
+            elif format == "json":
                 table.to_json(local_path=temp_file_path)
             else:
-                raise SystemError(f'Got (theoretically) impossible '
-                                  f'format option "{format}"')  # pragma: no cover
-
-            new_file = self.client.folder(folder_id).upload(file_path=temp_file_path,
-                                                            file_name=file_name)
+                raise SystemError(
+                    f"Got (theoretically) impossible " f'format option "{format}"'
+                )  # pragma: no cover
+
+            new_file = self.client.folder(folder_id).upload(
+                file_path=temp_file_path, file_name=file_name
+            )
         return new_file
 
     def download_file(self, path: str, local_path: str = None) -> str:
         """Download a Box object to a local file.
 
         `Args`:
             path: str
@@ -279,63 +285,66 @@
         if not local_path:
             # Temp file will be around as long as enclosing process is running,
             # which we need, because the Table we return will continue to use it.
             local_path = create_temp_file_for_path(path)
 
         file_id = self.get_item_id(path)
 
-        with open(local_path, 'wb') as output_file:
+        with open(local_path, "wb") as output_file:
             self.client.file(file_id).download_to(output_file)
 
         return local_path
 
-    def get_table(self, path, format='csv') -> Table:
+    def get_table(self, path, format="csv") -> Table:
         """Get a table that has been saved to Box in csv or JSON format.
 
         `Args`:
             path: str
                 The slash-separated path to the file containing the table.
             format: str
                  Format in which Table has been saved; for now, only 'csv' or 'json'.
 
         `Returns`: Table
             A Parsons Table.
         """
         file_id = self.get_item_id(path)
         return self.get_table_by_file_id(file_id=file_id, format=format)
 
-    def get_table_by_file_id(self, file_id, format='csv') -> Table:
+    def get_table_by_file_id(self, file_id, format="csv") -> Table:
         """Get a table that has been saved to Box in csv or JSON format.
 
         `Args`:
             file_id: str
                 The Box file_id of the table to be retrieved.
             format: str
                  Format in which Table has been saved; for now, only 'csv' or 'json'.
 
         `Returns`: Table
             A Parsons Table.
         """
         if format not in self.ALLOWED_FILE_FORMATS:
-            raise ValueError(f'Format argument to upload_table() must be in one '
-                             f'of {self.ALLOWED_FILE_FORMATS}; found "{format}"')
+            raise ValueError(
+                f"Format argument to upload_table() must be in one "
+                f'of {self.ALLOWED_FILE_FORMATS}; found "{format}"'
+            )
 
         # Temp file will be around as long as enclosing process is running,
         # which we need, because the Table we return will continue to use it.
         output_file_name = create_temp_file()
-        with open(output_file_name, 'wb') as output_file:
+        with open(output_file_name, "wb") as output_file:
             self.client.file(file_id).download_to(output_file)
 
-        if format == 'csv':
+        if format == "csv":
             return Table.from_csv(output_file_name)
-        elif format == 'json':
+        elif format == "json":
             return Table.from_json(output_file_name)
         else:
-            raise SystemError(f'Got (theoretically) impossible '
-                              f'format option "{format}"')  # pragma: no cover
+            raise SystemError(
+                f"Got (theoretically) impossible " f'format option "{format}"'
+            )  # pragma: no cover
 
     def get_item_id(self, path, base_folder_id=DEFAULT_FOLDER_ID) -> str:
         """Given a path-like object, try to return the id for the file or
         folder at the end of the path.
 
         *NOTE*: This method makes one API call for each level in
         `path`, so can be slow for long paths or intermediate folders
@@ -352,22 +361,22 @@
         `Returns`: Table
             A Parsons Table.
 
         """
         try:
             # Grab the leftmost element in the path - this is what we're
             # looking for in this folder.
-            if '/' in path:
-                this_element, path = path.split(sep='/', maxsplit=1)
-                if path == '':
+            if "/" in path:
+                this_element, path = path.split(sep="/", maxsplit=1)
+                if path == "":
                     raise ValueError('Illegal trailing "/" in file path')
 
             else:
                 this_element = path
-                path = ''
+                path = ""
 
             # Look in our current base_folder for an item whose name matches the
             # current element. If we're at initial, non-recursed call, base_folder
             # will be default folder.
             item_id = None
             for item in self.client.folder(folder_id=base_folder_id).get_items():
                 if item.name == this_element:
@@ -379,15 +388,15 @@
 
             # If there are no more elements left in path, this is the item we're after.
             if not len(path):
                 return item_id
 
             # If there *are* more elements in the path, we need to check that this item is
             # in fact a folder so we can recurse and search inside it.
-            if item.type != 'folder':
+            if item.type != "folder":
                 raise ValueError(f'Invalid folder "{this_element}"')
 
             return self.get_item_id(path=path, base_folder_id=item_id)
 
         # At the top level of the recursion, when we have the entire path, we want
         # to attach it to the error message. If caught at some inner level of the
         # recursion, just pass it on up.
```

### Comparing `parsons-1.0.0/parsons/capitol_canary/capitol_canary.py` & `parsons-1.1.0/parsons/capitol_canary/capitol_canary.py`

 * *Files 12% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 from parsons.utilities import check_env
 from parsons.utilities.api_connector import APIConnector
 from parsons.utilities.datetime import date_to_timestamp
 import logging
 
 logger = logging.getLogger(__name__)
 
-CAPITOL_CANARY_URI = 'https://api.phone2action.com/2.0/'
+CAPITOL_CANARY_URI = "https://api.phone2action.com/2.0/"
 
 
 class CapitolCanary(object):
     """
     Instantiate CapitolCanary Class
 
     `Args:`
@@ -23,44 +23,46 @@
             env variable set.
     `Returns:`
         CapitolCanary Class
     """
 
     def __init__(self, app_id=None, app_key=None):
         # check first for CapitolCanary branded app key and ID
-        cc_app_id = check_env.check('CAPITOLCANARY_APP_ID', None, optional=True)
-        cc_app_key = check_env.check('CAPITOLCANARY_APP_KEY', None, optional=True)
+        cc_app_id = check_env.check("CAPITOLCANARY_APP_ID", None, optional=True)
+        cc_app_key = check_env.check("CAPITOLCANARY_APP_KEY", None, optional=True)
 
-        self.app_id = cc_app_id or check_env.check('PHONE2ACTION_APP_ID', app_id)
-        self.app_key = cc_app_key or check_env.check('PHONE2ACTION_APP_KEY', app_key)
+        self.app_id = cc_app_id or check_env.check("PHONE2ACTION_APP_ID", app_id)
+        self.app_key = cc_app_key or check_env.check("PHONE2ACTION_APP_KEY", app_key)
         self.auth = HTTPBasicAuth(self.app_id, self.app_key)
         self.client = APIConnector(CAPITOL_CANARY_URI, auth=self.auth)
 
     def _paginate_request(self, url, args=None, page=None):
         # Internal pagination method
 
         if page is not None:
-            args['page'] = page
+            args["page"] = page
 
         r = self.client.get_request(url, params=args)
 
-        json = r['data']
+        json = r["data"]
 
         if page is not None:
             return json
 
         # If count of items is less than the total allowed per page, paginate
-        while r['pagination']['count'] == r['pagination']['per_page']:
+        while r["pagination"]["count"] == r["pagination"]["per_page"]:
 
-            r = self.client.get_request(r['pagination']['next_url'], args)
-            json.extend(r['data'])
+            r = self.client.get_request(r["pagination"]["next_url"], args)
+            json.extend(r["data"])
 
         return json
 
-    def get_advocates(self, state=None, campaign_id=None, updated_since=None, page=None):
+    def get_advocates(
+        self, state=None, campaign_id=None, updated_since=None, page=None
+    ):
         """
         Return advocates (person records).
 
         If no page is specified, the method will automatically paginate through the available
         advocates.
 
         `Args:`
@@ -85,56 +87,64 @@
                 * fields
                 * advocates
         """
 
         # Convert the passed in updated_since into a Unix timestamp (which is what the API wants)
         updated_since = date_to_timestamp(updated_since)
 
-        args = {'state': state,
-                'campaignid': campaign_id,
-                'updatedSince': updated_since}
+        args = {
+            "state": state,
+            "campaignid": campaign_id,
+            "updatedSince": updated_since,
+        }
 
-        logger.info('Retrieving advocates...')
-        json = self._paginate_request('advocates', args=args, page=page)
+        logger.info("Retrieving advocates...")
+        json = self._paginate_request("advocates", args=args, page=page)
 
         return self._advocates_tables(Table(json))
 
     def _advocates_tables(self, tbl):
         # Convert the advocates nested table into multiple tables
 
         tbls = {
-            'advocates': tbl,
-            'emails': Table(),
-            'phones': Table(),
-            'memberships': Table(),
-            'tags': Table(),
-            'ids': Table(),
-            'fields': Table(),
+            "advocates": tbl,
+            "emails": Table(),
+            "phones": Table(),
+            "memberships": Table(),
+            "tags": Table(),
+            "ids": Table(),
+            "fields": Table(),
         }
 
         if not tbl:
             return tbls
 
-        logger.info(f'Retrieved {tbl.num_rows} advocates...')
+        logger.info(f"Retrieved {tbl.num_rows} advocates...")
 
         # Unpack all of the single objects
         # The CapitolCanary API docs says that created_at and updated_at are dictionaries, but
         # the data returned from the server is a ISO8601 timestamp. - EHS, 05/21/2020
-        for c in ['address', 'districts']:
+        for c in ["address", "districts"]:
             tbl.unpack_dict(c)
 
         # Unpack all of the arrays
-        child_tables = [child for child in tbls.keys() if child != 'advocates']
+        child_tables = [child for child in tbls.keys() if child != "advocates"]
         for c in child_tables:
-            tbls[c] = tbl.long_table(['id'], c, key_rename={'id': 'advocate_id'})
+            tbls[c] = tbl.long_table(["id"], c, key_rename={"id": "advocate_id"})
 
         return tbls
 
-    def get_campaigns(self, state=None, zip=None, include_generic=False, include_private=False,
-                      include_content=True):
+    def get_campaigns(
+        self,
+        state=None,
+        zip=None,
+        include_generic=False,
+        include_private=False,
+        include_content=True,
+    ):
         """
         Returns a list of campaigns
 
         `Args:`
             state: str
                 Filter by US postal abbreviation for a state or territory e.g., "CA" "NY" or "DC"
             zip: int
@@ -147,44 +157,47 @@
                 If true, include campaign content fields, which may vary. This may cause
                 sync errors.
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        args = {'state': state,
-                'zip': zip,
-                'includeGeneric': str(include_generic),
-                'includePrivate': str(include_private)
-                }
+        args = {
+            "state": state,
+            "zip": zip,
+            "includeGeneric": str(include_generic),
+            "includePrivate": str(include_private),
+        }
 
-        tbl = Table(self.client.get_request('campaigns', params=args))
+        tbl = Table(self.client.get_request("campaigns", params=args))
         if tbl:
-            tbl.unpack_dict('updated_at')
+            tbl.unpack_dict("updated_at")
             if include_content:
-                tbl.unpack_dict('content')
+                tbl.unpack_dict("content")
 
         return tbl
 
-    def create_advocate(self,
-                        campaigns,
-                        first_name=None,
-                        last_name=None,
-                        email=None,
-                        phone=None,
-                        address1=None,
-                        address2=None,
-                        city=None,
-                        state=None,
-                        zip5=None,
-                        sms_optin=None,
-                        email_optin=None,
-                        sms_optout=None,
-                        email_optout=None,
-                        **kwargs):
+    def create_advocate(
+        self,
+        campaigns,
+        first_name=None,
+        last_name=None,
+        email=None,
+        phone=None,
+        address1=None,
+        address2=None,
+        city=None,
+        state=None,
+        zip5=None,
+        sms_optin=None,
+        email_optin=None,
+        sms_optout=None,
+        email_optout=None,
+        **kwargs,
+    ):
         """
         Create an advocate.
 
         If you want to opt an advocate into or out of SMS / email campaigns, you must provide
         the email address or phone number (accordingly).
 
         The list of arguments only partially covers the fields that can be set on the advocate.
@@ -235,75 +248,77 @@
             The int ID of the created advocate.
         """
 
         # Validate the passed in arguments
 
         if not campaigns:
             raise ValueError(
-                'When creating an advocate, you must specify one or more campaigns.')
+                "When creating an advocate, you must specify one or more campaigns."
+            )
 
         if not email and not phone:
             raise ValueError(
-                'When creating an advocate, you must provide an email address or a phone number.')
+                "When creating an advocate, you must provide an email address or a phone number."
+            )
 
         if (sms_optin or sms_optout) and not phone:
             raise ValueError(
-                'When opting an advocate in or out of SMS messages, you must specify a valid '
-                'phone and one or more campaigns')
+                "When opting an advocate in or out of SMS messages, you must specify a valid "
+                "phone and one or more campaigns"
+            )
 
         if (email_optin or email_optout) and not email:
             raise ValueError(
-                'When opting an advocate in or out of email messages, you must specify a valid '
-                'email address and one or more campaigns')
+                "When opting an advocate in or out of email messages, you must specify a valid "
+                "email address and one or more campaigns"
+            )
 
         # Align our arguments with the expected parameters for the API
         payload = {
-            'email': email,
-            'phone': phone,
-            'firstname': first_name,
-            'lastname': last_name,
-            'address1': address1,
-            'address2': address2,
-            'city': city,
-            'state': state,
-            'zip5': zip5,
-            'smsOptin': 1 if sms_optin else None,
-            'emailOptin': 1 if email_optin else None,
-            'smsOptout': 1 if sms_optout else None,
-            'emailOptout': 1 if email_optout else None,
+            "email": email,
+            "phone": phone,
+            "firstname": first_name,
+            "lastname": last_name,
+            "address1": address1,
+            "address2": address2,
+            "city": city,
+            "state": state,
+            "zip5": zip5,
+            "smsOptin": 1 if sms_optin else None,
+            "emailOptin": 1 if email_optin else None,
+            "smsOptout": 1 if sms_optout else None,
+            "emailOptout": 1 if email_optout else None,
         }
 
         # Clean up any keys that have a "None" value
-        payload = {
-            key: val
-            for key, val in payload.items()
-            if val is not None
-        }
+        payload = {key: val for key, val in payload.items() if val is not None}
 
         # Merge in any kwargs
         payload.update(kwargs)
 
         # Turn into a list of items so we can append multiple campaigns
-        campaign_keys = [('campaigns[]', val) for val in campaigns]
+        campaign_keys = [("campaigns[]", val) for val in campaigns]
         data = [(key, value) for key, value in payload.items()] + campaign_keys
 
         # Call into the CapitolCanary API
-        response = self.client.post_request('advocates', data=data)
-        return response['advocateid']
+        response = self.client.post_request("advocates", data=data)
+        return response["advocateid"]
 
-    def update_advocate(self,
-                        advocate_id,
-                        campaigns=None,
-                        email=None,
-                        phone=None,
-                        sms_optin=None,
-                        email_optin=None,
-                        sms_optout=None,
-                        email_optout=None,
-                        **kwargs):
+    def update_advocate(
+        self,
+        advocate_id,
+        campaigns=None,
+        email=None,
+        phone=None,
+        sms_optin=None,
+        email_optin=None,
+        sms_optout=None,
+        email_optout=None,
+        **kwargs,
+    ):
         """
         Update the fields of an advocate.
 
         If you want to opt an advocate into or out of SMS / email campaigns, you must provide
         the email address or phone number along with a list of campaigns.
 
         The list of arguments only partially covers the fields that can be updated on the advocate.
@@ -337,47 +352,45 @@
             **kwargs:
                 Additional fields on the advocate to update
         """
 
         # Validate the passed in arguments
         if (sms_optin or sms_optout) and not (phone and campaigns):
             raise ValueError(
-                'When opting an advocate in or out of SMS messages, you must specify a valid '
-                'phone and one or more campaigns')
+                "When opting an advocate in or out of SMS messages, you must specify a valid "
+                "phone and one or more campaigns"
+            )
 
         if (email_optin or email_optout) and not (email and campaigns):
             raise ValueError(
-                'When opting an advocate in or out of email messages, you must specify a valid '
-                'email address and one or more campaigns')
+                "When opting an advocate in or out of email messages, you must specify a valid "
+                "email address and one or more campaigns"
+            )
 
         # Align our arguments with the expected parameters for the API
         payload = {
-            'advocateid': advocate_id,
-            'campaigns': campaigns,
-            'email': email,
-            'phone': phone,
-            'smsOptin': 1 if sms_optin else None,
-            'emailOptin': 1 if email_optin else None,
-            'smsOptout': 1 if sms_optout else None,
-            'emailOptout': 1 if email_optout else None,
+            "advocateid": advocate_id,
+            "campaigns": campaigns,
+            "email": email,
+            "phone": phone,
+            "smsOptin": 1 if sms_optin else None,
+            "emailOptin": 1 if email_optin else None,
+            "smsOptout": 1 if sms_optout else None,
+            "emailOptout": 1 if email_optout else None,
             # remap first_name / last_name to be consistent with updated_advocates
-            'firstname': kwargs.pop('first_name', None),
-            'lastname': kwargs.pop('last_name', None),
+            "firstname": kwargs.pop("first_name", None),
+            "lastname": kwargs.pop("last_name", None),
         }
 
         # Clean up any keys that have a "None" value
-        payload = {
-            key: val
-            for key, val in payload.items()
-            if val is not None
-        }
+        payload = {key: val for key, val in payload.items() if val is not None}
 
         # Merge in any kwargs
         payload.update(kwargs)
 
         # Turn into a list of items so we can append multiple campaigns
         campaigns = campaigns or []
-        campaign_keys = [('campaigns[]', val) for val in campaigns]
+        campaign_keys = [("campaigns[]", val) for val in campaigns]
         data = [(key, value) for key, value in payload.items()] + campaign_keys
 
         # Call into the CapitolCanary API
-        self.client.post_request('advocates', data=data)
+        self.client.post_request("advocates", data=data)
```

### Comparing `parsons-1.0.0/parsons/civis/civisclient.py` & `parsons-1.1.0/parsons/civis/civisclient.py`

 * *Files 8% similar despite different names*

```diff
@@ -17,24 +17,26 @@
             Option settings for the client that are `described in the documentation <https://civis-python.readthedocs.io/en/stable/client.html#civis.APIClient>`_.
     `Returns:`
         Civis class
     """  # noqa: E501
 
     def __init__(self, db=None, api_key=None, **kwargs):
 
-        self.db = check_env.check('CIVIS_DATABASE', db)
-        self.api_key = check_env.check('CIVIS_API_KEY', api_key)
+        self.db = check_env.check("CIVIS_DATABASE", db)
+        self.api_key = check_env.check("CIVIS_API_KEY", api_key)
         self.client = civis.APIClient(api_key=api_key, **kwargs)
         """
         The Civis API client. Utilize this attribute to access to lower level and more
         advanced methods which might not be surfaced in Parsons. A list of the methods
         can be found by reading the Civis API client `documentation <https://civis-python.readthedocs.io/en/stable/client.html>`_.
         """  # noqa: E501
 
-    def query(self, sql, preview_rows=10, polling_interval=None, hidden=True, wait=True):
+    def query(
+        self, sql, preview_rows=10, polling_interval=None, hidden=True, wait=True
+    ):
         """
         Execute a SQL statement as a Civis query.
 
         Run a query that may return no results or where only a small
         preview is required. To execute a query that returns a large number
         of rows, see :func:`~civis.io.read_civis_sql`.
 
@@ -52,32 +54,47 @@
                 If ``True``, will wait for query to finish executing before exiting
                 the method. If ``False``, returns the future object.
         `Returns`
             Parsons Table or ``civis.CivisFuture``
                 See :ref:`parsons-table` for output options.
         """
 
-        fut = civis.io.query_civis(sql, self.db, preview_rows=preview_rows,
-                                   polling_interval=polling_interval, hidden=hidden)
+        fut = civis.io.query_civis(
+            sql,
+            self.db,
+            preview_rows=preview_rows,
+            polling_interval=polling_interval,
+            hidden=hidden,
+        )
 
         if not wait:
             return fut
 
         result = fut.result()
 
-        if result['result_rows'] is None:
+        if result["result_rows"] is None:
             return None
 
-        result['result_rows'].insert(0, result['result_columns'])
+        result["result_rows"].insert(0, result["result_columns"])
 
-        return Table(result['result_rows'])
+        return Table(result["result_rows"])
 
-    def table_import(self, table_obj, table, max_errors=None,
-                     existing_table_rows='fail', diststyle=None, distkey=None,
-                     sortkey1=None, sortkey2=None, wait=True, **civisargs):
+    def table_import(
+        self,
+        table_obj,
+        table,
+        max_errors=None,
+        existing_table_rows="fail",
+        diststyle=None,
+        distkey=None,
+        sortkey1=None,
+        sortkey2=None,
+        wait=True,
+        **civisargs
+    ):
         """
         Write the table to a Civis Redshift cluster. Additional key word
         arguments can passed to `civis.io.dataframe_to_civis()  <https://civis-python.readthedocs.io/en/v1.9.0/generated/civis.io.dataframe_to_civis.html#civis.io.dataframe_to_civis>`_ # noqa: E501
 
         `Args`
             table_obj: obj
                 A Parsons Table object
@@ -103,18 +120,25 @@
             wait: boolean
                 Wait for write job to complete before exiting method. If ``False``, returns
                 the future object.
         `Returns`
             ``None`` or ``civis.CivisFuture``
         """  # noqa: E501,E261
 
-        fut = civis.io.dataframe_to_civis(table_obj.to_dataframe(), database=self.db,
-                                          table=table, max_errors=max_errors,
-                                          existing_table_rows=existing_table_rows,
-                                          diststyle=diststyle, distkey=distkey,
-                                          sortkey1=sortkey1, sortkey2=sortkey2,
-                                          headers=True, **civisargs)
+        fut = civis.io.dataframe_to_civis(
+            table_obj.to_dataframe(),
+            database=self.db,
+            table=table,
+            max_errors=max_errors,
+            existing_table_rows=existing_table_rows,
+            diststyle=diststyle,
+            distkey=distkey,
+            sortkey1=sortkey1,
+            sortkey2=sortkey2,
+            headers=True,
+            **civisargs
+        )
 
         if wait:
             return fut.result()
 
         return fut
```

### Comparing `parsons-1.0.0/parsons/controlshift/controlshift.py` & `parsons-1.1.0/parsons/controlshift/controlshift.py`

 * *Files 5% similar despite different names*

```diff
@@ -22,39 +22,40 @@
             ``CONTROLSHIFT_CLIENT_SECRET`` env variable is set.
     `Returns:`
         Controlshift Class
     """
 
     def __init__(self, hostname=None, client_id=None, client_secret=None):
 
-        self.hostname = check_env.check('CONTROLSHIFT_HOSTNAME', hostname)
-        token_url = f'{self.hostname}/oauth/token'
+        self.hostname = check_env.check("CONTROLSHIFT_HOSTNAME", hostname)
+        token_url = f"{self.hostname}/oauth/token"
         self.client = OAuth2APIConnector(
             self.hostname,
-            client_id=check_env.check('CONTROLSHIFT_CLIENT_ID', client_id),
-            client_secret=check_env.check('CONTROLSHIFT_CLIENT_SECRET', client_secret),
+            client_id=check_env.check("CONTROLSHIFT_CLIENT_ID", client_id),
+            client_secret=check_env.check("CONTROLSHIFT_CLIENT_SECRET", client_secret),
             token_url=token_url,
-            auto_refresh_url=token_url
+            auto_refresh_url=token_url,
         )
 
     def get_petitions(self) -> Table:
         """
         Get a full list of all petitions, including ones that are unlaunched or otherwise not
         visible to the public.
 
         `Return:`
             Table Class
         """
         next_page = 1
         petitions = []
         while next_page:
             response = self.client.get_request(
-                f'{self.hostname}/api/v1/petitions', {'page': next_page})
-            next_page = response['meta']['next_page']
-            petitions.extend(response['petitions'])
+                f"{self.hostname}/api/v1/petitions", {"page": next_page}
+            )
+            next_page = response["meta"]["next_page"]
+            petitions.extend(response["petitions"])
 
         return Table(petitions)
 
         def get_signatures(self):
             pass
 
         def get_members(self):
```

### Comparing `parsons-1.0.0/parsons/copper/copper.py` & `parsons-1.1.0/parsons/copper/copper.py`

 * *Files 5% similar despite different names*

```diff
@@ -24,38 +24,38 @@
             env. variable set.
     `Returns:`
         Copper Class
     """
 
     def __init__(self, user_email=None, api_key=None):
 
-        self.api_key = check_env.check('COPPER_API_KEY', api_key)
-        self.user_email = check_env.check('COPPER_USER_EMAIL', user_email)
+        self.api_key = check_env.check("COPPER_API_KEY", api_key)
+        self.user_email = check_env.check("COPPER_USER_EMAIL", user_email)
         self.uri = COPPER_URI
 
     def base_request(self, endpoint, req_type, page=1, page_size=200, filters=None):
         # Internal Request Method
 
         url = self.uri + endpoint
 
         # Authentication must be done through headers, requests HTTPBasicAuth doesn't work
         headers = {
-            'X-PW-AccessToken': self.api_key,
-            'X-PW-Application': "developer_api",
-            'X-PW-UserEmail': self.user_email,
-            'Content-Type': "application/json"
+            "X-PW-AccessToken": self.api_key,
+            "X-PW-Application": "developer_api",
+            "X-PW-UserEmail": self.user_email,
+            "Content-Type": "application/json",
         }
 
         payload = {}
         if filters is not None:
             if len(filters) > 0 and isinstance(filters, dict):
                 payload.update(filters)
 
         # GET request with non-None data arg is malformed
-        if req_type == 'GET':
+        if req_type == "GET":
             return request(req_type, url, params=json.dumps(payload), headers=headers)
         else:
             payload["page_number"] = page
             payload["page_size"] = page_size
 
             return request(req_type, url, data=json.dumps(payload), headers=headers)
 
@@ -65,39 +65,35 @@
         page = 1
         total_pages = 2
         blob = []
         only_page = False
 
         if isinstance(filters, dict):
             # Assume user wants just that page if page_number specified in filters
-            if 'page_number' in filters:
-                page = filters['page_number']
+            if "page_number" in filters:
+                page = filters["page_number"]
                 # Ensure exactly one loop
                 total_pages = page
-                rows = f'{str(page_size)} or less'
+                rows = f"{str(page_size)} or less"
                 only_page = True
         else:
             filters = {}
 
         while page <= total_pages:
 
             r = self.base_request(
-                endpoint,
-                req_type,
-                page_size=page_size,
-                page=page,
-                filters=filters
+                endpoint, req_type, page_size=page_size, page=page, filters=filters
             )
 
             if page == 1:
-                if 'X-Pw-Total' in r.headers and not only_page:
-                    rows = r.headers['X-Pw-Total']
-                    total_pages = int(math.ceil(int(rows)/float(page_size)))
+                if "X-Pw-Total" in r.headers and not only_page:
+                    rows = r.headers["X-Pw-Total"]
+                    total_pages = int(math.ceil(int(rows) / float(page_size)))
                 else:
-                    rows = f'{str(page_size)} or less'
+                    rows = f"{str(page_size)} or less"
                     total_pages = 1
             logger.info(f"Retrieving page {page} of {total_pages}, total rows: {rows}")
             page += 1
 
             if r.text == "":
                 return []
             # Avoid too many layers of nesting if possible
@@ -128,15 +124,15 @@
             List of dicts of Parsons Tables:
                 * people
                 * people_emails
                 * people_phone_numbers
                 * people_custom_fields
                 * people_socials
                 * people_websites
-        """ # noqa: E501,E261
+        """  # noqa: E501,E261
 
         return self.get_standard_object("people", filters=filters, tidy=tidy)
 
     def get_companies(self, filters=None, tidy=False):
         """
         Get companies
 
@@ -153,15 +149,15 @@
         `Returns:`
             List of dicts of Parsons Tables:
                 * companies
                 * companies_phone_numbers
                 * companies_custom_fields
                 * companies_socials
                 * companies_websites
-        """ # noqa: E501,E261
+        """  # noqa: E501,E261
 
         return self.get_standard_object("companies", filters=filters, tidy=tidy)
 
     def get_activities(self, filters=None, tidy=False):
         """
         Get activities
 
@@ -173,15 +169,15 @@
                 If `True`: creates new table out of unpacked rows
                 If 'int': adds rows to original table if max rows per key <= given number
                 (so `tidy=0` guarantees new table)
 
         `Returns:`
             List of dicts of Parsons Tables:
                 * activities
-        """ # noqa: E501,E261
+        """  # noqa: E501,E261
 
         return self.get_standard_object("activities", filters=filters, tidy=tidy)
 
     def get_opportunities(self, filters=None, tidy=False):
         """
         Get opportunities (i.e. donations)
 
@@ -194,23 +190,25 @@
                 If 'int': adds rows to original table if max rows per key <= given number
                 (so `tidy=0` guarantees new table)
 
         `Returns:`
             List of dicts of Parsons Tables:
                 * opportunities
                 * opportunities_custom_fields
-        """ # noqa: E501,E261
+        """  # noqa: E501,E261
 
         return self.get_standard_object("opportunities", filters=filters, tidy=tidy)
 
     def get_standard_object(self, object_name, filters=None, tidy=False):
         # Retrieve and process a standard endpoint object (e.g. people, companies, etc.)
 
         logger.info(f"Retrieving {object_name} records.")
-        blob = self.paginate_request(f"/{object_name}/search", req_type='POST', filters=filters)
+        blob = self.paginate_request(
+            f"/{object_name}/search", req_type="POST", filters=filters
+        )
 
         return self.process_json(blob, object_name, tidy=tidy)
 
     def get_custom_fields(self):
         """
         Get custom fields
 
@@ -220,134 +218,139 @@
             See `Copper documentation <https://developer.copper.com/?version=latest#bf389290-0c19-46a7-85bf-f5e6884fa4e1>`_ for choices
 
         `Returns:`
             List of dicts of Parsons Tables:
                 * custom_fields
                 * custom_fields_available
                 * custom_fields_options
-        """ # noqa: E501,E261
+        """  # noqa: E501,E261
 
         logger.info("Retrieving custom fields.")
-        blob = self.paginate_request('/custom_field_definitions/', req_type='GET')
+        blob = self.paginate_request("/custom_field_definitions/", req_type="GET")
         return self.process_custom_fields(blob)
 
     def get_activity_types(self):
         """
         Get activity types
 
         `Args:`
             `filters: dict`
             Optional; pass additional parameters to filter the records returned.
             See `Copper documentation <https://developer.copper.com/?version=latest#6bd339f1-f0de-48b4-8c34-5a5e245e036f>`_ for choices
 
         `Returns:`
             List of dicts of Parsons Tables:
                 * activitiy_types
-        """ # noqa: E501,E261
+        """  # noqa: E501,E261
 
         logger.info("Retrieving activity types.")
 
-        response = self.paginate_request('/activity_types/', req_type='GET')
+        response = self.paginate_request("/activity_types/", req_type="GET")
         orig_table = Table(response)
-        at_user = orig_table.long_table([], 'user', prepend=False)
-        at_sys = orig_table.long_table([], 'system', prepend=False)
+        at_user = orig_table.long_table([], "user", prepend=False)
+        at_sys = orig_table.long_table([], "system", prepend=False)
         Table.concat(at_sys, at_user)
 
-        return [{'name': 'activity_types', 'tbl': at_sys}]
+        return [{"name": "activity_types", "tbl": at_sys}]
 
     def get_contact_types(self):
         """
         Get contact types
 
         `Args:`
             `filters: dict`
             Optional; pass additional parameters to filter the records returned.
             See `Copper documentation <https://developer.copper.com/?version=latest#8b6e6ed8-c594-4eed-a2af-586aa2100f09>`_ for choices
 
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
-        """ # noqa: E501,E261
+        """  # noqa: E501,E261
 
-        response = self.paginate_request('/contact_types/', req_type='GET')
+        response = self.paginate_request("/contact_types/", req_type="GET")
         return Table(response)
 
     def process_json(self, json_blob, obj_type, tidy=False):
         # Internal method for converting most types of json responses into a list of Parsons tables
 
         # Output goes here
         table_list = []
 
         # Original table & columns
         obj_table = Table(json_blob)
         cols = obj_table.get_columns_type_stats()
-        list_cols = [x['name'] for x in cols if 'list' in x['type']]
-        dict_cols = [x['name'] for x in cols if 'dict' in x['type']]
+        list_cols = [x["name"] for x in cols if "list" in x["type"]]
+        dict_cols = [x["name"] for x in cols if "dict" in x["type"]]
 
         # Unpack all list columns
         if len(list_cols) > 0:
             for l in list_cols:  # noqa E741
                 # Check for nested data
                 list_rows = obj_table.select_rows(
                     lambda row: isinstance(row[l], list)
                     and any(isinstance(x, dict) for x in row[l])
                 )
                 # Add separate long table for each column with nested data
                 if list_rows.num_rows > 0:
-                    logger.debug(l, 'is a nested column')
-                    if len([x for x in cols if x['name'] == l]) == 1:
-                        table_list.append({
-                            'name': f'{obj_type}_{l}',
-                            'tbl': obj_table.long_table(['id'], l)
-                        })
+                    logger.debug(l, "is a nested column")
+                    if len([x for x in cols if x["name"] == l]) == 1:
+                        table_list.append(
+                            {
+                                "name": f"{obj_type}_{l}",
+                                "tbl": obj_table.long_table(["id"], l),
+                            }
+                        )
                     else:
                         # Ignore if column doesn't exist (or has multiples)
                         continue
                 else:
                     if tidy is False:
-                        logger.debug(l, 'is a normal list column')
+                        logger.debug(l, "is a normal list column")
                         obj_table.unpack_list(l)
 
         # Unpack all dict columns
         if len(dict_cols) > 0 and tidy is False:
             for d in dict_cols:
-                logger.debug(d, 'is a dict column')
+                logger.debug(d, "is a dict column")
                 obj_table.unpack_dict(d)
 
         if tidy is not False:
             packed_cols = list_cols + dict_cols
             for p in packed_cols:
                 if p in obj_table.columns:
-                    logger.debug(p, 'needs to be unpacked into rows')
+                    logger.debug(p, "needs to be unpacked into rows")
 
                     # Determine whether or not to expand based on tidy
-                    unpacked_tidy = obj_table.unpack_nested_columns_as_rows(p, expand_original=tidy)
+                    unpacked_tidy = obj_table.unpack_nested_columns_as_rows(
+                        p, expand_original=tidy
+                    )
                     # Check if column was removed as sign it was unpacked into separate table
                     if p not in obj_table.columns:
-                        table_list.append({
-                            'name': f'{obj_type}_{p}',
-                            'tbl': unpacked_tidy
-                        })
+                        table_list.append(
+                            {"name": f"{obj_type}_{p}", "tbl": unpacked_tidy}
+                        )
                     else:
                         obj_table = unpacked_tidy
 
         # Original table will have had all nested columns removed
         if len(obj_table.columns) > 1:
-            table_list.append({'name': obj_type, 'tbl': obj_table})
+            table_list.append({"name": obj_type, "tbl": obj_table})
 
         return table_list
 
     def process_custom_fields(self, json_blob):
         # Internal method to convert custom fields responses into a list of Parsons tables
 
         # Original table & columns
         custom_fields = Table(json_blob)
 
         # Available On
-        available_on = custom_fields.long_table(['id'], 'available_on')
+        available_on = custom_fields.long_table(["id"], "available_on")
 
         # Options
-        options = custom_fields.long_table(['id', 'name'], 'options')
+        options = custom_fields.long_table(["id", "name"], "options")
 
-        return [{'name': 'custom_fields', 'tbl': custom_fields},
-                {'name': 'custom_fields_available', 'tbl': available_on},
-                {'name': 'custom_fields_options', 'tbl': options}]
+        return [
+            {"name": "custom_fields", "tbl": custom_fields},
+            {"name": "custom_fields_available", "tbl": available_on},
+            {"name": "custom_fields_options", "tbl": options},
+        ]
```

### Comparing `parsons-1.0.0/parsons/crowdtangle/crowdtangle.py` & `parsons-1.1.0/parsons/crowdtangle/crowdtangle.py`

 * *Files 8% similar despite different names*

```diff
@@ -21,61 +21,60 @@
             variable set.
     `Returns:`
         CrowdTangle Class
     """
 
     def __init__(self, api_key=None):
 
-        self.api_key = check_env.check('CT_API_KEY', api_key)
+        self.api_key = check_env.check("CT_API_KEY", api_key)
         self.uri = CT_URI
 
-    def _base_request(self, endpoint, req_type='GET', args=None):
+    def _base_request(self, endpoint, req_type="GET", args=None):
 
-        url = f'{self.uri}/{endpoint}'
-        base_args = {'token': self.api_key,
-                     'count': PAGE_SIZE}
+        url = f"{self.uri}/{endpoint}"
+        base_args = {"token": self.api_key, "count": PAGE_SIZE}
 
         # Add any args passed through to the base args
         if args is not None:
             base_args.update(args)
 
         r = request(req_type, url, params=base_args).json()
-        json = r['result']
+        json = r["result"]
         keys = list(json.keys())
         data = json[keys[0]]
 
-        while 'nextPage' in list(json['pagination'].keys()):
+        while "nextPage" in list(json["pagination"].keys()):
             logger.info(f"Retrieving {PAGE_SIZE} rows.")
             time.sleep(REQUEST_SLEEP)
-            next_url = json['pagination']['nextPage']
+            next_url = json["pagination"]["nextPage"]
             r = request(req_type, next_url).json()
-            json = r['result']
+            json = r["result"]
             data.extend(json[keys[0]])
 
         logger.info(f"Retrieved {len(data)} rows.")
 
         return data
 
     def _base_unpack(self, ParsonsTable):
 
         logger.debug("Working to unpack the Parsons Table...")
-        logger.debug(f'Starting with {len(ParsonsTable.columns)} columns...')
+        logger.debug(f"Starting with {len(ParsonsTable.columns)} columns...")
         sample = ParsonsTable[0]
 
         col_dict = {}
         for x in ParsonsTable.columns:
             col_dict[x] = str(type(sample[x]))
 
         for col in col_dict:
             if col_dict[col] == "<class 'dict'>":
                 ParsonsTable.unpack_dict(col)
             elif col_dict[x] == "<class 'list'>":
                 ParsonsTable.unpack_list(col)
 
-        logger.info(f'There are now {len(ParsonsTable.columns)} columns...')
+        logger.info(f"There are now {len(ParsonsTable.columns)} columns...")
         return ParsonsTable
 
     def _unpack(self, ParsonsTable):
 
         if ParsonsTable.num_rows == 0:
             return None
 
@@ -88,20 +87,28 @@
             second_count = len(ParsonsTable.columns)
 
         return ParsonsTable
 
     def _list_to_string(self, list_arg):
 
         if list_arg:
-            return ','.join(list_arg)
+            return ",".join(list_arg)
         else:
             return None
 
-    def get_posts(self, start_date=None, end_date=None, language=None, list_ids=None,
-                  min_interations=None, search_term=None, types=None):
+    def get_posts(
+        self,
+        start_date=None,
+        end_date=None,
+        language=None,
+        list_ids=None,
+        min_interations=None,
+        search_term=None,
+        types=None,
+    ):
         """
         Return a set of posts for the given parameters.
 
         See the `API documentation <https://github.com/CrowdTangle/API/wiki/Posts>`_
         for more information.
 
         .. warning::
@@ -150,29 +157,33 @@
                 that are not ``native_video``, ``youtube`` or ``vine``.
 
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        args = {'startDate': start_date,
-                'endDate': end_date,
-                'language': language,
-                'listIds': self._list_to_string(list_ids),
-                'minInteractions': min_interations,
-                'searchTerm': search_term,
-                'types': types}
+        args = {
+            "startDate": start_date,
+            "endDate": end_date,
+            "language": language,
+            "listIds": self._list_to_string(list_ids),
+            "minInteractions": min_interations,
+            "searchTerm": search_term,
+            "types": types,
+        }
 
         logger.info("Retrieving posts.")
-        pt = Table(self._base_request('posts', args=args))
-        logger.info(f'Retrieved {pt.num_rows} posts.')
+        pt = Table(self._base_request("posts", args=args))
+        logger.info(f"Retrieved {pt.num_rows} posts.")
         self._unpack(pt)
         return pt
 
-    def get_leaderboard(self, start_date=None, end_date=None, list_ids=None, account_ids=None):
+    def get_leaderboard(
+        self, start_date=None, end_date=None, list_ids=None, account_ids=None
+    ):
         """
         Return leaderboard data.
 
         See the `API documentation <https://github.com/CrowdTangle/API/wiki/Leaderboard>`_
         for more information.
 
         .. warning::
@@ -192,26 +203,29 @@
                 This and ``list_id`` are mutually exclusive; if both are sent, the
                 ``account_ids`` value will be used.
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        args = {'startDate': start_date,
-                'endDate': end_date,
-                'listIds': self._list_to_string(list_ids),
-                'accountIds': self._list_to_string(account_ids)}
+        args = {
+            "startDate": start_date,
+            "endDate": end_date,
+            "listIds": self._list_to_string(list_ids),
+            "accountIds": self._list_to_string(account_ids),
+        }
 
-        pt = Table(self._base_request('leaderboard', args=args))
-        logger.info(f'Retrieved {pt.num_rows} records from the leaderbooard.')
+        pt = Table(self._base_request("leaderboard", args=args))
+        logger.info(f"Retrieved {pt.num_rows} records from the leaderbooard.")
         self._unpack(pt)
         return pt
 
-    def get_links(self, link, start_date=None, end_date=None, include_summary=None,
-                  platforms=None):
+    def get_links(
+        self, link, start_date=None, end_date=None, include_summary=None, platforms=None
+    ):
         """
         Return up to 100 posts based on a specific link. It is strongly recommended to
         use the ``start_date`` parameter to limit queries to relevant dates.
 
         See the `API documentation <https://github.com/CrowdTangle/API/wiki/Links>`_
         for more information.
 
@@ -236,18 +250,20 @@
                 Filter by platforms
 
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        args = {'link': link,
-                'startDate': start_date,
-                'endDate': end_date,
-                'includeSummary': str(include_summary),
-                'platforms': self._list_to_string(platforms)}
+        args = {
+            "link": link,
+            "startDate": start_date,
+            "endDate": end_date,
+            "includeSummary": str(include_summary),
+            "platforms": self._list_to_string(platforms),
+        }
 
         logger.info("Retrieving posts based on link.")
-        pt = Table(self._base_request('links', args=args))
-        logger.info(f'Retrieved {pt.num_rows} links.')
+        pt = Table(self._base_request("links", args=args))
+        logger.info(f"Retrieved {pt.num_rows} links.")
         self._unpack(pt)
         return pt
```

### Comparing `parsons-1.0.0/parsons/databases/alchemy.py` & `parsons-1.1.0/parsons/databases/alchemy.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,41 +1,42 @@
 from sqlalchemy import create_engine, Table, MetaData
 import logging
 
 logger = logging.getLogger(__name__)
 
 
 class Alchemy:
-
     def generate_engine(self):
         """
         Generate a SQL Alchemy engine.
         """
 
         alchemy_url = self.generate_alchemy_url()
         return create_engine(alchemy_url, echo=False, convert_unicode=True)
 
     def generate_alchemy_url(self):
         """
         Generate a SQL Alchemy engine
         https://docs.sqlalchemy.org/en/14/core/engines.html#
         """
 
-        if self.dialect == 'redshift' or self.dialect == 'postgres':
-            connection_schema = 'postgresql+psycopg2'
-        elif self.dialect == 'mysql':
-            connection_schema = 'mysql+mysqlconnector'
-
-        params = [(self.username, self.username),
-                  (self.password, f':{self.password}'),
-                  (self.host, f'@{self.host}'),
-                  (self.port, f':{self.port}'),
-                  (self.db, f'/{self.db}')]
+        if self.dialect == "redshift" or self.dialect == "postgres":
+            connection_schema = "postgresql+psycopg2"
+        elif self.dialect == "mysql":
+            connection_schema = "mysql+mysqlconnector"
+
+        params = [
+            (self.username, self.username),
+            (self.password, f":{self.password}"),
+            (self.host, f"@{self.host}"),
+            (self.port, f":{self.port}"),
+            (self.db, f"/{self.db}"),
+        ]
 
-        url = f'{connection_schema}://'
+        url = f"{connection_schema}://"
 
         for i in params:
             if i[0]:
                 url += i[1]
 
         return url
```

### Comparing `parsons-1.0.0/parsons/databases/database/database.py` & `parsons-1.1.0/parsons/databases/database/database.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,12 @@
 import parsons.databases.database.constants as consts
 import ast
 
 
-class DatabaseCreateStatement():
-
+class DatabaseCreateStatement:
     def __init__(self):
         self.INT_TYPES = consts.INT_TYPES
         self.SMALLINT = consts.SMALLINT
         self.SMALLINT_MIN = consts.SMALLINT_MIN
         self.SMALLINT_MAX = consts.SMALLINT_MAX
         self.MEDIUMINT = consts.MEDIUMINT
         self.MEDIUMINT_MIN = consts.MEDIUMINT_MIN
@@ -100,16 +99,19 @@
         if type(val) in (int, float):
             return True
 
         # If it can be cast to a number and it doesn't contain underscores
         # then it's a valid sql number
         # Also check the first character is not zero
         try:
-            if ((float(val) or 1) and "_" not in val and
-                    (val in ("0", "0.0") or val[0] != "0")):
+            if (
+                (float(val) or 1)
+                and "_" not in val
+                and (val in ("0", "0.0") or val[0] != "0")
+            ):
                 return True
             else:
                 return False
 
         # If it can't be cast to a number in python
         # then it's not a number in sql
         except (TypeError, ValueError):
@@ -128,16 +130,17 @@
             bool
                 Whether or not the value is a valid sql boolean.
         """
         if not self.DO_PARSE_BOOLS:
             return
 
         if isinstance(val, bool) or (
-                type(val) in (int, str) and
-                str(val).upper() in self.TRUE_VALS + self.FALSE_VALS):
+            type(val) in (int, str)
+            and str(val).upper() in self.TRUE_VALS + self.FALSE_VALS
+        ):
             return True
         return False
 
     def detect_data_type(self, value, cmp_type=None):
         """Detect the higher of value's type cmp_type.
 
         1. check if it's a string
@@ -201,19 +204,19 @@
 
         # The value is very likely an int
         # let's get its size
         # If the compare types are empty and use the types of the current value
         if type_lit == int and cmp_type in (self.INT_TYPES + [None, "", self.BOOL]):
 
             # Use smallest possible int type above TINYINT
-            if (self.SMALLINT_MIN < val_lit < self.SMALLINT_MAX):
+            if self.SMALLINT_MIN < val_lit < self.SMALLINT_MAX:
                 return self.get_bigger_int(self.SMALLINT, cmp_type)
-            elif (self.MEDIUMINT_MIN < val_lit < self.MEDIUMINT_MAX):
+            elif self.MEDIUMINT_MIN < val_lit < self.MEDIUMINT_MAX:
                 return self.get_bigger_int(self.MEDIUMINT, cmp_type)
-            elif (self.INT_MIN < val_lit < self.INT_MAX):
+            elif self.INT_MIN < val_lit < self.INT_MAX:
                 return self.get_bigger_int(self.INT, cmp_type)
             else:
                 return self.BIGINT
 
         # Need to determine who makes it all the way down here
         return cmp_type
 
@@ -257,15 +260,15 @@
         if col.upper() in self.RESERVED_WORDS:
             col = self._rename_reserved_word(col, index)
 
         if col[0].isdigit():
             col = f"x_{col}"
 
         if len(col) > self.COL_NAME_MAX_LEN:
-            col = col[:self.COL_NAME_MAX_LEN]
+            col = col[: self.COL_NAME_MAX_LEN]
 
         return col
 
     def format_columns(self, cols, **kwargs):
         """Format the columns to meet database contraints.
 
         This method relies on ``format_column`` to handle most changes. It
```

### Comparing `parsons-1.0.0/parsons/databases/db_sync.py` & `parsons-1.1.0/parsons/databases/db_sync.py`

 * *Files 6% similar despite different names*

```diff
@@ -24,25 +24,38 @@
         retries: int
             The number of times to retry if there is an error processing a
             chunk of data. The default value is 0.
     `Returns:`
         A DBSync object.
     """
 
-    def __init__(self, source_db, destination_db, read_chunk_size=100_000, write_chunk_size=None,
-                 retries=0):
+    def __init__(
+        self,
+        source_db,
+        destination_db,
+        read_chunk_size=100_000,
+        write_chunk_size=None,
+        retries=0,
+    ):
 
         self.source_db = source_db
         self.dest_db = destination_db
         self.read_chunk_size = read_chunk_size
         self.write_chunk_size = write_chunk_size or read_chunk_size
         self.retries = retries
 
-    def table_sync_full(self, source_table, destination_table, if_exists='drop',
-                        order_by=None, verify_row_count=True, **kwargs):
+    def table_sync_full(
+        self,
+        source_table,
+        destination_table,
+        if_exists="drop",
+        order_by=None,
+        verify_row_count=True,
+        **kwargs,
+    ):
         """
         Full sync of table from a source database to a destination database. This will
         wipe all data from the destination table.
 
         `Args:`
             source_table: str
                 Full table path (e.g. ``my_schema.my_table``)
@@ -65,47 +78,59 @@
             ``None``
         """
 
         # Create the table objects
         source_tbl = self.source_db.table(source_table)
         destination_tbl = self.dest_db.table(destination_table)
 
-        logger.info(f'Syncing full table data from {source_table} to {destination_table}')
+        logger.info(
+            f"Syncing full table data from {source_table} to {destination_table}"
+        )
 
         # Drop or truncate if the destination table exists
         if destination_tbl.exists:
-            if if_exists == 'drop':
+            if if_exists == "drop":
                 destination_tbl.drop()
-            elif if_exists == 'truncate':
+            elif if_exists == "truncate":
                 self._check_column_match(source_tbl, destination_tbl)
                 destination_tbl.truncate()
-            elif if_exists == 'drop_if_needed':
+            elif if_exists == "drop_if_needed":
                 try:
                     self._check_column_match(source_tbl, destination_tbl)
                     destination_tbl.truncate()
                 except Exception:
                     logger.info(f"needed to drop {destination_tbl}...")
                     destination_tbl.drop()
             else:
-                raise ValueError('Invalid if_exists argument. Must be drop or truncate.')
+                raise ValueError(
+                    "Invalid if_exists argument. Must be drop or truncate."
+                )
 
         # Create the table, if needed.
         if not destination_tbl.exists:
             self.create_table(source_table, destination_table)
 
-        copied_rows = self.copy_rows(source_table, destination_table, None,
-                                     order_by, **kwargs)
+        copied_rows = self.copy_rows(
+            source_table, destination_table, None, order_by, **kwargs
+        )
 
         if verify_row_count:
             self._row_count_verify(source_tbl, destination_tbl)
 
-        logger.info(f'{source_table} synced: {copied_rows} total rows copied.')
+        logger.info(f"{source_table} synced: {copied_rows} total rows copied.")
 
-    def table_sync_incremental(self, source_table, destination_table, primary_key,
-                               distinct_check=True, verify_row_count=True, **kwargs):
+    def table_sync_incremental(
+        self,
+        source_table,
+        destination_table,
+        primary_key,
+        distinct_check=True,
+        verify_row_count=True,
+        **kwargs,
+    ):
         """
         Incremental sync of table from a source database to a destination database
         using an incremental primary key.
 
         `Args:`
             source_table: str
                 Full table path (e.g. ``my_schema.my_table``)
@@ -129,55 +154,73 @@
         # Create the table objects
         source_tbl = self.source_db.table(source_table)
         destination_tbl = self.dest_db.table(destination_table)
 
         # Check that the destination table exists. If it does not, then run a
         # full sync instead.
         if not destination_tbl.exists:
-            logger.info('Destination tables %s does not exist, running a full sync',
-                        destination_table)
-            self.table_sync_full(source_table, destination_table, order_by=primary_key, **kwargs)
+            logger.info(
+                "Destination tables %s does not exist, running a full sync",
+                destination_table,
+            )
+            self.table_sync_full(
+                source_table, destination_table, order_by=primary_key, **kwargs
+            )
             return
 
         # Check that the source table primary key is distinct
         if distinct_check and not source_tbl.distinct_primary_key(primary_key):
-            logger.info('Checking for distinct values for column %s in table %s',
-                        primary_key, source_table)
-            raise ValueError('{primary_key} is not distinct in source table.')
+            logger.info(
+                "Checking for distinct values for column %s in table %s",
+                primary_key,
+                source_table,
+            )
+            raise ValueError("{primary_key} is not distinct in source table.")
 
         # Get the max source table and destination table primary key
-        logger.debug('Calculating the maximum value for %s for source table %s', primary_key,
-                     source_table)
+        logger.debug(
+            "Calculating the maximum value for %s for source table %s",
+            primary_key,
+            source_table,
+        )
         source_max_pk = source_tbl.max_primary_key(primary_key)
-        logger.debug('Calculating the maximum value for %s for destination table %s', primary_key,
-                     destination_table)
+        logger.debug(
+            "Calculating the maximum value for %s for destination table %s",
+            primary_key,
+            destination_table,
+        )
         dest_max_pk = destination_tbl.max_primary_key(primary_key)
 
         # Check for a mismatch in row counts; if dest_max_pk is None, or destination is empty
         # and we don't have to worry about this check.
         if dest_max_pk is not None and dest_max_pk > source_max_pk:
-            raise ValueError('Destination DB primary key greater than source DB primary key.')
+            raise ValueError(
+                "Destination DB primary key greater than source DB primary key."
+            )
 
         # Do not copied if row counts are equal.
         elif dest_max_pk == source_max_pk:
-            logger.info('Tables are already in sync.')
+            logger.info("Tables are already in sync.")
             return None
 
         else:
-            rows_copied = self.copy_rows(source_table, destination_table, dest_max_pk,
-                                         primary_key, **kwargs)
+            rows_copied = self.copy_rows(
+                source_table, destination_table, dest_max_pk, primary_key, **kwargs
+            )
 
-            logger.info('Copied %s new rows to %s.', rows_copied, destination_table)
+            logger.info("Copied %s new rows to %s.", rows_copied, destination_table)
 
         if verify_row_count:
             self._row_count_verify(source_tbl, destination_tbl)
 
-        logger.info(f'{source_table} synced to {destination_table}.')
+        logger.info(f"{source_table} synced to {destination_table}.")
 
-    def copy_rows(self, source_table_name, destination_table_name, cutoff, order_by, **kwargs):
+    def copy_rows(
+        self, source_table_name, destination_table_name, cutoff, order_by, **kwargs
+    ):
         """
         Copy the rows from the source to the destination.
 
         `Args:`
             source_table_name: str
                 Full table path (e.g. ``my_schema.my_table``)
             destination_table_name: str
@@ -208,104 +251,125 @@
         # Keep going until we break out
         while True:
             try:
                 # Get the records to load into the database
                 if cutoff:
                     # If we have a cutoff, we are loading data incrementally -- filter out
                     # any data before our cutoff
-                    rows = source_table.get_new_rows(primary_key=order_by,
-                                                     cutoff_value=cutoff,
-                                                     offset=total_rows_downloaded,
-                                                     chunk_size=self.read_chunk_size)
+                    rows = source_table.get_new_rows(
+                        primary_key=order_by,
+                        cutoff_value=cutoff,
+                        offset=total_rows_downloaded,
+                        chunk_size=self.read_chunk_size,
+                    )
                 else:
                     # Get a chunk
-                    rows = source_table.get_rows(offset=total_rows_downloaded,
-                                                 chunk_size=self.read_chunk_size,
-                                                 order_by=order_by)
+                    rows = source_table.get_rows(
+                        offset=total_rows_downloaded,
+                        chunk_size=self.read_chunk_size,
+                        order_by=order_by,
+                    )
 
                 number_of_rows = rows.num_rows
                 total_rows_downloaded += number_of_rows
 
                 # If we didn't get any data, exit the loop -- there's nothing to load
                 if number_of_rows == 0:
                     # If we have any rows that are unwritten, flush them to the destination database
                     if rows_buffered > 0:
-                        logger.debug('Copying %s rows to %s', rows_buffered, destination_table_name)
-                        self.dest_db.copy(buffer, destination_table_name, if_exists='append',
-                                          **kwargs)
+                        logger.debug(
+                            "Copying %s rows to %s",
+                            rows_buffered,
+                            destination_table_name,
+                        )
+                        self.dest_db.copy(
+                            buffer, destination_table_name, if_exists="append", **kwargs
+                        )
                         total_rows_written += rows_buffered
 
                         # Reset the buffer
                         rows_buffered = 0
                         buffer = Table()
 
                     break
 
                 # Add the new rows to our buffer
                 buffer.concat(rows)
                 rows_buffered += number_of_rows
 
                 # If our buffer reaches our write threshold, write it out
                 if rows_buffered >= self.write_chunk_size:
-                    logger.debug('Copying %s rows to %s', rows_buffered, destination_table_name)
-                    self.dest_db.copy(buffer, destination_table_name, if_exists='append', **kwargs)
+                    logger.debug(
+                        "Copying %s rows to %s", rows_buffered, destination_table_name
+                    )
+                    self.dest_db.copy(
+                        buffer, destination_table_name, if_exists="append", **kwargs
+                    )
                     total_rows_written += rows_buffered
 
                     # Reset the buffer
                     rows_buffered = 0
                     buffer = Table()
 
             except Exception:
                 # Tick down the number of retries
                 retries_left -= 1
 
                 # If we are out of retries, fail
                 if retries_left == 0:
-                    logger.debug('No retries remaining')
+                    logger.debug("No retries remaining")
                     raise
 
                 # Otherwise, log the exception and try again
-                logger.exception('Unhandled error copying data; retrying')
+                logger.exception("Unhandled error copying data; retrying")
 
         return total_rows_written
 
     @staticmethod
     def _check_column_match(source_table_obj, destination_table_obj):
         """
         Ensure that the columns from each table match
         """
 
         if source_table_obj.columns != destination_table_obj.columns:
-            raise ValueError("""Destination table columns do not match source table columns.
-                             Consider dropping destination table and running a full sync.""")
+            raise ValueError(
+                """Destination table columns do not match source table columns.
+                             Consider dropping destination table and running a full sync."""
+            )
 
     @staticmethod
     def _row_count_verify(source_table_obj, destination_table_obj):
         """
         Ensure the the rows of the source table and the destination table match
         """
 
         source_row_count = source_table_obj.num_rows
         dest_row_count = destination_table_obj.num_rows
 
         if source_row_count != dest_row_count:
-            logger.warning((f'Table count mismatch. Source table contains {source_row_count}.',
-                           f' Destination table contains {dest_row_count}.'))
+            logger.warning(
+                (
+                    f"Table count mismatch. Source table contains {source_row_count}.",
+                    f" Destination table contains {dest_row_count}.",
+                )
+            )
             return False
 
-        logger.info('Source and destination table row counts match.')
+        logger.info("Source and destination table row counts match.")
         return True
 
     def create_table(self, source_table, destination_table):
         """
         Create the empty table in the destination database based on the source
         database schema structure. This method utilizes the Alchemy subclass.
         """
 
         # Try to create the destination using the source table's schema; if that doesn't work,
         # then we will lean on "copy" when loading the data to create the destination
         try:
             source_obj = self.source_db.get_table_object(source_table)
             self.dest_db.create_table(source_obj, destination_table)
         except Exception:
-            logger.warning('Unable to create destination table based on source table; we will '
-                           'fallback to using "copy" to create the destination.')
+            logger.warning(
+                "Unable to create destination table based on source table; we will "
+                'fallback to using "copy" to create the destination.'
+            )
```

### Comparing `parsons-1.0.0/parsons/databases/mysql/constants.py` & `parsons-1.1.0/parsons/databases/mysql/constants.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # Additional padding to add on to the maximum column width to account
 # for the addition of future data sets.
-VARCHAR_PAD = .25
+VARCHAR_PAD = 0.25
 
 COL_NAME_MAX_LEN = 64
 
 # List of varchar lengths to use for columns -- this list needs to be in order from smallest to
 # largest
 VARCHAR_STEPS = [32, 64, 128, 256, 512, 1024, 4096, 8192, 16384]
```

### Comparing `parsons-1.0.0/parsons/databases/mysql/create_table.py` & `parsons-1.1.0/parsons/databases/mysql/create_table.py`

 * *Files 8% similar despite different names*

```diff
@@ -4,15 +4,14 @@
 import petl
 import logging
 
 logger = logging.getLogger(__name__)
 
 
 class MySQLCreateTable(DatabaseCreateStatement):
-
     def __init__(self):
         super().__init__()
 
         self.VARCHAR_PAD = consts.VARCHAR_PAD
         self.COL_NAME_MAX_LEN = consts.COL_NAME_MAX_LEN
         self.RESERVED_WORDS = []
         self.VARCHAR_STEPS = consts.VARCHAR_STEPS
@@ -38,16 +37,16 @@
         # Iterate through each row in the column
         for row in column_rows:
 
             # Get the MySQL data type
             col_type = self.data_type(row, col_type)
 
             # Calculate width if a varchar
-            if col_type == 'varchar':
-                row_width = len(str(row.encode('utf-8')))
+            if col_type == "varchar":
+                row_width = len(str(row.encode("utf-8")))
 
                 # Evaluate width vs. current max width
                 if row_width > col_width:
                     col_width = row_width
 
         return col_type, col_width
 
@@ -55,15 +54,15 @@
         # Generate a dict of MySQL column types and widths for all columns
         # in a table.
 
         table_map = []
 
         for col in tbl.columns:
             col_type, col_width = self.evaluate_column(tbl.column_data(col))
-            col_map = {'name': col, 'type': col_type, 'width': col_width}
+            col_map = {"name": col, "type": col_type, "width": col_width}
             table_map.append(col_map)
 
         return table_map
 
     def create_statement(self, tbl, table_name, strict_length=True):
         # Generate create statement SQL for a given Parsons table.
 
@@ -73,19 +72,19 @@
         # Generate the table map
         table_map = self.evaluate_table(tbl)
 
         # Generate the column syntax
         column_syntax = []
         for c in table_map:
             if strict_length:
-                col_width = int(c['width'] + (self.VARCHAR_PAD * c['width']))
+                col_width = int(c["width"] + (self.VARCHAR_PAD * c["width"]))
             else:
-                col_width = self.round_longest(c['width'])
+                col_width = self.round_longest(c["width"])
 
-            if c['type'] == 'varchar':
+            if c["type"] == "varchar":
                 column_syntax.append(f"{c['name']} {c['type']}({col_width}) \n")
             else:
                 column_syntax.append(f"{c['name']} {c['type']} \n")
 
         # Generate full statement
         return f"CREATE TABLE {table_name} ( \n {','.join(column_syntax)});"
```

### Comparing `parsons-1.0.0/parsons/databases/mysql/mysql.py` & `parsons-1.1.0/parsons/databases/mysql/mysql.py`

 * *Files 2% similar despite different names*

```diff
@@ -35,19 +35,19 @@
         port: int
             Can be set by env variable ``MYSQL_PORT`` or argument.
     """
 
     def __init__(self, host=None, username=None, password=None, db=None, port=3306):
         super().__init__()
 
-        self.username = check_env.check('MYSQL_USERNAME', username)
-        self.password = check_env.check('MYSQL_PASSWORD', password)
-        self.host = check_env.check('MYSQL_HOST', host)
-        self.db = check_env.check('MYSQL_DB', db)
-        self.port = port or os.environ.get('MYSQL_PORT')
+        self.username = check_env.check("MYSQL_USERNAME", username)
+        self.password = check_env.check("MYSQL_PASSWORD", password)
+        self.host = check_env.check("MYSQL_HOST", host)
+        self.db = check_env.check("MYSQL_DB", db)
+        self.port = port or os.environ.get("MYSQL_PORT")
 
     @contextmanager
     def connection(self):
         """
         Generate a MySQL connection. The connection is set up as a python "context manager", so
         it will be closed automatically (and all queries committed) when the connection goes out
         of scope.
@@ -57,19 +57,21 @@
         ``with mysql.connection() as conn:``
 
         `Returns:`
             MySQL `connection` object
         """
 
         # Create a mysql connection and cursor
-        connection = mysql.connect(host=self.host,
-                                   user=self.username,
-                                   passwd=self.password,
-                                   database=self.db,
-                                   port=self.port)
+        connection = mysql.connect(
+            host=self.host,
+            user=self.username,
+            passwd=self.password,
+            database=self.db,
+            port=self.port,
+        )
 
         try:
             yield connection
         except mysql.Error:
             connection.rollback()
             raise
         else:
@@ -148,53 +150,55 @@
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
         with self.cursor(connection) as cursor:
 
             # The python connector can only execute a single sql statement, so we will
             # break up each statement and execute them separately.
-            for s in sql.strip().split(';'):
+            for s in sql.strip().split(";"):
                 if len(s) != 0:
-                    logger.debug(f'SQL Query: {sql}')
+                    logger.debug(f"SQL Query: {sql}")
                     cursor.execute(s, parameters)
 
             if commit:
                 connection.commit()
 
             # If the SQL query provides no response, then return None
             if not cursor.description:
-                logger.debug('Query returned 0 rows')
+                logger.debug("Query returned 0 rows")
                 return None
 
             else:
                 # Fetch the data in batches, and "pickle" the rows to a temp file.
                 # (We pickle rather than writing to, say, a CSV, so that we maintain
                 # all the type information for each field.)
                 temp_file = files.create_temp_file()
 
-                with open(temp_file, 'wb') as f:
+                with open(temp_file, "wb") as f:
                     # Grab the header
                     pickle.dump(cursor.column_names, f)
 
                     while True:
                         batch = cursor.fetchmany(QUERY_BATCH_SIZE)
                         if len(batch) == 0:
                             break
 
-                        logger.debug(f'Fetched {len(batch)} rows.')
+                        logger.debug(f"Fetched {len(batch)} rows.")
                         for row in batch:
                             pickle.dump(row, f)
 
                 # Load a Table from the file
                 final_tbl = Table(petl.frompickle(temp_file))
 
-                logger.debug(f'Query returned {final_tbl.num_rows} rows.')
+                logger.debug(f"Query returned {final_tbl.num_rows} rows.")
                 return final_tbl
 
-    def copy(self, tbl, table_name, if_exists='fail', chunk_size=1000, strict_length=True):
+    def copy(
+        self, tbl, table_name, if_exists="fail", chunk_size=1000, strict_length=True
+    ):
         """
         Copy a :ref:`parsons-table` to the database.
 
         .. note::
             This method utilizes extended inserts rather `LOAD DATA INFILE` since
             many MySQL Database configurations do not allow data files to be
             loaded. It results in a minor performance hit compared to `LOAD DATA`.
@@ -213,24 +217,26 @@
                 If the database table needs to be created, strict_length determines whether
                 the created table's column sizes will be sized to exactly fit the current data,
                 or if their size will be rounded up to account for future values being larger
                 then the current dataset. defaults to ``True``
         """
 
         if tbl.num_rows == 0:
-            logger.info('Parsons table is empty. Table will not be created.')
+            logger.info("Parsons table is empty. Table will not be created.")
             return None
 
         with self.connection() as connection:
 
             # Create table if not exists
             if self._create_table_precheck(connection, table_name, if_exists):
-                sql = self.create_statement(tbl, table_name, strict_length=strict_length)
+                sql = self.create_statement(
+                    tbl, table_name, strict_length=strict_length
+                )
                 self.query_with_connection(sql, connection, commit=False)
-                logger.info(f'Table {table_name} created.')
+                logger.info(f"Table {table_name} created.")
 
             # Chunk tables in batches of 1K rows, though this can be tuned and
             # optimized further.
             for t in tbl.chunk(chunk_size):
                 sql = self._insert_statement(t, table_name)
                 self.query_with_connection(sql, connection, commit=False)
 
@@ -267,30 +273,30 @@
                 If the table already exists, either ``fail``, ``append``, ``drop``,
                 or ``truncate`` the table.
         `Returns:`
             bool
                 True if the table needs to be created, False otherwise.
         """
 
-        if if_exists not in ['fail', 'truncate', 'append', 'drop']:
+        if if_exists not in ["fail", "truncate", "append", "drop"]:
             raise ValueError("Invalid value for `if_exists` argument")
 
         # If the table exists, evaluate the if_exists argument for next steps.
         if self.table_exists(table_name):
 
-            if if_exists == 'fail':
-                raise ValueError('Table already exists.')
+            if if_exists == "fail":
+                raise ValueError("Table already exists.")
 
-            if if_exists == 'truncate':
+            if if_exists == "truncate":
                 sql = f"TRUNCATE TABLE {table_name}"
                 self.query_with_connection(sql, connection, commit=False)
                 logger.info(f"{table_name} truncated.")
                 return False
 
-            if if_exists == 'drop':
+            if if_exists == "drop":
                 sql = f"DROP TABLE {table_name}"
                 self.query_with_connection(sql, connection, commit=False)
                 logger.info(f"{table_name} dropped.")
                 return True
 
         else:
             return True
@@ -298,16 +304,14 @@
     def table_exists(self, table_name):
         """
         Check if a table or view exists in the database.
 
         `Args:`
             table_name: str
                 The table name
-            view: boolean
-                Check to see if a view exists by the same name
 
         `Returns:`
             boolean
                 ``True`` if the table exists and ``False`` if it does not.
         """
 
         if self.query(f"SHOW TABLES LIKE '{table_name}'").first == table_name:
```

### Comparing `parsons-1.0.0/parsons/databases/postgres/postgres.py` & `parsons-1.1.0/parsons/databases/postgres/postgres.py`

 * *Files 3% similar despite different names*

```diff
@@ -24,35 +24,39 @@
             Required if env variable ``PGDATABASE`` not populated
         port: int
             Required if env variable ``PGPORT`` not populated.
         timeout: int
             Seconds to timeout if connection not established.
     """
 
-    def __init__(self, username=None, password=None, host=None, db=None, port=5432, timeout=10):
+    def __init__(
+        self, username=None, password=None, host=None, db=None, port=5432, timeout=10
+    ):
         super().__init__()
 
-        self.username = username or os.environ.get('PGUSER')
-        self.password = password or os.environ.get('PGPASSWORD')
-        self.host = host or os.environ.get('PGHOST')
-        self.db = db or os.environ.get('PGDATABASE')
-        self.port = port or os.environ.get('PGPORT')
+        self.username = username or os.environ.get("PGUSER")
+        self.password = password or os.environ.get("PGPASSWORD")
+        self.host = host or os.environ.get("PGHOST")
+        self.db = db or os.environ.get("PGDATABASE")
+        self.port = port or os.environ.get("PGPORT")
 
         # Check if there is a pgpass file. Psycopg2 will search for this file first when
         # creating a connection.
-        pgpass = os.path.isfile(os.path.expanduser('~/.pgpass'))
+        pgpass = os.path.isfile(os.path.expanduser("~/.pgpass"))
 
         if not any([self.username, self.password, self.host, self.db]) and not pgpass:
-            raise ValueError('Connection arguments missing. Please pass as a pgpass file, kwargs',
-                             'or env variables.')
+            raise ValueError(
+                "Connection arguments missing. Please pass as a pgpass file, kwargs",
+                "or env variables.",
+            )
 
         self.timeout = timeout
-        self.dialect = 'postgres'
+        self.dialect = "postgres"
 
-    def copy(self, tbl, table_name, if_exists='fail', strict_length=False):
+    def copy(self, tbl, table_name, if_exists="fail", strict_length=False):
         """
         Copy a :ref:`parsons-table` to Postgres.
 
         `Args:`
             tbl: parsons.Table
                 A Parsons table object
             table_name: str
@@ -70,24 +74,26 @@
         with self.connection() as connection:
 
             # Auto-generate table
             if self._create_table_precheck(connection, table_name, if_exists):
 
                 # Create the table
                 # To Do: Pass in the advanced configuration parameters.
-                sql = self.create_statement(tbl, table_name, strict_length=strict_length)
+                sql = self.create_statement(
+                    tbl, table_name, strict_length=strict_length
+                )
 
                 self.query_with_connection(sql, connection, commit=False)
-                logger.info(f'{table_name} created.')
+                logger.info(f"{table_name} created.")
 
             sql = f"COPY {table_name} FROM STDIN CSV HEADER;"
 
             with self.cursor(connection) as cursor:
                 cursor.copy_expert(sql, open(tbl.to_csv(), "r"))
-                logger.info(f'{tbl.num_rows} rows copied to {table_name}.')
+                logger.info(f"{tbl.num_rows} rows copied to {table_name}.")
 
     def table(self, table_name):
         # Return a Postgres table object
 
         return PostgresTable(self, table_name)
```

### Comparing `parsons-1.0.0/parsons/databases/postgres/postgres_core.py` & `parsons-1.1.0/parsons/databases/postgres/postgres_core.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,15 +13,14 @@
 # 100k rows per batch at ~1k bytes each = ~100MB per batch.
 QUERY_BATCH_SIZE = 100000
 
 logger = logging.getLogger(__name__)
 
 
 class PostgresCore(PostgresCreateStatement):
-
     @contextmanager
     def connection(self):
         """
         Generate a Postgres connection.
         The connection is set up as a python "context manager", so it will be closed
         automatically (and all queries committed) when the connection goes out of scope.
 
@@ -30,17 +29,22 @@
         ``with pg.connection() as conn:``
 
         `Returns:`
             Psycopg2 `connection` object
         """
 
         # Create a psycopg2 connection and cursor
-        conn = psycopg2.connect(user=self.username, password=self.password,
-                                host=self.host, dbname=self.db, port=self.port,
-                                connect_timeout=self.timeout)
+        conn = psycopg2.connect(
+            user=self.username,
+            password=self.password,
+            host=self.host,
+            dbname=self.db,
+            port=self.port,
+            connect_timeout=self.timeout,
+        )
 
         try:
             yield conn
         except psycopg2.Error:
             conn.rollback()
             raise
         else:
@@ -118,51 +122,51 @@
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
         with self.cursor(connection) as cursor:
 
-            logger.debug(f'SQL Query: {sql}')
+            logger.debug(f"SQL Query: {sql}")
             cursor.execute(sql, parameters)
 
             if commit:
                 connection.commit()
 
             # If the cursor is empty, don't cause an error
             if not cursor.description:
-                logger.debug('Query returned 0 rows')
+                logger.debug("Query returned 0 rows")
                 return None
 
             else:
 
                 # Fetch the data in batches, and "pickle" the rows to a temp file.
                 # (We pickle rather than writing to, say, a CSV, so that we maintain
                 # all the type information for each field.)
 
                 temp_file = files.create_temp_file()
 
-                with open(temp_file, 'wb') as f:
+                with open(temp_file, "wb") as f:
                     # Grab the header
                     header = [i[0] for i in cursor.description]
                     pickle.dump(header, f)
 
                     while True:
                         batch = cursor.fetchmany(QUERY_BATCH_SIZE)
                         if not batch:
                             break
 
-                        logger.debug(f'Fetched {len(batch)} rows.')
+                        logger.debug(f"Fetched {len(batch)} rows.")
                         for row in batch:
                             pickle.dump(list(row), f)
 
                 # Load a Table from the file
                 final_tbl = Table(petl.frompickle(temp_file))
 
-                logger.debug(f'Query returned {final_tbl.num_rows} rows.')
+                logger.debug(f"Query returned {final_tbl.num_rows} rows.")
                 return final_tbl
 
     def _create_table_precheck(self, connection, table_name, if_exists):
         """
         Helper to determine what to do when you need a table that may already exist.
 
         `Args:`
@@ -174,29 +178,29 @@
                 If the table already exists, either ``fail``, ``append``, ``drop``,
                 or ``truncate`` the table.
         `Returns:`
             bool
                 True if the table needs to be created, False otherwise.
         """
 
-        if if_exists not in ['fail', 'truncate', 'append', 'drop']:
+        if if_exists not in ["fail", "truncate", "append", "drop"]:
             raise ValueError("Invalid value for `if_exists` argument")
 
         # If the table exists, evaluate the if_exists argument for next steps.
         if self.table_exists_with_connection(table_name, connection):
 
-            if if_exists == 'fail':
-                raise ValueError('Table already exists.')
+            if if_exists == "fail":
+                raise ValueError("Table already exists.")
 
-            if if_exists == 'truncate':
+            if if_exists == "truncate":
                 truncate_sql = f"TRUNCATE TABLE {table_name};"
                 logger.info(f"Truncating {table_name}.")
                 self.query_with_connection(truncate_sql, connection, commit=False)
 
-            if if_exists == 'drop':
+            if if_exists == "drop":
                 logger.info(f"Dropping {table_name}.")
                 drop_sql = f"DROP TABLE {table_name};"
                 self.query_with_connection(drop_sql, connection, commit=False)
                 return True
 
             return False
 
@@ -221,15 +225,15 @@
             return self.table_exists_with_connection(table_name, connection, view)
 
     def table_exists_with_connection(self, table_name, connection, view=True):
 
         # Extract the table and schema from this. If no schema is detected then
         # will default to the public schema.
         try:
-            schema, table = table_name.lower().split('.', 1)
+            schema, table = table_name.lower().split(".", 1)
         except ValueError:
             schema, table = "public", table_name.lower()
 
         with self.cursor(connection) as cursor:
 
             # Check in pg tables for the table
             sql = f"""select count(*) from pg_tables where schemaname='{schema}' and
```

### Comparing `parsons-1.0.0/parsons/databases/postgres/postgres_create_statement.py` & `parsons-1.1.0/parsons/databases/redshift/rs_create_table.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,35 +1,32 @@
 from parsons.databases.database.database import DatabaseCreateStatement
-import parsons.databases.postgres.constants as consts
+import parsons.databases.redshift.constants as consts
 
 import petl
 import logging
 
 logger = logging.getLogger(__name__)
 
 
-class PostgresCreateStatement(DatabaseCreateStatement):
-
+class RedshiftCreateTable(DatabaseCreateStatement):
     def __init__(self):
         super().__init__()
 
         self.COL_NAME_MAX_LEN = consts.COL_NAME_MAX_LEN
         self.REPLACE_CHARS = consts.REPLACE_CHARS
 
-        # Postgres doesn't have a medium int
+        # Currently smallints are coded as ints
+        self.SMALLINT = self.INT
+        # Redshift doesn't have a medium int
         self.MEDIUMINT = self.INT
 
-        # Currently py floats are coded as Postgres decimals
-        self.FLOAT = consts.DECIMAL
+        # Currently py floats are coded as Redshift decimals
+        self.FLOAT = consts.FLOAT
 
-        # Max length of a Redshift VARCHAR column
         self.VARCHAR_MAX = consts.VARCHAR_MAX
-
-        # List of varchar lengths to use for columns -- this list needs to be in order from
-        # smallest to largest
         self.VARCHAR_STEPS = consts.VARCHAR_STEPS
 
     # the default behavior is f"{col}_"
     def _rename_reserved_word(self, col, index):
         """Return the renamed column.
 
         `Args`:
@@ -39,51 +36,62 @@
                 (Optional) The index of the column.
         `Returns`:
             str
                 The rename column.
         """
         return f"col_{index}"
 
-    def create_statement(self, tbl, table_name, padding=None, distkey=None, sortkey=None,
-                         varchar_max=None, varchar_truncate=True, columntypes=None,
-                         strict_length=True):
-        # Generate a table create statement. Distkeys and sortkeys are only used by
-        # Redshift and should not be passed when generating a create statement for
-        # Postgres.
+    def create_statement(
+        self,
+        tbl,
+        table_name,
+        padding=None,
+        distkey=None,
+        sortkey=None,
+        varchar_max=None,
+        varchar_truncate=True,
+        columntypes=None,
+        strict_length=True,
+    ):
 
-        if tbl.num_rows == 0:
-            raise ValueError('Table is empty. Must have 1 or more rows.')
+        # Warn the user if they don't provide a DIST key or a SORT key
+        self._log_key_warning(distkey=distkey, sortkey=sortkey, method="copy")
+
+        # Generate a table create statement
 
         # Validate and rename column names if needed
         tbl.table = petl.setheader(tbl.table, self.column_name_validate(tbl.columns))
 
+        if tbl.num_rows == 0:
+            raise ValueError("Table is empty. Must have 1 or more rows.")
+
         mapping = self.generate_data_types(tbl)
 
         if padding:
-            mapping['longest'] = self.vc_padding(mapping, padding)
+            mapping["longest"] = self.vc_padding(mapping, padding)
         elif not strict_length:
-            mapping['longest'] = self.vc_step(mapping)
+            mapping["longest"] = self.vc_step(mapping)
 
         if varchar_max:
-            mapping['longest'] = self.vc_max(mapping, varchar_max)
+            mapping["longest"] = self.vc_max(mapping, varchar_max)
 
         if varchar_truncate:
-            mapping['longest'] = self.vc_trunc(mapping)
+            mapping["longest"] = self.vc_trunc(mapping)
 
-        mapping['longest'] = self.vc_validate(mapping)
+        mapping["longest"] = self.vc_validate(mapping)
 
         # Add any provided column type overrides
         if columntypes:
-            for i in range(len(mapping['headers'])):
-                col = mapping['headers'][i]
+            for i in range(len(mapping["headers"])):
+                col = mapping["headers"][i]
                 if columntypes.get(col):
-                    mapping['type_list'][i] = columntypes[col]
+                    mapping["type_list"][i] = columntypes[col]
 
         # Enclose in quotes
-        mapping['headers'] = [f'"{h}"'for h in mapping['headers']]
+        mapping["headers"] = ['"{}"'.format(h) for h in mapping["headers"]]
 
         return self.create_sql(table_name, mapping, distkey=distkey, sortkey=sortkey)
 
     # This is for backwards compatability
     def data_type(self, val, current_type):
         return self.detect_data_type(val, current_type)
 
@@ -97,103 +105,142 @@
         longest, type_list = [], []
 
         cont = petl.records(table.table)
 
         # Populate empty values for the columns
         for col in table.columns:
             longest.append(0)
-            type_list.append('')
+            type_list.append("")
 
         for row in cont:
             for i in range(len(row)):
                 # NA is the csv null value
-                if type_list[i] == 'varchar' or row[i] in ['NA', '']:
+                if type_list[i] == "varchar" or row[i] in ["NA", ""]:
                     pass
                 else:
                     var_type = self.data_type(row[i], type_list[i])
                     type_list[i] = var_type
 
                 # Calculate width
-                width = len(str(row[i]).encode('utf-8'))
-                if width > longest[i]:
-                    longest[i] = width
+                if len(str(row[i]).encode("utf-8")) > longest[i]:
+                    longest[i] = len(str(row[i]).encode("utf-8"))
 
-        # In L140 'NA' and '' will be skipped
+        # In L138 'NA' and '' will be skipped
         # If the entire column is either one of those (or a mix of the two)
         # the type will be empty.
         # Fill with a default varchar
-        type_list = [typ or 'varchar' for typ in type_list]
+        type_list = [typ or "varchar" for typ in type_list]
 
-        return {'longest': longest,
-                'headers': table.columns,
-                'type_list': type_list}
+        return {"longest": longest, "headers": table.columns, "type_list": type_list}
 
     def vc_padding(self, mapping, padding):
         # Pad the width of a varchar column
 
-        return [int(c + (c * padding)) for c in mapping['longest']]
+        return [int(c + (c * padding)) for c in mapping["longest"]]
 
     def vc_step(self, mapping):
-        return [self.round_longest(c) for c in mapping['longest']]
+        return [self.round_longest(c) for c in mapping["longest"]]
 
     def vc_max(self, mapping, columns):
         # Set the varchar width of a column to the maximum
 
         for c in columns:
 
             try:
-                idx = mapping['headers'].index(c)
-                mapping['longest'][idx] = self.VARCHAR_MAX
+                idx = mapping["headers"].index(c)
+                mapping["longest"][idx] = self.VARCHAR_MAX
 
             except KeyError as error:
-                logger.error('Could not find column name provided.')
+                logger.error("Could not find column name provided.")
                 raise error
 
-        return mapping['longest']
+        return mapping["longest"]
 
     def vc_trunc(self, mapping):
 
-        return [self.VARCHAR_MAX if c > self.VARCHAR_MAX else c for c in mapping['longest']]
+        return [
+            self.VARCHAR_MAX if c > self.VARCHAR_MAX else c for c in mapping["longest"]
+        ]
 
     def vc_validate(self, mapping):
 
-        return [1 if c == 0 else c for c in mapping['longest']]
+        return [1 if c == 0 else c for c in mapping["longest"]]
 
     def create_sql(self, table_name, mapping, distkey=None, sortkey=None):
         # Generate the sql to create the table
 
-        statement = f'create table {table_name} ('
+        statement = "create table {} (".format(table_name)
 
-        for i in range(len(mapping['headers'])):
-            if mapping['type_list'][i] == 'varchar':
-                statement = (statement + '\n  {} varchar({}),').format(str(mapping['headers'][i])
-                                                                       .lower(),
-                                                                       str(mapping['longest'][i]))
+        for i in range(len(mapping["headers"])):
+            if mapping["type_list"][i] == "varchar":
+                statement = (statement + "\n  {} varchar({}),").format(
+                    str(mapping["headers"][i]).lower(), str(mapping["longest"][i])
+                )
             else:
-                statement = (statement + '\n  ' + '{} {}' + ',').format(str(mapping['headers'][i])
-                                                                        .lower(),
-                                                                        mapping['type_list'][i])
+                statement = (statement + "\n  " + "{} {}" + ",").format(
+                    str(mapping["headers"][i]).lower(), mapping["type_list"][i]
+                )
 
-        statement = statement[:-1] + ') '
+        statement = statement[:-1] + ") "
 
         if distkey:
-            statement += f'\ndistkey({distkey}) '
+            statement += "\ndistkey({}) ".format(distkey)
 
-        if sortkey:
-            statement += f'\nsortkey({sortkey})'
+        if sortkey and isinstance(sortkey, list):
+            statement += "\ncompound sortkey("
+            statement += ", ".join(sortkey)
+            statement += ")"
+        elif sortkey:
+            statement += "\nsortkey({})".format(sortkey)
 
-        statement += ';'
+        statement += ";"
 
         return statement
 
     # This is for backwards compatability
     def column_name_validate(self, columns):
         return self.format_columns(columns, col_prefix="col_")
 
     @staticmethod
+    def _log_key_warning(distkey=None, sortkey=None, method=""):
+        # Log a warning message advising the user about DIST and SORT keys
+
+        if distkey and sortkey:
+            return
+
+        keys = [
+            (
+                distkey,
+                "DIST",
+                "https://aws.amazon.com/about-aws/whats-new/2019/08/amazon-redshift-"
+                "now-recommends-distribution-keys-for-improved-query-performance/",
+            ),
+            (
+                sortkey,
+                "SORT",
+                "https://docs.amazonaws.cn/en_us/redshift/latest/dg/c_best-practices-"
+                "sort-key.html",
+            ),
+        ]
+        warning = "".join(
+            [
+                "You didn't provide a {} key to method `parsons.redshift.Redshift.{}`.\n"
+                "You can learn about best practices here:\n{}.\n".format(
+                    keyname, method, keyinfo
+                )
+                for key, keyname, keyinfo in keys
+                if not key
+            ]
+        )
+
+        warning += "You may be able to further optimize your queries."
+
+        logger.warning(warning)
+
+    @staticmethod
     def round_longest(longest):
         # Find the value that will work best to fit our longest column value
         for step in consts.VARCHAR_STEPS:
             # Make sure we have padding
             if longest < step / 2:
                 return step
```

### Comparing `parsons-1.0.0/parsons/databases/redshift/redshift.py` & `parsons-1.1.0/parsons/databases/redshift/redshift.py`

 * *Files 14% similar despite different names*

```diff
@@ -21,16 +21,21 @@
 # data sets into memory.
 # 100k rows per batch at ~1k bytes each = ~100MB per batch.
 QUERY_BATCH_SIZE = 100000
 
 logger = logging.getLogger(__name__)
 
 
-class Redshift(RedshiftCreateTable, RedshiftCopyTable, RedshiftTableUtilities, RedshiftSchema,
-               Alchemy):
+class Redshift(
+    RedshiftCreateTable,
+    RedshiftCopyTable,
+    RedshiftTableUtilities,
+    RedshiftSchema,
+    Alchemy,
+):
     """
     A Redshift class to connect to database.
 
     Args:
         username: str
             Required if env variable ``REDSHIFT_USERNAME`` not populated
         password: str
@@ -60,38 +65,49 @@
             be provided in the Redshift copy command that does not require an access key.
         use_env_token: bool
             Controls use of the ``AWS_SESSION_TOKEN`` environment variable for S3. Defaults
             to ``True``. Set to ``False`` in order to ignore the ``AWS_SESSION_TOKEN`` environment
             variable even if the ``aws_session_token`` argument was not passed in.
     """
 
-    def __init__(self, username=None, password=None, host=None, db=None, port=None,
-                 timeout=10, s3_temp_bucket=None,
-                 aws_access_key_id=None, aws_secret_access_key=None, iam_role=None,
-                 use_env_token=True):
+    def __init__(
+        self,
+        username=None,
+        password=None,
+        host=None,
+        db=None,
+        port=None,
+        timeout=10,
+        s3_temp_bucket=None,
+        aws_access_key_id=None,
+        aws_secret_access_key=None,
+        iam_role=None,
+        use_env_token=True,
+    ):
         super().__init__()
 
         try:
-            self.username = username or os.environ['REDSHIFT_USERNAME']
-            self.password = password or os.environ['REDSHIFT_PASSWORD']
-            self.host = host or os.environ['REDSHIFT_HOST']
-            self.db = db or os.environ['REDSHIFT_DB']
-            self.port = port or os.environ['REDSHIFT_PORT']
+            self.username = username or os.environ["REDSHIFT_USERNAME"]
+            self.password = password or os.environ["REDSHIFT_PASSWORD"]
+            self.host = host or os.environ["REDSHIFT_HOST"]
+            self.db = db or os.environ["REDSHIFT_DB"]
+            self.port = port or os.environ["REDSHIFT_PORT"]
         except KeyError as error:
-            logger.error("Connection info missing. Most include as kwarg or "
-                         "env variable.")
+            logger.error(
+                "Connection info missing. Most include as kwarg or " "env variable."
+            )
             raise error
 
         self.timeout = timeout
-        self.dialect = 'redshift'
-        self.s3_temp_bucket = s3_temp_bucket or os.environ.get('S3_TEMP_BUCKET')
+        self.dialect = "redshift"
+        self.s3_temp_bucket = s3_temp_bucket or os.environ.get("S3_TEMP_BUCKET")
         # Set prefix for temp S3 bucket paths that include subfolders
         self.s3_temp_bucket_prefix = None
-        if self.s3_temp_bucket and '/' in self.s3_temp_bucket:
-            split_temp_bucket_name = self.s3_temp_bucket.split('/', 1)
+        if self.s3_temp_bucket and "/" in self.s3_temp_bucket:
+            split_temp_bucket_name = self.s3_temp_bucket.split("/", 1)
             self.s3_temp_bucket = split_temp_bucket_name[0]
             self.s3_temp_bucket_prefix = split_temp_bucket_name[1]
         self.use_env_token = use_env_token
         # We don't check/load the environment variables for aws_* here
         # because the logic in S3() and rs_copy_table.py does already.
         self.aws_access_key_id = aws_access_key_id
         self.aws_secret_access_key = aws_secret_access_key
@@ -109,17 +125,22 @@
         ``with rs.connection() as conn:``
 
         `Returns:`
             Psycopg2 ``connection`` object
         """
 
         # Create a psycopg2 connection and cursor
-        conn = psycopg2.connect(user=self.username, password=self.password,
-                                host=self.host, dbname=self.db, port=self.port,
-                                connect_timeout=self.timeout)
+        conn = psycopg2.connect(
+            user=self.username,
+            password=self.password,
+            host=self.host,
+            dbname=self.db,
+            port=self.port,
+            connect_timeout=self.timeout,
+        )
         try:
             yield conn
 
             conn.commit()
         finally:
             conn.close()
 
@@ -196,64 +217,87 @@
                 See :ref:`parsons-table` for output options.
         """
 
         # To Do: Have it return an ordered dict to return the
         #        rows in the correct order
 
         with self.cursor(connection) as cursor:
-
-            if 'credentials' not in sql:
-                logger.debug(f'SQL Query: {sql}')
+            if "credentials" not in sql:
+                logger.debug(f"SQL Query: {sql}")
             cursor.execute(sql, parameters)
 
             if commit:
                 connection.commit()
 
             # If the cursor is empty, don't cause an error
             if not cursor.description:
-                logger.debug('Query returned 0 rows')
+                logger.debug("Query returned 0 rows")
                 return None
 
             else:
-
                 # Fetch the data in batches, and "pickle" the rows to a temp file.
                 # (We pickle rather than writing to, say, a CSV, so that we maintain
                 # all the type information for each field.)
 
                 temp_file = files.create_temp_file()
 
-                with open(temp_file, 'wb') as f:
+                with open(temp_file, "wb") as f:
                     # Grab the header
                     header = [i[0] for i in cursor.description]
                     pickle.dump(header, f)
 
                     while True:
                         batch = cursor.fetchmany(QUERY_BATCH_SIZE)
                         if not batch:
                             break
 
-                        logger.debug(f'Fetched {len(batch)} rows.')
+                        logger.debug(f"Fetched {len(batch)} rows.")
                         for row in batch:
                             pickle.dump(list(row), f)
 
                 # Load a Table from the file
                 final_tbl = Table(petl.frompickle(temp_file))
 
-                logger.debug(f'Query returned {final_tbl.num_rows} rows.')
+                logger.debug(f"Query returned {final_tbl.num_rows} rows.")
                 return final_tbl
 
-    def copy_s3(self, table_name, bucket, key, manifest=False, data_type='csv',
-                csv_delimiter=',', compression=None, if_exists='fail', max_errors=0,
-                distkey=None, sortkey=None, padding=None, varchar_max=None,
-                statupdate=True, compupdate=True, ignoreheader=1, acceptanydate=True,
-                dateformat='auto', timeformat='auto', emptyasnull=True,
-                blanksasnull=True, nullas=None, acceptinvchars=True, truncatecolumns=False,
-                columntypes=None, specifycols=None,
-                aws_access_key_id=None, aws_secret_access_key=None, bucket_region=None,
-                strict_length=True, template_table=None):
+    def copy_s3(
+        self,
+        table_name,
+        bucket,
+        key,
+        manifest=False,
+        data_type="csv",
+        csv_delimiter=",",
+        compression=None,
+        if_exists="fail",
+        max_errors=0,
+        distkey=None,
+        sortkey=None,
+        padding=None,
+        varchar_max=None,
+        statupdate=True,
+        compupdate=True,
+        ignoreheader=1,
+        acceptanydate=True,
+        dateformat="auto",
+        timeformat="auto",
+        emptyasnull=True,
+        blanksasnull=True,
+        nullas=None,
+        acceptinvchars=True,
+        truncatecolumns=False,
+        columntypes=None,
+        specifycols=None,
+        aws_access_key_id=None,
+        aws_secret_access_key=None,
+        bucket_region=None,
+        strict_length=True,
+        template_table=None,
+    ):
         """
         Copy a file from s3 to Redshift.
 
         `Args:`
             table_name: str
                 The table name and schema (``tmc.cool_table``) to point the file.
             bucket: str
@@ -347,67 +391,111 @@
 
         `Returns`
             Parsons Table or ``None``
                 See :ref:`parsons-table` for output options.
         """
 
         with self.connection() as connection:
-
             if self._create_table_precheck(connection, table_name, if_exists):
                 if template_table:
-                    sql = f'CREATE TABLE {table_name} (LIKE {template_table})'
+                    sql = f"CREATE TABLE {table_name} (LIKE {template_table})"
                 else:
                     # Grab the object from s3
                     from parsons.aws.s3 import S3
-                    s3 = S3(aws_access_key_id=aws_access_key_id,
-                            aws_secret_access_key=aws_secret_access_key,
-                            use_env_token=self.use_env_token)
+
+                    s3 = S3(
+                        aws_access_key_id=aws_access_key_id,
+                        aws_secret_access_key=aws_secret_access_key,
+                        use_env_token=self.use_env_token,
+                    )
 
                     local_path = s3.get_file(bucket, key)
-                    if data_type == 'csv':
+                    if data_type == "csv":
                         tbl = Table.from_csv(local_path, delimiter=csv_delimiter)
                     else:
                         raise TypeError("Invalid data type provided")
 
                     # Create the table
-                    sql = self.create_statement(tbl, table_name, padding=padding,
-                                                distkey=distkey, sortkey=sortkey,
-                                                varchar_max=varchar_max,
-                                                columntypes=columntypes,
-                                                strict_length=strict_length)
+                    sql = self.create_statement(
+                        tbl,
+                        table_name,
+                        padding=padding,
+                        distkey=distkey,
+                        sortkey=sortkey,
+                        varchar_max=varchar_max,
+                        columntypes=columntypes,
+                        strict_length=strict_length,
+                    )
 
                 self.query_with_connection(sql, connection, commit=False)
-                logger.info(f'{table_name} created.')
+                logger.info(f"{table_name} created.")
 
             # Copy the table
-            copy_sql = self.copy_statement(table_name, bucket, key, manifest=manifest,
-                                           data_type=data_type, csv_delimiter=csv_delimiter,
-                                           compression=compression, max_errors=max_errors,
-                                           statupdate=statupdate, compupdate=compupdate,
-                                           aws_access_key_id=aws_access_key_id,
-                                           aws_secret_access_key=aws_secret_access_key,
-                                           ignoreheader=ignoreheader, acceptanydate=acceptanydate,
-                                           emptyasnull=emptyasnull, blanksasnull=blanksasnull,
-                                           nullas=nullas, acceptinvchars=acceptinvchars,
-                                           truncatecolumns=truncatecolumns,
-                                           specifycols=specifycols,
-                                           dateformat=dateformat, timeformat=timeformat,
-                                           bucket_region=bucket_region)
+            copy_sql = self.copy_statement(
+                table_name,
+                bucket,
+                key,
+                manifest=manifest,
+                data_type=data_type,
+                csv_delimiter=csv_delimiter,
+                compression=compression,
+                max_errors=max_errors,
+                statupdate=statupdate,
+                compupdate=compupdate,
+                aws_access_key_id=aws_access_key_id,
+                aws_secret_access_key=aws_secret_access_key,
+                ignoreheader=ignoreheader,
+                acceptanydate=acceptanydate,
+                emptyasnull=emptyasnull,
+                blanksasnull=blanksasnull,
+                nullas=nullas,
+                acceptinvchars=acceptinvchars,
+                truncatecolumns=truncatecolumns,
+                specifycols=specifycols,
+                dateformat=dateformat,
+                timeformat=timeformat,
+                bucket_region=bucket_region,
+            )
 
             self.query_with_connection(copy_sql, connection, commit=False)
-            logger.info(f'Data copied to {table_name}.')
+            logger.info(f"Data copied to {table_name}.")
 
-    def copy(self, tbl, table_name, if_exists='fail', max_errors=0, distkey=None,
-             sortkey=None, padding=None, statupdate=None, compupdate=None, acceptanydate=True,
-             emptyasnull=True, blanksasnull=True, nullas=None, acceptinvchars=True,
-             dateformat='auto', timeformat='auto', varchar_max=None, truncatecolumns=False,
-             columntypes=None, specifycols=None, alter_table=False, alter_table_cascade=False,
-             aws_access_key_id=None, aws_secret_access_key=None, iam_role=None,
-             cleanup_s3_file=True, template_table=None, temp_bucket_region=None,
-             strict_length=True, csv_encoding='utf-8'):
+    def copy(
+        self,
+        tbl,
+        table_name,
+        if_exists="fail",
+        max_errors=0,
+        distkey=None,
+        sortkey=None,
+        padding=None,
+        statupdate=None,
+        compupdate=None,
+        acceptanydate=True,
+        emptyasnull=True,
+        blanksasnull=True,
+        nullas=None,
+        acceptinvchars=True,
+        dateformat="auto",
+        timeformat="auto",
+        varchar_max=None,
+        truncatecolumns=False,
+        columntypes=None,
+        specifycols=None,
+        alter_table=False,
+        alter_table_cascade=False,
+        aws_access_key_id=None,
+        aws_secret_access_key=None,
+        iam_role=None,
+        cleanup_s3_file=True,
+        template_table=None,
+        temp_bucket_region=None,
+        strict_length=True,
+        csv_encoding="utf-8",
+    ):
         """
         Copy a :ref:`parsons-table` to Redshift.
 
         `Args:`
             tbl: obj
                 A Parsons Table.
             table_name: str
@@ -525,79 +613,106 @@
         # Specify the columns for a copy statement.
         if specifycols or (specifycols is None and template_table):
             cols = tbl.columns
         else:
             cols = None
 
         with self.connection() as connection:
-
             # Check to see if the table exists. If it does not or if_exists = drop, then
             # create the new table.
             if self._create_table_precheck(connection, table_name, if_exists):
                 if template_table:
                     # Copy the schema from the template table
-                    sql = f'CREATE TABLE {table_name} (LIKE {template_table})'
+                    sql = f"CREATE TABLE {table_name} (LIKE {template_table})"
                 else:
-                    sql = self.create_statement(tbl, table_name, padding=padding,
-                                                distkey=distkey, sortkey=sortkey,
-                                                varchar_max=varchar_max,
-                                                columntypes=columntypes,
-                                                strict_length=strict_length)
+                    sql = self.create_statement(
+                        tbl,
+                        table_name,
+                        padding=padding,
+                        distkey=distkey,
+                        sortkey=sortkey,
+                        varchar_max=varchar_max,
+                        columntypes=columntypes,
+                        strict_length=strict_length,
+                    )
                 self.query_with_connection(sql, connection, commit=False)
-                logger.info(f'{table_name} created.')
+                logger.info(f"{table_name} created.")
 
             # If alter_table is True, then alter table if the table column widths
             # are wider than the existing table.
             if alter_table:
                 self.alter_varchar_column_widths(
-                    tbl, table_name, drop_dependencies=alter_table_cascade)
+                    tbl, table_name, drop_dependencies=alter_table_cascade
+                )
 
             # Upload the table to S3
-            key = self.temp_s3_copy(tbl, aws_access_key_id=aws_access_key_id,
-                                    aws_secret_access_key=aws_secret_access_key,
-                                    csv_encoding=csv_encoding)
+            key = self.temp_s3_copy(
+                tbl,
+                aws_access_key_id=aws_access_key_id,
+                aws_secret_access_key=aws_secret_access_key,
+                csv_encoding=csv_encoding,
+            )
 
             try:
                 # Copy to Redshift database.
-                copy_args = {'max_errors': max_errors,
-                             'ignoreheader': 1,
-                             'statupdate': statupdate,
-                             'compupdate': compupdate,
-                             'acceptanydate': acceptanydate,
-                             'dateformat': dateformat,
-                             'timeformat': timeformat,
-                             'blanksasnull': blanksasnull,
-                             'nullas': nullas,
-                             'emptyasnull': emptyasnull,
-                             'acceptinvchars': acceptinvchars,
-                             'truncatecolumns': truncatecolumns,
-                             'specifycols': cols,
-                             'aws_access_key_id': aws_access_key_id,
-                             'aws_secret_access_key': aws_secret_access_key,
-                             'compression': 'gzip',
-                             'bucket_region': temp_bucket_region}
+                copy_args = {
+                    "max_errors": max_errors,
+                    "ignoreheader": 1,
+                    "statupdate": statupdate,
+                    "compupdate": compupdate,
+                    "acceptanydate": acceptanydate,
+                    "dateformat": dateformat,
+                    "timeformat": timeformat,
+                    "blanksasnull": blanksasnull,
+                    "nullas": nullas,
+                    "emptyasnull": emptyasnull,
+                    "acceptinvchars": acceptinvchars,
+                    "truncatecolumns": truncatecolumns,
+                    "specifycols": cols,
+                    "aws_access_key_id": aws_access_key_id,
+                    "aws_secret_access_key": aws_secret_access_key,
+                    "compression": "gzip",
+                    "bucket_region": temp_bucket_region,
+                }
 
                 # Copy from S3 to Redshift
-                sql = self.copy_statement(table_name, self.s3_temp_bucket, key, **copy_args)
+                sql = self.copy_statement(
+                    table_name, self.s3_temp_bucket, key, **copy_args
+                )
                 sql_censored = sql_helpers.redact_credentials(sql)
 
-                logger.debug(f'Copy SQL command: {sql_censored}')
+                logger.debug(f"Copy SQL command: {sql_censored}")
                 self.query_with_connection(sql, connection, commit=False)
 
-                logger.info(f'Data copied to {table_name}.')
+                logger.info(f"Data copied to {table_name}.")
 
             # Clean up the S3 bucket.
             finally:
                 if key and cleanup_s3_file:
                     self.temp_s3_delete(key)
 
-    def unload(self, sql, bucket, key_prefix, manifest=True, header=True, delimiter='|',
-               compression='gzip', add_quotes=True, null_as=None, escape=True, allow_overwrite=True,
-               parallel=True, max_file_size='6.2 GB', aws_region=None, aws_access_key_id=None,
-               aws_secret_access_key=None):
+    def unload(
+        self,
+        sql,
+        bucket,
+        key_prefix,
+        manifest=True,
+        header=True,
+        delimiter="|",
+        compression="gzip",
+        add_quotes=True,
+        null_as=None,
+        escape=True,
+        allow_overwrite=True,
+        parallel=True,
+        max_file_size="6.2 GB",
+        aws_region=None,
+        aws_access_key_id=None,
+        aws_secret_access_key=None,
+    ):
         """
         Unload Redshift data to S3 Bucket. This is a more efficient method than running a query
         to export data as it can export in parallel and directly into an S3 bucket. Consider
         using this for exports of 10MM or more rows.
 
         sql: str
             The SQL string to execute to generate the data to unload.
@@ -673,24 +788,32 @@
         if escape:
             statement += "ESCAPE \n"
         if allow_overwrite:
             statement += "ALLOWOVERWRITE \n"
         if aws_region:
             statement += f"REGION {aws_region} \n"
 
-        logger.info(f'Unloading data to s3://{bucket}/{key_prefix}')
+        logger.info(f"Unloading data to s3://{bucket}/{key_prefix}")
         # Censor sensitive data
         statement_censored = sql_helpers.redact_credentials(statement)
         logger.debug(statement_censored)
 
         return self.query(statement)
 
-    def generate_manifest(self, buckets, aws_access_key_id=None, aws_secret_access_key=None,
-                          mandatory=True, prefix=None, manifest_bucket=None, manifest_key=None,
-                          path=None):
+    def generate_manifest(
+        self,
+        buckets,
+        aws_access_key_id=None,
+        aws_secret_access_key=None,
+        mandatory=True,
+        prefix=None,
+        manifest_bucket=None,
+        manifest_key=None,
+        path=None,
+    ):
         """
         Given a list of S3 buckets, generate a manifest file (JSON format). A manifest file
         allows you to copy multiple files into a single table at once. Once the manifest is
         generated, you can pass it with the :func:`~parsons.redshift.Redshift.copy_s3` method.
 
         AWS keys are not required if ``AWS_ACCESS_KEY_ID`` and
         ``AWS_SECRET_ACCESS_KEY`` environmental variables set.
@@ -714,53 +837,66 @@
                 Optional key name for S3 bucket to write file
 
         `Returns:`
             ``dict`` of manifest
         """
 
         from parsons.aws import S3
-        s3 = S3(aws_access_key_id=aws_access_key_id,
-                aws_secret_access_key=aws_secret_access_key,
-                use_env_token=self.use_env_token)
+
+        s3 = S3(
+            aws_access_key_id=aws_access_key_id,
+            aws_secret_access_key=aws_secret_access_key,
+            use_env_token=self.use_env_token,
+        )
 
         # Deal with a single bucket being passed, rather than list.
         if isinstance(buckets, str):
             buckets = [buckets]
 
         # Generate manifest file
-        manifest = {'entries': []}
+        manifest = {"entries": []}
         for bucket in buckets:
-
             # Retrieve list of files in bucket
             key_list = s3.list_keys(bucket, prefix=prefix)
             for key in key_list:
-                manifest['entries'].append({
-                    'url': '/'.join(['s3:/', bucket, key]),
-                    'mandatory': mandatory
-                })
+                manifest["entries"].append(
+                    {"url": "/".join(["s3:/", bucket, key]), "mandatory": mandatory}
+                )
 
-        logger.info('Manifest generated.')
+        logger.info("Manifest generated.")
 
         # Save the file to s3 bucket if provided
         if manifest_key and manifest_bucket:
             # Dump the manifest to a temp JSON file
             manifest_path = files.create_temp_file()
-            with open(manifest_path, 'w') as manifest_file_obj:
+            with open(manifest_path, "w") as manifest_file_obj:
                 json.dump(manifest, manifest_file_obj, sort_keys=True, indent=4)
 
             # Upload the file to S3
             s3.put_file(manifest_bucket, manifest_key, manifest_path)
 
-            logger.info(f'Manifest saved to s3://{manifest_bucket}/{manifest_key}')
+            logger.info(f"Manifest saved to s3://{manifest_bucket}/{manifest_key}")
 
         return manifest
 
-    def upsert(self, table_obj, target_table, primary_key, vacuum=True, distinct_check=True,
-               cleanup_temp_table=True, alter_table=True, alter_table_cascade=False,
-               from_s3=False, distkey=None, sortkey=None, **copy_args):
+    def upsert(
+        self,
+        table_obj,
+        target_table,
+        primary_key,
+        vacuum=True,
+        distinct_check=True,
+        cleanup_temp_table=True,
+        alter_table=True,
+        alter_table_cascade=False,
+        from_s3=False,
+        distkey=None,
+        sortkey=None,
+        **copy_args,
+    ):
         """
         Preform an upsert on an existing table. An upsert is a function in which rows
         in a table are updated and inserted at the same time.
 
         `Args:`
             table_obj: obj
                 A Parsons table object
@@ -800,118 +936,124 @@
 
         # Set distkey and sortkey to argument or primary key. These keys will be used
         # for the staging table and, if it does not already exist, the destination table.
         distkey = distkey or primary_keys[0]
         sortkey = sortkey or primary_key
 
         if not self.table_exists(target_table):
-            logger.info('Target table does not exist. Copying into newly \
-                         created target table.')
+            logger.info(
+                "Target table does not exist. Copying into newly \
+                         created target table."
+            )
             self.copy(table_obj, target_table, distkey=distkey, sortkey=sortkey)
             return None
 
         if alter_table and table_obj:
             # Make target table column widths match incoming table, if necessary
-            self.alter_varchar_column_widths(table_obj, target_table,
-                                             drop_dependencies=alter_table_cascade)
+            self.alter_varchar_column_widths(
+                table_obj, target_table, drop_dependencies=alter_table_cascade
+            )
 
-        noise = f'{random.randrange(0, 10000):04}'[:4]
-        date_stamp = datetime.datetime.now().strftime('%Y%m%d_%H%M')
+        noise = f"{random.randrange(0, 10000):04}"[:4]
+        date_stamp = datetime.datetime.now().strftime("%Y%m%d_%H%M")
         # Generate a temp table like "table_tmp_20200210_1230_14212"
-        staging_tbl = '{}_stg_{}_{}'.format(target_table, date_stamp, noise)
+        staging_tbl = "{}_stg_{}_{}".format(target_table, date_stamp, noise)
 
         if distinct_check:
-            primary_keys_statement = ', '.join(primary_keys)
-            diff = self.query(f'''
+            primary_keys_statement = ", ".join(primary_keys)
+            diff = self.query(
+                f"""
                 select (
                     select count(*)
                     from {target_table}
                 ) - (
                     SELECT COUNT(*) from (
                         select distinct {primary_keys_statement}
                         from {target_table}
                     )
                 ) as total_count
-            ''').first
+            """
+            ).first
             if diff > 0:
-                raise ValueError('Primary key column contains duplicate values.')
+                raise ValueError("Primary key column contains duplicate values.")
 
         with self.connection() as connection:
-
             try:
                 # Copy to a staging table
-                logger.info(f'Building staging table: {staging_tbl}')
-                if 'compupdate' not in copy_args:
+                logger.info(f"Building staging table: {staging_tbl}")
+                if "compupdate" not in copy_args:
                     # Especially with a lot of columns, compupdate=True can
                     # cause a lot of processing/analysis by Redshift before upload.
                     # Since this is a temporary table, setting compression for each
                     # column is not impactful barely impactful
                     # https://docs.aws.amazon.com/redshift/latest/dg/c_Loading_tables_auto_compress.html
                     copy_args = dict(copy_args, compupdate=False)
 
                 if from_s3:
                     if table_obj is not None:
                         raise ValueError(
-                            'upsert(... from_s3=True) requires the first argument (table_obj)'
-                            ' to be None. from_s3 and table_obj are mutually exclusive.'
+                            "upsert(... from_s3=True) requires the first argument (table_obj)"
+                            " to be None. from_s3 and table_obj are mutually exclusive."
                         )
-                    self.copy_s3(staging_tbl,
-                                 template_table=target_table,
-                                 **copy_args)
+                    self.copy_s3(staging_tbl, template_table=target_table, **copy_args)
                 else:
-                    self.copy(table_obj, staging_tbl,
-                              template_table=target_table,
-                              alter_table=False,  # We just did our own alter table above
-                              distkey=distkey,
-                              sortkey=sortkey,
-                              **copy_args)
+                    self.copy(
+                        table_obj,
+                        staging_tbl,
+                        template_table=target_table,
+                        alter_table=False,  # We just did our own alter table above
+                        distkey=distkey,
+                        sortkey=sortkey,
+                        **copy_args,
+                    )
 
-                staging_table_name = staging_tbl.split('.')[1]
-                target_table_name = target_table.split('.')[1]
+                staging_table_name = staging_tbl.split(".")[1]
+                target_table_name = target_table.split(".")[1]
 
                 # Delete rows
                 comparisons = [
-                    f'{staging_table_name}.{primary_key} = {target_table_name}.{primary_key}'
+                    f"{staging_table_name}.{primary_key} = {target_table_name}.{primary_key}"
                     for primary_key in primary_keys
                 ]
-                where_clause = ' and '.join(comparisons)
+                where_clause = " and ".join(comparisons)
 
                 sql = f"""
                        DELETE FROM {target_table}
                        USING {staging_tbl}
                        WHERE {where_clause}
                        """
                 self.query_with_connection(sql, connection, commit=False)
-                logger.debug(f'Target rows deleted from {target_table}.')
+                logger.debug(f"Target rows deleted from {target_table}.")
 
                 # Insert rows
                 # ALTER TABLE APPEND would be more efficient, but you can't run it in a
                 # transaction block. It's worth the performance hit to not commit until the
                 # end.
                 sql = f"""
                        INSERT INTO {target_table}
                        SELECT * FROM {staging_tbl};
                        """
 
                 self.query_with_connection(sql, connection, commit=False)
-                logger.info(f'Target rows inserted to {target_table}')
+                logger.info(f"Target rows inserted to {target_table}")
 
             finally:
                 if cleanup_temp_table:
                     # Drop the staging table
-                    self.query_with_connection(f"DROP TABLE IF EXISTS {staging_tbl};",
-                                               connection, commit=False)
-                    logger.info(f'{staging_tbl} staging table dropped.')
+                    self.query_with_connection(
+                        f"DROP TABLE IF EXISTS {staging_tbl};", connection, commit=False
+                    )
+                    logger.info(f"{staging_tbl} staging table dropped.")
 
         # Vacuum table. You must commit when running this type of transaction.
         if vacuum:
             with self.connection() as connection:
                 connection.set_session(autocommit=True)
-                self.query_with_connection(f'VACUUM {target_table};', connection)
-                logger.info(f'{target_table} vacuumed.')
+                self.query_with_connection(f"VACUUM {target_table};", connection)
+                logger.info(f"{target_table} vacuumed.")
 
     def drop_dependencies_for_cols(self, schema, table, cols):
         fmt_cols = ", ".join([f"'{c}'" for c in cols])
         sql_depend = f"""
             select
                 distinct dependent_ns.nspname||'.'||dependent_view.relname as table_name
             from pg_depend
@@ -933,17 +1075,19 @@
                 and pg_attribute.attname in ({fmt_cols})
             ;
         """
 
         with self.connection() as connection:
             connection.set_session(autocommit=True)
             tbl = self.query_with_connection(sql_depend, connection)
-            dropped_views = [row['table_name'] for row in tbl]
+            dropped_views = [row["table_name"] for row in tbl]
             if dropped_views:
-                sql_drop = "\n".join([f"drop view {view};" for view in dropped_views])
+                sql_drop = "\n".join(
+                    [f"drop view {view} CASCADE;" for view in dropped_views]
+                )
                 tbl = self.query_with_connection(sql_drop, connection)
                 logger.info(f"Dropped the following views: {dropped_views}")
 
         return tbl
 
     def alter_varchar_column_widths(self, tbl, table_name, drop_dependencies=False):
         """
@@ -965,32 +1109,40 @@
 
         # Create a list of column names and max width for string values.
         pc = {c: tbl.get_column_max_width(c) for c in tbl.columns}
 
         # Determine the max width of the varchar columns in the Redshift table
         s, t = self.split_full_table_name(table_name)
         cols = self.get_columns(s, t)
-        rc = {k: v['max_length'] for k, v in cols.items() if v['data_type'] == 'character varying'}  # noqa: E501, E261
+        rc = {
+            k: v["max_length"]
+            for k, v in cols.items()
+            if v["data_type"] == "character varying"
+        }  # noqa: E501, E261
 
         # Figure out if any of the destination table varchar columns are smaller than the
         # associated Parsons table columns. If they are, then alter column types to expand
         # their width.
         for c in set(rc.keys()).intersection(set(pc.keys())):
             if rc[c] < pc[c] and rc[c] != 65535:
-                logger.info(f'{c} not wide enough. Expanding column width.')
+                logger.info(f"{c} not wide enough. Expanding column width.")
                 # If requested size is larger than Redshift will allow,
                 # automatically set to Redshift's max varchar width
                 new_size = 65535
                 if pc[c] < new_size:
                     new_size = pc[c]
                 if drop_dependencies:
                     self.drop_dependencies_for_cols(s, t, [c])
-                self.alter_table_column_type(table_name, c, 'varchar', varchar_width=new_size)
-
-    def alter_table_column_type(self, table_name, column_name, data_type, varchar_width=None):
+                self.alter_table_column_type(
+                    table_name, c, "varchar", varchar_width=new_size
+                )
+
+    def alter_table_column_type(
+        self, table_name, column_name, data_type, varchar_width=None
+    ):
         """
         Alter a column type of an existing table.
 
         table_name: str
             The table name (ex. ``my_schema.my_table``).
         column_name: str
             The target column name
@@ -1004,15 +1156,15 @@
 
         if varchar_width:
             sql += f"({varchar_width})"
 
         with self.connection() as connection:
             connection.set_session(autocommit=True)
             self.query_with_connection(sql, connection)
-            logger.info(f'Altered {table_name} {column_name}.')
+            logger.info(f"Altered {table_name} {column_name}.")
 
     def table(self, table_name):
         # Return a Redshift table object
 
         return RedshiftTable(self, table_name)
```

### Comparing `parsons-1.0.0/parsons/databases/redshift/rs_copy_table.py` & `parsons-1.1.0/parsons/databases/redshift/rs_copy_table.py`

 * *Files 6% similar despite different names*

```diff
@@ -13,39 +13,60 @@
     aws_access_key_id = None
     aws_secret_access_key = None
     iam_role = None
 
     def __init__(self, use_env_token=True):
         self.use_env_token = use_env_token
 
-    def copy_statement(self, table_name, bucket, key, manifest=False,
-                       data_type='csv', csv_delimiter=',', max_errors=0,
-                       statupdate=None, compupdate=None, ignoreheader=1, acceptanydate=True,
-                       dateformat='auto', timeformat='auto', emptyasnull=True,
-                       blanksasnull=True, nullas=None, acceptinvchars=True, truncatecolumns=False,
-                       specifycols=None, aws_access_key_id=None, aws_secret_access_key=None,
-                       compression=None, bucket_region=None):
+    def copy_statement(
+        self,
+        table_name,
+        bucket,
+        key,
+        manifest=False,
+        data_type="csv",
+        csv_delimiter=",",
+        max_errors=0,
+        statupdate=None,
+        compupdate=None,
+        ignoreheader=1,
+        acceptanydate=True,
+        dateformat="auto",
+        timeformat="auto",
+        emptyasnull=True,
+        blanksasnull=True,
+        nullas=None,
+        acceptinvchars=True,
+        truncatecolumns=False,
+        specifycols=None,
+        aws_access_key_id=None,
+        aws_secret_access_key=None,
+        compression=None,
+        bucket_region=None,
+    ):
 
         # Source / Destination
-        source = f's3://{bucket}/{key}'
+        source = f"s3://{bucket}/{key}"
 
         # Add column list for mapping or if there are fewer columns on source file
         col_list = f"({', '.join(specifycols)})" if specifycols is not None else ""
 
         sql = f"copy {table_name}{col_list} \nfrom '{source}' \n"
 
         # Generate credentials
         sql += self.get_creds(aws_access_key_id, aws_secret_access_key)
 
         # Other options
         if manifest:
             sql += "manifest \n"
         if bucket_region:
             sql += f"region '{bucket_region}'\n"
-            logger.info('Copying data from S3 bucket %s in region %s', bucket, bucket_region)
+            logger.info(
+                "Copying data from S3 bucket %s in region %s", bucket, bucket_region
+            )
         sql += f"maxerror {max_errors} \n"
 
         # Redshift has some default behavior when statupdate is left out
         # vs when it is explicitly set as on or off.
         if statupdate is not None:
             if statupdate:
                 sql += "statupdate on\n"
@@ -74,23 +95,23 @@
             sql += f"null as {nullas}"
         if acceptinvchars:
             sql += "acceptinvchars \n"
         if truncatecolumns:
             sql += "truncatecolumns \n"
 
         # Data Type
-        if data_type == 'csv':
+        if data_type == "csv":
             sql += f"csv delimiter '{csv_delimiter}' \n"
         else:
-            raise TypeError('Invalid data type specified.')
+            raise TypeError("Invalid data type specified.")
 
-        if compression == 'gzip':
-            sql += 'gzip \n'
+        if compression == "gzip":
+            sql += "gzip \n"
 
-        sql += ';'
+        sql += ";"
 
         return sql
 
     def get_creds(self, aws_access_key_id, aws_secret_access_key):
 
         if aws_access_key_id and aws_secret_access_key:
             # When we have credentials, then we don't need to set them again
@@ -101,53 +122,66 @@
             return f"credentials 'aws_iam_role={self.iam_role}'\n"
 
         elif self.aws_access_key_id and self.aws_secret_access_key:
 
             aws_access_key_id = self.aws_access_key_id
             aws_secret_access_key = self.aws_secret_access_key
 
-        elif 'AWS_ACCESS_KEY_ID' in os.environ and 'AWS_SECRET_ACCESS_KEY' in os.environ:
+        elif (
+            "AWS_ACCESS_KEY_ID" in os.environ and "AWS_SECRET_ACCESS_KEY" in os.environ
+        ):
 
-            aws_access_key_id = os.environ['AWS_ACCESS_KEY_ID']
-            aws_secret_access_key = os.environ['AWS_SECRET_ACCESS_KEY']
+            aws_access_key_id = os.environ["AWS_ACCESS_KEY_ID"]
+            aws_secret_access_key = os.environ["AWS_SECRET_ACCESS_KEY"]
 
         else:
 
             s3 = S3(use_env_token=self.use_env_token)
             creds = s3.aws.session.get_credentials()
             aws_access_key_id = creds.access_key
             aws_secret_access_key = creds.secret_key
 
         return "credentials 'aws_access_key_id={};aws_secret_access_key={}'\n".format(
-            aws_access_key_id,
-            aws_secret_access_key)
+            aws_access_key_id, aws_secret_access_key
+        )
 
-    def temp_s3_copy(self, tbl, aws_access_key_id=None, aws_secret_access_key=None,
-                     csv_encoding='utf-8'):
+    def temp_s3_copy(
+        self,
+        tbl,
+        aws_access_key_id=None,
+        aws_secret_access_key=None,
+        csv_encoding="utf-8",
+    ):
 
         if not self.s3_temp_bucket:
-            raise KeyError(("Missing S3_TEMP_BUCKET, needed for transferring data to Redshift. "
-                            "Must be specified as env vars or kwargs"
-                            ))
+            raise KeyError(
+                (
+                    "Missing S3_TEMP_BUCKET, needed for transferring data to Redshift. "
+                    "Must be specified as env vars or kwargs"
+                )
+            )
 
         # Coalesce S3 Key arguments
         aws_access_key_id = aws_access_key_id or self.aws_access_key_id
         aws_secret_access_key = aws_secret_access_key or self.aws_secret_access_key
 
-        self.s3 = S3(aws_access_key_id=aws_access_key_id,
-                     aws_secret_access_key=aws_secret_access_key, use_env_token=self.use_env_token)
+        self.s3 = S3(
+            aws_access_key_id=aws_access_key_id,
+            aws_secret_access_key=aws_secret_access_key,
+            use_env_token=self.use_env_token,
+        )
 
         hashed_name = hash(time.time())
         key = f"{S3_TEMP_KEY_PREFIX}/{hashed_name}.csv.gz"
         if self.s3_temp_bucket_prefix:
-            key = self.s3_temp_bucket_prefix + '/' + key
+            key = self.s3_temp_bucket_prefix + "/" + key
 
         # Convert table to compressed CSV file, to optimize the transfers to S3 and to
         # Redshift.
-        local_path = tbl.to_csv(temp_file_compression='gzip', encoding=csv_encoding)
+        local_path = tbl.to_csv(temp_file_compression="gzip", encoding=csv_encoding)
         # Copy table to bucket
         self.s3.put_file(self.s3_temp_bucket, key, local_path)
 
         return key
 
     def temp_s3_delete(self, key):
```

### Comparing `parsons-1.0.0/parsons/databases/redshift/rs_schema.py` & `parsons-1.1.0/parsons/databases/redshift/rs_schema.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,8 @@
 class RedshiftSchema(object):
-
     def schema_exists(self, schema):
         sql = f"select * from pg_namespace where nspname = '{schema}'"
         res = self.query(sql)
         return res.num_rows > 0
 
     def create_schema_with_permissions(self, schema, group=None):
         """
@@ -15,33 +14,33 @@
                 The schema name
             group: str
                 The Redshift group name
             type: str
                 The type of permissions to grant. Supports `select`, `all`, etc. (For
                 full list, see the
                 `Redshift GRANT docs <https://docs.aws.amazon.com/redshift/latest/dg/r_GRANT.html>`_)
-        """ # noqa: E501,E261
+        """  # noqa: E501,E261
 
         if not self.schema_exists(schema):
             self.query(f"create schema {schema}")
             self.query(f"grant usage on schema {schema} to group {group}")
 
-    def grant_schema_permissions(self, schema, group, permissions_type='select'):
+    def grant_schema_permissions(self, schema, group, permissions_type="select"):
         """
         Grants a Redshift group permissions to all tables within an existing schema.
 
         `Args:`
             schema: str
                 The schema name
             group: str
                 The Redshift group name
             type: str
                 The type of permissions to grant. Supports `select`, `all`, etc. (For
                 full list, see the
                 `Redshift GRANT docs <https://docs.aws.amazon.com/redshift/latest/dg/r_GRANT.html>`_)
-        """ # noqa: E501,E261
+        """  # noqa: E501,E261
 
         sql = f"""
             grant usage on schema {schema} to group {group};
             grant {permissions_type} on all tables in schema {schema} to group {group};
         """
         self.query(sql)
```

### Comparing `parsons-1.0.0/parsons/databases/redshift/rs_table_utilities.py` & `parsons-1.1.0/parsons/databases/redshift/rs_table_utilities.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 import logging
+
 # import pkgutil
 
 logger = logging.getLogger(__name__)
 
 
 class RedshiftTableUtilities(object):
-
     def __init__(self):
         pass
 
     def table_exists(self, table_name, view=True):
         """
         Check if a table or view exists in the database.
 
@@ -23,42 +23,46 @@
             boolean
                 ``True`` if the table exists and ``False`` if it does not.
         """
         with self.connection() as connection:
             return self.table_exists_with_connection(table_name, connection, view)
 
     def table_exists_with_connection(self, table_name, connection, view=True):
-        table_name = table_name.lower().split('.')
+        table_name = table_name.lower().split(".")
         table_name = [x.strip() for x in table_name]
 
         # Check in pg tables for the table
         sql = """select count(*) from pg_tables where schemaname='{}' and
-                 tablename='{}';""".format(table_name[0], table_name[1])
+                 tablename='{}';""".format(
+            table_name[0], table_name[1]
+        )
 
         # TODO maybe convert these queries to use self.query_with_connection
 
         with self.cursor(connection) as cursor:
 
             cursor.execute(sql)
             result = cursor.fetchone()[0]
 
             # Check in the pg_views for the table
             if view:
                 sql = """select count(*) from pg_views where schemaname='{}' and
-                         viewname='{}';""".format(table_name[0], table_name[1])
+                         viewname='{}';""".format(
+                    table_name[0], table_name[1]
+                )
 
             cursor.execute(sql)
             result += cursor.fetchone()[0]
 
         # If in either, return boolean
         if result >= 1:
-            logger.debug(f'{table_name[0]}.{table_name[1]} exists.')
+            logger.debug(f"{table_name[0]}.{table_name[1]} exists.")
             return True
         else:
-            logger.debug(f'{table_name[0]}.{table_name[1]} does NOT exist.')
+            logger.debug(f"{table_name[0]}.{table_name[1]} does NOT exist.")
             return False
 
     def get_row_count(self, table_name):
         """
         Return the row count of a table.
 
         **SQL Code**
@@ -71,15 +75,15 @@
             table_name: str
                 The schema and name (e.g. ``myschema.mytable``) of the table.
         `Returns:`
             int
         """
 
         count_query = self.query(f"select count(*) from {table_name}")
-        return count_query[0]['count']
+        return count_query[0]["count"]
 
     def rename_table(self, table_name, new_table_name):
         """
         Rename an existing table.
 
         .. note::
             You cannot move schemas when renaming a table. Instead, utilize
@@ -109,41 +113,41 @@
                 Name of existing schema and table (e.g. ``my_schema.old_table``)
             new_table: str
                 New name of schema and table (e.g. ``my_schema.newtable``)
             drop_original: boolean
                 Drop the source table.
         Returns:
                 None
-        """ # noqa: E501,E261
+        """  # noqa: E501,E261
 
         # To Do: Add the grants
         # To Do: Argument for if the table exists?
         # To Do: Add the ignore extra kwarg.
 
         create_sql = f"create table {new_table} (like {source_table});"
         alter_sql = f"alter table {new_table} append from {source_table}"
 
-        logger.info(f'Creating empty {new_table} from {source_table}.')
+        logger.info(f"Creating empty {new_table} from {source_table}.")
         self.query(create_sql)
 
         with self.connection() as conn:
 
             #  An ALTER TABLE statement can't be run within a block, meaning
             #  that it needs to be committed on running. To enable this,
             #  the connection must be set to autocommit.
 
             conn.set_session(autocommit=True)
-            logger.info(f'Moving data from {source_table} to {new_table}.')
+            logger.info(f"Moving data from {source_table} to {new_table}.")
             self.query_with_connection(alter_sql, conn)
 
         if drop_source_table:
             self.query(f"drop table {source_table};")
-            logger.info(f'{source_table} dropped.')
+            logger.info(f"{source_table} dropped.")
 
-        logger.info(f'{source_table} data moved from {new_table}  .')
+        logger.info(f"{source_table} data moved from {new_table}  .")
 
     def _create_table_precheck(self, connection, table_name, if_exists):
         """
         Helper to determine what to do when you need a table that may already exist.
 
         `Args:`
             connection: obj
@@ -154,38 +158,39 @@
                 If the table already exists, either ``fail``, ``append``, ``drop``,
                 or ``truncate`` the table.
         `Returns:`
             bool
                 True if the table needs to be created, False otherwise.
         """
 
-        if if_exists not in ['fail', 'truncate', 'append', 'drop']:
+        if if_exists not in ["fail", "truncate", "append", "drop"]:
             raise ValueError("Invalid value for `if_exists` argument")
 
         exists = self.table_exists_with_connection(table_name, connection)
 
-        if exists and if_exists in ['fail', 'truncate', 'append']:
-            if if_exists == 'fail':
-                raise ValueError('Table already exists.')
-            elif if_exists == 'truncate':
+        if exists and if_exists in ["fail", "truncate", "append"]:
+            if if_exists == "fail":
+                raise ValueError("Table already exists.")
+            elif if_exists == "truncate":
                 truncate_sql = f"truncate table {table_name}"
                 self.query_with_connection(truncate_sql, connection, commit=False)
 
         else:
-            if exists and if_exists == 'drop':
+            if exists and if_exists == "drop":
                 logger.debug(f"Table {table_name} exist, will drop...")
                 drop_sql = f"drop table {table_name};\n"
                 self.query_with_connection(drop_sql, connection, commit=False)
 
             return True
 
         return False
 
-    def populate_table_from_query(self, query, destination_table, if_exists='fail', distkey=None,
-                                  sortkey=None):
+    def populate_table_from_query(
+        self, query, destination_table, if_exists="fail", distkey=None, sortkey=None
+    ):
         """
         Populate a Redshift table with the results of a SQL query, creating the table if it
         doesn't yet exist.
 
         `Args:`
             query: str
                 The SQL query
@@ -196,15 +201,17 @@
                 or ``truncate`` the table.
             distkey: str
                 The column to use as the distkey for the table.
             sortkey: str
                 The column to use as the sortkey for the table.
         """
         with self.connection() as conn:
-            should_create = self._create_table_precheck(conn, destination_table, if_exists)
+            should_create = self._create_table_precheck(
+                conn, destination_table, if_exists
+            )
 
             if should_create:
                 logger.info(f"Creating table {destination_table} from query...")
                 sql = f"create table {destination_table}"
                 if distkey:
                     sql += f" distkey({distkey})"
                 if sortkey:
@@ -212,18 +219,24 @@
                 sql += f" as {query}"
             else:
                 logger.info(f"Inserting data into {destination_table} from query...")
                 sql = f"insert into {destination_table} ({query})"
 
             self.query_with_connection(sql, conn, commit=False)
 
-        logger.info(f'{destination_table} created from query')
+        logger.info(f"{destination_table} created from query")
 
-    def duplicate_table(self, source_table, destination_table, where_clause='',
-                        if_exists='fail', drop_source_table=False):
+    def duplicate_table(
+        self,
+        source_table,
+        destination_table,
+        where_clause="",
+        if_exists="fail",
+        drop_source_table=False,
+    ):
         """
         Create a copy of an existing table (or subset of rows) in a new
         table. It will inherit encoding, sortkey and distkey.
 
         `Args:`
             source_table: str
                 Name of existing schema and table (e.g. ``myschema.oldtable``)
@@ -235,32 +248,34 @@
                 If the table already exists, either ``fail``, ``append``, ``drop``,
                 or ``truncate`` the table.
             drop_source_table: boolean
                 Drop the source table
         """
 
         with self.connection() as conn:
-            should_create = self._create_table_precheck(conn, destination_table, if_exists)
+            should_create = self._create_table_precheck(
+                conn, destination_table, if_exists
+            )
 
             if should_create:
-                logger.info(f'Creating {destination_table} from {source_table}...')
+                logger.info(f"Creating {destination_table} from {source_table}...")
                 create_sql = f"create table {destination_table} (like {source_table})"
                 self.query_with_connection(create_sql, conn, commit=False)
 
             logger.info(f"Transferring data to {destination_table} from {source_table}")
             select_sql = f"select * from {source_table} {where_clause}"
             insert_sql = f"insert into {destination_table} ({select_sql})"
             self.query_with_connection(insert_sql, conn, commit=False)
 
             if drop_source_table:
-                logger.info(f'Dropping table {source_table}...')
+                logger.info(f"Dropping table {source_table}...")
                 drop_sql = f"drop table {source_table}"
                 self.query_with_connection(drop_sql, conn, commit=False)
 
-        logger.info(f'{destination_table} created from {source_table}.')
+        logger.info(f"{destination_table} created from {source_table}.")
 
     def union_tables(self, new_table_name, tables, union_all=True, view=False):
         """
         Union a series of table into a new table.
 
         Args:
             new_table_name: str
@@ -387,20 +402,20 @@
             from information_schema.columns
             where table_name = '{table_name}'
             and table_schema = '{schema}'
             order by ordinal_position
         """
 
         return {
-            row['column_name']: {
-                'data_type': row['data_type'],
-                'max_length': row['max_length'],
-                'max_precision': row['max_precision'],
-                'max_scale': row['max_scale'],
-                'is_nullable': row['is_nullable'] == 'YES',
+            row["column_name"]: {
+                "data_type": row["data_type"],
+                "max_length": row["max_length"],
+                "max_precision": row["max_precision"],
+                "max_scale": row["max_scale"],
+                "is_nullable": row["is_nullable"] == "YES",
             }
             for row in self.query(query)
         }
 
     def get_columns_list(self, schema, table_name):
         """
         Gets the just the column names for a table.
@@ -454,15 +469,15 @@
             Must be a Redshift superuser to run this method.
 
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        logger.info('Retrieving running and queued queries.')
+        logger.info("Retrieving running and queued queries.")
 
         # Lifted from Redshift Utils https://github.com/awslabs/amazon-redshift-utils/blob/master/src/AdminScripts/running_queues.sql # noqa: E501
         sql = """
               select trim(u.usename) as user,
                 s.pid,
                 q.xid,
                 q.query,
@@ -510,15 +525,17 @@
         `Args:`
             table_name: str
                 Schema and table name
             value_column: str
                 The column containing the values
         """
 
-        return self.query(f'SELECT MAX({value_column}) value from {table_name}')[0]['value']
+        return self.query(f"SELECT MAX({value_column}) value from {table_name}")[0][
+            "value"
+        ]
 
     def get_object_type(self, object_name):
         """
         Get object type.
 
         One of `view`, `table`, `index`, `sequence`, or `TOAST table`.
 
@@ -545,15 +562,15 @@
         """
         tbl = self.query(sql_obj_type)
 
         if tbl.num_rows == 0:
             logger.info(f"{object_name} doesn't exist.")
             return None
 
-        return tbl[0]['object_name']
+        return tbl[0]["object_name"]
 
     def is_view(self, object_name):
         """
         Return true if the object is a view.
 
         `Args:`
             object_name: str
@@ -622,16 +639,16 @@
 
         conditions = []
         if schema:
             conditions.append(f"schemaname like '{schema}'")
         if table:
             conditions.append(f"tablename like '{table}'")
 
-        conditions_str = ' and '.join(conditions)
-        where_clause = f"where {conditions_str}" if conditions_str else ''
+        conditions_str = " and ".join(conditions)
+        where_clause = f"where {conditions_str}" if conditions_str else ""
 
         # ddl_query = pkgutil.get_data(
         #     __name__, "queries/v_generate_tbl_ddl.sql").decode()
         sql_get_ddl = f"""
             select *
             from admin.v_generate_tbl_ddl
             {where_clause}
@@ -639,24 +656,24 @@
         ddl_table = self.query(sql_get_ddl)
 
         if ddl_table.num_rows == 0:
             logger.info(f"No tables matching {schema} and {table}.")
             return None
 
         def join_sql_parts(columns, rows):
-            return [f"{columns[1]}.{columns[2]}",
-                    '\n'.join([row[4] for row in rows])]
+            return [f"{columns[1]}.{columns[2]}", "\n".join([row[4] for row in rows])]
 
         # The query returns the sql over multiple rows
         # We need to join then into a single row
         ddl_table.reduce_rows(
-            ['table_id', 'schemaname', 'tablename'],
+            ["table_id", "schemaname", "tablename"],
             join_sql_parts,
-            ['tablename', 'ddl'],
-            presorted=True)
+            ["tablename", "ddl"],
+            presorted=True,
+        )
 
         return ddl_table.to_dicts()
 
     def get_view_definition(self, view):
         """
         Get the view definition (i.e. the create statement).
 
@@ -697,16 +714,16 @@
 
         conditions = []
         if schema:
             conditions.append(f"schemaname like '{schema}'")
         if view:
             conditions.append(f"g.viewname like '{view}'")
 
-        conditions_str = ' and '.join(conditions)
-        where_clause = f"where {conditions_str}" if conditions_str else ''
+        conditions_str = " and ".join(conditions)
+        where_clause = f"where {conditions_str}" if conditions_str else ""
 
         # ddl_query = pkgutil.get_data(
         #     __name__, "queries/v_generate_view_ddl.sql").decode()
         sql_get_ddl = f"""
             select schemaname || '.' || viewname as viewname, ddl
             from admin.v_generate_view_ddl g
             {where_clause}
```

### Comparing `parsons-1.0.0/parsons/databases/table.py` & `parsons-1.1.0/parsons/databases/table.py`

 * *Files 6% similar despite different names*

```diff
@@ -28,20 +28,22 @@
         return self.db.query(f"SELECT COUNT(*) FROM {self.table}").first
 
     def max_primary_key(self, primary_key):
         """
         Get the maximum primary key in the table.
         """
 
-        return self.db.query(f"""
+        return self.db.query(
+            f"""
             SELECT {primary_key}
             FROM {self.table}
             ORDER BY {primary_key} DESC
             LIMIT 1
-        """).first
+        """
+        ).first
 
     def distinct_primary_key(self, primary_key):
         """
         Check if the passed primary key column is distinct.
         """
 
         sql = f"""
@@ -121,15 +123,15 @@
         It will select every value greater than the provided value.
         """
 
         if cutoff_value is not None:
             where_clause = f"WHERE {primary_key} > %s"
             parameters = [cutoff_value]
         else:
-            where_clause = ''
+            where_clause = ""
             parameters = []
 
         sql = f"""
                SELECT
                *
                FROM {self.table}
                {where_clause}
@@ -144,21 +146,21 @@
         return self.db.query(sql, parameters)
 
     def drop(self, cascade=False):
         """
         Drop the table.
         """
 
-        sql = f'DROP TABLE {self.table}'
+        sql = f"DROP TABLE {self.table}"
         if cascade:
-            sql += ' CASCADE'
+            sql += " CASCADE"
 
         self.db.query(sql)
-        logger.info(f'{self.table} dropped.')
+        logger.info(f"{self.table} dropped.")
 
     def truncate(self):
         """
         Truncate the table.
         """
 
-        self.db.query(f'TRUNCATE TABLE {self.table}')
-        logger.info(f'{self.table} truncated.')
+        self.db.query(f"TRUNCATE TABLE {self.table}")
+        logger.info(f"{self.table} truncated.")
```

### Comparing `parsons-1.0.0/parsons/donorbox/donorbox.py` & `parsons-1.1.0/parsons/donorbox/donorbox.py`

 * *Files 1% similar despite different names*

```diff
@@ -20,16 +20,16 @@
                 argument or set as ``DONORBOX_ACCOUNT_EMAIL`` environment variable.
             donorbox_api_key: str
                 The API key generated by Donorbox for your account. Can be passed as
                 argument or set as ``DONORBOX_API_KEY`` environment variable.
     """
 
     def __init__(self, email=None, api_key=None):
-        self.email = check_env.check('DONORBOX_ACCOUNT_EMAIL', email)
-        self.api_key = check_env.check('DONORBOX_API_KEY', api_key)
+        self.email = check_env.check("DONORBOX_ACCOUNT_EMAIL", email)
+        self.api_key = check_env.check("DONORBOX_API_KEY", api_key)
         self.uri = URI
         self.client = APIConnector(self.uri, auth=(self.email, self.api_key))
 
     def get_campaigns(self, **kwargs):
         """
         Get information on campaigns.
 
@@ -136,15 +136,17 @@
             per_page: int
                 Optional. Results per page when using pagination. Default is 50, maximum is 100.
 
         `Returns`:
             Parsons Table
         """
         if "donor_id" in kwargs:
-            kwargs["id"] = kwargs.pop("donor_id")  # switch to Donorbox's (less specific) name
+            kwargs["id"] = kwargs.pop(
+                "donor_id"
+            )  # switch to Donorbox's (less specific) name
         data = self.client.get_request("donors", params=kwargs)
         return Table(data)
 
     def get_plans(self, **kwargs):
         """
         Get information on plans.
 
@@ -208,16 +210,18 @@
         Valid formats: YYYY-mm-dd YYYY/mm/dd YYYYmmdd dd-mm-YYYY
 
         date_string: str
             Required. Date in a string format to be checked against Donorbox's valid options.
 
         `Returns`: None
         """
-        valid_formats = ['%Y-%m-%d', '%d-%m-%Y', '%Y/%m/%d', '%Y%m%d']
+        valid_formats = ["%Y-%m-%d", "%d-%m-%Y", "%Y/%m/%d", "%Y%m%d"]
         for str_format in valid_formats:
             try:
                 datetime.datetime.strptime(date_string, str_format)
                 return
             except ValueError:
                 continue
-        raise ValueError(f"The date you supplied, {date_string}, is not a valid Donorbox format." +
-                         "Try the following formats: YYYY-mm-dd YYYY/mm/dd YYYYmmdd dd-mm-YYYY")
+        raise ValueError(
+            f"The date you supplied, {date_string}, is not a valid Donorbox format."
+            + "Try the following formats: YYYY-mm-dd YYYY/mm/dd YYYYmmdd dd-mm-YYYY"
+        )
```

### Comparing `parsons-1.0.0/parsons/etl/etl.py` & `parsons-1.1.0/parsons/etl/etl.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,15 +1,14 @@
 import petl
 import logging
 
 logger = logging.getLogger(__name__)
 
 
 class ETL(object):
-
     def __init__(self):
 
         pass
 
     def add_column(self, column, value=None, index=None):
         """
         Add a column to your table
@@ -77,16 +76,17 @@
             fill_value:
                 A fixed or calculated value
         `Returns:`
             `Parsons Table` and also updates self
         """
 
         if callable(fill_value):
-            self.table = petl.convert(self.table, column_name, lambda _, r: fill_value(r),
-                                      pass_row=True)
+            self.table = petl.convert(
+                self.table, column_name, lambda _, r: fill_value(r), pass_row=True
+            )
         else:
             self.table = petl.update(self.table, column_name, fill_value)
 
         return self
 
     def fillna_column(self, column_name, fill_value):
         """
@@ -98,19 +98,28 @@
             fill_value:
                 A fixed or calculated value
         `Returns:`
             `Parsons Table` and also updates self
         """
 
         if callable(fill_value):
-            self.table = petl.convert(self.table, column_name, lambda _, r: fill_value(r),
-                                      where=lambda r: r[column_name] is None, pass_row=True)
+            self.table = petl.convert(
+                self.table,
+                column_name,
+                lambda _, r: fill_value(r),
+                where=lambda r: r[column_name] is None,
+                pass_row=True,
+            )
         else:
-            self.table = petl.update(self.table, column_name, fill_value,
-                                     where=lambda r: r[column_name] is None)
+            self.table = petl.update(
+                self.table,
+                column_name,
+                fill_value,
+                where=lambda r: r[column_name] is None,
+            )
 
         return self
 
     def move_column(self, column, index):
         """
         Move a column
 
@@ -157,16 +166,16 @@
             int
         """
 
         max_width = 0
 
         for v in petl.values(self.table, column):
 
-            if len(str(v).encode('utf-8')) > max_width:
-                max_width = len(str(v).encode('utf-8'))
+            if len(str(v).encode("utf-8")) > max_width:
+                max_width = len(str(v).encode("utf-8"))
 
         return max_width
 
     def convert_columns_to_str(self):
         """
         Convenience function to convert all non-string or mixed columns in a
         Parsons table to string (e.g. for comparison)
@@ -185,16 +194,16 @@
             if x is None:
                 return ""
             return str(x)
 
         for col in cols:
             # If there's more than one type (or no types), convert to str
             # Also if there is one type and it's not str, convert to str
-            if len(col['type']) != 1 or col['type'][0] != 'str':
-                self.convert_column(col['name'], str_or_empty)
+            if len(col["type"]) != 1 or col["type"][0] != "str":
+                self.convert_column(col["name"], str_or_empty)
 
         return self
 
     def coalesce_columns(self, dest_column, source_columns, remove_source_columns=True):
         """
         Coalesces values from one or more source columns into a destination column, by selecting
         the first non-empty value. If the destination column doesn't exist, it will be added.
@@ -208,23 +217,25 @@
                 Whether to remove the source columns after the coalesce. If the destination
                 column is also one of the source columns, it will not be removed.
         `Returns:`
             `Parsons Table` and also updates self
         """
 
         if dest_column in self.columns:
+
             def convert_fn(value, row):
                 for source_col in source_columns:
                     if row.get(source_col):
                         return row[source_col]
 
             logger.debug(f"Coalescing {source_columns} into {dest_column}")
             self.convert_column(dest_column, convert_fn, pass_row=True)
 
         else:
+
             def add_fn(row):
                 for source_col in source_columns:
                     if row.get(source_col):
                         return row[source_col]
 
             logger.debug(f"Creating new column {dest_column} from {source_columns}")
             self.add_column(dest_column, add_fn)
@@ -264,15 +275,15 @@
             print (tbl)
             >> {{'first_name': 'Jane', 'last_name': 'Doe', 'date_of_birth': '1908-01-01'}}
         """
 
         for col in self.columns:
 
             if not exact_match:
-                cleaned_col = col.lower().replace('_', '').replace(' ', '')
+                cleaned_col = col.lower().replace("_", "").replace(" ", "")
             else:
                 cleaned_col = col
 
             for k, v in column_map.items():
                 for i in v:
                     if cleaned_col == i:
                         self.rename_column(col, k)
@@ -317,16 +328,16 @@
             # remove it from the list of columns to coalesce
             for item in coalesce_list:
                 if item not in self.columns:
                     coalesce_list.remove(item)
             # if the key from the mapping dict already exists in the table,
             # rename it so it can be coalesced with other possible columns
             if key in self.columns:
-                self.rename_column(key, f'{key}_temp')
-                coalesce_list.insert(0, f'{key}_temp')
+                self.rename_column(key, f"{key}_temp")
+                coalesce_list.insert(0, f"{key}_temp")
 
             # coalesce columns
             self.coalesce_columns(key, coalesce_list, remove_source_columns=True)
 
         return self
 
     def get_column_types(self, column):
@@ -351,16 +362,18 @@
             list
                 A list of dicts
         `Returns:`
             list
                 A list of dicts, each containing a column 'name' and a 'type' list
         """
 
-        return [{'name': col, 'type': self.get_column_types(col)}
-                for col in self.table.columns()]
+        return [
+            {"name": col, "type": self.get_column_types(col)}
+            for col in self.table.columns()
+        ]
 
     def convert_table(self, *args):
         """
         Transform all cells in a table via arbitrary functions, method invocations or dictionary
         translations. This method is useful for cleaning fields and data hygiene functions such
         as regex. This method leverages the petl ``convert()`` method. Example usage can be
         found `here` <https://petl.readthedocs.io/en/v0.24/transform.html#petl.convert>`_.
@@ -372,17 +385,24 @@
             `Parsons Table` and also updates self
         """  # noqa: W605
 
         self.convert_column(self.columns, *args)
 
         return self
 
-    def unpack_dict(self, column, keys=None, include_original=False,
-                    sample_size=5000, missing=None, prepend=True,
-                    prepend_value=None):
+    def unpack_dict(
+        self,
+        column,
+        keys=None,
+        include_original=False,
+        sample_size=5000,
+        missing=None,
+        prepend=True,
+        prepend_value=None,
+    ):
         """
         Unpack dictionary values from one column into separate columns
 
         `Args:`
             column: str
                 The column name to unpack
             keys: list
@@ -403,26 +423,36 @@
         """
 
         if prepend:
             if prepend_value is None:
                 prepend_value = column
 
             self.table = petl.convert(
-                self.table,
-                column,
-                lambda v: self._prepend_dict(v, prepend_value))
+                self.table, column, lambda v: self._prepend_dict(v, prepend_value)
+            )
 
         self.table = petl.unpackdict(
-            self.table, column, keys=keys, includeoriginal=include_original,
-            samplesize=sample_size, missing=missing)
+            self.table,
+            column,
+            keys=keys,
+            includeoriginal=include_original,
+            samplesize=sample_size,
+            missing=missing,
+        )
 
         return self
 
-    def unpack_list(self, column, include_original=False, missing=None, replace=False,
-                    max_columns=None):
+    def unpack_list(
+        self,
+        column,
+        include_original=False,
+        missing=None,
+        replace=False,
+        max_columns=None,
+    ):
         """
         Unpack list values from one column into separate columns. Numbers the
         columns.
 
         .. code-block:: python
 
           # Begin with a list in column
@@ -471,26 +501,31 @@
         # If max columns provided, set max columns
         if col_count > 0 and max_columns:
             col_count = max_columns
 
         # Create new column names "COL_01, COL_02"
         new_cols = []
         for i in range(col_count):
-            new_cols.append(column + '_' + str(i))
+            new_cols.append(column + "_" + str(i))
 
-        tbl = petl.unpack(self.table, column, new_cols,
-                          include_original=include_original, missing=missing)
+        tbl = petl.unpack(
+            self.table,
+            column,
+            new_cols,
+            include_original=include_original,
+            missing=missing,
+        )
 
         if replace:
             self.table = tbl
 
         else:
             return tbl
 
-    def unpack_nested_columns_as_rows(self, column, key='id', expand_original=False):
+    def unpack_nested_columns_as_rows(self, column, key="id", expand_original=False):
         """
         Unpack list or dict values from one column into separate rows.
         Not recommended for JSON columns (i.e. lists of dicts), but can handle columns
         with any mix of types. Makes use of PETL's `melt()` method.
 
         `Args:`
             column: str
@@ -504,96 +539,113 @@
                 Removes packed list and dict rows from original either way.
         `Returns:`
             If `expand_original`, original table with packed rows replaced by unpacked rows
             Otherwise, standalone table with key column and unpacked values only
         """
 
         if isinstance(expand_original, int) and expand_original is not True:
-            lengths = {len(row[column]) for row in self if isinstance(row[column], (dict, list))}
+            lengths = {
+                len(row[column])
+                for row in self
+                if isinstance(row[column], (dict, list))
+            }
             max_len = sorted(lengths, reverse=True)[0]
             if max_len > expand_original:
                 expand_original = False
 
         if expand_original:
             # Include all columns and filter out other non-dict types in table_list
             table = self
             table_list = table.select_rows(lambda row: isinstance(row[column], list))
         else:
             # Otherwise, include only key and column, but keep all non-dict types in table_list
             table = self.cut(key, column)
-            table_list = table.select_rows(lambda row: not isinstance(row[column], dict))
+            table_list = table.select_rows(
+                lambda row: not isinstance(row[column], dict)
+            )
 
         # All the columns other than column to ignore while melting
         ignore_cols = table.columns
         ignore_cols.remove(column)
 
         # Unpack lists as separate columns
         table_list.unpack_list(column, replace=True)
 
         # Rename the columns to retain only the number
         for col in table_list.columns:
-            if f'{column}_' in col:
-                table_list.rename_column(col, col.replace(f'{column}_', ""))
+            if f"{column}_" in col:
+                table_list.rename_column(col, col.replace(f"{column}_", ""))
 
         # Filter dicts and unpack as separate columns
         table_dict = table.select_rows(lambda row: isinstance(row[column], dict))
         table_dict.unpack_dict(column, prepend=False)
 
         from parsons.etl.table import Table
 
         # Use melt to pivot both sets of columns into their own Tables and clean out None values
         melted_list = Table(petl.melt(table_list.table, ignore_cols))
         melted_dict = Table(petl.melt(table_dict.table, ignore_cols))
 
-        melted_list.remove_null_rows('value')
-        melted_dict.remove_null_rows('value')
+        melted_list.remove_null_rows("value")
+        melted_dict.remove_null_rows("value")
 
-        melted_list.rename_column('variable', column)
-        melted_dict.rename_column('variable', column)
+        melted_list.rename_column("variable", column)
+        melted_dict.rename_column("variable", column)
 
         # Combine the list and dict Tables
         melted_list.concat(melted_dict)
 
         import hashlib
 
         if expand_original:
             # Add unpacked rows to the original table (minus packed rows)
-            orig = self.select_rows(lambda row: not isinstance(row[column], (dict, list)))
+            orig = self.select_rows(
+                lambda row: not isinstance(row[column], (dict, list))
+            )
             orig.concat(melted_list)
             # Add unique id column by hashing all the other fields
-            if 'uid' not in self.columns:
-                orig.add_column('uid', lambda row: hashlib.md5(
-                    str.encode(
-                        ''.join([str(x) for x in row])
-                        )
-                    ).hexdigest())
-                orig.move_column('uid', 0)
+            if "uid" not in self.columns:
+                orig.add_column(
+                    "uid",
+                    lambda row: hashlib.md5(
+                        str.encode("".join([str(x) for x in row]))
+                    ).hexdigest(),
+                )
+                orig.move_column("uid", 0)
 
             # Rename value column in case this is done again to this Table
-            orig.rename_column('value', f'{column}_value')
+            orig.rename_column("value", f"{column}_value")
 
             # Keep column next to column_value
             orig.move_column(column, -1)
             output = orig
         else:
             orig = self.remove_column(column)
             # Add unique id column by hashing all the other fields
-            melted_list.add_column('uid', lambda row: hashlib.md5(
-                str.encode(
-                    ''.join([str(x) for x in row])
-                    )
-                ).hexdigest())
-            melted_list.move_column('uid', 0)
+            melted_list.add_column(
+                "uid",
+                lambda row: hashlib.md5(
+                    str.encode("".join([str(x) for x in row]))
+                ).hexdigest(),
+            )
+            melted_list.move_column("uid", 0)
             output = melted_list
 
         self = orig
         return output
 
-    def long_table(self, key, column, key_rename=None, retain_original=False,
-                   prepend=True, prepend_value=None):
+    def long_table(
+        self,
+        key,
+        column,
+        key_rename=None,
+        retain_original=False,
+        prepend=True,
+        prepend_value=None,
+    ):
         """
         Create a new long parsons table from a column, including the foreign
         key.
 
         .. code-block:: python
 
            # Begin with nested dicts in a column
@@ -640,16 +692,16 @@
 
         if type(key) == str:
             key = [key]
 
         lt = self.cut(*key, column)  # Create a table of key and column
         lt.unpack_list(column, replace=True)  # Unpack the list
         lt.table = petl.melt(lt.table, key)  # Melt into a long table
-        lt = lt.cut(*key, 'value')  # Get rid of column names created in unpack
-        lt.rename_column('value', column)  # Rename 'value' to old column name
+        lt = lt.cut(*key, "value")  # Get rid of column names created in unpack
+        lt.rename_column("value", column)  # Rename 'value' to old column name
         lt.remove_null_rows(column)  # Remove null values
 
         # If a new key name is specified, rename
         if key_rename:
             for k, v in key_rename.items():
                 lt.rename_column(k, v)
 
@@ -738,15 +790,15 @@
     def _prepend_dict(self, dict_obj, prepend):
         # Internal method to rename dict keys
 
         new_dict = {}
 
         for k, v in dict_obj.items():
 
-            new_dict[prepend + '_' + k] = v
+            new_dict[prepend + "_" + k] = v
 
         return new_dict
 
     def stack(self, *tables, missing=None):
         """
         Stack Parsons tables on top of one another.
 
@@ -800,32 +852,41 @@
             rows: int
                 The number of rows of each new Parsons table
         `Returns:`
             List of Parsons tables
         """
 
         from parsons.etl import Table
-        return [Table(petl.rowslice(self.table, i, i+rows)) for i in range(0, self.num_rows, rows)]
+
+        return [
+            Table(petl.rowslice(self.table, i, i + rows))
+            for i in range(0, self.num_rows, rows)
+        ]
 
     @staticmethod
     def get_normalized_column_name(column_name):
         """
         Returns a column name with whitespace removed, non-alphanumeric characters removed, and
         everything lowercased.
 
         `Returns:`
             str
                 Normalized column name
         """
 
         column_name = column_name.lower().strip()
-        return ''.join(c for c in column_name if c.isalnum())
+        return "".join(c for c in column_name if c.isalnum())
 
-    def match_columns(self, desired_columns, fuzzy_match=True, if_extra_columns='remove',
-                      if_missing_columns='add'):
+    def match_columns(
+        self,
+        desired_columns,
+        fuzzy_match=True,
+        if_extra_columns="remove",
+        if_missing_columns="add",
+    ):
         """
         Changes the column names and ordering in this Table to match a list of desired column
         names.
 
         `Args:`
             desired_columns: list
                 Ordered list of desired column names
@@ -844,15 +905,17 @@
 
         `Returns:`
             `Parsons Table` and also updates self
         """
 
         from parsons.etl import Table  # Just trying to avoid recursive imports.
 
-        normalize_fn = Table.get_normalized_column_name if fuzzy_match else (lambda s: s)
+        normalize_fn = (
+            Table.get_normalized_column_name if fuzzy_match else (lambda s: s)
+        )
 
         # Create a mapping of our "normalized" name to the original column name
         current_columns_normalized = {
             normalize_fn(col): col for col in reversed(self.columns)
         }
 
         # Track any columns we need to add to our current table from our desired columns
@@ -867,66 +930,69 @@
 
         # Loop through our desired columns -- the columns we want to see in our final table
         for desired_column in desired_columns:
             normalized_desired = normalize_fn(desired_column)
             # Try to find our desired column in our Table
             if normalized_desired not in current_columns_normalized:
                 # If we can't find our desired column in our current columns, then it's "missing"
-                if if_missing_columns == 'fail':
+                if if_missing_columns == "fail":
                     # If our missing strategy is to fail, raise an exception
                     raise TypeError(f"Table is missing column {desired_column}")
-                elif if_missing_columns == 'add':
+                elif if_missing_columns == "add":
                     # We have to add to our table
                     columns_to_add.append(desired_column)
                     # We will need to remember this column when we cut down to desired columns
                     cut_columns.append(desired_column)
                     # This will be in the final table
                     final_header.append(desired_column)
-                elif if_missing_columns != 'ignore':
+                elif if_missing_columns != "ignore":
                     # If it's not ignore, add, or fail, then it's not a valid strategy
-                    raise TypeError(f"Invalid option {if_missing_columns} for "
-                                    "argument `if_missing_columns`")
+                    raise TypeError(
+                        f"Invalid option {if_missing_columns} for "
+                        "argument `if_missing_columns`"
+                    )
             else:
                 # We have found this in our current columns, so take it out of our list to search
                 current_column = current_columns_normalized.pop(normalized_desired)
                 # Add the column to our intermediate table as the old column name
                 cut_columns.append(current_column)
                 # Add to our final header list as the "desired" name
                 final_header.append(desired_column)
 
         # Look for any "extra" columns from our current table that aren't in our desired columns
         for current_column in current_columns_normalized.values():
             # Figure out what to do with our "extra" columns
-            if if_extra_columns == 'fail':
+            if if_extra_columns == "fail":
                 # If our missing strategy is to fail, raise an exception
                 raise TypeError(f"Table has extra column {current_column}")
-            elif if_extra_columns == 'ignore':
+            elif if_extra_columns == "ignore":
                 # If we're "ignore"ing our extra columns, we should keep them by adding them to
                 # our intermediate and final columns list
                 cut_columns.append(current_column)
                 final_header.append(current_column)
-            elif if_extra_columns != 'remove':
+            elif if_extra_columns != "remove":
                 # If it's not ignore, add, or fail, then it's not a valid strategy
-                raise TypeError(f"Invalid option {if_extra_columns} for "
-                                "argument `if_extra_columns`")
+                raise TypeError(
+                    f"Invalid option {if_extra_columns} for "
+                    "argument `if_extra_columns`"
+                )
 
         # Add any columns we need to add
         for column in columns_to_add:
             self.table = petl.addfield(self.table, column, None)
 
         # Cut down to just the columns we care about
         self.table = petl.cut(self.table, *cut_columns)
 
         # Rename any columns
         self.table = petl.setheader(self.table, final_header)
 
         return self
 
-    def reduce_rows(self, columns, reduce_func, headers, presorted=False,
-                    **kwargs):
+    def reduce_rows(self, columns, reduce_func, headers, presorted=False, **kwargs):
         """
         Group rows by a column or columns, then reduce the groups to a single row.
 
         Based on the `rowreduce petl function <https://petl.readthedocs.io/en/stable/transform.html#petl.transform.reductions.rowreduce>`_.
 
         For example, the output from the query to get a table's definition is
         returned as one component per row. The `reduce_rows` method can be used
@@ -984,23 +1050,24 @@
                 should match the length of the list returned by the reduce
                 function.
             presorted: bool
                 If false, the row will be sorted.
         `Returns:`
             `Parsons Table` and also updates self
 
-        """ # noqa: E501,E261
+        """  # noqa: E501,E261
 
         self.table = petl.rowreduce(
             self.table,
             columns,
             reduce_func,
             header=headers,
             presorted=presorted,
-            **kwargs)
+            **kwargs,
+        )
 
         return self
 
     def sort(self, columns=None, reverse=False):
         """
         Sort the rows a table.
 
@@ -1076,16 +1143,16 @@
             *args: Any
                 The arguements to pass to the petl function.
             **kwargs: Any
                 The keyword arguements to pass to the petl function.
         `Returns:`
             `parsons.Table` or `petl` table
         """  # noqa: E501
-        update_table = kwargs.pop('update_table', False)
-        to_petl = kwargs.pop('to_petl', False)
+        update_table = kwargs.pop("update_table", False)
+        to_petl = kwargs.pop("to_petl", False)
 
         if update_table:
             self.table = getattr(petl, petl_method)(self.table, *args, **kwargs)
 
         if to_petl:
             return getattr(petl, petl_method)(self.table, *args, **kwargs)
```

### Comparing `parsons-1.0.0/parsons/etl/table.py` & `parsons-1.1.0/parsons/etl/table.py`

 * *Files 2% similar despite different names*

```diff
@@ -78,15 +78,15 @@
 
         elif isinstance(index, slice):
             tblslice = petl.rowslice(self.table, index.start, index.stop, index.step)
             return [row for row in tblslice]
 
         else:
 
-            raise TypeError('You must pass a string or an index as a value.')
+            raise TypeError("You must pass a string or an index as a value.")
 
     def __bool__(self):
 
         # Try to get a single row from our table
         head_one = petl.head(self.table)
 
         # See if our single row is empty
@@ -104,14 +104,17 @@
         """
         `Returns:`
             int
                 Number of rows in the table
         """
         return petl.nrows(self.table)
 
+    def __len__(self):
+        return self.num_rows
+
     @property
     def data(self):
         """
         Returns an iterable object for iterating over the raw data rows as tuples
         (without field names)
         """
         return petl.data(self.table)
@@ -149,21 +152,23 @@
             dict
                 A dictionary of the row with the column as the key and the cell
                 as the value.
         """
 
         self._index_count += 1
         if self._index_count >= DIRECT_INDEX_WARNING_COUNT:
-            logger.warning("""
+            logger.warning(
+                """
                 You have indexed directly into this Table multiple times. This can be inefficient,
                 as data transformations you've made will be computed _each time_ you index into the
                 Table. If you are accessing many rows of data, consider switching to this style of
                 iteration, which is much more efficient:
                 `for row in table:`
-                """)
+                """
+            )
 
         return petl.dicts(self.table)[row_index]
 
     def column_data(self, column_name):
         """
         Returns the data in the column as a list.
 
@@ -175,15 +180,15 @@
                 A list of data in the column.
         """
 
         if column_name in self.columns:
             return list(self.table[column_name])
 
         else:
-            raise ValueError('Column name not found.')
+            raise ValueError("Column name not found.")
 
     def materialize(self):
         """
         "Materializes" a Table, meaning all data is loaded into memory and all pending
         transformations are applied.
 
         Use this if petl's lazy-loading behavior is causing you problems, eg. if you want to read
@@ -212,15 +217,15 @@
 
         # Load the data in batches, and "pickle" the rows to a temp file.
         # (We pickle rather than writing to, say, a CSV, so that we maintain
         # all the type information for each field.)
 
         file_path = file_path or files.create_temp_file()
 
-        with open(file_path, 'wb') as handle:
+        with open(file_path, "wb") as handle:
             for row in self.table:
                 pickle.dump(list(row), handle)
 
         # Load a Table from the file
         self.table = petl.frompickle(file_path)
 
         return file_path
```

### Comparing `parsons-1.0.0/parsons/etl/tofrom.py` & `parsons-1.1.0/parsons/etl/tofrom.py`

 * *Files 8% similar despite different names*

```diff
@@ -2,17 +2,15 @@
 import json
 import io
 import gzip
 from parsons.utilities import files, zip_archive
 
 
 class ToFrom(object):
-
-    def to_dataframe(self, index=None, exclude=None, columns=None,
-                     coerce_float=False):
+    def to_dataframe(self, index=None, exclude=None, columns=None, coerce_float=False):
         """
         Outputs table as a Pandas Dataframe
 
         `Args:`
             index: str, list
                 Field of array to use as the index, alternately a specific set
                 of input labels to use
@@ -25,20 +23,33 @@
                 columns in the result (any names not found in the data will
                 become all-NA columns)
         `Returns:`
             dataframe
                 Pandas DataFrame object
         """
 
-        return petl.todataframe(self.table, index=index, exclude=exclude,
-                                columns=columns, coerce_float=coerce_float)
-
-    def to_html(self, local_path=None, encoding=None, errors='strict',
-                index_header=False, caption=None, tr_style=None,
-                td_styles=None, truncate=None):
+        return petl.todataframe(
+            self.table,
+            index=index,
+            exclude=exclude,
+            columns=columns,
+            coerce_float=coerce_float,
+        )
+
+    def to_html(
+        self,
+        local_path=None,
+        encoding=None,
+        errors="strict",
+        index_header=False,
+        caption=None,
+        tr_style=None,
+        td_styles=None,
+        truncate=None,
+    ):
         """
         Outputs table to html.
 
         .. warning::
                 If a file already exists at the given location, it will be
                 overwritten.
 
@@ -66,28 +77,38 @@
             str
                 The path of the new file
         """
 
         if not local_path:
             local_path = files.create_temp_file(suffix=".html")
 
-        petl.tohtml(self.table,
-                    source=local_path,
-                    encoding=encoding,
-                    errors=errors,
-                    caption=caption,
-                    index_header=index_header,
-                    tr_style=tr_style,
-                    td_styles=td_styles,
-                    truncate=truncate)
+        petl.tohtml(
+            self.table,
+            source=local_path,
+            encoding=encoding,
+            errors=errors,
+            caption=caption,
+            index_header=index_header,
+            tr_style=tr_style,
+            td_styles=td_styles,
+            truncate=truncate,
+        )
 
         return local_path
 
-    def to_csv(self, local_path=None, temp_file_compression=None, encoding=None, errors='strict',
-               write_header=True, csv_name=None, **csvargs):
+    def to_csv(
+        self,
+        local_path=None,
+        temp_file_compression=None,
+        encoding=None,
+        errors="strict",
+        write_header=True,
+        csv_name=None,
+        **csvargs,
+    ):
         """
         Outputs table to a CSV. Additional key word arguments are passed to ``csv.writer()``. So,
         e.g., to override the delimiter from the default CSV dialect, provide the delimiter
         keyword argument.
 
         .. warning::
                 If a file already exists at the given location, it will be
@@ -118,36 +139,40 @@
         `Returns:`
             str
                 The path of the new file
         """  # noqa: W605
 
         # If a zip archive.
         if files.zip_check(local_path, temp_file_compression):
-            return self.to_zip_csv(archive_path=local_path,
-                                   encoding=encoding,
-                                   errors=errors,
-                                   write_header=write_header,
-                                   csv_name=csv_name,
-                                   **csvargs)
+            return self.to_zip_csv(
+                archive_path=local_path,
+                encoding=encoding,
+                errors=errors,
+                write_header=write_header,
+                csv_name=csv_name,
+                **csvargs,
+            )
 
         if not local_path:
-            suffix = '.csv' + files.suffix_for_compression_type(temp_file_compression)
+            suffix = ".csv" + files.suffix_for_compression_type(temp_file_compression)
             local_path = files.create_temp_file(suffix=suffix)
 
         # Create normal csv/.gzip
-        petl.tocsv(self.table,
-                   source=local_path,
-                   encoding=encoding,
-                   errors=errors,
-                   write_header=write_header,
-                   **csvargs)
+        petl.tocsv(
+            self.table,
+            source=local_path,
+            encoding=encoding,
+            errors=errors,
+            write_header=write_header,
+            **csvargs,
+        )
 
         return local_path
 
-    def append_csv(self, local_path, encoding=None, errors='strict', **csvargs):
+    def append_csv(self, local_path, encoding=None, errors="strict", **csvargs):
         """
         Appends table to an existing CSV.
 
         Additional additional key word arguments
         are passed to ``csv.writer()``. So, e.g., to override the delimiter
         from the default CSV dialect, provide the delimiter keyword argument.
 
@@ -164,23 +189,29 @@
                 ``csv_writer`` optional arguments
 
         `Returns:`
             str
                 The path of the file
         """  # noqa: W605
 
-        petl.appendcsv(self.table,
-                       source=local_path,
-                       encoding=encoding,
-                       errors=errors,
-                       **csvargs)
+        petl.appendcsv(
+            self.table, source=local_path, encoding=encoding, errors=errors, **csvargs
+        )
         return local_path
 
-    def to_zip_csv(self, archive_path=None, csv_name=None, encoding=None,
-                   errors='strict', write_header=True, if_exists='replace', **csvargs):
+    def to_zip_csv(
+        self,
+        archive_path=None,
+        csv_name=None,
+        encoding=None,
+        errors="strict",
+        write_header=True,
+        if_exists="replace",
+        **csvargs,
+    ):
         """
         Outputs table to a CSV in a zip archive. Additional key word arguments are passed to
         ``csv.writer()``. So, e.g., to override the delimiter from the default CSV dialect,
         provide the delimiter keyword argument. Use thismethod if you would like to write
         multiple csv files to the same archive.
 
         .. warning::
@@ -208,25 +239,32 @@
 
         `Returns:`
             str
                 The path of the archive
         """  # noqa: W605
 
         if not archive_path:
-            archive_path = files.create_temp_file(suffix='.zip')
+            archive_path = files.create_temp_file(suffix=".zip")
 
-        cf = self.to_csv(encoding=encoding, errors=errors, write_header=write_header, **csvargs)
+        cf = self.to_csv(
+            encoding=encoding, errors=errors, write_header=write_header, **csvargs
+        )
 
         if not csv_name:
-            csv_name = files.extract_file_name(archive_path, include_suffix=False) + '.csv'
-
-        return zip_archive.create_archive(archive_path, cf, file_name=csv_name,
-                                          if_exists=if_exists)
-
-    def to_json(self, local_path=None, temp_file_compression=None, line_delimited=False):
+            csv_name = (
+                files.extract_file_name(archive_path, include_suffix=False) + ".csv"
+            )
+
+        return zip_archive.create_archive(
+            archive_path, cf, file_name=csv_name, if_exists=if_exists
+        )
+
+    def to_json(
+        self, local_path=None, temp_file_compression=None, line_delimited=False
+    ):
         """
         Outputs table to a JSON file
 
         .. warning::
                 If a file already exists at the given location, it will be
                 overwritten.
 
@@ -245,58 +283,69 @@
 
         `Returns:`
             str
                 The path of the new file
         """
 
         if not local_path:
-            suffix = '.json' + files.suffix_for_compression_type(temp_file_compression)
+            suffix = ".json" + files.suffix_for_compression_type(temp_file_compression)
             local_path = files.create_temp_file(suffix=suffix)
 
         # Note we don't use the much simpler petl.tojson(), since that method reads the whole
         # table into memory before writing to file.
 
         if files.is_gzip_path(local_path):
             open_fn = gzip.open
-            mode = 'w+t'
+            mode = "w+t"
         else:
             open_fn = open
-            mode = 'w'
+            mode = "w"
 
         with open_fn(local_path, mode) as file:
             if not line_delimited:
-                file.write('[')
+                file.write("[")
 
             i = 0
             for row in self:
                 if i:
                     if not line_delimited:
-                        file.write(',')
-                    file.write('\n')
+                        file.write(",")
+                    file.write("\n")
                 i += 1
                 json.dump(row, file)
 
             if not line_delimited:
-                file.write(']')
+                file.write("]")
 
         return local_path
 
     def to_dicts(self):
         """
         Output table as a list of dicts.
 
         `Returns:`
             list
         """
 
         return list(petl.dicts(self.table))
 
-    def to_sftp_csv(self, remote_path, host, username, password, port=22, encoding=None,
-                    compression=None, errors='strict', write_header=True,
-                    rsa_private_key_file=None, **csvargs):
+    def to_sftp_csv(
+        self,
+        remote_path,
+        host,
+        username,
+        password,
+        port=22,
+        encoding=None,
+        compression=None,
+        errors="strict",
+        write_header=True,
+        rsa_private_key_file=None,
+        **csvargs,
+    ):
         """
         Writes the table to a CSV file on a remote SFTP server
 
         `Args:`
             remote_path: str
                 The remote path of the file. If it ends in '.gz', the file will be compressed.
             host: str
@@ -324,22 +373,38 @@
         from parsons.sftp import SFTP
 
         sftp = SFTP(host, username, password, port, rsa_private_key_file)
 
         compression = files.compression_type_for_path(remote_path)
 
         local_path = self.to_csv(
-            temp_file_compression=compression, encoding=encoding, errors=errors,
-            write_header=write_header, **csvargs)
+            temp_file_compression=compression,
+            encoding=encoding,
+            errors=errors,
+            write_header=write_header,
+            **csvargs,
+        )
         sftp.put_file(local_path, remote_path)
 
-    def to_s3_csv(self, bucket, key, aws_access_key_id=None,
-                  aws_secret_access_key=None, compression=None, encoding=None,
-                  errors='strict', write_header=True, acl='bucket-owner-full-control',
-                  public_url=False, public_url_expires=3600, **csvargs):
+    def to_s3_csv(
+        self,
+        bucket,
+        key,
+        aws_access_key_id=None,
+        aws_secret_access_key=None,
+        compression=None,
+        encoding=None,
+        errors="strict",
+        write_header=True,
+        acl="bucket-owner-full-control",
+        public_url=False,
+        public_url_expires=3600,
+        use_env_token=True,
+        **csvargs,
+    ):
         """
         Writes the table to an s3 object as a CSV
 
         `Args:`
             bucket: str
                 The s3 bucket to upload to
             key: str
@@ -361,46 +426,67 @@
                 Include header in output
             public_url: boolean
                 Create a public link to the file
             public_url_expire: 3600
                 The time, in seconds, until the url expires if ``public_url`` set to ``True``.
             acl: str
                 The S3 permissions on the file
+            use_env_token: boolean
+                Controls use of the ``AWS_SESSION_TOKEN`` environment variable for S3. Defaults
+                to ``True``. Set to ``False`` in order to ignore the ``AWS_SESSION_TOKEN`` env
+                variable even if the ``aws_session_token`` argument was not passed in.
             \**csvargs: kwargs
                 ``csv_writer`` optional arguments
         `Returns:`
             Public url if specified. If not ``None``.
         """  # noqa: W605
 
         compression = compression or files.compression_type_for_path(key)
 
-        csv_name = files.extract_file_name(key, include_suffix=False) + '.csv'
+        csv_name = files.extract_file_name(key, include_suffix=False) + ".csv"
 
         # Save the CSV as a temp file
-        local_path = self.to_csv(temp_file_compression=compression,
-                                 encoding=encoding,
-                                 errors=errors,
-                                 write_header=write_header,
-                                 csv_name=csv_name,
-                                 **csvargs)
+        local_path = self.to_csv(
+            temp_file_compression=compression,
+            encoding=encoding,
+            errors=errors,
+            write_header=write_header,
+            csv_name=csv_name,
+            **csvargs,
+        )
 
         # Put the file on S3
         from parsons.aws import S3
-        self.s3 = S3(aws_access_key_id=aws_access_key_id,
-                     aws_secret_access_key=aws_secret_access_key)
+
+        self.s3 = S3(
+            aws_access_key_id=aws_access_key_id,
+            aws_secret_access_key=aws_secret_access_key,
+            use_env_token=use_env_token,
+        )
         self.s3.put_file(bucket, key, local_path, acl=acl)
 
         if public_url:
             return self.s3.get_url(bucket, key, expires_in=public_url_expires)
         else:
             return None
 
-    def to_gcs_csv(self, bucket_name, blob_name, app_creds=None, project=None, compression=None,
-                   encoding=None, errors='strict', write_header=True, public_url=False,
-                   public_url_expires=60, **csvargs):
+    def to_gcs_csv(
+        self,
+        bucket_name,
+        blob_name,
+        app_creds=None,
+        project=None,
+        compression=None,
+        encoding=None,
+        errors="strict",
+        write_header=True,
+        public_url=False,
+        public_url_expires=60,
+        **csvargs,
+    ):
         """
         Writes the table to a Google Cloud Storage blob as a CSV.
 
         `Args:`
             bucket_name: str
                 The bucket to upload to
             blob_name: str
@@ -430,35 +516,46 @@
                 ``csv_writer`` optional arguments
         `Returns:`
             Public url if specified. If not ``None``.
         """  # noqa: W605
 
         compression = compression or files.compression_type_for_path(blob_name)
 
-        csv_name = files.extract_file_name(blob_name, include_suffix=False) + '.csv'
+        csv_name = files.extract_file_name(blob_name, include_suffix=False) + ".csv"
 
         # Save the CSV as a temp file
-        local_path = self.to_csv(temp_file_compression=compression,
-                                 encoding=encoding,
-                                 errors=errors,
-                                 write_header=write_header,
-                                 csv_name=csv_name,
-                                 **csvargs)
+        local_path = self.to_csv(
+            temp_file_compression=compression,
+            encoding=encoding,
+            errors=errors,
+            write_header=write_header,
+            csv_name=csv_name,
+            **csvargs,
+        )
 
         from parsons.google.google_cloud_storage import GoogleCloudStorage
+
         gcs = GoogleCloudStorage(app_creds=app_creds, project=project)
         gcs.put_blob(bucket_name, blob_name, local_path)
 
         if public_url:
             return gcs.get_url(bucket_name, blob_name, expires_in=public_url_expires)
         else:
             return None
 
-    def to_redshift(self, table_name, username=None, password=None, host=None,
-                    db=None, port=None, **copy_args):
+    def to_redshift(
+        self,
+        table_name,
+        username=None,
+        password=None,
+        host=None,
+        db=None,
+        port=None,
+        **copy_args,
+    ):
         """
         Write a table to a Redshift database. Note, this requires you to pass
         AWS S3 credentials or store them as environmental variables.
 
         Args:
             table_name: str
                 The table name and schema (``my_schema.my_table``) to point the file.
@@ -476,19 +573,28 @@
                 See :func:`~parsons.databases.Redshift.copy`` for options.
 
         Returns:
             ``None``
         """  # noqa: W605
 
         from parsons.databases.redshift import Redshift
+
         rs = Redshift(username=username, password=password, host=host, db=db, port=port)
         rs.copy(self, table_name, **copy_args)
 
-    def to_postgres(self, table_name, username=None, password=None, host=None,
-                    db=None, port=None, **copy_args):
+    def to_postgres(
+        self,
+        table_name,
+        username=None,
+        password=None,
+        host=None,
+        db=None,
+        port=None,
+        **copy_args,
+    ):
         """
         Write a table to a Postgres database.
 
         Args:
             table_name: str
                 The table name and schema (``my_schema.my_table``) to point the file.
             username: str
@@ -505,24 +611,36 @@
                 See :func:`~parsons.databases.Postgres.copy`` for options.
 
         Returns:
             ``None``
         """  # noqa: W605
 
         from parsons.databases.postgres import Postgres
+
         pg = Postgres(username=username, password=password, host=host, db=db, port=port)
         pg.copy(self, table_name, **copy_args)
 
     def to_petl(self):
 
         return self.table
 
-    def to_civis(self, table, api_key=None, db=None, max_errors=None,
-                 existing_table_rows='fail', diststyle=None, distkey=None,
-                 sortkey1=None, sortkey2=None, wait=True, **civisargs):
+    def to_civis(
+        self,
+        table,
+        api_key=None,
+        db=None,
+        max_errors=None,
+        existing_table_rows="fail",
+        diststyle=None,
+        distkey=None,
+        sortkey1=None,
+        sortkey2=None,
+        wait=True,
+        **civisargs,
+    ):
         """
         Write the table to a Civis Redshift cluster. Additional key word
         arguments can passed to `civis.io.dataframe_to_civis()
         <https://civis-python.readthedocs.io/en/v1.9.0/generated/civis.io.dataframe_to_civis.html#civis.io.dataframe_to_civis>`_ # noqa: E501
 
         `Args`
             table: str
@@ -551,20 +669,28 @@
             sortkey2: str
                 The second column in a compound sortkey for the table.
             wait: boolean
                 Wait for write job to complete before exiting method.
         """
 
         from parsons.civis.civisclient import CivisClient
+
         civis = CivisClient(db=db, api_key=api_key)
         return civis.table_import(
-            self, table, max_errors=max_errors,
-            existing_table_rows=existing_table_rows, diststyle=diststyle,
-            distkey=distkey, sortkey1=sortkey1, sortkey2=sortkey2, wait=wait,
-            **civisargs)
+            self,
+            table,
+            max_errors=max_errors,
+            existing_table_rows=existing_table_rows,
+            diststyle=diststyle,
+            distkey=distkey,
+            sortkey1=sortkey1,
+            sortkey2=sortkey2,
+            wait=wait,
+            **civisargs,
+        )
 
     @classmethod
     def from_csv(cls, local_path, **csvargs):
         """
         Create a ``parsons table`` object from a CSV file
 
         `Args:`
@@ -581,15 +707,15 @@
         remote_prefixes = ["http://", "https://", "ftp://", "s3://"]
         if any(map(local_path.startswith, remote_prefixes)):
             is_remote_file = True
         else:
             is_remote_file = False
 
         if not is_remote_file and not files.has_data(local_path):
-            raise ValueError('CSV file is empty')
+            raise ValueError("CSV file is empty")
 
         return cls(petl.fromcsv(local_path, **csvargs))
 
     @classmethod
     def from_csv_string(cls, str, **csvargs):
         """
         Create a ``parsons table`` object from a string representing a CSV.
@@ -600,15 +726,15 @@
             **csvargs: kwargs
                 ``csv_reader`` optional arguments
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        bytesio = io.BytesIO(str.encode('utf-8'))
+        bytesio = io.BytesIO(str.encode("utf-8"))
         memory_source = petl.io.sources.MemorySource(bytesio.read())
         return cls(petl.fromcsv(memory_source, **csvargs))
 
     @classmethod
     def from_columns(cls, cols, header=None):
         """
         Create a ``parsons table`` from a list of lists organized as columns
@@ -647,24 +773,25 @@
 
         if line_delimited:
             if files.is_gzip_path(local_path):
                 open_fn = gzip.open
             else:
                 open_fn = open
 
-            with open_fn(local_path, 'r') as file:
+            with open_fn(local_path, "r") as file:
                 rows = [json.loads(line) for line in file]
             return cls(rows)
 
         else:
             return cls(petl.fromjson(local_path, header=header))
 
     @classmethod
-    def from_redshift(cls, sql, username=None, password=None, host=None,
-                      db=None, port=None):
+    def from_redshift(
+        cls, sql, username=None, password=None, host=None, db=None, port=None
+    ):
         """
         Create a ``parsons table`` from a Redshift query.
 
         To pull an entire Redshift table, use a query like ``SELECT * FROM tablename``.
 
         `Args:`
             sql: str
@@ -682,19 +809,22 @@
 
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
         from parsons.databases.redshift import Redshift
+
         rs = Redshift(username=username, password=password, host=host, db=db, port=port)
         return rs.query(sql)
 
     @classmethod
-    def from_postgres(cls, sql, username=None, password=None, host=None, db=None, port=None):
+    def from_postgres(
+        cls, sql, username=None, password=None, host=None, db=None, port=None
+    ):
         """
         Args:
             sql: str
                 A valid SQL statement
             username: str
                 Required if env variable ``PGUSER`` not populated
             password: str
@@ -704,20 +834,28 @@
             db: str
                 Required if env variable ``PGDATABASE`` not populated
             port: int
                 Required if env variable ``PGPORT`` not populated.
         """
 
         from parsons.databases.postgres import Postgres
+
         pg = Postgres(username=username, password=password, host=host, db=db, port=port)
         return pg.query(sql)
 
     @classmethod
-    def from_s3_csv(cls, bucket, key, from_manifest=False, aws_access_key_id=None,
-                    aws_secret_access_key=None, **csvargs):
+    def from_s3_csv(
+        cls,
+        bucket,
+        key,
+        from_manifest=False,
+        aws_access_key_id=None,
+        aws_secret_access_key=None,
+        **csvargs,
+    ):
         """
         Create a ``parsons table`` from a key in an S3 bucket.
 
         `Args:`
             bucket: str
                 The S3 bucket.
             key: str
@@ -732,14 +870,15 @@
             \**csvargs: kwargs
                 ``csv_reader`` optional arguments
         `Returns:`
             `parsons.Table` object
         """  # noqa: W605
 
         from parsons.aws import S3
+
         s3 = S3(aws_access_key_id, aws_secret_access_key)
 
         if from_manifest:
             with open(s3.get_file(bucket, key)) as fd:
                 manifest = json.load(fd)
 
             s3_keys = [x["url"] for x in manifest["entries"]]
@@ -748,15 +887,15 @@
             s3_keys = [f"s3://{bucket}/{key}"]
 
         tbls = []
         for key in s3_keys:
             # TODO handle urls that end with '/', i.e. urls that point to "folders"
             _, _, bucket_, key_ = key.split("/", 3)
             file_ = s3.get_file(bucket_, key_)
-            if files.compression_type_for_path(key_) == 'zip':
+            if files.compression_type_for_path(key_) == "zip":
                 file_ = zip_archive.unzip_archive(file_)
 
             tbls.append(petl.fromcsv(file_, **csvargs))
 
         return cls(petl.cat(*tbls))
 
     @classmethod
```

### Comparing `parsons-1.0.0/parsons/facebook_ads/facebook_ads.py` & `parsons-1.1.0/parsons/facebook_ads/facebook_ads.py`

 * *Files 5% similar despite different names*

```diff
@@ -34,60 +34,64 @@
     # The data columns that are valid for creating a custom audience.
     # Feel free to add more variants to capture common column names, as long as they're fairly
     # unambiguous.
     # IMPORTANT - Keep these maps in sync with the comments in the ``add_users_to_custom_audience``
     # method!
     # TODO add support for parsing full names from one column
     KeyMatchMap = {
-        FBKeySchema.email: ['email', 'email address', 'voterbase_email'],
-        FBKeySchema.fn: ['fn', 'first', 'first name', 'vb_tsmart_first_name'],
-        FBKeySchema.ln: ['ln', 'last', 'last name', 'vb_tsmart_last_name'],
+        FBKeySchema.email: ["email", "email address", "voterbase_email"],
+        FBKeySchema.fn: ["fn", "first", "first name", "vb_tsmart_first_name"],
+        FBKeySchema.ln: ["ln", "last", "last name", "vb_tsmart_last_name"],
         FBKeySchema.phone: [
-            'phone',
-            'phone number',
-            'cell',
-            'landline',
-            'vb_voterbase_phone',
-            'vb_voterbase_phone_wireless'
-            ],
-        FBKeySchema.ct: ['ct', 'city', 'vb_vf_reg_city', 'vb_tsmart_city'],
+            "phone",
+            "phone number",
+            "cell",
+            "landline",
+            "vb_voterbase_phone",
+            "vb_voterbase_phone_wireless",
+        ],
+        FBKeySchema.ct: ["ct", "city", "vb_vf_reg_city", "vb_tsmart_city"],
         FBKeySchema.st: [
-            'st',
-            'state',
-            'state code',
-            'vb_vf_source_state',
-            'vb_tsmart_state',
-            'vb_vf_reg_state',
-            'vb_vf_reg_cass_state'
-            ],
-        FBKeySchema.zip: ['zip', 'zip code', 'vb_vf_reg_zip', 'vb_tsmart_zip'],
-        FBKeySchema.country: ['country', 'country code'],
+            "st",
+            "state",
+            "state code",
+            "vb_vf_source_state",
+            "vb_tsmart_state",
+            "vb_vf_reg_state",
+            "vb_vf_reg_cass_state",
+        ],
+        FBKeySchema.zip: ["zip", "zip code", "vb_vf_reg_zip", "vb_tsmart_zip"],
+        FBKeySchema.country: ["country", "country code"],
         # Yes, it's not kosher to confuse gender and sex. However, gender is all that FB
         # supports in their audience targeting.
-        FBKeySchema.gen: ['gen', 'gender', 'sex', 'vb_voterbase_gender'],
-        FBKeySchema.doby: ['doby', 'dob year', 'birth year'],
-        FBKeySchema.dobm: ['dobm', 'dob month', 'birth month'],
-        FBKeySchema.dobd: ['dobd', 'dob day', 'birth day'],
+        FBKeySchema.gen: ["gen", "gender", "sex", "vb_voterbase_gender"],
+        FBKeySchema.doby: ["doby", "dob year", "birth year"],
+        FBKeySchema.dobm: ["dobm", "dob month", "birth month"],
+        FBKeySchema.dobd: ["dobd", "dob day", "birth day"],
     }
 
     PreprocessKeyMatchMap = {
         # Data in this column will be parsed into the FBKeySchema.dobX keys.
-        "DOB YYYYMMDD": ['dob', 'vb_voterbase_dob', 'vb_tsmart_dob']
+        "DOB YYYYMMDD": ["dob", "vb_voterbase_dob", "vb_tsmart_dob"]
     }
 
-    def __init__(self, app_id=None, app_secret=None, access_token=None, ad_account_id=None):
+    def __init__(
+        self, app_id=None, app_secret=None, access_token=None, ad_account_id=None
+    ):
 
         try:
-            self.app_id = app_id or os.environ['FB_APP_ID']
-            self.app_secret = app_secret or os.environ['FB_APP_SECRET']
-            self.access_token = access_token or os.environ['FB_ACCESS_TOKEN']
-            self.ad_account_id = ad_account_id or os.environ['FB_AD_ACCOUNT_ID']
+            self.app_id = app_id or os.environ["FB_APP_ID"]
+            self.app_secret = app_secret or os.environ["FB_APP_SECRET"]
+            self.access_token = access_token or os.environ["FB_ACCESS_TOKEN"]
+            self.ad_account_id = ad_account_id or os.environ["FB_AD_ACCOUNT_ID"]
         except KeyError as error:
-            logger.error("FB Marketing API credentials missing. Must be specified as env vars "
-                         "or kwargs")
+            logger.error(
+                "FB Marketing API credentials missing. Must be specified as env vars "
+                "or kwargs"
+            )
             raise error
 
         FacebookAdsApi.init(self.app_id, self.app_secret, self.access_token)
         self.ad_account = AdAccount("act_%s" % self.ad_account_id)
 
     @staticmethod
     def _get_match_key_for_column(column):
@@ -113,24 +117,21 @@
 
     @staticmethod
     def _preprocess_dob_column(table, column):
         # Parse the DOB column into 3 new columns, and remove the original column
         # TODO Throw an error if the values are not 6 characters long?
 
         table.add_column(
-            FBKeySchema.doby,
-            lambda row: row[column][:4] if row[column] else None
+            FBKeySchema.doby, lambda row: row[column][:4] if row[column] else None
         )
         table.add_column(
-            FBKeySchema.dobm,
-            lambda row: row[column][4:6] if row[column] else None
+            FBKeySchema.dobm, lambda row: row[column][4:6] if row[column] else None
         )
         table.add_column(
-            FBKeySchema.dobd,
-            lambda row: row[column][6:8] if row[column] else None
+            FBKeySchema.dobd, lambda row: row[column][6:8] if row[column] else None
         )
         table.remove_column(column)
 
     @staticmethod
     def _preprocess_users_table(table):
         # Handle columns that require special parsing
         for column in table.columns:
@@ -183,28 +184,28 @@
 
         # For each of the FB match keys, create a new column from the source column.
         # If there are more than one source cols for a given FB match key, we'll pick
         # the first non-empty value for each row.
 
         for fb_key, orig_cols in fb_keys_to_orig_cols.items():
             value_fn = (
-                lambda bound_cols:
-                    lambda row:
-                        FacebookAds._get_first_non_empty_value_from_dict(row, bound_cols)
+                lambda bound_cols: lambda row: FacebookAds._get_first_non_empty_value_from_dict(
+                    row, bound_cols
+                )
             )(orig_cols)
 
             # A little trickery here to handle the case where one of the "orig_cols" is already
             # named like the "fb_key".
-            t.add_column(fb_key+"_fb_temp_col", value_fn)
+            t.add_column(fb_key + "_fb_temp_col", value_fn)
             t.remove_column(*orig_cols)
-            t.rename_column(fb_key+"_fb_temp_col", fb_key)
+            t.rename_column(fb_key + "_fb_temp_col", fb_key)
 
         # Convert None values to empty strings. Otherwise the FB SDK chokes.
         petl_table = t.to_petl()
-        t = Table(petl_table.replaceall(None, ''))
+        t = Table(petl_table.replaceall(None, ""))
 
         return t
 
     @staticmethod
     def _get_match_schema_and_data(table):
         # Grab the raw data as a list of tuples
         data_list = [row for row in table.data]
@@ -238,45 +239,55 @@
             ID of the created audience
         """
 
         if not self._is_valid_data_source(data_source):
             raise KeyError("Invalid data_source provided")
 
         params = {
-            'name': name,
-            'subtype': 'CUSTOM',
-            'description': description,
-            'customer_file_source': data_source,
+            "name": name,
+            "subtype": "CUSTOM",
+            "description": description,
+            "customer_file_source": data_source,
         }
 
         res = self.ad_account.create_custom_audience(params=params)
-        return res['id']
+        return res["id"]
 
     def delete_custom_audience(self, audience_id):
         """
         Deletes a FB custom audience.
 
         `Args:`
             audience_id: str
                 The ID of the custom audience to delete.
         """
 
         CustomAudience(audience_id).api_delete()
 
     @staticmethod
-    def _add_batch_to_custom_audience(app_id, app_secret, access_token, audience_id, schema,
-                                      batch, added_so_far, total_rows):
+    def _add_batch_to_custom_audience(
+        app_id,
+        app_secret,
+        access_token,
+        audience_id,
+        schema,
+        batch,
+        added_so_far,
+        total_rows,
+    ):
         # Since this method runs in parallel, we need to re-initialize the Facebook API each time
         # to avoid SSL-related errors. Basically, the FacebookAdsApi python framework isn't
         # built to run in parallel.
         FacebookAdsApi.init(app_id, app_secret, access_token)
 
         # Note that the FB SDK handles basic normalization and hashing of the data
         CustomAudience(audience_id).add_users(schema, batch, is_raw=True)
-        logger.info(f"Added {added_so_far+len(batch)}/{total_rows} users to custom audience...")
+        logger.info(
+            f"Added {added_so_far+len(batch)}/{total_rows} users to custom audience..."
+        )
 
     def add_users_to_custom_audience(self, audience_id, users_table):
         """
         Adds user data to a custom audience.
 
         Each user row in the provided table should have at least one of the supported columns
         defined. Otherwise the row will be ignored. Beyond that, the rows may have any other
@@ -343,38 +354,48 @@
 
         `Args:`
             audience_id: str
                 The ID of the custom audience to delete.
             users_table: obj
                 Parsons table
 
-        """ # noqa: E501,E261
+        """  # noqa: E501,E261
 
-        logger.info(f"Adding custom audience users from provided table with "
-                    f"{users_table.num_rows} rows")
+        logger.info(
+            f"Adding custom audience users from provided table with "
+            f"{users_table.num_rows} rows"
+        )
 
         match_table = FacebookAds.get_match_table_for_users_table(users_table)
         if not match_table.columns:
-            raise KeyError("No valid columns found for audience matching. "
-                           "See FacebookAds.KeyMatchMap for supported columns")
+            raise KeyError(
+                "No valid columns found for audience matching. "
+                "See FacebookAds.KeyMatchMap for supported columns"
+            )
 
         num_rows = match_table.num_rows
         logger.info(f"Found {num_rows} rows with valid FB matching keys")
         logger.info(f"Using FB matching keys: {match_table.columns}")
 
         (schema, data) = FacebookAds._get_match_schema_and_data(match_table)
 
         # Use the FB API to add users, respecting the limit per API call.
         # Process and upload batches in parallel, to improve performance.
 
         batch_size = MAX_FB_AUDIENCE_API_USERS
 
         parallel_jobs = (
             delayed(FacebookAds._add_batch_to_custom_audience)(
-                self.app_id, self.app_secret, self.access_token, audience_id, schema,
-                data[i:i+batch_size], i, num_rows
+                self.app_id,
+                self.app_secret,
+                self.access_token,
+                audience_id,
+                schema,
+                data[i : i + batch_size],
+                i,
+                num_rows,
             )
             for i in range(0, len(data), batch_size)
         )
 
-        n_jobs = os.environ.get('PARSONS_NUM_PARALLEL_JOBS', 4)
+        n_jobs = os.environ.get("PARSONS_NUM_PARALLEL_JOBS", 4)
         Parallel(n_jobs=n_jobs)(parallel_jobs)
```

### Comparing `parsons-1.0.0/parsons/freshdesk/freshdesk.py` & `parsons-1.1.0/parsons/freshdesk/freshdesk.py`

 * *Files 6% similar despite different names*

```diff
@@ -22,51 +22,58 @@
             env variable set.
     `Returns:`
         Freshdesk class
     """
 
     def __init__(self, domain, api_key):
 
-        self.api_key = check_env.check('FRESHDESK_API_KEY', api_key)
-        self.domain = check_env.check('FRESHDESK_DOMAIN', domain)
-        self.uri = f'https://{self.domain}.freshdesk.com/api/v2/'
-        self.client = APIConnector(self.uri, auth=(self.api_key, 'x'))
+        self.api_key = check_env.check("FRESHDESK_API_KEY", api_key)
+        self.domain = check_env.check("FRESHDESK_DOMAIN", domain)
+        self.uri = f"https://{self.domain}.freshdesk.com/api/v2/"
+        self.client = APIConnector(self.uri, auth=(self.api_key, "x"))
 
     def _get_request(self, endpoint, params=None):
-        base_params = {'per_page': PAGE_SIZE}
+        base_params = {"per_page": PAGE_SIZE}
 
         if params:
             base_params.update(params)
 
-        r = self.client.request(endpoint, 'GET', params=base_params)
+        r = self.client.request(endpoint, "GET", params=base_params)
         self.client.validate_response(r)
         data = r.json()
 
         # Paginate
-        while 'link' in r.headers.keys():
-            logger.info(f'Retrieving another page of {PAGE_SIZE} records.')
-            url = re.search('<(.*)>', r.headers['link']).group(1)
-            r = self.client.request(url, 'GET', params=params)
+        while "link" in r.headers.keys():
+            logger.info(f"Retrieving another page of {PAGE_SIZE} records.")
+            url = re.search("<(.*)>", r.headers["link"]).group(1)
+            r = self.client.request(url, "GET", params=params)
             self.client.validate_response(r)
             data.extend(r.json())
 
         return data
 
     @staticmethod
     def _transform_table(tbl, expand_custom_fields=None):
         if tbl.num_rows > 0:
-            tbl.move_column('id', 0)
+            tbl.move_column("id", 0)
             tbl.sort()
             if expand_custom_fields:
-                tbl.unpack_dict('custom_fields', prepend=False)
+                tbl.unpack_dict("custom_fields", prepend=False)
 
         return tbl
 
-    def get_tickets(self, ticket_type=None, requester_id=None, requester_email=None,
-                    company_id=None, updated_since='2016-01-01', expand_custom_fields=False):
+    def get_tickets(
+        self,
+        ticket_type=None,
+        requester_id=None,
+        requester_email=None,
+        company_id=None,
+        updated_since="2016-01-01",
+        expand_custom_fields=False,
+    ):
         """
         List tickets.
 
         See the `API Docs <https://developers.freshdesk.com/api/#list_all_tickets>`_
         for more information.
 
         .. warning::
@@ -93,26 +100,36 @@
             expand_custom_fields: boolean
                 Expand nested custom fields to their own columns.
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        params = {'filter': ticket_type,
-                  'requester_id': requester_id,
-                  'requester_email': requester_email,
-                  'company_id': company_id,
-                  'updated_since': updated_since}
+        params = {
+            "filter": ticket_type,
+            "requester_id": requester_id,
+            "requester_email": requester_email,
+            "company_id": company_id,
+            "updated_since": updated_since,
+        }
 
-        tbl = Table(self._get_request('tickets', params=params))
-        logger.info(f'Found {tbl.num_rows} tickets.')
+        tbl = Table(self._get_request("tickets", params=params))
+        logger.info(f"Found {tbl.num_rows} tickets.")
         return self._transform_table(tbl, expand_custom_fields)
 
-    def get_contacts(self, email=None, mobile=None, phone=None, company_id=None,
-                     state=None, updated_since=None, expand_custom_fields=None):
+    def get_contacts(
+        self,
+        email=None,
+        mobile=None,
+        phone=None,
+        company_id=None,
+        state=None,
+        updated_since=None,
+        expand_custom_fields=None,
+    ):
         """
         Get contacts.
 
         See the `API Docs <https://developers.freshdesk.com/api/#list_all_contacts>`_
         for more information.
 
         `Args:`
@@ -131,23 +148,25 @@
             expand_custom_fields: boolean
                 Expand nested custom fields to their own columns.
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        params = {'email': email,
-                  'mobile': mobile,
-                  'phone': phone,
-                  'company_id': company_id,
-                  'state': state,
-                  '_updated_since': updated_since}
+        params = {
+            "email": email,
+            "mobile": mobile,
+            "phone": phone,
+            "company_id": company_id,
+            "state": state,
+            "_updated_since": updated_since,
+        }
 
-        tbl = Table(self._get_request('contacts', params=params))
-        logger.info(f'Found {tbl.num_rows} contacts.')
+        tbl = Table(self._get_request("contacts", params=params))
+        logger.info(f"Found {tbl.num_rows} contacts.")
         return self._transform_table(tbl, expand_custom_fields)
 
     def get_companies(self, expand_custom_fields=False):
         """
         List companies.
 
         See the `API Docs <https://developers.freshdesk.com/api/#list_all_companies>`_
@@ -157,16 +176,16 @@
             expand_custom_fields: boolean
                 Expand nested custom fields to their own columns.
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        tbl = Table(self._get_request('companies'))
-        logger.info(f'Found {tbl.num_rows} companies.')
+        tbl = Table(self._get_request("companies"))
+        logger.info(f"Found {tbl.num_rows} companies.")
         return self._transform_table(tbl, expand_custom_fields)
 
     def get_agents(self, email=None, mobile=None, phone=None, state=None):
         """
         List agents.
 
         See the `API Docs <https://developers.freshdesk.com/api/#list_all_agents>`_
@@ -182,18 +201,15 @@
             state: str
                 Filter by state
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        params = {'email': email,
-                  'mobile': mobile,
-                  'phone': phone,
-                  'state': state}
-        tbl = Table(self._get_request('agents', params=params))
-        logger.info(f'Found {tbl.num_rows} agents.')
+        params = {"email": email, "mobile": mobile, "phone": phone, "state": state}
+        tbl = Table(self._get_request("agents", params=params))
+        logger.info(f"Found {tbl.num_rows} agents.")
         tbl = self._transform_table(tbl)
-        tbl = tbl.unpack_dict('contact', prepend=False)
-        tbl.remove_column('signature')  # Removing since raw HTML might cause issues.
+        tbl = tbl.unpack_dict("contact", prepend=False)
+        tbl.remove_column("signature")  # Removing since raw HTML might cause issues.
 
         return tbl
```

### Comparing `parsons-1.0.0/parsons/geocode/census_geocoder.py` & `parsons-1.1.0/parsons/geocode/census_geocoder.py`

 * *Files 11% similar despite different names*

```diff
@@ -18,21 +18,21 @@
     `Args:`
         benchmark: str
             The US Census benchmark file to utilize. By default the current benchmark is used,
             but other options can found `here <https://geocoding.geo.census.gov/geocoder/benchmarks>`_.
         vintage: str
             The US Census vintage file to utilize. By default the current vintage is used, but
             other options can be found `here <https://geocoding.geo.census.gov/geocoder/vintages?form>`_.
-    """ # noqa E501
+    """  # noqa E501
 
-    def __init__(self, benchmark='Public_AR_Current', vintage='Current_Current'):
+    def __init__(self, benchmark="Public_AR_Current", vintage="Current_Current"):
 
         self.cg = censusgeocode.CensusGeocode(benchmark=benchmark, vintage=vintage)
 
-    def geocode_onelineaddress(self, address, return_type='geographies'):
+    def geocode_onelineaddress(self, address, return_type="geographies"):
         """
         Geocode a single line address. Does not require parsing of city and zipcode field. Returns
         geocode as well as other census block data. If the service is unable to geocode the address
         it will return an empty list.
 
         `Args:`
             address: str
@@ -44,16 +44,22 @@
             dict
         """
 
         geo = self.cg.onelineaddress(address, returntype=return_type)
         self._log_result(geo)
         return geo
 
-    def geocode_address(self, address_line, city=None, state=None, zipcode=None,
-                        return_type='geographies'):
+    def geocode_address(
+        self,
+        address_line,
+        city=None,
+        state=None,
+        zipcode=None,
+        return_type="geographies",
+    ):
         """
         Geocode an address by specifying address fields. Returns the geocode as well as other
         census block data.
 
         `Args:`
             address_line: str
                 A valid address line
@@ -80,49 +86,57 @@
 
         The table must **only** include the following columns in the following order.
 
         .. list-table::
             :widths: 40
             :header-rows: 1
 
-            * - Column Data
-            * - Unique ID
-            * - Street
-            * - City
-            * - State
-            * - Zipcode
+            * - Column Names
+            * - id (must be unique)
+            * - street
+            * - city
+            * - state
+            * - zip
 
         `Args:`
             table: Parsons Table
                 A Parsons table
         `Returns:`
             A Parsons table
         """
 
-        logger.info(f'Geocoding {table.num_rows} records.')
+        logger.info(f"Geocoding {table.num_rows} records.")
+        if set(table.columns) != {"street", "city", "state", "zip"}:
+            msg = (
+                "Table must ONLY include `['id', 'street', 'city', 'state', 'zip']` as"
+                + "columns. Tip: try using `table.cut()`"
+            )
+            raise ValueError(msg)
+
         chunked_tables = table.chunk(BATCH_SIZE)
         batch_count = 1
         records_processed = 0
 
         geocoded_tbl = Table([[]])
         for tbl in chunked_tables:
+
             geocoded_tbl.concat(Table(petl.fromdicts(self.cg.addressbatch(tbl))))
             records_processed += tbl.num_rows
-            logger.info(f'{records_processed} of {table.num_rows} records processed.')
+            logger.info(f"{records_processed} of {table.num_rows} records processed.")
             batch_count += 1
 
         return geocoded_tbl
 
     def _log_result(self, dict):
         # Internal method to log the result of the geocode
 
         if len(dict) == 0:
-            logger.info('Unable to geocode record.')
+            logger.info("Unable to geocode record.")
         else:
-            logger.info('Record geocoded.')
+            logger.info("Record geocoded.")
 
     def get_coordinates_data(self, latitude, longitude):
         """
         Return census data on coordinates.
 
         `Args`
             latitude: int
@@ -130,12 +144,12 @@
             longitude: int
                 A valid longitude in the United States
         `Returns:`
            dict
         """
 
         geo = self.cg.coordinates(x=longitude, y=latitude)
-        if len(geo['States']) == 0:
-            logger.info('Coordinate not found.')
+        if len(geo["States"]) == 0:
+            logger.info("Coordinate not found.")
         else:
-            logger.info('Coordinate processed.')
+            logger.info("Coordinate processed.")
         return geo
```

### Comparing `parsons-1.0.0/parsons/github/github.py` & `parsons-1.1.0/parsons/github/github.py`

 * *Files 4% similar despite different names*

```diff
@@ -13,40 +13,43 @@
 
 
 def _wrap_method(decorator, method):
     @wraps(method)
     def _wrapper(self, *args, **kwargs):
         bound_method = partial(method.__get__(self, type(self)))
         return decorator(bound_method)(*args, **kwargs)
+
     return _wrapper
 
 
 def decorate_methods(decorator):
     # Based on Django's django.utils.decorators.method_decorator
     def decorate(cls):
         for method in dir(cls):
             # Don't decorate dunder methods
             if method.startswith("__"):
                 continue
             cls_method = getattr(cls, method)
             if callable(cls_method):
                 setattr(cls, method, _wrap_method(decorator, cls_method))
         return cls
+
     return decorate
 
 
 def wrap_github_404(func):
     @wraps(func)
     def _wrapped_func(*args, **kwargs):
         try:
             return (func)(*args, **kwargs)
         except UnknownObjectException:
             raise ParsonsGitHubError(
                 "Couldn't find the object you referenced, maybe you need to log in?"
             )
+
     return _wrapped_func
 
 
 class ParsonsGitHubError(Exception):
     pass
 
 
@@ -68,17 +71,19 @@
         access_token: Optional[str]
             Access token to use for credentials. Can be set with ``GITHUB_ACCESS_TOKEN`` environment
             variable.
     """
 
     def __init__(self, username=None, password=None, access_token=None):
 
-        self.username = check_env.check('GITHUB_USERNAME', username, optional=True)
-        self.password = check_env.check('GITHUB_PASSWORD', password, optional=True)
-        self.access_token = check_env.check('GITHUB_ACCESS_TOKEN', access_token, optional=True)
+        self.username = check_env.check("GITHUB_USERNAME", username, optional=True)
+        self.password = check_env.check("GITHUB_PASSWORD", password, optional=True)
+        self.access_token = check_env.check(
+            "GITHUB_ACCESS_TOKEN", access_token, optional=True
+        )
 
         if self.username and self.password:
             self.client = PyGithub(self.username, self.password)
         elif self.access_token:
             self.client = PyGithub(self.access_token)
         else:
             self.client = PyGithub()
@@ -164,15 +169,15 @@
                 Page size. Defaults to 100.
 
         Returns:
             ``Table``
                 Table with page of user repos
         """
 
-        logger.info(f'Listing page {page} of repos for user {username}')
+        logger.info(f"Listing page {page} of repos for user {username}")
 
         return self._as_table(
             self.client.get_user(username).get_repos(), page=page, page_size=page_size
         )
 
     def list_organization_repos(self, organization_name, page=None, page_size=100):
         """List organization repos with pagination, returning a ``Table``
@@ -186,15 +191,17 @@
                 Page size. Defaults to 100.
 
         Returns:
             ``Table``
                 Table with page of organization repos
         """
 
-        logger.info(f'Listing page {page} of repos for organization {organization_name}')
+        logger.info(
+            f"Listing page {page} of repos for organization {organization_name}"
+        )
 
         return self._as_table(
             self.client.get_organization(organization_name).get_repos(),
             page=page,
             page_size=page_size,
         )
 
@@ -210,17 +217,28 @@
         Returns:
             dict
                 Issue information
         """
 
         return self.client.get_repo(repo_name).get_issue(number=issue_number).raw_data
 
-    def list_repo_issues(self, repo_name, state="open", assignee=None, creator=None, mentioned=None,
-                         labels=[], sort="created", direction="desc", since=None, page=None,
-                         page_size=100):
+    def list_repo_issues(
+        self,
+        repo_name,
+        state="open",
+        assignee=None,
+        creator=None,
+        mentioned=None,
+        labels=[],
+        sort="created",
+        direction="desc",
+        since=None,
+        page=None,
+        page_size=100,
+    ):
         """List issues for a given repo
 
         Args:
             repo_name: str
                 Full repo name (account/name)
             state: str
                 State of issues to return. One of "open", "closed", "all". Defaults to "open".
@@ -245,27 +263,27 @@
                 Page size. Defaults to 100.
 
         Returns:
             ``Table``
                 Table with page of repo issues
         """
 
-        logger.info(f'Listing page {page} of issues for repo {repo_name}')
+        logger.info(f"Listing page {page} of issues for repo {repo_name}")
 
         kwargs_dict = {"state": state, "sort": sort, "direction": direction}
         if assignee:
             kwargs_dict["assignee"] = assignee
         if creator:
             kwargs_dict["creator"] = creator
         if mentioned:
             kwargs_dict["mentioned"] = mentioned
         if len(labels) > 0:
             kwargs_dict["labels"] = ",".join(labels)
         if since:
-            kwargs_dict["since"] = f'{since.isoformat()[:19]}Z'
+            kwargs_dict["since"] = f"{since.isoformat()[:19]}Z"
 
         return self._as_table(
             self.client.get_repo(repo_name).get_issues(**kwargs_dict),
             page=page,
             page_size=page_size,
         )
 
@@ -281,16 +299,24 @@
         Returns:
             dict
                 Pull request information
         """
 
         return self.client.get_repo(repo_name).get_pull(pull_request_number).raw_data
 
-    def list_repo_pull_requests(self, repo_name, state="open", base=None, sort="created",
-                                direction="desc", page=None, page_size=100):
+    def list_repo_pull_requests(
+        self,
+        repo_name,
+        state="open",
+        base=None,
+        sort="created",
+        direction="desc",
+        page=None,
+        page_size=100,
+    ):
         """Lists pull requests for a given repo
 
         Args:
             repo_name: str
                 Full repo name (account/name)
             state: str
                 One of "open, "closed", "all". Defaults to "open".
@@ -307,22 +333,24 @@
                 Page size. Defaults to 100.
 
         Returns:
             ``Table``
                 Table with page of repo pull requests
         """
 
-        logger.info(f'Listing page {page} of pull requests for repo {repo_name}')
+        logger.info(f"Listing page {page} of pull requests for repo {repo_name}")
 
         kwargs_dict = {"state": state, "sort": sort, "direction": direction}
         if base:
             kwargs_dict["base"] = base
 
         self._as_table(
-            self.client.get_repo(repo_name).get_pulls(**kwargs_dict), page=page, page_size=page_size
+            self.client.get_repo(repo_name).get_pulls(**kwargs_dict),
+            page=page,
+            page_size=page_size,
         )
 
     def list_repo_contributors(self, repo_name, page=None, page_size=100):
         """Lists contributors for a given repo
 
         Args:
             repo_name: str
@@ -333,18 +361,20 @@
                 Page size. Defaults to 100.
 
         Returns:
             ``Table``
                 Table with page of repo contributors
         """
 
-        logger.info(f'Listing page {page} of contributors for repo {repo_name}')
+        logger.info(f"Listing page {page} of contributors for repo {repo_name}")
 
         return self._as_table(
-            self.client.get_repo(repo_name).get_contributors(), page=page, page_size=page_size
+            self.client.get_repo(repo_name).get_contributors(),
+            page=page,
+            page_size=page_size,
         )
 
     def download_file(self, repo_name, path, branch=None, local_path=None):
         """Download a file from a repo by path and branch. Defaults to the repo's default branch if
         branch is not supplied.
 
         Uses the download_url directly rather than the API because the API only supports contents up
@@ -372,39 +402,46 @@
         if not local_path:
             local_path = files.create_temp_file_for_path(path)
 
         repo = self.client.get_repo(repo_name)
         if branch is None:
             branch = repo.default_branch
 
-        logger.info(f'Downloading {path} from {repo_name}, branch {branch} to {local_path}')
+        logger.info(
+            f"Downloading {path} from {repo_name}, branch {branch} to {local_path}"
+        )
 
         headers = None
         if self.access_token:
             headers = {
-                'Authorization': f'token {self.access_token}',
+                "Authorization": f"token {self.access_token}",
             }
 
-        res = requests.get(f'https://raw.githubusercontent.com/{repo_name}/{branch}/{path}',
-                           headers=headers)
+        res = requests.get(
+            f"https://raw.githubusercontent.com/{repo_name}/{branch}/{path}",
+            headers=headers,
+        )
 
         if res.status_code == 404:
             raise UnknownObjectException(status=404, data=res.content)
         elif res.status_code != 200:
             raise ParsonsGitHubError(
-                f'Error downloading {path} from repo {repo_name}: {res.content}')
+                f"Error downloading {path} from repo {repo_name}: {res.content}"
+            )
 
-        with open(local_path, 'wb') as f:
+        with open(local_path, "wb") as f:
             f.write(res.content)
 
-        logger.info(f'Downloaded {path} to {local_path}')
+        logger.info(f"Downloaded {path} to {local_path}")
 
         return local_path
 
-    def download_table(self, repo_name, path, branch=None, local_path=None, delimiter=','):
+    def download_table(
+        self, repo_name, path, branch=None, local_path=None, delimiter=","
+    ):
         """Download a CSV file from a repo by path and branch as a Parsons Table.
 
         Args:
             repo_name: str
                 Full repo name (account/name)
             path: str
                 Path from the repo base directory
```

### Comparing `parsons-1.0.0/parsons/google/google_admin.py` & `parsons-1.1.0/parsons/google/google_admin.py`

 * *Files 21% similar despite different names*

```diff
@@ -21,48 +21,56 @@
     `Returns:`
         GoogleAdmin Class
     """
 
     def __init__(self, app_creds=None, sub=None):
         setup_google_application_credentials(app_creds)
 
-        self.client = ServiceAccountCredentials.from_json_keyfile_name(
-            os.environ['GOOGLE_APPLICATION_CREDENTIALS'],
-            ['https://www.googleapis.com/auth/admin.directory.group']
-        ).create_delegated(sub).authorize(httplib2.Http())
+        self.client = (
+            ServiceAccountCredentials.from_json_keyfile_name(
+                os.environ["GOOGLE_APPLICATION_CREDENTIALS"],
+                ["https://www.googleapis.com/auth/admin.directory.group"],
+            )
+            .create_delegated(sub)
+            .authorize(httplib2.Http())
+        )
 
     def _paginate_request(self, endpoint, collection, params=None):
         # Build query params
         param_arr = []
-        param_str = ''
+        param_str = ""
         if params:
             for key, value in params.items():
-                param_arr.append(key + '=' + value)
-            param_str = '?' + '&'.join(param_arr)
+                param_arr.append(key + "=" + value)
+            param_str = "?" + "&".join(param_arr)
 
         # Make API call
-        req_url = 'https://admin.googleapis.com/admin/directory/v1/' + endpoint
+        req_url = "https://admin.googleapis.com/admin/directory/v1/" + endpoint
 
         # Return type from Google Admin is a tuple of length 2. Extract desired result from 2nd item
         # in tuple and convert to json
-        res = json.loads(self.client.request(req_url + param_str, 'GET')[1].decode('utf-8'))
+        res = json.loads(
+            self.client.request(req_url + param_str, "GET")[1].decode("utf-8")
+        )
 
         # Paginate
         ret = []
         if collection in res:
             ret = res[collection]
 
-            while('nextPageToken' in res):
-                if param_arr[-1][0:10] != 'pageToken=':
-                    param_arr.append('pageToken=' + res['nextPageToken'])
+            while "nextPageToken" in res:
+                if param_arr[-1][0:10] != "pageToken=":
+                    param_arr.append("pageToken=" + res["nextPageToken"])
                 else:
-                    param_arr[-1] = 'pageToken=' + res['nextPageToken']
-                res = json.loads(self.client.request(
-                    req_url + '?' + '&'.join(param_arr), 'GET'
-                )[1].decode('utf-8'))
+                    param_arr[-1] = "pageToken=" + res["nextPageToken"]
+                res = json.loads(
+                    self.client.request(req_url + "?" + "&".join(param_arr), "GET")[
+                        1
+                    ].decode("utf-8")
+                )
                 ret += res[collection]
 
         return Table(ret)
 
     def get_aliases(self, group_key, params=None):
         """
         Get aliases for a group. `Google Admin API Documentation <https://developers.google.com/\
@@ -72,35 +80,39 @@
             group_key: str
                 The Google group id
             params: dict
                 A dictionary of fields for the GET request
         `Returns:`
             Table Class
         """
-        return self._paginate_request('groups/' + group_key + '/aliases', 'aliases', params)
+        return self._paginate_request(
+            "groups/" + group_key + "/aliases", "aliases", params
+        )
 
     def get_all_group_members(self, group_key, params=None):
         """
         Get all members in a group. `Google Admin API Documentation <https://developers.google.com/\
         admin-sdk/directory/v1/guides/manage-group-members#get_all_members>`_
 
         `Args:`
             group_key: str
                 The Google group id
             params: dict
                 A dictionary of fields for the GET request
         `Returns:`
             Table Class
         """
-        return self._paginate_request('groups/' + group_key + '/members', 'members', params)
+        return self._paginate_request(
+            "groups/" + group_key + "/members", "members", params
+        )
 
     def get_all_groups(self, params=None):
         """
         Get all groups in a domain or account. `Google Admin API Documentation <https://developers.\
         google.com/admin-sdk/directory/v1/guides/manage-groups#get_all_domain_groups>`_
         `Args:`
             params: dict
                 A dictionary of fields for the GET request.
         `Returns:`
             Table Class
         """
-        return self._paginate_request('groups', 'groups', params)
+        return self._paginate_request("groups", "groups", params)
```

### Comparing `parsons-1.0.0/parsons/google/google_bigquery.py` & `parsons-1.1.0/parsons/google/google_bigquery.py`

 * *Files 7% similar despite different names*

```diff
@@ -10,53 +10,53 @@
 from parsons.etl import Table
 from parsons.google.utitities import setup_google_application_credentials
 from parsons.google.google_cloud_storage import GoogleCloudStorage
 from parsons.utilities import check_env
 from parsons.utilities.files import create_temp_file
 
 BIGQUERY_TYPE_MAP = {
-    'str': 'STRING',
-    'float': 'FLOAT',
-    'int': 'INTEGER',
-    'bool': 'BOOLEAN',
-    'datetime.datetime': 'DATETIME',
-    'datetime.date': 'DATE',
-    'datetime.time': 'TIME',
-    'dict': 'RECORD',
-    'NoneType': 'STRING'
+    "str": "STRING",
+    "float": "FLOAT",
+    "int": "INTEGER",
+    "bool": "BOOLEAN",
+    "datetime.datetime": "DATETIME",
+    "datetime.date": "DATE",
+    "datetime.time": "TIME",
+    "dict": "RECORD",
+    "NoneType": "STRING",
 }
 
 # Max number of rows that we query at a time, so we can avoid loading huge
 # data sets into memory.
 # 100k rows per batch at ~1k bytes each = ~100MB per batch.
 QUERY_BATCH_SIZE = 100000
 
 
 def get_table_ref(client, table_name):
     # Helper function to build a TableReference for our table
     parsed = parse_table_name(table_name)
-    dataset_ref = client.dataset(parsed['dataset'])
-    return dataset_ref.table(parsed['table'])
+    dataset_ref = client.dataset(parsed["dataset"])
+    return dataset_ref.table(parsed["table"])
 
 
 def parse_table_name(table_name):
     # Helper function to parse out the different components of a table ID
-    parts = table_name.split('.')
+    parts = table_name.split(".")
     parts.reverse()
     parsed = {
-        'project': None,
-        'dataset': None,
-        'table': None,
+        "project": None,
+        "dataset": None,
+        "table": None,
     }
     if len(parts) > 0:
-        parsed['table'] = parts[0]
+        parsed["table"] = parts[0]
     if len(parts) > 1:
-        parsed['dataset'] = parts[1]
+        parsed["dataset"] = parts[1]
     if len(parts) > 2:
-        parsed['project'] = parts[2]
+        parsed["project"] = parts[2]
     return parsed
 
 
 class GoogleBigQuery:
     """
     Class for querying BigQuery table and returning the data as Parsons tables.
 
@@ -94,18 +94,26 @@
         # We will not create the client until we need to use it, since creating the client
         # without valid GOOGLE_APPLICATION_CREDENTIALS raises an exception.
         # This attribute will be used to hold the client once we have created it.
         self._client = None
 
         self._dbapi = dbapi
 
-        self.dialect = 'bigquery'
+        self.dialect = "bigquery"
 
-    def copy(self, table_obj, table_name, if_exists='fail',
-             tmp_gcs_bucket=None, gcs_client=None, job_config=None, **load_kwargs):
+    def copy(
+        self,
+        table_obj,
+        table_name,
+        if_exists="fail",
+        tmp_gcs_bucket=None,
+        gcs_client=None,
+        job_config=None,
+        **load_kwargs,
+    ):
         """
         Copy a :ref:`parsons-table` into Google BigQuery via Google Cloud Storage.
 
         `Args:`
             table_obj: obj
                 The Parsons Table to copy into BigQuery.
             table_name: str
@@ -121,19 +129,21 @@
             job_config: object
                 A LoadJobConfig object to provide to the underlying call to load_table_from_uri
                 on the BigQuery client. The function will create its own if not provided.
             **load_kwargs: kwargs
                 Arguments to pass to the underlying load_table_from_uri call on the BigQuery
                 client.
         """
-        tmp_gcs_bucket = check_env.check('GCS_TEMP_BUCKET', tmp_gcs_bucket)
+        tmp_gcs_bucket = check_env.check("GCS_TEMP_BUCKET", tmp_gcs_bucket)
 
-        if if_exists not in ['fail', 'truncate', 'append', 'drop']:
-            raise ValueError(f'Unexpected value for if_exists: {if_exists}, must be one of '
-                             '"append", "drop", "truncate", or "fail"')
+        if if_exists not in ["fail", "truncate", "append", "drop"]:
+            raise ValueError(
+                f"Unexpected value for if_exists: {if_exists}, must be one of "
+                '"append", "drop", "truncate", or "fail"'
+            )
 
         table_exists = self.table_exists(table_name)
 
         if not job_config:
             job_config = bigquery.LoadJobConfig()
 
         if not job_config.schema:
@@ -142,33 +152,37 @@
         if not job_config.create_disposition:
             job_config.create_disposition = bigquery.CreateDisposition.CREATE_IF_NEEDED
         job_config.skip_leading_rows = 1
         job_config.source_format = bigquery.SourceFormat.CSV
         job_config.write_disposition = bigquery.WriteDisposition.WRITE_EMPTY
 
         if table_exists:
-            if if_exists == 'fail':
-                raise ValueError('Table already exists.')
-            elif if_exists == 'drop':
+            if if_exists == "fail":
+                raise ValueError("Table already exists.")
+            elif if_exists == "drop":
                 self.delete_table(table_name)
-            elif if_exists == 'append':
+            elif if_exists == "append":
                 job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND
-            elif if_exists == 'truncate':
+            elif if_exists == "truncate":
                 job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE
 
         gcs_client = gcs_client or GoogleCloudStorage()
-        temp_blob_name = f'{uuid.uuid4()}.csv'
-        temp_blob_uri = gcs_client.upload_table(table_obj, tmp_gcs_bucket, temp_blob_name)
+        temp_blob_name = f"{uuid.uuid4()}.csv"
+        temp_blob_uri = gcs_client.upload_table(
+            table_obj, tmp_gcs_bucket, temp_blob_name
+        )
 
         # load CSV from Cloud Storage into BigQuery
         table_ref = get_table_ref(self.client, table_name)
         try:
             load_job = self.client.load_table_from_uri(
-                temp_blob_uri, table_ref,
-                job_config=job_config, **load_kwargs,
+                temp_blob_uri,
+                table_ref,
+                job_config=job_config,
+                **load_kwargs,
             )
             load_job.result()
         finally:
             gcs_client.delete_blob(tmp_gcs_bucket, temp_blob_name)
 
     def delete_table(self, table_name):
         """
@@ -222,15 +236,15 @@
 
         # We will use a temp file to cache the results so that they are not all living
         # in memory. We'll use pickle to serialize the results to file in order to maintain
         # the proper data types (e.g. integer).
         temp_filename = create_temp_file()
 
         wrote_header = False
-        with open(temp_filename, 'wb') as temp_file:
+        with open(temp_filename, "wb") as temp_file:
             # Track whether we got data, since if we don't get any results we need to return None
             got_results = False
             while True:
                 batch = cursor.fetchmany(QUERY_BATCH_SIZE)
                 if len(batch) == 0:
                     break
 
@@ -287,18 +301,18 @@
 
         return self._client
 
     def _generate_schema(self, tbl):
         stats = tbl.get_columns_type_stats()
         fields = []
         for stat in stats:
-            petl_types = stat['type']
-            best_type = 'str' if 'str' in petl_types else petl_types[0]
+            petl_types = stat["type"]
+            best_type = "str" if "str" in petl_types else petl_types[0]
             field_type = self._bigquery_type(best_type)
-            field = bigquery.schema.SchemaField(stat['name'], field_type)
+            field = bigquery.schema.SchemaField(stat["name"], field_type)
             fields.append(field)
         return fields
 
     @staticmethod
     def _bigquery_type(tp):
         return BIGQUERY_TYPE_MAP[tp]
 
@@ -328,8 +342,10 @@
         bq_table = self.db.client.get_table(table_ref)
 
         # BigQuery wants the schema when we load the data, so we will grab it from the table
         job_config = bigquery.LoadJobConfig()
         job_config.schema = bq_table.schema
 
         empty_table = Table([])
-        self.db.copy(empty_table, self.table, if_exists='truncate', job_config=job_config)
+        self.db.copy(
+            empty_table, self.table, if_exists="truncate", job_config=job_config
+        )
```

### Comparing `parsons-1.0.0/parsons/google/google_civic.py` & `parsons-1.1.0/parsons/google/google_civic.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,61 +1,61 @@
 from parsons.utilities import check_env
 import requests
 from parsons.etl import Table
 
-URI = 'https://www.googleapis.com/civicinfo/v2/'
+URI = "https://www.googleapis.com/civicinfo/v2/"
 
 
 class GoogleCivic(object):
     """
     `Args:`
         api_key : str
             A valid Google api key. Not required if ``GOOGLE_CIVIC_API_KEY``
             env variable set.
     `Returns:`
         class
     """
 
     def __init__(self, api_key=None):
 
-        self.api_key = check_env.check('GOOGLE_CIVIC_API_KEY', api_key)
+        self.api_key = check_env.check("GOOGLE_CIVIC_API_KEY", api_key)
         self.uri = URI
 
     def request(self, url, args=None):
         # Internal request method
 
         if not args:
             args = {}
 
-        args['key'] = self.api_key
+        args["key"] = self.api_key
 
         r = requests.get(url, params=args)
 
         return r.json()
 
     def get_elections(self):
         """
         Get a collection of information about elections and voter information.
 
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        url = self.uri + 'elections'
+        url = self.uri + "elections"
 
-        return Table((self.request(url))['elections'])
+        return Table((self.request(url))["elections"])
 
     def _get_voter_info(self, election_id, address):
         # Internal method to call voter info end point. Portions of this are
         # parsed for other methods.
 
-        url = self.uri + 'voterinfo'
+        url = self.uri + "voterinfo"
 
-        args = {'address': address, 'electionId': election_id}
+        args = {"address": address, "electionId": election_id}
 
         return self.request(url, args=args)
 
     def get_polling_location(self, election_id, address):
         """
         Get polling location information for a given address.
 
@@ -68,17 +68,17 @@
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
         r = self._get_voter_info(election_id, address)
 
-        return r['pollingLocations']
+        return r["pollingLocations"]
 
-    def get_polling_locations(self, election_id, table, address_field='address'):
+    def get_polling_locations(self, election_id, table, address_field="address"):
         """
         Get polling location information for a table of addresses.
 
         `Args:`
             election_id: int
                 A valid election id. Election ids can be found by running the
                 :meth:`get_elections` method.
@@ -93,26 +93,26 @@
 
         polling_locations = []
 
         # Iterate through the rows of the table
         for row in table:
             loc = self.get_polling_location(election_id, row[address_field])
             # Insert original passed address
-            loc[0]['passed_address'] = row[address_field]
+            loc[0]["passed_address"] = row[address_field]
 
             # Add to list of lists
             polling_locations.append(loc[0])
 
         # Unpack values
         tbl = Table(polling_locations)
-        tbl.unpack_dict('address', prepend_value='polling')
-        tbl.unpack_list('sources', replace=True)
-        tbl.unpack_dict('sources_0', prepend_value='source')
-        tbl.rename_column('polling_line1', 'polling_address')
+        tbl.unpack_dict("address", prepend_value="polling")
+        tbl.unpack_list("sources", replace=True)
+        tbl.unpack_dict("sources_0", prepend_value="source")
+        tbl.rename_column("polling_line1", "polling_address")
 
         # Resort columns
-        tbl.move_column('pollingHours', len(tbl.columns))
-        tbl.move_column('notes', len(tbl.columns))
-        tbl.move_column('polling_locationName', 1)
-        tbl.move_column('polling_address', 2)
+        tbl.move_column("pollingHours", len(tbl.columns))
+        tbl.move_column("notes", len(tbl.columns))
+        tbl.move_column("polling_locationName", 1)
+        tbl.move_column("polling_address", 2)
 
         return tbl
```

### Comparing `parsons-1.0.0/parsons/google/google_cloud_storage.py` & `parsons-1.1.0/parsons/google/google_cloud_storage.py`

 * *Files 2% similar despite different names*

```diff
@@ -52,33 +52,33 @@
         Returns a list of buckets
 
         `Returns:`
             List of buckets
         """
 
         buckets = [b.name for b in self.client.list_buckets()]
-        logger.info(f'Found {len(buckets)}.')
+        logger.info(f"Found {len(buckets)}.")
         return buckets
 
     def bucket_exists(self, bucket_name):
         """
         Verify that a bucket exists
 
         `Args:`
             bucket_name: str
                 The name of the bucket
         `Returns:`
             boolean
         """
 
         if bucket_name in self.list_buckets():
-            logger.info(f'{bucket_name} exists.')
+            logger.info(f"{bucket_name} exists.")
             return True
         else:
-            logger.info(f'{bucket_name} does not exist.')
+            logger.info(f"{bucket_name} does not exist.")
             return False
 
     def get_bucket(self, bucket_name):
         """
         Returns a bucket object
 
         `Args:`
@@ -87,17 +87,17 @@
         `Returns:`
             GoogleCloud Storage bucket
         """
 
         if self.client.lookup_bucket(bucket_name):
             bucket = self.client.get_bucket(bucket_name)
         else:
-            raise google.cloud.exceptions.NotFound('Bucket not found')
+            raise google.cloud.exceptions.NotFound("Bucket not found")
 
-        logger.info(f'Returning {bucket_name} object')
+        logger.info(f"Returning {bucket_name} object")
         return bucket
 
     def create_bucket(self, bucket_name):
         """
         Create a bucket.
 
         `Args:`
@@ -106,15 +106,15 @@
         `Returns:`
             ``None``
         """
 
         # To Do: Allow user to set all of the bucket parameters
 
         self.client.create_bucket(bucket_name)
-        logger.info(f'Created {bucket_name} bucket.')
+        logger.info(f"Created {bucket_name} bucket.")
 
     def delete_bucket(self, bucket_name, delete_blobs=False):
         """
         Delete a bucket. Will fail if not empty unless ``delete_blobs`` argument
         is set to ``True``.
 
         `Args:`
@@ -124,15 +124,15 @@
                 Delete blobs in the bucket, if it is not empty
         `Returns:`
             ``None``
         """
 
         bucket = self.get_bucket(bucket_name)
         bucket.delete(force=delete_blobs)
-        logger.info(f'{bucket_name} bucket deleted.')
+        logger.info(f"{bucket_name} bucket deleted.")
 
     def list_blobs(self, bucket_name, max_results=None, prefix=None):
         """
         List all of the blobs in a bucket
 
         `Args:`
             bucket_name: str
@@ -141,17 +141,19 @@
                 TBD
             prefix_filter: str
                 A prefix to filter files
         `Returns:`
             A list of blob names
         """
 
-        blobs = self.client.list_blobs(bucket_name, max_results=max_results, prefix=prefix)
+        blobs = self.client.list_blobs(
+            bucket_name, max_results=max_results, prefix=prefix
+        )
         lst = [b.name for b in blobs]
-        logger.info(f'Found {len(lst)} in {bucket_name} bucket.')
+        logger.info(f"Found {len(lst)} in {bucket_name} bucket.")
 
         return lst
 
     def blob_exists(self, bucket_name, blob_name):
         """
         Verify that a blob exists in the specified bucket
 
@@ -161,18 +163,18 @@
             blob_name: str
                 The name of the blob
         `Returns:`
             boolean
         """
 
         if blob_name in self.list_blobs(bucket_name):
-            logger.info(f'{blob_name} exists.')
+            logger.info(f"{blob_name} exists.")
             return True
         else:
-            logger.info(f'{blob_name} does not exist.')
+            logger.info(f"{blob_name} does not exist.")
             return False
 
     def get_blob(self, bucket_name, blob_name):
         """
         Get a blob object
 
         `Args`:
@@ -182,15 +184,15 @@
                 A blob name
         `Returns:`
             A Google Storage blob object
         """
 
         bucket = self.get_bucket(bucket_name)
         blob = bucket.get_blob(blob_name)
-        logger.debug(f'Got {blob_name} object from {bucket_name} bucket.')
+        logger.debug(f"Got {blob_name} object from {bucket_name} bucket.")
         return blob
 
     def put_blob(self, bucket_name, blob_name, local_path):
         """
         Puts a blob (aka file) in a bucket
 
         `Args:`
@@ -206,15 +208,15 @@
 
         bucket = self.get_bucket(bucket_name)
         blob = storage.Blob(blob_name, bucket)
 
         with open(local_path, "rb") as f:
             blob.upload_from_file(f)
 
-        logger.info(f'{blob_name} put in {bucket_name} bucket.')
+        logger.info(f"{blob_name} put in {bucket_name} bucket.")
 
     def download_blob(self, bucket_name, blob_name, local_path=None):
         """
         Gets a blob from a bucket
 
         `Args:`
             bucket_name: str
@@ -227,23 +229,23 @@
                 when the script is done running.
         `Returns:`
             str
                 The path of the downloaded file
         """
 
         if not local_path:
-            local_path = files.create_temp_file_for_path('TEMPTHING')
+            local_path = files.create_temp_file_for_path("TEMPTHING")
 
         bucket = storage.Bucket(self.client, name=bucket_name)
         blob = storage.Blob(blob_name, bucket)
 
-        logger.info(f'Downloading {blob_name} from {bucket_name} bucket.')
-        with open(local_path, 'wb') as f:
+        logger.info(f"Downloading {blob_name} from {bucket_name} bucket.")
+        with open(local_path, "wb") as f:
             blob.download_to_file(f, client=self.client)
-        logger.info(f'{blob_name} saved to {local_path}.')
+        logger.info(f"{blob_name} saved to {local_path}.")
 
         return local_path
 
     def delete_blob(self, bucket_name, blob_name):
         """
         Delete a blob
 
@@ -254,17 +256,19 @@
                 The blob name
         `Returns:`
             ``None``
         """
 
         blob = self.get_blob(bucket_name, blob_name)
         blob.delete()
-        logger.info(f'{blob_name} blob in {bucket_name} bucket deleted.')
+        logger.info(f"{blob_name} blob in {bucket_name} bucket deleted.")
 
-    def upload_table(self, table, bucket_name, blob_name, data_type='csv', default_acl=None):
+    def upload_table(
+        self, table, bucket_name, blob_name, data_type="csv", default_acl=None
+    ):
         """
         Load the data from a Parsons table into a blob.
 
         `Args:`
             table: obj
                 A :ref:`parsons-table`
             bucket_name: str
@@ -273,30 +277,36 @@
                 The name of the blob to upload the data into.
             data_type: str
                 The file format to use when writing the data. One of: `csv` or `json`
         """
         bucket = storage.Bucket(self.client, name=bucket_name)
         blob = storage.Blob(blob_name, bucket)
 
-        if data_type == 'csv':
+        if data_type == "csv":
             local_file = table.to_csv()
-            content_type = 'text/csv'
-        elif data_type == 'json':
+            content_type = "text/csv"
+        elif data_type == "json":
             local_file = table.to_json()
-            content_type = 'application/json'
+            content_type = "application/json"
         else:
-            raise ValueError(f'Unknown data_type value ({data_type}): must be one of: csv or json')
+            raise ValueError(
+                f"Unknown data_type value ({data_type}): must be one of: csv or json"
+            )
 
         try:
-            blob.upload_from_filename(local_file, content_type=content_type, client=self.client,
-                                      predefined_acl=default_acl)
+            blob.upload_from_filename(
+                local_file,
+                content_type=content_type,
+                client=self.client,
+                predefined_acl=default_acl,
+            )
         finally:
             files.close_temp_file(local_file)
 
-        return f'gs://{bucket_name}/{blob_name}'
+        return f"gs://{bucket_name}/{blob_name}"
 
     def get_url(self, bucket_name, blob_name, expires_in=60):
         """
         Generates a presigned url for a blob
 
         `Args:`
             bucket_name: str
@@ -308,11 +318,13 @@
         `Returns:`
             url:
                 A link to download the object
         """
 
         bucket = self.client.bucket(bucket_name)
         blob = bucket.blob(blob_name)
-        url = blob.generate_signed_url(version="v4",
-                                       expiration=datetime.timedelta(minutes=expires_in),
-                                       method="GET")
+        url = blob.generate_signed_url(
+            version="v4",
+            expiration=datetime.timedelta(minutes=expires_in),
+            method="GET",
+        )
         return url
```

### Comparing `parsons-1.0.0/parsons/google/google_sheets.py` & `parsons-1.1.0/parsons/google/google_sheets.py`

 * *Files 4% similar despite different names*

```diff
@@ -24,39 +24,45 @@
             In order to use account impersonation, pass in the email address of the account to be
             impersonated as a string.
     """
 
     def __init__(self, google_keyfile_dict=None, subject=None):
 
         scope = [
-            'https://spreadsheets.google.com/feeds',
-            'https://www.googleapis.com/auth/drive',
-            ]
+            "https://spreadsheets.google.com/feeds",
+            "https://www.googleapis.com/auth/drive",
+        ]
 
-        setup_google_application_credentials(google_keyfile_dict, 'GOOGLE_DRIVE_CREDENTIALS')
-        google_credential_file = open(os.environ['GOOGLE_DRIVE_CREDENTIALS'])
+        setup_google_application_credentials(
+            google_keyfile_dict, "GOOGLE_DRIVE_CREDENTIALS"
+        )
+        google_credential_file = open(os.environ["GOOGLE_DRIVE_CREDENTIALS"])
         credentials_dict = json.load(google_credential_file)
 
         credentials = Credentials.from_service_account_info(
             credentials_dict, scopes=scope, subject=subject
         )
 
         self.gspread_client = gspread.authorize(credentials)
 
     def _get_worksheet(self, spreadsheet_id, worksheet=0):
         # Internal method to retrieve a worksheet object.
 
         # Check if the worksheet is an integer, if so find the sheet by index
         if isinstance(worksheet, int):
-            return self.gspread_client.open_by_key(spreadsheet_id).get_worksheet(worksheet)
+            return self.gspread_client.open_by_key(spreadsheet_id).get_worksheet(
+                worksheet
+            )
 
         elif isinstance(worksheet, str):
             idx = self.list_worksheets(spreadsheet_id).index(worksheet)
             try:
-                return self.gspread_client.open_by_key(spreadsheet_id).get_worksheet(idx)
+                return self.gspread_client.open_by_key(spreadsheet_id).get_worksheet(
+                    idx
+                )
             except:  # noqa: E722
                 raise ValueError(f"Couldn't find worksheet {worksheet}")
 
         else:
             raise ValueError(f"Couldn't find worksheet index or title {worksheet}")
 
     def list_worksheets(self, spreadsheet_id):
@@ -107,19 +113,27 @@
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
         worksheet = self._get_worksheet(spreadsheet_id, worksheet)
         tbl = Table(worksheet.get_all_values())
-        logger.info(f'Retrieved worksheet with {tbl.num_rows} rows.')
+        logger.info(f"Retrieved worksheet with {tbl.num_rows} rows.")
         return tbl
 
-    def share_spreadsheet(self, spreadsheet_id, sharee, share_type='user', role='reader',
-                          notify=True, notify_message=None, with_link=False):
+    def share_spreadsheet(
+        self,
+        spreadsheet_id,
+        sharee,
+        share_type="user",
+        role="reader",
+        notify=True,
+        notify_message=None,
+        with_link=False,
+    ):
         """
         Share a spreadsheet with a user, group of users, domain and/or the public.
 
         `Args:`
             spreadsheet_id: str
                 The ID of the spreadsheet (Tip: Get this from the spreadsheet URL)
             sharee: str
@@ -136,17 +150,23 @@
             email_message: str
                 The email to be sent if notify kwarg set to True.
             with_link: boolean
                 Whether a link is required for this permission.
         """
 
         spreadsheet = self.gspread_client.open_by_key(spreadsheet_id)
-        spreadsheet.share(sharee, share_type, role, notify=notify,
-                          email_message=notify_message, with_link=with_link)
-        logger.info(f'Shared spreadsheet {spreadsheet_id}.')
+        spreadsheet.share(
+            sharee,
+            share_type,
+            role,
+            notify=notify,
+            email_message=notify_message,
+            with_link=with_link,
+        )
+        logger.info(f"Shared spreadsheet {spreadsheet_id}.")
 
     def get_spreadsheet_permissions(self, spreadsheet_id):
         """
         List the permissioned users and groups for a spreadsheet.
 
         `Args:`
             spreadsheet_id: str
@@ -154,15 +174,15 @@
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
         spreadsheet = self.gspread_client.open_by_key(spreadsheet_id)
         tbl = Table(spreadsheet.list_permissions())
-        logger.info(f'Retrieved permissions for {spreadsheet_id} spreadsheet.')
+        logger.info(f"Retrieved permissions for {spreadsheet_id} spreadsheet.")
         return tbl
 
     def create_spreadsheet(self, title, editor_email=None, folder_id=None):
         """
         Creates a new Google spreadsheet. Optionally shares the new doc with
         the given email address. Optionally creates the sheet in a specified folder.
 
@@ -184,31 +204,31 @@
 
         spreadsheet = self.gspread_client.create(title, folder_id=folder_id)
 
         if editor_email:
             self.gspread_client.insert_permission(
                 spreadsheet.id,
                 editor_email,
-                perm_type='user',
-                role='writer',
+                perm_type="user",
+                role="writer",
             )
 
-        logger.info(f'Created spreadsheet {spreadsheet.id}')
+        logger.info(f"Created spreadsheet {spreadsheet.id}")
         return spreadsheet.id
 
     def delete_spreadsheet(self, spreadsheet_id):
         """
         Deletes a Google spreadsheet.
 
         `Args:`
             spreadsheet_id: str
                 The ID of the spreadsheet (Tip: Get this from the spreadsheet URL)
         """
         self.gspread_client.del_spreadsheet(spreadsheet_id)
-        logger.info(f'Deleted spreadsheet {spreadsheet_id}')
+        logger.info(f"Deleted spreadsheet {spreadsheet_id}")
 
     def add_sheet(self, spreadsheet_id, title=None, rows=100, cols=25):
         """
         Adds a sheet to a Google spreadsheet.
 
         `Args:`
             spreadsheet_id: str
@@ -221,19 +241,20 @@
         `Returns:`
             str
                 The sheet index
         """
         spreadsheet = self.gspread_client.open_by_key(spreadsheet_id)
         spreadsheet.add_worksheet(title, rows, cols)
         sheet_count = len(spreadsheet.worksheets())
-        logger.info('Created worksheet.')
-        return (sheet_count-1)
+        logger.info("Created worksheet.")
+        return sheet_count - 1
 
-    def append_to_sheet(self, spreadsheet_id, table, worksheet=0, user_entered_value=False,
-                        **kwargs):
+    def append_to_sheet(
+        self, spreadsheet_id, table, worksheet=0, user_entered_value=False, **kwargs
+    ):
         """
         Append data from a Parsons table to a Google sheet. Note that the table's columns are
         ignored, as we'll be keeping whatever header row already exists in the Google sheet.
 
         `Args:`
             spreadsheet_id: str
                 The ID of the spreadsheet (Tip: Get this from the spreadsheet URL)
@@ -244,46 +265,49 @@
                 0.
             user_entered_value: bool (optional)
                 If True, will submit cell values as entered (required for entering formulas).
                 Otherwise, values will be entered as strings or numbers only.
         """
 
         # This is in here to ensure backwards compatibility with previous versions of Parsons.
-        if 'sheet_index' in kwargs:
-            worksheet = kwargs['sheet_index']
-            logger.warning('Argument deprecated. Use worksheet instead.')
+        if "sheet_index" in kwargs:
+            worksheet = kwargs["sheet_index"]
+            logger.warning("Argument deprecated. Use worksheet instead.")
 
         sheet = self._get_worksheet(spreadsheet_id, worksheet)
 
         # Grab the existing data, so we can figure out where to start adding new data as a batch.
         # TODO Figure out a way to do a batch append without having to read the whole sheet first.
         # Maybe use gspread's low-level batch_update().
         existing_table = self.get_worksheet(spreadsheet_id, worksheet)
 
         # If the existing sheet is blank, then just overwrite the table.
         if existing_table.num_rows == 0:
-            return self.overwrite_sheet(spreadsheet_id, table, worksheet, user_entered_value)
+            return self.overwrite_sheet(
+                spreadsheet_id, table, worksheet, user_entered_value
+            )
 
         cells = []
         for row_num, row in enumerate(table.data):
             for col_num, cell in enumerate(row):
                 # Add 2 to allow for the header row, and for google sheets indexing starting at 1
                 sheet_row_num = existing_table.num_rows + row_num + 2
                 cells.append(gspread.Cell(sheet_row_num, col_num + 1, row[col_num]))
 
-        value_input_option = 'RAW'
+        value_input_option = "RAW"
         if user_entered_value:
-            value_input_option = 'USER_ENTERED'
+            value_input_option = "USER_ENTERED"
 
         # Update the data in one batch
         sheet.update_cells(cells, value_input_option=value_input_option)
-        logger.info(f'Appended {table.num_rows} rows to worksheet.')
+        logger.info(f"Appended {table.num_rows} rows to worksheet.")
 
-    def overwrite_sheet(self, spreadsheet_id, table, worksheet=0, user_entered_value=False,
-                        **kwargs):
+    def overwrite_sheet(
+        self, spreadsheet_id, table, worksheet=0, user_entered_value=False, **kwargs
+    ):
         """
         Replace the data in a Google sheet with a Parsons table, using the table's columns as the
         first row.
 
         `Args:`
             spreadsheet_id: str
                 The ID of the spreadsheet (Tip: Get this from the spreadsheet URL)
@@ -294,37 +318,37 @@
                 0.
             user_entered_value: bool (optional)
                 If True, will submit cell values as entered (required for entering formulas).
                 Otherwise, values will be entered as strings or numbers only.
         """
 
         # This is in here to ensure backwards compatibility with previous versions of Parsons.
-        if 'sheet_index' in kwargs:
-            worksheet = kwargs['sheet_index']
-            logger.warning('Argument deprecated. Use worksheet instead.')
+        if "sheet_index" in kwargs:
+            worksheet = kwargs["sheet_index"]
+            logger.warning("Argument deprecated. Use worksheet instead.")
 
         sheet = self._get_worksheet(spreadsheet_id, worksheet)
         sheet.clear()
 
-        value_input_option = 'RAW'
+        value_input_option = "RAW"
         if user_entered_value:
-            value_input_option = 'USER_ENTERED'
+            value_input_option = "USER_ENTERED"
 
         # Add header row
         sheet.append_row(table.columns, value_input_option=value_input_option)
 
         cells = []
         for row_num, row in enumerate(table.data):
             for col_num, cell in enumerate(row):
                 # We start at row #2 to keep room for the header row we added above
                 cells.append(gspread.Cell(row_num + 2, col_num + 1, row[col_num]))
 
         # Update the data in one batch
         sheet.update_cells(cells, value_input_option=value_input_option)
-        logger.info('Overwrote worksheet.')
+        logger.info("Overwrote worksheet.")
 
     def format_cells(self, spreadsheet_id, range, cell_format, worksheet=0):
         """
         Format the cells of a worksheet.
 
         `Args:`
             spreadsheet_id: str
@@ -365,26 +389,26 @@
                         }
                     }, worksheet=0)
 
         """  # noqa: E501,E261
 
         ws = self._get_worksheet(spreadsheet_id, worksheet)
         ws.format(range, cell_format)
-        logger.info('Formatted worksheet')
+        logger.info("Formatted worksheet")
 
     def read_sheet(self, spreadsheet_id, sheet_index=0):
         # Deprecated method v0.14 of Parsons.
 
-        logger.warning('Deprecated method. Use get_worksheet() instead.')
+        logger.warning("Deprecated method. Use get_worksheet() instead.")
         return self.get_worksheet(spreadsheet_id, sheet_index)
 
     def read_sheet_with_title(self, spreadsheet_id, title):
         # Deprecated method v0.14 of Parsons.
 
-        logger.warning('Deprecated method. Use get_worksheet() instead.')
+        logger.warning("Deprecated method. Use get_worksheet() instead.")
         return self.get_worksheet(spreadsheet_id, title)
 
     def get_sheet_index_with_title(self, spreadsheet_id, title):
         # Deprecated method v0.14 of Parsons.
 
-        logger.warning('Deprecated method. Use get_worksheet_index   instead.')
+        logger.warning("Deprecated method. Use get_worksheet_index   instead.")
         return self.get_worksheet_index(spreadsheet_id, title)
```

### Comparing `parsons-1.0.0/parsons/google/utitities.py` & `parsons-1.1.0/parsons/google/utitities.py`

 * *Files 14% similar despite different names*

```diff
@@ -2,22 +2,23 @@
 from parsons.utilities import files
 from parsons.utilities import check_env
 import json
 import os
 
 
 def setup_google_application_credentials(
-        app_creds: t.Union[t.Dict, str, None],
-        env_var_name: str = 'GOOGLE_APPLICATION_CREDENTIALS') -> None:
+    app_creds: t.Union[t.Dict, str, None],
+    env_var_name: str = "GOOGLE_APPLICATION_CREDENTIALS",
+) -> None:
     # Detect if app_creds is a dict, path string or json string, and if it is a
     # json string, then convert it to a temporary file. Then set the
     # environmental variable.
     credentials = check_env.check(env_var_name, app_creds)
     try:
         if type(credentials) is dict:
             credentials = json.dumps(credentials)
         if json.loads(credentials):
-            creds_path = files.string_to_temp_file(credentials, suffix='.json')
+            creds_path = files.string_to_temp_file(credentials, suffix=".json")
     except ValueError:
         creds_path = credentials
 
     os.environ[env_var_name] = creds_path
```

### Comparing `parsons-1.0.0/parsons/hustle/hustle.py` & `parsons-1.1.0/parsons/hustle/hustle.py`

 * *Files 9% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 from parsons.utilities import check_env, json_format
 import datetime
 from parsons.hustle.column_map import LEAD_COLUMN_MAP
 import logging
 
 logger = logging.getLogger(__name__)
 
-HUSTLE_URI = 'https://api.hustle.com/v1/'
+HUSTLE_URI = "https://api.hustle.com/v1/"
 PAGE_LIMIT = 1000
 
 
 class Hustle(object):
     """
     Instantiate Hustle Class
 
@@ -25,31 +25,35 @@
     `Returns:`
         Hustle Class
     """
 
     def __init__(self, client_id, client_secret):
 
         self.uri = HUSTLE_URI
-        self.client_id = check_env.check('HUSTLE_CLIENT_ID', client_id)
-        self.client_secret = check_env.check('HUSTLE_CLIENT_SECRET', client_secret)
+        self.client_id = check_env.check("HUSTLE_CLIENT_ID", client_id)
+        self.client_secret = check_env.check("HUSTLE_CLIENT_SECRET", client_secret)
         self.token_expiration = None
         self._get_auth_token(client_id, client_secret)
 
     def _get_auth_token(self, client_id, client_secret):
         # Generate a temporary authorization token
 
-        data = {'client_id': client_id,
-                'client_secret': client_secret,
-                'grant_type': 'client_credentials'}
+        data = {
+            "client_id": client_id,
+            "client_secret": client_secret,
+            "grant_type": "client_credentials",
+        }
 
-        r = request('POST', self.uri + 'oauth/token', data=data)
+        r = request("POST", self.uri + "oauth/token", data=data)
         logger.debug(r.json())
 
-        self.auth_token = r.json()['access_token']
-        self.token_expiration = datetime.datetime.now() + datetime.timedelta(seconds=7200)
+        self.auth_token = r.json()["access_token"]
+        self.token_expiration = datetime.datetime.now() + datetime.timedelta(
+            seconds=7200
+        )
         logger.info("Authentication token generated")
 
     def _token_check(self):
         # Tokens are only valid for 7200 seconds. This checks to make sure that it has
         # not expired and generate another one if it has.
 
         logger.debug("Checking token expiration.")
@@ -58,47 +62,49 @@
             logger.info("Refreshing authentication token.")
             self._get_auth_token(self.client_id, self.client_secret)
 
         else:
 
             pass
 
-    def _request(self, endpoint, req_type='GET', args=None, payload=None, raise_on_error=True):
+    def _request(
+        self, endpoint, req_type="GET", args=None, payload=None, raise_on_error=True
+    ):
 
         url = self.uri + endpoint
         self._token_check()
 
-        headers = {'Authorization': f'Bearer {self.auth_token}'}
+        headers = {"Authorization": f"Bearer {self.auth_token}"}
 
         parameters = {}
-        if req_type == 'GET':
-            parameters = {'limit': PAGE_LIMIT}
+        if req_type == "GET":
+            parameters = {"limit": PAGE_LIMIT}
 
         if args:
             parameters.update(args)
 
         r = request(req_type, url, params=parameters, json=payload, headers=headers)
 
         self._error_check(r, raise_on_error)
 
         # If a single item return the dict
-        if 'items' not in r.json().keys():
+        if "items" not in r.json().keys():
 
             return r.json()
 
         else:
-            result = r.json()['items']
+            result = r.json()["items"]
 
         # Pagination
-        while r.json()['pagination']['hasNextPage'] == 'true':
+        while r.json()["pagination"]["hasNextPage"] == "true":
 
-            parameters['cursor'] = r.json['pagination']['cursor']
+            parameters["cursor"] = r.json["pagination"]["cursor"]
             r = request(req_type, url, params=parameters, headers=headers)
             self._error_check(r, raise_on_error)
-            result.append(r.json()['items'])
+            result.append(r.json()["items"])
 
         return result
 
     def _error_check(self, r, raise_on_error):
         # Check for errors
 
         if r.status_code in (200, 201):
@@ -126,34 +132,36 @@
                 The group id.
 
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        tbl = Table(self._request(f'groups/{group_id}/agents'))
-        logger.info(f'Got {tbl.num_rows} agents from {group_id} group.')
+        tbl = Table(self._request(f"groups/{group_id}/agents"))
+        logger.info(f"Got {tbl.num_rows} agents from {group_id} group.")
         return tbl
 
     def get_agent(self, agent_id):
         """
         Get a single agent.
 
         `Args:`
             agent_id: str
                 The agent id.
         `Returns:`
             dict
         """
 
-        r = self._request(f'agents/{agent_id}')
-        logger.info(f'Got {agent_id} agent.')
+        r = self._request(f"agents/{agent_id}")
+        logger.info(f"Got {agent_id} agent.")
         return r
 
-    def create_agent(self, group_id, name, full_name, phone_number, send_invite=False, email=None):
+    def create_agent(
+        self, group_id, name, full_name, phone_number, send_invite=False, email=None
+    ):
         """
         Create an agent.
 
         `Args:`
             group_id: str
                 The group id to assign the agent.
             name: str
@@ -166,25 +174,29 @@
                 Send an invitation to the agent.
             email:
                 The email address of the agent.
         `Returns:`
             dict
         """
 
-        agent = {'name': name,
-                 'fullName': full_name,
-                 'phoneNumber': phone_number,
-                 'sendInvite': send_invite,
-                 'email': email}
+        agent = {
+            "name": name,
+            "fullName": full_name,
+            "phoneNumber": phone_number,
+            "sendInvite": send_invite,
+            "email": email,
+        }
 
         # Remove empty args in dictionary
         agent = json_format.remove_empty_keys(agent)
 
-        logger.info(f'Generating {full_name} agent.')
-        return self._request(f'groups/{group_id}/agents', req_type="POST", payload=agent)
+        logger.info(f"Generating {full_name} agent.")
+        return self._request(
+            f"groups/{group_id}/agents", req_type="POST", payload=agent
+        )
 
     def update_agent(self, agent_id, name=None, full_name=None, send_invite=False):
         """
         Update an agent.
 
         `Args:`
             agent_id: str
@@ -197,93 +209,91 @@
                 The valid phone number of the agent.
             send_invite: boolean
                 Send an invitation to the agent.
         `Returns:`
             dict
         """
 
-        agent = {'name': name,
-                 'fullName': full_name,
-                 'sendInvite': send_invite}
+        agent = {"name": name, "fullName": full_name, "sendInvite": send_invite}
 
         # Remove empty args in dictionary
         agent = json_format.remove_empty_keys(agent)
 
-        logger.info(f'Updating agent {agent_id}.')
-        return self._request(f'agents/{agent_id}', req_type="PUT", payload=agent)
+        logger.info(f"Updating agent {agent_id}.")
+        return self._request(f"agents/{agent_id}", req_type="PUT", payload=agent)
 
     def get_organizations(self):
         """
         Get organizations.
 
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        tbl = Table(self._request('organizations'))
-        logger.info(f'Got {tbl.num_rows} organizations.')
+        tbl = Table(self._request("organizations"))
+        logger.info(f"Got {tbl.num_rows} organizations.")
         return tbl
 
     def get_organization(self, organization_id):
         """
         Get a single organization.
 
         `Args:`
             organization_id: str
                 The organization id.
         `Returns:`
             dict
         """
 
-        r = self._request(f'organizations/{organization_id}')
-        logger.info(f'Got {organization_id} organization.')
+        r = self._request(f"organizations/{organization_id}")
+        logger.info(f"Got {organization_id} organization.")
         return r
 
     def get_groups(self, organization_id):
         """
         Get a list of groups.
 
         `Args:`
             organization_id: str
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        tbl = Table(self._request(f'organizations/{organization_id}/groups'))
-        logger.info(f'Got {tbl.num_rows} groups.')
+        tbl = Table(self._request(f"organizations/{organization_id}/groups"))
+        logger.info(f"Got {tbl.num_rows} groups.")
         return tbl
 
     def get_group(self, group_id):
         """
         Get a single group.
 
         `Args:`
             group_id: str
                 The group id.
         """
 
-        r = self._request(f'groups/{group_id}')
-        logger.info(f'Got {group_id} group.')
+        r = self._request(f"groups/{group_id}")
+        logger.info(f"Got {group_id} group.")
         return r
 
     def get_lead(self, lead_id):
         """
         Get a single lead..
 
         `Args`:
             lead_id: str
                 The lead id.
         `Returns:`
             dict
         """
 
-        r = self._request(f'leads/{lead_id}')
-        logger.info(f'Got {lead_id} lead.')
+        r = self._request(f"leads/{lead_id}")
+        logger.info(f"Got {lead_id} lead.")
         return r
 
     def get_leads(self, organization_id=None, group_id=None):
         """
         Get leads metadata. One of ``organization_id`` and ``group_id`` must be passed
         as an argument. If both are passed, an error will be raised.
 
@@ -294,32 +304,44 @@
                 The group id.
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
         if organization_id is None and group_id is None:
-            raise ValueError('Either organization_id or group_id required.')
+            raise ValueError("Either organization_id or group_id required.")
 
         if organization_id is not None and group_id is not None:
-            raise ValueError('Only one of organization_id and group_id may be populated.')
+            raise ValueError(
+                "Only one of organization_id and group_id may be populated."
+            )
 
         if organization_id:
-            endpoint = f'organizations/{organization_id}/leads'
-            logger.info(f'Retrieving {organization_id} organization leads.')
+            endpoint = f"organizations/{organization_id}/leads"
+            logger.info(f"Retrieving {organization_id} organization leads.")
         if group_id:
-            endpoint = f'groups/{group_id}/leads'
-            logger.info(f'Retrieving {group_id} group leads.')
+            endpoint = f"groups/{group_id}/leads"
+            logger.info(f"Retrieving {group_id} group leads.")
 
         tbl = Table(self._request(endpoint))
-        logger.info(f'Got {tbl.num_rows} leads.')
+        logger.info(f"Got {tbl.num_rows} leads.")
         return tbl
 
-    def create_lead(self, group_id, phone_number, first_name, last_name=None, email=None,
-                    notes=None, follow_up=None, custom_fields=None, tag_ids=None):
+    def create_lead(
+        self,
+        group_id,
+        phone_number,
+        first_name,
+        last_name=None,
+        email=None,
+        notes=None,
+        follow_up=None,
+        custom_fields=None,
+        tag_ids=None,
+    ):
         """
 
         Create a lead.
 
         `Args:`
             group_id: str
                 The group id to assign the leads.
@@ -340,28 +362,29 @@
                 value as the value.
             tag_ids: list
                 A list of tag ids.
         `Returns:`
                 ``None``
         """
 
-        lead = {'firstName': first_name,
-                'lastName': last_name,
-                'email': email,
-                'phoneNumber': phone_number,
-                'notes': notes,
-                'followUp': follow_up,
-                'customFields': custom_fields,
-                'tagIds': tag_ids
-                }
+        lead = {
+            "firstName": first_name,
+            "lastName": last_name,
+            "email": email,
+            "phoneNumber": phone_number,
+            "notes": notes,
+            "followUp": follow_up,
+            "customFields": custom_fields,
+            "tagIds": tag_ids,
+        }
 
         # Remove empty args in dictionary
         lead = json_format.remove_empty_keys(lead)
-        logger.info(f'Generating lead for {first_name} {last_name}.')
-        return self._request(f'groups/{group_id}/leads', req_type="POST", payload=lead)
+        logger.info(f"Generating lead for {first_name} {last_name}.")
+        return self._request(f"groups/{group_id}/leads", req_type="POST", payload=lead)
 
     def create_leads(self, table, group_id=None):
         """
         Create multiple leads. All unrecognized fields will be passed as custom fields. Column
         names must map to the following names.
 
         .. list-table::
@@ -390,47 +413,65 @@
                 value.
         `Returns:`
             A table of created ids with associated lead id.
         """
 
         table.map_columns(LEAD_COLUMN_MAP)
 
-        arg_list = ['first_name', 'last_name', 'email', 'phone_number', 'follow_up',
-                    'tag_ids', 'group_id']
+        arg_list = [
+            "first_name",
+            "last_name",
+            "email",
+            "phone_number",
+            "follow_up",
+            "tag_ids",
+            "group_id",
+        ]
 
         created_leads = []
 
         for row in table:
 
-            lead = {'group_id': group_id}
+            lead = {"group_id": group_id}
             custom_fields = {}
 
             # Check for column names that map to arguments, if not assign
             # to custom fields
             for k, v in row.items():
                 if k in arg_list:
                     lead[k] = v
                 else:
                     custom_fields[k] = v
 
-            lead['custom_fields'] = custom_fields
+            lead["custom_fields"] = custom_fields
 
             # Group Id check
-            if not group_id and 'group_id' not in table.columns:
-                raise ValueError('Group Id must be passed as an argument or a column value.')
+            if not group_id and "group_id" not in table.columns:
+                raise ValueError(
+                    "Group Id must be passed as an argument or a column value."
+                )
             if group_id:
-                lead['group_id'] = group_id
+                lead["group_id"] = group_id
 
             created_leads.append(self.create_lead(**lead))
 
         logger.info(f"Created {table.num_rows} leads.")
         return Table(created_leads)
 
-    def update_lead(self, lead_id, first_name=None, last_name=None, email=None,
-                    global_opt_out=None, notes=None, follow_up=None, tag_ids=None):
+    def update_lead(
+        self,
+        lead_id,
+        first_name=None,
+        last_name=None,
+        email=None,
+        global_opt_out=None,
+        notes=None,
+        follow_up=None,
+        tag_ids=None,
+    ):
         """
         Update a lead.
 
         `Args`:
             lead_id: str
                 The lead id
             first_name: str
@@ -447,52 +488,54 @@
                 Follow up for the lead
             tag_ids: list
                 Tags to apply to lead
         `Returns:`
             dict
         """
 
-        lead = {'leadId': lead_id,
-                'firstName': first_name,
-                'lastName': last_name,
-                'email': email,
-                'globalOptedOut': global_opt_out,
-                'notes': notes,
-                'followUp': follow_up,
-                'tagIds': tag_ids}
+        lead = {
+            "leadId": lead_id,
+            "firstName": first_name,
+            "lastName": last_name,
+            "email": email,
+            "globalOptedOut": global_opt_out,
+            "notes": notes,
+            "followUp": follow_up,
+            "tagIds": tag_ids,
+        }
 
         # Remove empty args in dictionary
         lead = json_format.remove_empty_keys(lead)
 
-        logger.info(f'Updating lead for {first_name} {last_name}.')
-        return self._request(f'leads/{lead_id}', req_type="PUT", payload=lead)
+        logger.info(f"Updating lead for {first_name} {last_name}.")
+        return self._request(f"leads/{lead_id}", req_type="PUT", payload=lead)
 
     def get_tags(self, organization_id):
         """
         Get an organization's tags.
 
         `Args:`
             organization_id: str
                 The organization id.
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        tbl = Table(self._request(f'organizations/{organization_id}/tags'))
-        logger.info(f'Got {tbl.num_rows} tags for {organization_id} organization.')
+        tbl = Table(self._request(f"organizations/{organization_id}/tags"))
+        logger.info(f"Got {tbl.num_rows} tags for {organization_id} organization.")
         return tbl
 
     def get_tag(self, tag_id):
         """
         Get a single tag.
 
         `Args:`
             tag_id: str
                 The tag id.
         `Returns:`
             dict
         """
 
-        r = self._request(f'tags/{tag_id}')
-        logger.info(f'Got {tag_id} tag.')
+        r = self._request(f"tags/{tag_id}")
+        logger.info(f"Got {tag_id} tag.")
         return r
```

### Comparing `parsons-1.0.0/parsons/mailchimp/mailchimp.py` & `parsons-1.1.0/parsons/mailchimp/mailchimp.py`

 * *Files 10% similar despite different names*

```diff
@@ -3,37 +3,46 @@
 from parsons.etl import Table
 from parsons.utilities import check_env
 from parsons.utilities.api_connector import APIConnector
 
 logger = logging.getLogger(__name__)
 
 
-class Mailchimp():
+class Mailchimp:
     """
     Instantiate Mailchimp Class
 
     `Args:`
         api_key:
             The Mailchimp-provided application key. Not required if
             ``MAILCHIMP_API_KEY`` env variable set.
     `Returns:`
         Mailchimp Class
     """
 
     def __init__(self, api_key=None):
-        self.api_key = check_env.check('MAILCHIMP_API_KEY', api_key)
+        self.api_key = check_env.check("MAILCHIMP_API_KEY", api_key)
         self.domain = re.findall("(?<=-).+$", self.api_key)[0]
-        self.uri = f'https://{self.domain}.api.mailchimp.com/3.0/'
-        self.client = APIConnector(self.uri, auth=('x', self.api_key))
+        self.uri = f"https://{self.domain}.api.mailchimp.com/3.0/"
+        self.client = APIConnector(self.uri, auth=("x", self.api_key))
 
-    def get_lists(self, fields=None, exclude_fields=None,
-                  count=None, offset=None, before_date_created=None,
-                  since_date_created=None, before_campaign_last_sent=None,
-                  since_campaign_last_sent=None, email=None, sort_field=None,
-                  sort_dir=None):
+    def get_lists(
+        self,
+        fields=None,
+        exclude_fields=None,
+        count=None,
+        offset=None,
+        before_date_created=None,
+        since_date_created=None,
+        before_campaign_last_sent=None,
+        since_campaign_last_sent=None,
+        email=None,
+        sort_field=None,
+        sort_dir=None,
+    ):
         """
         Get a table of lists under the account based on query parameters. Note
         that argument descriptions here are sourced from Mailchimp's official
         API documentation.
 
         `Args:`
             fields: list of strings
@@ -70,40 +79,54 @@
                 Returns files sorted by the specified field.
             sort_dir: string, can only be 'ASC', 'DESC', or None
                 Determines the order direction for sorted results.
 
         `Returns:`
             Table Class
         """
-        params = {'fields': fields,
-                  'exclude_fields': exclude_fields,
-                  'count': count,
-                  'offset': offset,
-                  'before_date_created': before_date_created,
-                  'since_date_created': since_date_created,
-                  'before_campaign_last_sent': before_campaign_last_sent,
-                  'since_campaign_last_sent': since_campaign_last_sent,
-                  'email': email,
-                  'sort_field': sort_field,
-                  'sort_dir': sort_dir}
-
-        response = self.client.get_request('lists', params=params)
-        tbl = Table(response['lists'])
-        logger.info(f'Found {tbl.num_rows} lists.')
+        params = {
+            "fields": fields,
+            "exclude_fields": exclude_fields,
+            "count": count,
+            "offset": offset,
+            "before_date_created": before_date_created,
+            "since_date_created": since_date_created,
+            "before_campaign_last_sent": before_campaign_last_sent,
+            "since_campaign_last_sent": since_campaign_last_sent,
+            "email": email,
+            "sort_field": sort_field,
+            "sort_dir": sort_dir,
+        }
+
+        response = self.client.get_request("lists", params=params)
+        tbl = Table(response["lists"])
+        logger.info(f"Found {tbl.num_rows} lists.")
         if tbl.num_rows > 0:
             return tbl
         else:
             return Table()
 
-    def get_campaigns(self, fields=None, exclude_fields=None,
-                      count=None, offset=None, type=None, status=None,
-                      before_send_time=None, since_send_time=None,
-                      before_create_time=None, since_create_time=None,
-                      list_id=None, folder_id=None, member_id=None,
-                      sort_field=None, sort_dir=None):
+    def get_campaigns(
+        self,
+        fields=None,
+        exclude_fields=None,
+        count=None,
+        offset=None,
+        type=None,
+        status=None,
+        before_send_time=None,
+        since_send_time=None,
+        before_create_time=None,
+        since_create_time=None,
+        list_id=None,
+        folder_id=None,
+        member_id=None,
+        sort_field=None,
+        sort_dir=None,
+    ):
         """
         Get a table of campaigns under the account based on query parameters.
         Note that argument descriptions here are sourced from Mailchimp's
         official API documentation.
 
         `Args:`
             fields: list of strings
@@ -149,47 +172,63 @@
                 Returns files sorted by the specified field.
             sort_dir: string, can only be 'ASC', 'DESC', or None
                 Determines the order direction for sorted results.
 
         `Returns:`
             Table Class
         """
-        params = {'fields': fields,
-                  'exclude_fields': exclude_fields,
-                  'count': count,
-                  'offset': offset,
-                  'type': type,
-                  'status': status,
-                  'before_send_time': before_send_time,
-                  'since_send_time': since_send_time,
-                  'before_create_time': before_create_time,
-                  'since_create_time': since_create_time,
-                  'list_id': list_id,
-                  'folder_id': folder_id,
-                  'member_id': member_id,
-                  'sort_field': sort_field,
-                  'sort_dir': sort_dir}
-
-        response = self.client.get_request('campaigns', params=params)
-        tbl = Table(response['campaigns'])
-        logger.info(f'Found {tbl.num_rows} campaigns.')
+        params = {
+            "fields": fields,
+            "exclude_fields": exclude_fields,
+            "count": count,
+            "offset": offset,
+            "type": type,
+            "status": status,
+            "before_send_time": before_send_time,
+            "since_send_time": since_send_time,
+            "before_create_time": before_create_time,
+            "since_create_time": since_create_time,
+            "list_id": list_id,
+            "folder_id": folder_id,
+            "member_id": member_id,
+            "sort_field": sort_field,
+            "sort_dir": sort_dir,
+        }
+
+        response = self.client.get_request("campaigns", params=params)
+        tbl = Table(response["campaigns"])
+        logger.info(f"Found {tbl.num_rows} campaigns.")
         if tbl.num_rows > 0:
             return tbl
         else:
             return Table()
 
-    def get_members(self, list_id, fields=None,
-                    exclude_fields=None, count=None, offset=None,
-                    email_type=None, status=None, since_timestamp_opt=None,
-                    before_timestamp_opt=None, since_last_changed=None,
-                    before_last_changed=None, unique_email_id=None,
-                    vip_only=False, interest_category_id=None,
-                    interest_ids=None, interest_match=None, sort_field=None,
-                    sort_dir=None, since_last_campaign=None,
-                    unsubscribed_since=None):
+    def get_members(
+        self,
+        list_id,
+        fields=None,
+        exclude_fields=None,
+        count=None,
+        offset=None,
+        email_type=None,
+        status=None,
+        since_timestamp_opt=None,
+        before_timestamp_opt=None,
+        since_last_changed=None,
+        before_last_changed=None,
+        unique_email_id=None,
+        vip_only=False,
+        interest_category_id=None,
+        interest_ids=None,
+        interest_match=None,
+        sort_field=None,
+        sort_dir=None,
+        since_last_campaign=None,
+        unsubscribed_since=None,
+    ):
         """
         Get a table of members in a list based on query parameters. Note that
         argument descriptions here are sourced from Mailchimp's official API
         documentation.
 
         `Args:`
             list_id: string
@@ -262,45 +301,53 @@
                 Filter subscribers by those unsubscribed since a specific date.
                 Using any status other than unsubscribed with this filter will
                 result in an error.
 
         `Returns:`
             Table Class
         """
-        params = {'fields': fields,
-                  'exclude_fields': exclude_fields,
-                  'count': count,
-                  'offset': offset,
-                  'email_type': email_type,
-                  'status': status,
-                  'since_timestamp_opt': since_timestamp_opt,
-                  'before_timestamp_opt': before_timestamp_opt,
-                  'since_last_changed': since_last_changed,
-                  'before_last_changed': before_last_changed,
-                  'unqiue_email_id': unique_email_id,
-                  'vip_only': vip_only,
-                  'interest_category_id': interest_category_id,
-                  'interest_ids': interest_ids,
-                  'interest_match': interest_match,
-                  'sort_field': sort_field,
-                  'sort_dir': sort_dir,
-                  'since_last_campaign': since_last_campaign,
-                  'unsubscribed_since': unsubscribed_since}
-
-        response = self.client.get_request(f'lists/{list_id}/members', params=params)
-        tbl = Table(response['members'])
-        logger.info(f'Found {tbl.num_rows} members.')
+        params = {
+            "fields": fields,
+            "exclude_fields": exclude_fields,
+            "count": count,
+            "offset": offset,
+            "email_type": email_type,
+            "status": status,
+            "since_timestamp_opt": since_timestamp_opt,
+            "before_timestamp_opt": before_timestamp_opt,
+            "since_last_changed": since_last_changed,
+            "before_last_changed": before_last_changed,
+            "unqiue_email_id": unique_email_id,
+            "vip_only": vip_only,
+            "interest_category_id": interest_category_id,
+            "interest_ids": interest_ids,
+            "interest_match": interest_match,
+            "sort_field": sort_field,
+            "sort_dir": sort_dir,
+            "since_last_campaign": since_last_campaign,
+            "unsubscribed_since": unsubscribed_since,
+        }
+
+        response = self.client.get_request(f"lists/{list_id}/members", params=params)
+        tbl = Table(response["members"])
+        logger.info(f"Found {tbl.num_rows} members.")
         if tbl.num_rows > 0:
             return tbl
         else:
             return Table()
 
-    def get_campaign_emails(self, campaign_id, fields=None,
-                            exclude_fields=None, count=None, offset=None,
-                            since=None):
+    def get_campaign_emails(
+        self,
+        campaign_id,
+        fields=None,
+        exclude_fields=None,
+        count=None,
+        offset=None,
+        since=None,
+    ):
         """
         Get a table of individual emails from a campaign based on query
         parameters. Note that argument descriptions here are sourced from
         Mailchimp's official API documentation.
 
         `Args:`
             campaign_id: string
@@ -322,30 +369,34 @@
                 Restrict results to email activity events that occur after a
                 specific time. We recommend ISO 8601 time format:
                 2015-10-21T15:41:36+00:00.
 
         `Returns:`
             Table Class
         """
-        params = {'fields': fields,
-                  'exclude_fields': exclude_fields,
-                  'count': count,
-                  'offset': offset,
-                  'since': since}
-
-        response = self.client.get_request(f'reports/{campaign_id}/email-activity',
-                                           params=params)
-        tbl = Table(response['emails'])
+        params = {
+            "fields": fields,
+            "exclude_fields": exclude_fields,
+            "count": count,
+            "offset": offset,
+            "since": since,
+        }
+
+        response = self.client.get_request(
+            f"reports/{campaign_id}/email-activity", params=params
+        )
+        tbl = Table(response["emails"])
         if tbl.num_rows > 0:
             return tbl
         else:
             return Table()
 
-    def get_unsubscribes(self, campaign_id, fields=None,
-                         exclude_fields=None, count=None, offset=None):
+    def get_unsubscribes(
+        self, campaign_id, fields=None, exclude_fields=None, count=None, offset=None
+    ):
         """
         Get a table of unsubscribes associated with a campaign based on query
         parameters. Note that argument descriptions here are sourced from
         Mailchimp's official API documentation.
 
         `Args:`
             campaign_id: string
@@ -363,20 +414,23 @@
                 The number of records from a collection to skip. Iterating over
                 large collections with this parameter can be slow. Default
                 value is 0.
 
         `Returns:`
             Table Class
         """
-        params = {'fields': fields,
-                  'exclude_fields': exclude_fields,
-                  'count': count,
-                  'offset': offset}
-
-        response = self.client.get_request(f'reports/{campaign_id}/unsubscribed',
-                                           params=params)
-        tbl = Table(response['unsubscribes'])
-        logger.info(f'Found {tbl.num_rows} unsubscribes for {campaign_id}.')
+        params = {
+            "fields": fields,
+            "exclude_fields": exclude_fields,
+            "count": count,
+            "offset": offset,
+        }
+
+        response = self.client.get_request(
+            f"reports/{campaign_id}/unsubscribed", params=params
+        )
+        tbl = Table(response["unsubscribes"])
+        logger.info(f"Found {tbl.num_rows} unsubscribes for {campaign_id}.")
         if tbl.num_rows > 0:
             return tbl
         else:
             return Table()
```

### Comparing `parsons-1.0.0/parsons/mobilize_america/ma.py` & `parsons-1.1.0/parsons/mobilize_america/ma.py`

 * *Files 4% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 import re
 import os
 import logging
 import collections.abc
 
 logger = logging.getLogger(__name__)
 
-MA_URI = 'https://api.mobilize.us/v1/'
+MA_URI = "https://api.mobilize.us/v1/"
 
 
 class MobilizeAmerica(object):
     """
     Instantiate MobilizeAmerica Class
 
     api_key: str
@@ -22,72 +22,71 @@
     `Returns:`
         MobilizeAmerica Class
     """
 
     def __init__(self, api_key=None):
 
         self.uri = MA_URI
-        self.api_key = api_key or os.environ.get('MOBILIZE_AMERICA_API_KEY')
+        self.api_key = api_key or os.environ.get("MOBILIZE_AMERICA_API_KEY")
 
         if not self.api_key:
-            logger.info('Mobilize America API Key missing. Calling methods that rely on private'
-                        ' endpoints will fail.')
+            logger.info(
+                "Mobilize America API Key missing. Calling methods that rely on private"
+                " endpoints will fail."
+            )
 
-    def _request(self, url, req_type='GET', post_data=None, args=None, auth=False):
+    def _request(self, url, req_type="GET", post_data=None, args=None, auth=False):
         if auth:
 
             if not self.api_key:
-                raise TypeError('This method requires an api key.')
+                raise TypeError("This method requires an api key.")
             else:
-                header = {'Authorization': 'Bearer ' + self.api_key}
+                header = {"Authorization": "Bearer " + self.api_key}
 
         else:
             header = None
 
         r = _request(req_type, url, json=post_data, params=args, headers=header)
 
         r.raise_for_status()
 
-        if 'error' in r.json():
-            raise ValueError('API Error:' + str(r.json()['error']))
+        if "error" in r.json():
+            raise ValueError("API Error:" + str(r.json()["error"]))
 
         return r
 
-    def _request_paginate(self, url, req_type='GET', args=None, auth=False):
+    def _request_paginate(self, url, req_type="GET", args=None, auth=False):
 
         r = self._request(url, req_type=req_type, args=args, auth=auth)
 
-        json = r.json()['data']
+        json = r.json()["data"]
 
-        while r.json()['next']:
+        while r.json()["next"]:
 
-            r = self._request(r.json()['next'], req_type=req_type, auth=auth)
-            json.extend(r.json()['data'])
+            r = self._request(r.json()["next"], req_type=req_type, auth=auth)
+            json.extend(r.json()["data"])
 
         return json
 
     def _time_parse(self, time_arg):
         # Parse the date filters
 
-        trans = [('>=', 'gte_'),
-                 ('>', 'gt_'),
-                 ('<=', 'lte_'),
-                 ('<', 'lt_')]
+        trans = [(">=", "gte_"), (">", "gt_"), ("<=", "lte_"), ("<", "lt_")]
 
         if time_arg:
 
-            time = re.sub('<=|<|>=|>', '', time_arg)
+            time = re.sub("<=|<|>=|>", "", time_arg)
             time = date_to_timestamp(time)
-            time_filter = re.search('<=|<|>=|>', time_arg).group()
+            time_filter = re.search("<=|<|>=|>", time_arg).group()
 
             for i in trans:
                 if time_filter == i[0]:
                     return i[1] + str(time)
 
-            raise ValueError('Invalid time operator. Must be one of >=, >, <= or >.')
+            raise ValueError("Invalid time operator. Must be one of >=, >, <= or >.")
 
         return time_arg
 
     def get_organizations(self, updated_since=None):
         """
         Return all active organizations on the platform.
 
@@ -95,34 +94,48 @@
             updated_since: str
                 Filter to organizations updated since given date (ISO Date)
         `Returns`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        return Table(self._request_paginate(self.uri + 'organizations',
-                                            args={
-                                                'updated_since': date_to_timestamp(updated_since)
-                                            }))
+        return Table(
+            self._request_paginate(
+                self.uri + "organizations",
+                args={"updated_since": date_to_timestamp(updated_since)},
+            )
+        )
 
     def get_promoted_organizations(self, organization_id):
         """
         Return all organizations promoted by the given organization.
 
         `Args:`
             organization_id: int
                 ID of the organization to query.
         `Returns`
             Parsons Table
         """
-        url = self.uri + 'organizations/' + str(organization_id) + '/promoted_organizations'
+        url = (
+            self.uri
+            + "organizations/"
+            + str(organization_id)
+            + "/promoted_organizations"
+        )
         return Table(self._request_paginate(url, auth=True))
 
-    def get_events(self, organization_id=None, updated_since=None, timeslot_start=None,
-                   timeslot_end=None, timeslots_table=False, max_timeslots=None):
+    def get_events(
+        self,
+        organization_id=None,
+        updated_since=None,
+        timeslot_start=None,
+        timeslot_end=None,
+        timeslots_table=False,
+        max_timeslots=None,
+    ):
         """
         Fetch all public events on the platform.
 
         `Args:`
             organization_id: list or int
                 Filter events by a single or multiple organization ids
             updated_since: str
@@ -154,48 +167,57 @@
         `Returns`
             :ref:`parsons.Table <parsons-table>`, dict, list[:ref:`parsons.Table <parsons-table>`]
         """
 
         if isinstance(organization_id, (str, int)):
             organization_id = [organization_id]
 
-        args = {'organization_id': organization_id,
-                'updated_since': date_to_timestamp(updated_since),
-                'timeslot_start': self._time_parse(timeslot_start),
-                'timeslot_end': self._time_parse(timeslot_end)}
+        args = {
+            "organization_id": organization_id,
+            "updated_since": date_to_timestamp(updated_since),
+            "timeslot_start": self._time_parse(timeslot_start),
+            "timeslot_end": self._time_parse(timeslot_end),
+        }
 
-        tbl = Table(self._request_paginate(self.uri + 'events', args=args))
+        tbl = Table(self._request_paginate(self.uri + "events", args=args))
 
         if tbl.num_rows > 0:
 
-            tbl.unpack_dict('sponsor')
-            tbl.unpack_dict('location', prepend=False)
-            tbl.unpack_dict('location', prepend=False)  # Intentional duplicate
-            tbl.table = petl.convert(tbl.table, 'address_lines', lambda v: ' '.join(v))
+            tbl.unpack_dict("sponsor")
+            tbl.unpack_dict("location", prepend=False)
+            tbl.unpack_dict("location", prepend=False)  # Intentional duplicate
+            tbl.table = petl.convert(tbl.table, "address_lines", lambda v: " ".join(v))
 
             if timeslots_table:
 
-                timeslots_tbl = tbl.long_table(['id'], 'timeslots', 'event_id')
-                return {'events': tbl, 'timeslots': timeslots_tbl}
+                timeslots_tbl = tbl.long_table(["id"], "timeslots", "event_id")
+                return {"events": tbl, "timeslots": timeslots_tbl}
 
             elif max_timeslots == 0:
-                tbl.remove_column('timeslots')
+                tbl.remove_column("timeslots")
 
             else:
-                tbl.unpack_list('timeslots', replace=True, max_columns=max_timeslots)
+                tbl.unpack_list("timeslots", replace=True, max_columns=max_timeslots)
                 cols = tbl.columns
                 for c in cols:
-                    if re.search('timeslots', c, re.IGNORECASE) is not None:
+                    if re.search("timeslots", c, re.IGNORECASE) is not None:
                         tbl.unpack_dict(c)
                         tbl.materialize()
 
         return tbl
 
-    def get_events_organization(self, organization_id, updated_since=None, timeslot_start=None,
-                                timeslot_end=None, timeslots_table=False, max_timeslots=None):
+    def get_events_organization(
+        self,
+        organization_id,
+        updated_since=None,
+        timeslot_start=None,
+        timeslot_end=None,
+        timeslots_table=False,
+        max_timeslots=None,
+    ):
         """
         Fetch all public events for an organization. This includes both events owned
         by the organization (as indicated by the organization field on the event object)
         and events of other organizations promoted by this specified organization.
 
         .. note::
             API Key Required
@@ -249,44 +271,48 @@
 
                 If ``max_timeslots`` is 0, no timeslot columns will be included.
 
         `Returns`
             :ref:`parsons.Table <parsons-table>`, dict, list[:ref:`parsons.Table <parsons-table>`]
         """
 
-        args = {'updated_since': date_to_timestamp(updated_since),
-                'timeslot_start': self._time_parse(timeslot_start),
-                'timeslot_end': self._time_parse(timeslot_end),
-                }
-
-        tbl = Table(self._request_paginate(
-            self.uri + 'organizations/' + str(organization_id) + '/events',
-            args=args,
-            auth=True))
+        args = {
+            "updated_since": date_to_timestamp(updated_since),
+            "timeslot_start": self._time_parse(timeslot_start),
+            "timeslot_end": self._time_parse(timeslot_end),
+        }
+
+        tbl = Table(
+            self._request_paginate(
+                self.uri + "organizations/" + str(organization_id) + "/events",
+                args=args,
+                auth=True,
+            )
+        )
 
         if tbl.num_rows > 0:
 
-            tbl.unpack_dict('sponsor')
-            tbl.unpack_dict('location', prepend=False)
-            tbl.unpack_dict('location', prepend=False)  # Intentional duplicate
-            tbl.table = petl.convert(tbl.table, 'address_lines', lambda v: ' '.join(v))
+            tbl.unpack_dict("sponsor")
+            tbl.unpack_dict("location", prepend=False)
+            tbl.unpack_dict("location", prepend=False)  # Intentional duplicate
+            tbl.table = petl.convert(tbl.table, "address_lines", lambda v: " ".join(v))
 
             if timeslots_table:
 
-                timeslots_tbl = tbl.long_table(['id'], 'timeslots', 'event_id')
-                return {'events': tbl, 'timeslots': timeslots_tbl}
+                timeslots_tbl = tbl.long_table(["id"], "timeslots", "event_id")
+                return {"events": tbl, "timeslots": timeslots_tbl}
 
             elif max_timeslots == 0:
-                tbl.remove_column('timeslots')
+                tbl.remove_column("timeslots")
 
             else:
-                tbl.unpack_list('timeslots', replace=True, max_columns=max_timeslots)
+                tbl.unpack_list("timeslots", replace=True, max_columns=max_timeslots)
                 cols = tbl.columns
                 for c in cols:
-                    if re.search('timeslots', c, re.IGNORECASE) is not None:
+                    if re.search("timeslots", c, re.IGNORECASE) is not None:
                         tbl.unpack_dict(c)
                         tbl.materialize()
 
         return tbl
 
     def get_events_deleted(self, organization_id=None, updated_since=None):
         """
@@ -301,18 +327,20 @@
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
         if isinstance(organization_id, (str, int)):
             organization_id = [organization_id]
 
-        args = {'organization_id': organization_id,
-                'updated_since': date_to_timestamp(updated_since)}
+        args = {
+            "organization_id": organization_id,
+            "updated_since": date_to_timestamp(updated_since),
+        }
 
-        return Table(self._request_paginate(self.uri + 'events/deleted', args=args))
+        return Table(self._request_paginate(self.uri + "events/deleted", args=args))
 
     def get_people(self, organization_id, updated_since=None):
         """
         Fetch all people (volunteers) who are affiliated with an organization(s).
 
         .. note::
             API Key Required
@@ -328,16 +356,16 @@
         """
         if isinstance(organization_id, collections.abc.Iterable):
             data = Table()
             for id in organization_id:
                 data.concat(self.get_people(id, updated_since))
             return data
         else:
-            url = self.uri + 'organizations/' + str(organization_id) + '/people'
-            args = {'updated_since': date_to_timestamp(updated_since)}
+            url = self.uri + "organizations/" + str(organization_id) + "/people"
+            args = {"updated_since": date_to_timestamp(updated_since)}
             return Table(self._request_paginate(url, args=args, auth=True))
 
     def get_attendances(self, organization_id, updated_since=None):
         """
         Fetch all attendances which were either promoted by the organization or
         were for events owned by the organization.
 
@@ -349,10 +377,10 @@
                 Filter attendances by an organization id
             updated_since: str
                 Filter to attendances updated since given date (ISO Date)
         `Returns`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
-        url = self.uri + 'organizations/' + str(organization_id) + '/attendances'
-        args = {'updated_since': date_to_timestamp(updated_since)}
+        url = self.uri + "organizations/" + str(organization_id) + "/attendances"
+        args = {"updated_since": date_to_timestamp(updated_since)}
         return Table(self._request_paginate(url, args=args, auth=True))
```

### Comparing `parsons-1.0.0/parsons/newmode/newmode.py` & `parsons-1.1.0/parsons/newmode/newmode.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,15 +3,14 @@
 from parsons.etl import Table
 import logging
 
 logger = logging.getLogger(__name__)
 
 
 class Newmode:
-
     def __init__(self, api_user=None, api_password=None, api_version=None):
         """
         Args:
             api_user: str
                 The Newmode api user. Not required if ``NEWMODE_API_USER`` env variable is
                 passed.
             api_password: str
@@ -19,21 +18,21 @@
                 passed.
             api_version: str
                 The Newmode api version. Defaults to "v1.0" or the value of ``NEWMODE_API_VERSION``
                 env variable.
         Returns:
             Newmode class
         """
-        self.api_user = check_env.check('NEWMODE_API_USER', api_user)
-        self.api_password = check_env.check('NEWMODE_API_PASSWORD', api_password)
+        self.api_user = check_env.check("NEWMODE_API_USER", api_user)
+        self.api_password = check_env.check("NEWMODE_API_PASSWORD", api_password)
 
         if api_version is None:
             api_version = "v1.0"
 
-        self.api_version = check_env.check('NEWMODE_API_VERSION', api_version)
+        self.api_version = check_env.check("NEWMODE_API_VERSION", api_version)
 
         self.client = Client(api_user, api_password, api_version)
 
     def convert_to_table(self, data):
         # Internal method to create a Parsons table from a data element.
         table = None
         if type(data) is list:
@@ -95,15 +94,15 @@
         Returns:
             Targets information as table.
         """
         targets = self.client.lookupTargets(tool_id, search, params=params)
         if targets:
             data = []
             for key in targets:
-                if key != '_links':
+                if key != "_links":
                     data.append(targets[key])
             return self.convert_to_table(data)
         else:
             logging.warning("Empty targets returned")
             return self.convert_to_table([])
 
     def get_action(self, tool_id, params={}):
@@ -136,18 +135,18 @@
             params:
                 Extra parameters sent to New/Mode library.
         Returns:
             Action link (if otl) or sid.
         """
         action = self.client.runAction(tool_id, payload, params=params)
         if action:
-            if 'link' in action:
-                return action['link']
+            if "link" in action:
+                return action["link"]
             else:
-                return action['sid']
+                return action["sid"]
         else:
             logging.warning("Error in response")
             return None
 
     def get_target(self, target_id, params={}):
         """
         Get specific target.
```

### Comparing `parsons-1.0.0/parsons/ngpvan/activist_codes.py` & `parsons-1.1.0/parsons/ngpvan/activist_codes.py`

 * *Files 8% similar despite different names*

```diff
@@ -4,68 +4,72 @@
 from parsons.ngpvan.utilities import action_parse
 import logging
 
 logger = logging.getLogger(__name__)
 
 
 class ActivistCodes(object):
-
     def __init__(self, van_connection):
 
         self.connection = van_connection
 
     def get_activist_codes(self):
         """
         Get activist codes.
 
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        tbl = Table(self.connection.get_request('activistCodes'))
-        logger.info(f'Found {tbl.num_rows} activist codes.')
+        tbl = Table(self.connection.get_request("activistCodes"))
+        logger.info(f"Found {tbl.num_rows} activist codes.")
         return tbl
 
     def get_activist_code(self, activist_code_id):
         """
         Get an activist code.
 
         `Args:`
             activist_code_id : int
                 The activist code id.
         `Returns:`
             dict
                 The activist code
         """
 
-        r = self.connection.get_request(f'activistCodes/{activist_code_id}')
-        logger.info(f'Found activist code {activist_code_id}.')
+        r = self.connection.get_request(f"activistCodes/{activist_code_id}")
+        logger.info(f"Found activist code {activist_code_id}.")
         return r
 
-    def toggle_activist_code(self, id, activist_code_id, action, id_type='vanid',
-                             omit_contact=True):
+    def toggle_activist_code(
+        self, id, activist_code_id, action, id_type="vanid", omit_contact=True
+    ):
         # Internal method to apply/remove activist codes. Was previously a public method,
         # but for the sake of simplicity, breaking out into two public  methods.
 
-        response = {"activistCodeId": activist_code_id,
-                    "action": action_parse(action),
-                    "type": "activistCode",
-                    "omitActivistCodeContactHistory": omit_contact
-                    }
+        response = {
+            "activistCodeId": activist_code_id,
+            "action": action_parse(action),
+            "type": "activistCode",
+            "omitActivistCodeContactHistory": omit_contact,
+        }
 
         r = self.apply_response(id, response, id_type, omit_contact=omit_contact)
 
-        logger.info(f'{id_type.upper()} {id} {action.capitalize()} ' +
-                    f'activist code {activist_code_id}')
+        logger.info(
+            f"{id_type.upper()} {id} {action.capitalize()} "
+            + f"activist code {activist_code_id}"
+        )
 
         return r
 
-    def apply_activist_code(self, id, activist_code_id, id_type='vanid',
-                            omit_contact=True):
+    def apply_activist_code(
+        self, id, activist_code_id, id_type="vanid", omit_contact=True
+    ):
         """
         Apply an activist code to or from a person.
 
         `Args:`
             id: str
                 A valid person id
             activist_code_id: int
@@ -78,21 +82,19 @@
             omit_contact: boolean
                 If set to false the contact history will be updated with a contact
                 attempt.
         Returns:
             ``None``
         """
 
-        return self.toggle_activist_code(id,
-                                         activist_code_id,
-                                         'Apply',
-                                         id_type=id_type,
-                                         omit_contact=omit_contact)
+        return self.toggle_activist_code(
+            id, activist_code_id, "Apply", id_type=id_type, omit_contact=omit_contact
+        )
 
-    def remove_activist_code(self, id, activist_code_id, id_type='vanid'):
+    def remove_activist_code(self, id, activist_code_id, id_type="vanid"):
         """
         Remove an activist code to or from a person.
 
         `Args:`
             id: str
                 A valid person id
             activist_code_id: int
@@ -102,8 +104,10 @@
             id_type: str
                 A known person identifier type available on this VAN instance
                 such as ``dwid``
         Returns:
             ``None``
         """
 
-        return self.toggle_activist_code(id, activist_code_id, 'Remove', id_type=id_type)
+        return self.toggle_activist_code(
+            id, activist_code_id, "Remove", id_type=id_type
+        )
```

### Comparing `parsons-1.0.0/parsons/ngpvan/bulk_import.py` & `parsons-1.1.0/parsons/ngpvan/bulk_import.py`

 * *Files 13% similar despite different names*

```diff
@@ -6,48 +6,46 @@
 import uuid
 import csv
 
 logger = logging.getLogger(__name__)
 
 
 class BulkImport(object):
-
     def __init__(self):
-
         pass
 
     def get_bulk_import_resources(self):
         """
         Get bulk import resources that available to the user. These define
         the types of bulk imports that you can run. These might include
         ``Contacts``, ``ActivistCodes``, ``ContactsActivistCodes`` and others.
 
         `Returns:`
             list
                 A list of resources.
         """
 
-        r = self.connection.get_request('bulkImportJobs/resources')
-        logger.info(f'Found {len(r)} bulk import resources.')
+        r = self.connection.get_request("bulkImportJobs/resources")
+        logger.info(f"Found {len(r)} bulk import resources.")
         return r
 
     def get_bulk_import_job(self, job_id):
         """
         Get a bulk import job status.
 
         `Args:`
             job_id : int
                 The bulk import job id.
         `Returns:`
             dict
                 The bulk import job
         """
 
-        r = self.connection.get_request(f'bulkImportJobs/{job_id}')
-        logger.info(f'Found bulk import job {job_id}.')
+        r = self.connection.get_request(f"bulkImportJobs/{job_id}")
+        logger.info(f"Found bulk import job {job_id}.")
         return r
 
     def get_bulk_import_job_results(self, job_id):
         """
         Get result file of a bulk upload job. This will include one row
         per record processed as well as the status of each.
 
@@ -60,45 +58,45 @@
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
         r = self.get_bulk_import_job(job_id)
         logger.info(f"Bulk Import Job Status: {r['status']}")
-        if r['status'] == 'Completed':
-            return Table.from_csv(r['resultFiles'][0]['url'])
+        if r["status"] == "Completed":
+            return Table.from_csv(r["resultFiles"][0]["url"])
 
         return None
 
     def get_bulk_import_mapping_types(self):
         """
         Get bulk import mapping types.
 
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        tbl = Table(self.connection.get_request('bulkImportMappingTypes'))
-        logger.info(f'Found {tbl.num_rows} bulk import mapping types.')
+        tbl = Table(self.connection.get_request("bulkImportMappingTypes"))
+        logger.info(f"Found {tbl.num_rows} bulk import mapping types.")
         return tbl
 
     def get_bulk_import_mapping_type(self, type_name):
         """
         Get a single bulk import mapping type.
 
         `Args:`
             type_name: str
         `Returns`:
             dict
                 A mapping type json
         """
 
-        r = self.connection.get_request(f'bulkImportMappingTypes/{type_name}')
-        logger.info(f'Found {type_name} bulk import mapping type.')
+        r = self.connection.get_request(f"bulkImportMappingTypes/{type_name}")
+        logger.info(f"Found {type_name} bulk import mapping type.")
         return r
 
     def get_bulk_import_mapping_type_fields(self, type_name, field_name):
         """
         Get data about a field in a mapping type.
 
         `Args:`
@@ -107,53 +105,71 @@
             field_name: str
                 The field name
         `Returns:`
             dict
                 A mapping type fields json
         """
 
-        r = self.connection.get_request(f'bulkImportMappingTypes/{type_name}/{field_name}/values')
-        logger.info(f'Found {type_name} bulk import mapping type field values.')
+        r = self.connection.get_request(
+            f"bulkImportMappingTypes/{type_name}/{field_name}/values"
+        )
+        logger.info(f"Found {type_name} bulk import mapping type field values.")
         return r
 
-    def post_bulk_import(self, tbl, url_type, resource_type, mapping_types,
-                         description, result_fields=None, **url_kwargs):
+    def post_bulk_import(
+        self,
+        tbl,
+        url_type,
+        resource_type,
+        mapping_types,
+        description,
+        result_fields=None,
+        **url_kwargs,
+    ):
         # Internal method to post bulk imports.
 
         # Move to cloud storage
         file_name = str(uuid.uuid1())
-        url = cloud_storage.post_file(tbl,
-                                      url_type,
-                                      file_path=file_name + '.zip',
-                                      quoting=csv.QUOTE_ALL,
-                                      **url_kwargs)
-        logger.info(f'Table uploaded to {url_type}.')
+        url = cloud_storage.post_file(
+            tbl,
+            url_type,
+            file_path=file_name + ".zip",
+            quoting=csv.QUOTE_ALL,
+            **url_kwargs,
+        )
+        logger.info(f"Table uploaded to {url_type}.")
 
         # Generate request json
-        json = {"description": description,
-                "file": {
-                    "columnDelimiter": 'csv',
-                    "columns": [{'name': c} for c in tbl.columns],
-                    "fileName": file_name + '.csv',
-                    "hasHeader": "True",
-                    "hasQuotes": "True",
-                    "sourceUrl": url},
-                "actions": [{"resultFileSizeKbLimit": 5000,
-                             "resourceType": resource_type,
-                             "actionType": "loadMappedFile",
-                             "mappingTypes": mapping_types}]
+        json = {
+            "description": description,
+            "file": {
+                "columnDelimiter": "csv",
+                "columns": [{"name": c} for c in tbl.columns],
+                "fileName": file_name + ".csv",
+                "hasHeader": "True",
+                "hasQuotes": "True",
+                "sourceUrl": url,
+            },
+            "actions": [
+                {
+                    "resultFileSizeKbLimit": 5000,
+                    "resourceType": resource_type,
+                    "actionType": "loadMappedFile",
+                    "mappingTypes": mapping_types,
                 }
+            ],
+        }
 
         if result_fields:
-            result_fields = [{'name': c} for c in result_fields]
-            json['actions'][0]['columnsToIncludeInResultsFile'] = result_fields
+            result_fields = [{"name": c} for c in result_fields]
+            json["actions"][0]["columnsToIncludeInResultsFile"] = result_fields
 
-        r = self.connection.post_request('bulkImportJobs', json=json)
+        r = self.connection.post_request("bulkImportJobs", json=json)
         logger.info(f"Bulk upload {r['jobId']} created.")
-        return r['jobId']
+        return r["jobId"]
 
     def bulk_apply_activist_codes(self, tbl, url_type, **url_kwargs):
         """
         Bulk apply activist codes.
 
         The table may include the following columns. The first column
         must be ``vanid``.
@@ -188,20 +204,22 @@
                 Arguments to configure your cloud storage url type. See
                 :ref:`Cloud Storage <cloud-storage>` for more details.
         `Returns:`
             int
                 The bulk import job id
         """
 
-        return self.post_bulk_import(tbl,
-                                     url_type,
-                                     'ContactsActivistCodes',
-                                     [{"name": "ActivistCode"}],
-                                     'Activist Code Upload',
-                                     **url_kwargs)
+        return self.post_bulk_import(
+            tbl,
+            url_type,
+            "ContactsActivistCodes",
+            [{"name": "ActivistCode"}],
+            "Activist Code Upload",
+            **url_kwargs,
+        )
 
     def bulk_upsert_contacts(self, tbl, url_type, result_fields=None, **url_kwargs):
         """
         Bulk create or update contact records. Provide a Parsons table of contact data to
         create or update records.
 
         .. note::
@@ -306,36 +324,84 @@
         `Returns:`
             int
                 The bulk import job id
         """
 
         tbl = tbl.map_columns(COLUMN_MAP, exact_match=False)
 
-        return self.post_bulk_import(tbl,
-                                     url_type,
-                                     'Contacts',
-                                     [{'name': 'CreateOrUpdateContact'}],
-                                     'Create Or Update Contact Records',
-                                     result_fields=result_fields,
-                                     **url_kwargs)
+        return self.post_bulk_import(
+            tbl,
+            url_type,
+            "Contacts",
+            [{"name": "CreateOrUpdateContact"}],
+            "Create Or Update Contact Records",
+            result_fields=result_fields,
+            **url_kwargs,
+        )
+
+    def bulk_apply_suppressions(self, tbl, url_type, **url_kwargs):
+        """
+        Bulk apply contact suppressions codes.
+
+        The table may include the following columns. The first column
+        must be ``vanid``.
+
+        .. list-table::
+            :widths: 25 25
+            :header-rows: 1
+
+            * - Column Name
+              - Required
+              - Description
+            * - ``vanid``
+              - Yes
+              - A valid VANID primary key
+            * - ``suppressionid``
+              - Yes
+              - A valid suppression id
+
+        `Args:`
+            table: Parsons table
+                A Parsons table.
+            url_type: str
+                The cloud file storage to use to post the file (``S3`` or ``GCS``).
+                See :ref:`Cloud Storage <cloud-storage>` for more details.
+            **url_kwargs: kwargs
+                Arguments to configure your cloud storage url type. See
+                :ref:`Cloud Storage <cloud-storage>` for more details.
+        `Returns:`
+            int
+                The bulk import job id
+        """
+
+        return self.post_bulk_import(
+            tbl,
+            url_type,
+            "Contacts",
+            [{"name": "Suppressions"}],
+            "Apply Suppressions",
+            **url_kwargs,
+        )
 
 
 # This is a column mapper that is used to accept additional column names and provide
 # flexibility for the user.
 
-COLUMN_MAP = {'firstname': ['fn', 'first'],
-              'middlename': ['mn', 'middle'],
-              'lastname': ['ln', 'last'],
-              'dob': ['dateofbirth', 'birthdate'],
-              'sex': ['gender'],
-              'addressline1': ['address', 'addressline1', 'address1'],
-              'addressline2': ['addressline2', 'address2'],
-              'addressline3': ['addressline3', 'address3'],
-              'city': [],
-              'stateorprovince': ['state', 'st'],
-              'countrycode': ['country'],
-              'displayasentered': [],
-              'cellphone': ['cell'],
-              'cellphonecountrycode': ['cellcountrycode'],
-              'phone': ['home', 'homephone'],
-              'phonecountrycode': ['phonecountrycode'],
-              'email': ['emailaddress']}
+COLUMN_MAP = {
+    "firstname": ["fn", "first"],
+    "middlename": ["mn", "middle"],
+    "lastname": ["ln", "last"],
+    "dob": ["dateofbirth", "birthdate"],
+    "sex": ["gender"],
+    "addressline1": ["address", "addressline1", "address1"],
+    "addressline2": ["addressline2", "address2"],
+    "addressline3": ["addressline3", "address3"],
+    "city": [],
+    "stateorprovince": ["state", "st"],
+    "countrycode": ["country"],
+    "displayasentered": [],
+    "cellphone": ["cell"],
+    "cellphonecountrycode": ["cellcountrycode"],
+    "phone": ["home", "homephone"],
+    "phonecountrycode": ["phonecountrycode"],
+    "email": ["emailaddress"],
+}
```

### Comparing `parsons-1.0.0/parsons/ngpvan/canvass_responses.py` & `parsons-1.1.0/parsons/ngpvan/canvass_responses.py`

 * *Files 6% similar despite different names*

```diff
@@ -3,50 +3,49 @@
 from parsons.etl.table import Table
 import logging
 
 logger = logging.getLogger(__name__)
 
 
 class CanvassResponses(object):
-
     def __init__(self, van_connection):
 
         self.connection = van_connection
 
     def get_canvass_responses_contact_types(self):
         """
         Get canvass response contact types.
 
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        tbl = Table(self.connection.get_request('canvassResponses/contactTypes'))
-        logger.info(f'Found {tbl.num_rows} canvass response contact types.')
+        tbl = Table(self.connection.get_request("canvassResponses/contactTypes"))
+        logger.info(f"Found {tbl.num_rows} canvass response contact types.")
         return tbl
 
     def get_canvass_responses_input_types(self):
         """
         Get canvass response input types.
 
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        tbl = Table(self.connection.get_request('canvassResponses/inputTypes'))
-        logger.info(f'Found {tbl.num_rows} canvass response input types.')
+        tbl = Table(self.connection.get_request("canvassResponses/inputTypes"))
+        logger.info(f"Found {tbl.num_rows} canvass response input types.")
         return tbl
 
     def get_canvass_responses_result_codes(self):
         """
         Get canvass response result codes.
 
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        tbl = Table(self.connection.get_request('canvassResponses/resultCodes'))
-        logger.info(f'Found {tbl.num_rows} canvass response result codes.')
+        tbl = Table(self.connection.get_request("canvassResponses/resultCodes"))
+        logger.info(f"Found {tbl.num_rows} canvass response result codes.")
         return tbl
```

### Comparing `parsons-1.0.0/parsons/ngpvan/changed_entities.py` & `parsons-1.1.0/parsons/ngpvan/changed_entities.py`

 * *Files 8% similar despite different names*

```diff
@@ -6,48 +6,58 @@
 
 logger = logging.getLogger(__name__)
 
 RETRY_RATE = 10
 
 
 class ChangedEntities(object):
-
     def __init__(self):
 
         pass
 
     def get_changed_entity_resources(self):
         """
         Get changed entity resources available to the API user.
 
         `Returns:`
             list
         """
 
-        r = self.connection.get_request('changedEntityExportJobs/resources')
-        logger.info(f'Found {len(r)} changed entity resources.')
+        r = self.connection.get_request("changedEntityExportJobs/resources")
+        logger.info(f"Found {len(r)} changed entity resources.")
         return r
 
     def get_changed_entity_resource_fields(self, resource_type):
         """
         Get export fields avaliable for each changed entity resource.
 
         `Args:`
             resource_type: str
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        tbl = Table(self.connection.get_request(f'changedEntityExportJobs/fields/{resource_type}'))
-        logger.info(f'Found {tbl.num_rows} fields for {resource_type}.')
+        tbl = Table(
+            self.connection.get_request(
+                f"changedEntityExportJobs/fields/{resource_type}"
+            )
+        )
+        logger.info(f"Found {tbl.num_rows} fields for {resource_type}.")
         return tbl
 
-    def get_changed_entities(self, resource_type, date_from, date_to=None, include_inactive=False,
-                             requested_fields=None, custom_fields=None):
+    def get_changed_entities(
+        self,
+        resource_type,
+        date_from,
+        date_to=None,
+        include_inactive=False,
+        requested_fields=None,
+        custom_fields=None,
+    ):
         """
         Get modified records for VAN from up to 90 days in the past.
 
         `Args:`
             resource_type: str
                 The type of resource to export. Use the :py:meth:`~parsons.ngpvan.changed_entities.ChangedEntities.get_changed_entity_resources`
                 to get a list of potential entities.
@@ -74,26 +84,26 @@
         json = {
             "dateChangedFrom": date_from,
             "dateChangedTo": date_to,
             "resourceType": resource_type,
             "requestedFields": requested_fields,
             "requestedCustomFieldIds": custom_fields,
             "fileSizeKbLimit": 100000,
-            "includeInactive": include_inactive
+            "includeInactive": include_inactive,
         }
 
-        r = self.connection.post_request('changedEntityExportJobs', json=json)
+        r = self.connection.post_request("changedEntityExportJobs", json=json)
 
         while True:
-            status = self._get_changed_entity_job(r['exportJobId'])
-            if status['jobStatus'] in ['Pending', 'InProcess']:
-                logger.info('Waiting on export file.')
+            status = self._get_changed_entity_job(r["exportJobId"])
+            if status["jobStatus"] in ["Pending", "InProcess"]:
+                logger.info("Waiting on export file.")
                 time.sleep(RETRY_RATE)
-            elif status['jobStatus'] == 'Complete':
-                return Table.from_csv(status['files'][0]['downloadUrl'])
+            elif status["jobStatus"] == "Complete":
+                return Table.from_csv(status["files"][0]["downloadUrl"])
             else:
-                raise ValueError(status['message'])
+                raise ValueError(status["message"])
 
     def _get_changed_entity_job(self, job_id):
 
-        r = self.connection.get_request(f'changedEntityExportJobs/{job_id}')
+        r = self.connection.get_request(f"changedEntityExportJobs/{job_id}")
         return r
```

### Comparing `parsons-1.0.0/parsons/ngpvan/codes.py` & `parsons-1.1.0/parsons/ngpvan/codes.py`

 * *Files 4% similar despite different names*

```diff
@@ -2,21 +2,21 @@
 from parsons.etl.table import Table
 import logging
 
 logger = logging.getLogger(__name__)
 
 
 class Codes(object):
-
     def __init__(self, van_connection):
 
         self.connection = van_connection
 
-    def get_codes(self, name=None, supported_entities=None, parent_code_id=None,
-                  code_type=None):
+    def get_codes(
+        self, name=None, supported_entities=None, parent_code_id=None, code_type=None
+    ):
         """
         Get codes.
 
         `Args:`
             name : str
                 Filter by name of code.
             supported_entities: str
@@ -26,57 +26,64 @@
             code_type: str
                 Filter by code type.
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        params = {'name': name,
-                  'supportedEntities': supported_entities,
-                  'parentCodeId': parent_code_id,
-                  'codeType': code_type,
-                  '$top': 200
-                  }
+        params = {
+            "name": name,
+            "supportedEntities": supported_entities,
+            "parentCodeId": parent_code_id,
+            "codeType": code_type,
+            "$top": 200,
+        }
 
-        tbl = Table(self.connection.get_request('codes', params=params))
-        logger.info(f'Found {tbl.num_rows} codes.')
+        tbl = Table(self.connection.get_request("codes", params=params))
+        logger.info(f"Found {tbl.num_rows} codes.")
         return tbl
 
     def get_code(self, code_id):
         """
         Get a code.
 
         `Args:`
             code_id : int
                 The code id.
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        c = self.connection.get_request(f'codes/{code_id}')
+        c = self.connection.get_request(f"codes/{code_id}")
         logger.debug(c)
-        logger.info(f'Found code {code_id}.')
+        logger.info(f"Found code {code_id}.")
         return c
 
     def get_code_types(self):
         """
         Get code types.
 
         `Returns:`
             list
                 A list of code types.
         """
 
-        lst = self.connection.get_request('codeTypes')
-        logger.info(f'Found {len(lst)} code types.')
+        lst = self.connection.get_request("codeTypes")
+        logger.info(f"Found {len(lst)} code types.")
         return lst
 
-    def create_code(self, name=None, parent_code_id=None, description=None,
-                    code_type='SourceCode', supported_entities=None):
+    def create_code(
+        self,
+        name=None,
+        parent_code_id=None,
+        description=None,
+        code_type="SourceCode",
+        supported_entities=None,
+    ):
         """
         Create a code.
 
         `Args:`
             name: str
                 The name of the code.
             parent_code_id: int
@@ -102,33 +109,47 @@
                          'name': 'Locations',
                          'start_time': '12-31-2018T13:00:00',
                          'end_time': '12-31-2018T14:00:00'
                         }
                     ]
         """
 
-        json = {"parentCodeId": parent_code_id,
-                "name": name,
-                "codeType": code_type,
-                "description": description}
+        json = {
+            "parentCodeId": parent_code_id,
+            "name": name,
+            "codeType": code_type,
+            "description": description,
+        }
 
         if supported_entities:
 
-            se = [{'name': s['name'],
-                   'isSearchable': s['is_searchable'],
-                   'isApplicable': s['is_applicable']} for s in supported_entities]
+            se = [
+                {
+                    "name": s["name"],
+                    "isSearchable": s["is_searchable"],
+                    "isApplicable": s["is_applicable"],
+                }
+                for s in supported_entities
+            ]
 
-            json['supportedEntities'] = se
+            json["supportedEntities"] = se
 
-        r = self.connection.post_request('codes', json=json)
-        logger.info(f'Code {r} created.')
+        r = self.connection.post_request("codes", json=json)
+        logger.info(f"Code {r} created.")
         return r
 
-    def update_code(self, code_id, name=None, parent_code_id=None, description=None,
-                    code_type='SourceCode', supported_entities=None):
+    def update_code(
+        self,
+        code_id,
+        name=None,
+        parent_code_id=None,
+        description=None,
+        code_type="SourceCode",
+        supported_entities=None,
+    ):
         """
         Update a code.
 
         `Args:`
             code_id: int
                 The code id.
             name: str
@@ -159,53 +180,58 @@
                         }
                     ]
         """
 
         post_data = {}
 
         if name:
-            post_data['name'] = name
+            post_data["name"] = name
         if parent_code_id:
-            post_data['parentCodeId'] = parent_code_id
+            post_data["parentCodeId"] = parent_code_id
         if code_type:
-            post_data['codeType'] = code_type
+            post_data["codeType"] = code_type
         if description:
-            post_data['description'] = description
+            post_data["description"] = description
 
         if supported_entities:
 
-            se = [{'name': s['name'],
-                   'isSearchable': s['is_searchable'],
-                   'isApplicable': s['is_applicable']} for s in supported_entities]
-            post_data['supportedEntities'] = se
+            se = [
+                {
+                    "name": s["name"],
+                    "isSearchable": s["is_searchable"],
+                    "isApplicable": s["is_applicable"],
+                }
+                for s in supported_entities
+            ]
+            post_data["supportedEntities"] = se
 
-        r = self.connection.put_request(f'codes/{code_id}', json=post_data)
-        logger.info(f'Code {code_id} updated.')
+        r = self.connection.put_request(f"codes/{code_id}", json=post_data)
+        logger.info(f"Code {code_id} updated.")
         return r
 
     def delete_code(self, code_id):
         """
         Delete a code.
 
         `Args:`
             code_id: int
                 The code id.
         `Returns:`
             ``None``
         """
 
-        r = self.connection.delete_request(f'codes/{code_id}')
-        logger.info(f'Code {code_id} deleted.')
+        r = self.connection.delete_request(f"codes/{code_id}")
+        logger.info(f"Code {code_id} deleted.")
         return r
 
     def get_code_supported_entities(self):
         """
         Get code supported entities.
 
         `Returns:`
             list
                 A list of code supported entities.
         """
 
-        lst = self.connection.get_request('codes/supportedEntities')
-        logger.info(f'Found {len(lst)} code supported entities.')
+        lst = self.connection.get_request("codes/supportedEntities")
+        logger.info(f"Found {len(lst)} code supported entities.")
         return lst
```

### Comparing `parsons-1.0.0/parsons/ngpvan/contact_notes.py` & `parsons-1.1.0/parsons/ngpvan/contact_notes.py`

 * *Files 10% similar despite different names*

```diff
@@ -3,15 +3,14 @@
 from parsons.etl.table import Table
 import logging
 
 logger = logging.getLogger(__name__)
 
 
 class ContactNotes(object):
-
     def __init__(self, van_connection):
         self.connection = van_connection
 
     def get_contact_notes(self, van_id):
         """
         Get custom fields.
 
@@ -19,19 +18,21 @@
             van_id : str
                 VAN ID for the person to get notes for.
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        tbl = Table(self.connection.get_request(f'people/{van_id}/notes'))
-        logger.info(f'Found {tbl.num_rows} custom fields.')
+        tbl = Table(self.connection.get_request(f"people/{van_id}/notes"))
+        logger.info(f"Found {tbl.num_rows} custom fields.")
         return tbl
 
-    def create_contact_note(self, van_id, text, is_view_restricted, note_category_id=None):
+    def create_contact_note(
+        self, van_id, text, is_view_restricted, note_category_id=None
+    ):
         """
         Create a contact note
 
         `Args:`
             van_id: str
                 VAN ID for the person this note will be applied to.
             text: str
@@ -42,14 +43,14 @@
                 in the current context.
             note_category_id: int
                 Optional; if set, the note category for this note.
         `Returns:`
             int
               The note ID.
         """
-        note = {'text': text, 'isViewRestricted': is_view_restricted}
+        note = {"text": text, "isViewRestricted": is_view_restricted}
         if note_category_id is not None:
-            note['category'] = {'noteCategoryId': note_category_id}
+            note["category"] = {"noteCategoryId": note_category_id}
 
-        r = self.connection.post_request(f'people/{van_id}/notes', json=note)
-        logger.info(f'Contact note {r} created.')
+        r = self.connection.post_request(f"people/{van_id}/notes", json=note)
+        logger.info(f"Contact note {r} created.")
         return r
```

### Comparing `parsons-1.0.0/parsons/ngpvan/custom_fields.py` & `parsons-1.1.0/parsons/ngpvan/custom_fields.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,39 +1,38 @@
 from parsons.etl.table import Table
 import logging
 
 logger = logging.getLogger(__name__)
 
 
-class CustomFields():
-
+class CustomFields:
     def __init__(self, van_connection):
 
         self.connection = van_connection
 
-    def get_custom_fields(self, field_type='contacts'):
+    def get_custom_fields(self, field_type="contacts"):
         """
         Get custom fields.
 
         `Args:`
             field_type : str
                 Filter by custom field group type. Must be one of ``contacts`` or
                 ``contributions``.
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        params = {'customFieldsGroupType': field_type.capitalize()}
+        params = {"customFieldsGroupType": field_type.capitalize()}
 
-        tbl = Table(self.connection.get_request('customFields', params=params))
-        logger.info(f'Found {tbl.num_rows} custom fields.')
+        tbl = Table(self.connection.get_request("customFields", params=params))
+        logger.info(f"Found {tbl.num_rows} custom fields.")
         return tbl
 
-    def get_custom_fields_values(self, field_type='contacts'):
+    def get_custom_fields_values(self, field_type="contacts"):
         """
         Get custom field values as a long table.
 
         `Args:`
             field_type : str
                 Filter by custom field group type. Must be one of ``contacts`` or
                 ``contributions``.
@@ -42,32 +41,38 @@
                 See :ref:`parsons-table` for output options.
         """
 
         tbl = self.get_custom_fields()
 
         # Some custom fields do no have associated values. If this is the case then
         # we should return an empty Table, but with the expected columns.
-        if tbl.get_column_types('availableValues') == ['NoneType']:
-            logger.info('Found 0 custom field values.')
-            return Table([{'customFieldId': None,
-                           'id': None,
-                           'name': None,
-                           'parentValueId': None}])
+        if tbl.get_column_types("availableValues") == ["NoneType"]:
+            logger.info("Found 0 custom field values.")
+            return Table(
+                [
+                    {
+                        "customFieldId": None,
+                        "id": None,
+                        "name": None,
+                        "parentValueId": None,
+                    }
+                ]
+            )
 
         else:
-            logger.info(f'Found {tbl.num_rows} custom field values.')
-            return tbl.long_table('customFieldId', 'availableValues', prepend=False)
+            logger.info(f"Found {tbl.num_rows} custom field values.")
+            return tbl.long_table("customFieldId", "availableValues", prepend=False)
 
     def get_custom_field(self, custom_field_id):
         """
         Get a custom field.
 
         `Args:`
             custom_field_id: int
                 A valid custom field id.
         `Returns:`
             A json.
         """
 
-        r = self.connection.get_request(f'customFields/{custom_field_id}')
-        logger.info(f'Found custom field {custom_field_id}.')
+        r = self.connection.get_request(f"customFields/{custom_field_id}")
+        logger.info(f"Found custom field {custom_field_id}.")
         return r
```

### Comparing `parsons-1.0.0/parsons/ngpvan/events.py` & `parsons-1.1.0/parsons/ngpvan/events.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,26 +1,40 @@
 """NGPVAN Events Endpoints"""
 
-from parsons.etl.table import Table
 import logging
 
+from parsons.etl.table import Table
+
 logger = logging.getLogger(__name__)
 
 
 class Events(object):
-
     def __init__(self, van_connection):
 
         self.connection = van_connection
 
-    def get_events(self, code_ids=None, event_type_ids=None, rep_event_id=None,
-                   starting_after=None, starting_before=None, district_field=None,
-                   expand_fields=['locations', 'codes', 'shifts', 'roles', 'notes',
-                                  'financialProgram', 'ticketCategories',
-                                  'onlineForms']):
+    def get_events(
+        self,
+        code_ids=None,
+        event_type_ids=None,
+        rep_event_id=None,
+        starting_after=None,
+        starting_before=None,
+        district_field=None,
+        expand_fields=[
+            "locations",
+            "codes",
+            "shifts",
+            "roles",
+            "notes",
+            "financialProgram",
+            "ticketCategories",
+            "onlineForms",
+        ],
+    ):
         """
         Get events.
 
         `Args:`
             code_ids: str
                 Filter by code id.
             event_type_ids: str
@@ -40,33 +54,45 @@
                 ``onlineForms``.
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
         if expand_fields:
-            expand_fields = ','.join(expand_fields)
+            expand_fields = ",".join(expand_fields)
 
-        params = {'codeIds': code_ids,
-                  'eventTypeIds': event_type_ids,
-                  'inRepetitionWithEventId': rep_event_id,
-                  'startingAfter': starting_after,
-                  'startingBefore': starting_before,
-                  'districtFieldValue': district_field,
-                  '$top': 50,
-                  '$expand': expand_fields
-                  }
+        params = {
+            "codeIds": code_ids,
+            "eventTypeIds": event_type_ids,
+            "inRepetitionWithEventId": rep_event_id,
+            "startingAfter": starting_after,
+            "startingBefore": starting_before,
+            "districtFieldValue": district_field,
+            "$top": 50,
+            "$expand": expand_fields,
+        }
 
-        tbl = Table(self.connection.get_request('events', params=params))
-        logger.info(f'Found {tbl.num_rows} events.')
+        tbl = Table(self.connection.get_request("events", params=params))
+        logger.info(f"Found {tbl.num_rows} events.")
         return tbl
 
-    def get_event(self, event_id, expand_fields=['locations', 'codes', 'shifts', 'roles',
-                                                 'notes', 'financialProgram', 'ticketCategories',
-                                                 'voterRegistrationBatches']):
+    def get_event(
+        self,
+        event_id,
+        expand_fields=[
+            "locations",
+            "codes",
+            "shifts",
+            "roles",
+            "notes",
+            "financialProgram",
+            "ticketCategories",
+            "voterRegistrationBatches",
+        ],
+    ):
         """
         Get an event.
 
         `Args:`
             event_id: int
                 The event id.
             expand_fields: list
@@ -76,24 +102,40 @@
                 ``ticketCategories``, ``voterRegistrationBatches`.`
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
         if expand_fields:
-            expand_fields = ','.join(expand_fields)
+            expand_fields = ",".join(expand_fields)
 
-        r = self.connection.get_request(f'events/{event_id}', params={'$expand': expand_fields})
-        logger.info(f'Found event {event_id}.')
+        r = self.connection.get_request(
+            f"events/{event_id}", params={"$expand": expand_fields}
+        )
+        logger.info(f"Found event {event_id}.")
         return r
 
-    def create_event(self, name, short_name, start_date, end_date, event_type_id,
-                     roles, shifts=None, description=None, editable=False,
-                     publicly_viewable=False, location_ids=None, code_ids=None, notes=None,
-                     district_field_value=None, voter_registration_batches=None):
+    def create_event(
+        self,
+        name,
+        short_name,
+        start_date,
+        end_date,
+        event_type_id,
+        roles,
+        shifts=None,
+        description=None,
+        editable=False,
+        publicly_viewable=False,
+        location_ids=None,
+        code_ids=None,
+        notes=None,
+        district_field_value=None,
+        voter_registration_batches=None,
+    ):
         """
         Create an event
 
         `Args:`
             name: str
                 A name for this event, no longer than 500 characters.
             short_name: str
@@ -143,60 +185,68 @@
                 A list of notes
         `Returns:`
             int
               The event code.
         """
 
         if shifts is None:
-            shifts = [{'name': 'Default Shift',
-                       'startTime': start_date,
-                       'endTime': end_date}]
+            shifts = [
+                {"name": "Default Shift", "startTime": start_date, "endTime": end_date}
+            ]
         else:
-            shifts = [{'name': s['name'],
-                       'startTime': s['start_time'],
-                       'endTime': s['end_time']} for s in shifts]
-
-        event = {'name': name,
-                 'shortName': short_name,
-                 'description': description,
-                 'startDate': start_date,
-                 'endDate': end_date,
-                 'eventType': {'eventTypeId': event_type_id},
-                 'isOnlyEditableByCreatingUser': str(editable).lower(),
-                 'isPubliclyViewable': publicly_viewable,
-                 'notes': notes,
-                 'shifts': shifts,
-                 'roles': [{'roleId': r} for r in roles],
-                 'districtFieldValue': district_field_value,
-                 'voterRegistrationBatches': voter_registration_batches
-                 }
+            shifts = [
+                {
+                    "name": s["name"],
+                    "startTime": s["start_time"],
+                    "endTime": s["end_time"],
+                }
+                for s in shifts
+            ]
+
+        event = {
+            "name": name,
+            "shortName": short_name,
+            "description": description,
+            "startDate": start_date,
+            "endDate": end_date,
+            "eventType": {"eventTypeId": event_type_id},
+            "isOnlyEditableByCreatingUser": str(editable).lower(),
+            "isPubliclyViewable": publicly_viewable,
+            "notes": notes,
+            "shifts": shifts,
+            "roles": [{"roleId": r} for r in roles],
+            "districtFieldValue": district_field_value,
+            "voterRegistrationBatches": voter_registration_batches,
+        }
 
         if location_ids:
-            event['locations'] = [{'locationId': l} for l in location_ids],  # noqa E741
+            event["locations"] = (
+                [{"locationId": location_id} for location_id in location_ids],
+            )
 
         if code_ids:
-            event['codes'] = [{'codeID': c} for c in code_ids]
+            event["codes"] = [{"codeID": c} for c in code_ids]
 
-        r = self.connection.post_request('events', json=event)
-        logger.info(f'Event {r} created.')
+        r = self.connection.post_request("events", json=event)
+        logger.info(f"Event {r} created.")
         return r
 
     def delete_event(self, event_id):
         """
         Delete an event.
 
         `Args:`
             event_id: int
                 The event id.
         `Returns:`
             ``None``
         """
 
-        r = self.connection.delete_request(f'events/{event_id}')
-        logger.info(f'Event {event_id} deleted.')
+        r = self.connection.delete_request(f"events/{event_id}")
+        logger.info(f"Event {event_id} deleted.")
         return r
 
     def add_event_shift(self, event_id, shift_name, start_time, end_time):
         """
         Add shifts to an event
 
         `Args:`
@@ -209,28 +259,25 @@
             end_time: str
                 The end time of the shift (``iso8601`` formatted date).
         `Returns:`
             int
               The shift id.
         """
 
-        shift = {'name': shift_name,
-                 'startTime': start_time,
-                 'endTime': end_time
-                 }
+        shift = {"name": shift_name, "startTime": start_time, "endTime": end_time}
 
-        r = self.connection.post_request(f'events/{event_id}/shifts', json=shift)
-        logger.info(f'Shift {r} added.')
+        r = self.connection.post_request(f"events/{event_id}/shifts", json=shift)
+        logger.info(f"Shift {r} added.")
         return r
 
     def get_event_types(self):
         """
         Get event types.
 
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        tbl = Table(self.connection.get_request('events/types'))
-        logger.info(f'Found {tbl.num_rows} events.')
+        tbl = Table(self.connection.get_request("events/types"))
+        logger.info(f"Found {tbl.num_rows} events.")
         return tbl
```

### Comparing `parsons-1.0.0/parsons/ngpvan/locations.py` & `parsons-1.1.0/parsons/ngpvan/locations.py`

 * *Files 10% similar despite different names*

```diff
@@ -3,52 +3,58 @@
 from parsons.etl.table import Table
 import logging
 
 logger = logging.getLogger(__name__)
 
 
 class Locations(object):
-
     def __init__(self, van_connection):
 
         self.connection = van_connection
 
     def get_locations(self, name=None):
         """
         Get locations.
 
         `Args:`
             name: str
                 Filter locations by name.
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
-         """
+        """
 
-        tbl = Table(self.connection.get_request('locations', params={'name': name}))
-        logger.info(f'Found {tbl.num_rows} locations.')
+        tbl = Table(self.connection.get_request("locations", params={"name": name}))
+        logger.info(f"Found {tbl.num_rows} locations.")
         return self._unpack_loc(tbl)
 
     def get_location(self, location_id):
         """
         Get a location.
 
         `Args:`
             location_id: int
                 The location id.
         `Returns:`
             dict
         """
 
-        r = self.connection.get_request(f'locations/{location_id}')
-        logger.info(f'Found location {location_id}.')
+        r = self.connection.get_request(f"locations/{location_id}")
+        logger.info(f"Found location {location_id}.")
         return r
 
-    def create_location(self, name, address_line1=None, address_line2=None, city=None,
-                        state=None, zip_code=None):
+    def create_location(
+        self,
+        name,
+        address_line1=None,
+        address_line2=None,
+        city=None,
+        state=None,
+        zip_code=None,
+    ):
         """
         Find or create a location. If location already exists, will return location id.
 
         `Args:`
             name: str
                 A name for this location, no longer than 50 characters.
             address_line1: str
@@ -62,48 +68,50 @@
             zip_code: str
                 ZIP, ZIP+4, Postal Code, Post code, etc.
             `Returns:`
                 int
                     A location id.
         """
 
-        location = {'name': name,
-                    'address': {
-                        'addressLine1': address_line1,
-                        'addressLine2': address_line2,
-                        'city': city,
-                        'stateOrProvince': state,
-                        'zipOrPostalCode': zip_code
-                    }}
+        location = {
+            "name": name,
+            "address": {
+                "addressLine1": address_line1,
+                "addressLine2": address_line2,
+                "city": city,
+                "stateOrProvince": state,
+                "zipOrPostalCode": zip_code,
+            },
+        }
 
-        r = self.connection.post_request('locations/findOrCreate', json=location)
-        logger.info(f'Location {r} created.')
+        r = self.connection.post_request("locations/findOrCreate", json=location)
+        logger.info(f"Location {r} created.")
         return r
 
     def delete_location(self, location_id):
         """
         Delete a location.
 
         `Args:`
             location_id: int
                 The location id
         `Returns:`
             ``None``
         """
 
-        r = self.connection.delete_request(f'locations/{location_id}')
-        logger.info(f'Location {location_id} deleted.')
+        r = self.connection.delete_request(f"locations/{location_id}")
+        logger.info(f"Location {location_id} deleted.")
         return r
 
     def _unpack_loc(self, table):
         # Internal method to unpack location json
 
         if isinstance(table, tuple):
             return table
 
-        if 'address' in table.columns:
-            table.unpack_dict('address', prepend=False)
+        if "address" in table.columns:
+            table.unpack_dict("address", prepend=False)
 
-        if 'geoLocation' in table.columns:
-            table.unpack_dict('geoLocation', prepend=False)
+        if "geoLocation" in table.columns:
+            table.unpack_dict("geoLocation", prepend=False)
 
         return table
```

### Comparing `parsons-1.0.0/parsons/ngpvan/people.py` & `parsons-1.1.0/parsons/ngpvan/people.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,21 +1,30 @@
 from parsons.utilities import json_format
 import logging
 
 logger = logging.getLogger(__name__)
 
 
 class People(object):
-
     def __init__(self, van_connection):
 
         self.connection = van_connection
 
-    def find_person(self, first_name=None, last_name=None, date_of_birth=None, email=None,
-                    phone=None, phone_type=None, street_number=None, street_name=None, zip=None):
+    def find_person(
+        self,
+        first_name=None,
+        last_name=None,
+        date_of_birth=None,
+        email=None,
+        phone=None,
+        phone_type=None,
+        street_number=None,
+        street_name=None,
+        zip=None,
+    ):
         """
         Find a person record.
 
         .. note::
             Person find must include the following minimum combinations to conduct
             a search.
 
@@ -42,26 +51,26 @@
                 Street Name
             zip: str
                 5 digit zip code
         `Returns:`
             A person dict object
         """
 
-        logger.info(f'Finding {first_name} {last_name}.')
+        logger.info(f"Finding {first_name} {last_name}.")
 
         return self._people_search(
             first_name=first_name,
             last_name=last_name,
             date_of_birth=date_of_birth,
             email=email,
             phone=phone,
             phone_type=phone_type,
             street_number=street_number,
             street_name=street_name,
-            zip=zip
+            zip=zip,
         )
 
     def find_person_json(self, match_json):
         """
         Find a person record based on json data.
 
         .. note::
@@ -82,21 +91,32 @@
             match_json: dict
                 A dictionary of values to match against.
             fields: The fields to return. Leave as default for all available fields
         `Returns:`
             A person dict object
         """
 
-        logger.info('Finding a match for json details.')
+        logger.info("Finding a match for json details.")
 
         return self._people_search(match_json=match_json)
 
-    def update_person(self, id=None, id_type='vanid', first_name=None, last_name=None,
-                      date_of_birth=None, email=None, phone=None, phone_type=None,
-                      street_number=None, street_name=None, zip=None):
+    def update_person(
+        self,
+        id=None,
+        id_type="vanid",
+        first_name=None,
+        last_name=None,
+        date_of_birth=None,
+        email=None,
+        phone=None,
+        phone_type=None,
+        street_number=None,
+        street_name=None,
+        zip=None,
+    ):
         """
         Update a person record based on a provided ID. All other arguments provided will be
         updated on the record.
 
         .. warning::
             This method can only be run on MyMembers, EveryAction, MyCampaign databases.
 
@@ -137,18 +157,18 @@
             date_of_birth=date_of_birth,
             email=email,
             phone=phone,
             phone_type=phone_type,
             street_number=street_number,
             street_name=street_name,
             zip=zip,
-            create=True
+            create=True,
         )
 
-    def update_person_json(self, id, id_type='vanid', match_json=None):
+    def update_person_json(self, id, id_type="vanid", match_json=None):
         """
         Update a person record based on a provided ID within the match_json dict.
 
         .. note::
             A full list of possible values for the json, and its structure can be found
             `here <https://docs.ngpvan.com/reference/people#peoplevanid>`_.
 
@@ -160,18 +180,30 @@
                 Defaults to ``vanid``.
             match_json: dict
                 A dictionary of values to match against and save.
         `Returns:`
             A person dict
         """
 
-        return self._people_search(id=id, id_type=id_type, match_json=match_json, create=True)
+        return self._people_search(
+            id=id, id_type=id_type, match_json=match_json, create=True
+        )
 
-    def upsert_person(self, first_name=None, last_name=None, date_of_birth=None, email=None,
-                      phone=None, phone_type=None, street_number=None, street_name=None, zip=None):
+    def upsert_person(
+        self,
+        first_name=None,
+        last_name=None,
+        date_of_birth=None,
+        email=None,
+        phone=None,
+        phone_type=None,
+        street_number=None,
+        street_name=None,
+        zip=None,
+    ):
         """
         Create or update a person record.
 
         .. note::
             Person find must include the following minimum combinations.
 
               - first_name, last_name, email
@@ -213,15 +245,15 @@
             date_of_birth=date_of_birth,
             email=email,
             phone=phone,
             phone_type=phone_type,
             street_number=street_number,
             street_name=street_name,
             zip=zip,
-            create=True
+            create=True,
         )
 
     def upsert_person_json(self, match_json):
         """
         Create or update a person record.
 
         .. note::
@@ -246,97 +278,141 @@
                 A dictionary of values to match against and save.
         `Returns:`
             A person dict
         """
 
         return self._people_search(match_json=match_json, create=True)
 
-    def _people_search(self, id=None, id_type=None, first_name=None, last_name=None,
-                       date_of_birth=None, email=None, phone=None, phone_type='H',
-                       street_number=None, street_name=None, zip=None, match_json=None,
-                       create=False):
+    def _people_search(
+        self,
+        id=None,
+        id_type=None,
+        first_name=None,
+        last_name=None,
+        date_of_birth=None,
+        email=None,
+        phone=None,
+        phone_type="H",
+        street_number=None,
+        street_name=None,
+        zip=None,
+        match_json=None,
+        create=False,
+    ):
         # Internal method to hit the people find/create endpoints
 
         addressLine1 = None
         if street_name and street_number:
-            addressLine1 = f'{street_number} {street_name}'
+            addressLine1 = f"{street_number} {street_name}"
 
         # Check to see if a match map has been provided
         if not match_json:
             json = {"firstName": first_name, "lastName": last_name}
 
             # Will fail if empty dicts are provided, hence needed to add if exist
             if email:
-                json['emails'] = [{'email': email}]
+                json["emails"] = [{"email": email}]
             if phone:  # To Do: Strip out non-integers from phone
-                json['phones'] = [{'phoneNumber': phone, 'phoneType': phone_type}]
+                json["phones"] = [{"phoneNumber": phone, "phoneType": phone_type}]
             if date_of_birth:
-                json['dateOfBirth'] = date_of_birth
+                json["dateOfBirth"] = date_of_birth
             if zip or addressLine1:
-                json['addresses'] = [{}]
+                json["addresses"] = [{}]
                 if zip:
-                    json['addresses'][0]['zipOrPostalCode'] = zip
+                    json["addresses"][0]["zipOrPostalCode"] = zip
                 if addressLine1:
-                    json['addresses'][0]['addressLine1'] = addressLine1
+                    json["addresses"][0]["addressLine1"] = addressLine1
         else:
             json = match_json
-            if 'vanId' in match_json:
-                id = match_json['vanId']
+            if "vanId" in match_json:
+                id = match_json["vanId"]
 
-        url = 'people/'
+        url = "people/"
 
         if id:
 
             if create:
-                id_type = '' if id_type in ('vanid', None) else f"{id_type}:"
+                id_type = "" if id_type in ("vanid", None) else f"{id_type}:"
                 url += id_type + str(id)
             else:
                 return self.get_person(id, id_type=id_type)
 
         else:
-            url += 'find'
+            url += "find"
 
             if create:
-                url += 'OrCreate'
+                url += "OrCreate"
             else:
                 # Ensure that the minimum combination of fields were passed
                 json_flat = json_format.flatten_json(json)
                 self._valid_search(**json_flat)
 
         return self.connection.post_request(url, json=json)
 
-    def _valid_search(self, firstName=None, lastName=None, email=None, phoneNumber=None,
-                      dateOfBirth=None, addressLine1=None, zipOrPostalCode=None, **kwargs):
+    def _valid_search(
+        self,
+        firstName=None,
+        lastName=None,
+        email=None,
+        phoneNumber=None,
+        dateOfBirth=None,
+        addressLine1=None,
+        zipOrPostalCode=None,
+        **kwargs,
+    ):
         # Internal method to check if a search is valid, kwargs are ignored
 
-        if (None in [firstName, lastName, email] and
-            None in [firstName, lastName, phoneNumber] and
-            None in [firstName, lastName, zipOrPostalCode, dateOfBirth] and
-            None in [firstName, lastName, addressLine1, zipOrPostalCode] and
-                None in [email]):
+        if (
+            None in [firstName, lastName, email]
+            and None in [firstName, lastName, phoneNumber]
+            and None in [firstName, lastName, zipOrPostalCode, dateOfBirth]
+            and None in [firstName, lastName, addressLine1, zipOrPostalCode]
+            and None in [email]
+        ):
 
-            raise ValueError("""
+            raise ValueError(
+                """
                              Person find must include the following minimum
                              combinations to conduct a search.
                                 - first_name, last_name, email
                                 - first_name, last_name, phone
                                 - first_name, last_name, zip, dob
                                 - first_name, last_name, street_number, street_name, zip
                                 - email
-                            """)
+                            """
+            )
 
         return True
 
-    def get_person(self, id, id_type='vanid', expand_fields=[
-                   'contribution_history', 'addresses', 'phones', 'emails',
-                   'codes', 'custom_fields', 'external_ids', 'preferences',
-                   'recorded_addresses', 'reported_demographics', 'suppressions',
-                   'cases', 'custom_properties', 'districts', 'election_records',
-                   'membership_statuses', 'notes', 'organization_roles',
-                   'disclosure_field_values']):
+    def get_person(
+        self,
+        id,
+        id_type="vanid",
+        expand_fields=[
+            "contribution_history",
+            "addresses",
+            "phones",
+            "emails",
+            "codes",
+            "custom_fields",
+            "external_ids",
+            "preferences",
+            "recorded_addresses",
+            "reported_demographics",
+            "suppressions",
+            "cases",
+            "custom_properties",
+            "districts",
+            "election_records",
+            "membership_statuses",
+            "notes",
+            "organization_roles",
+            "disclosure_field_values",
+        ],
+    ):
         """
         Returns a single person record using their VANID or external id.
 
         `Args:`
             id: str
                 A valid id
             id_type: str
@@ -351,32 +427,40 @@
                 ``districts``, ``election_records``, ``membership_statuses``, ``notes``,
                 ``organization_roles``, ``scores``, ``disclosure_field_values``.
         `Returns:`
             A person dict
         """
 
         # Change end point based on id type
-        url = 'people/'
+        url = "people/"
 
-        id_type = '' if id_type in ('vanid', None) else f"{id_type}:"
+        id_type = "" if id_type in ("vanid", None) else f"{id_type}:"
         url += id_type + str(id)
 
         # Removing the fields that are not returned in MyVoters
-        NOT_IN_MYVOTERS = ['codes', 'contribution_history', 'organization_roles']
+        NOT_IN_MYVOTERS = ["codes", "contribution_history", "organization_roles"]
 
         if self.connection.db_code == 0:
             expand_fields = [v for v in expand_fields if v not in NOT_IN_MYVOTERS]
 
-        expand_fields = ','.join([json_format.arg_format(f) for f in expand_fields])
+        expand_fields = ",".join([json_format.arg_format(f) for f in expand_fields])
 
         logger.info(f'Getting person with {id_type or "vanid"} of {id} at url {url}')
-        return self.connection.get_request(url, params={'$expand': expand_fields})
+        return self.connection.get_request(url, params={"$expand": expand_fields})
 
-    def apply_canvass_result(self, id, result_code_id, id_type='vanid', contact_type_id=None,
-                             input_type_id=None, date_canvassed=None, phone=None):
+    def apply_canvass_result(
+        self,
+        id,
+        result_code_id,
+        id_type="vanid",
+        contact_type_id=None,
+        input_type_id=None,
+        date_canvassed=None,
+        phone=None,
+    ):
         """
         Apply a canvass result to a person. Use this end point for attempts that do not
         result in a survey response or an activist code (e.g. Not Home).
 
         `Args:`
             id: str
                 A valid person id
@@ -394,22 +478,37 @@
                 `Optional`; ISO 8601 formatted date. Defaults to todays date
             phone: str
                 `Optional`; Phone number of any type (Work, Cell, Home)
         `Returns:`
             ``None``
         """
 
-        logger.info(f'Applying result code {result_code_id} to {id_type} {id}.')
-        self.apply_response(id, None, id_type=id_type, contact_type_id=contact_type_id,
-                            input_type_id=input_type_id, date_canvassed=date_canvassed,
-                            result_code_id=result_code_id, phone=phone)
-
-    def toggle_volunteer_action(self, id, volunteer_activity_id, action, id_type='vanid',
-                                result_code_id=None, contact_type_id=None, input_type_id=None,
-                                date_canvassed=None):
+        logger.info(f"Applying result code {result_code_id} to {id_type} {id}.")
+        self.apply_response(
+            id,
+            None,
+            id_type=id_type,
+            contact_type_id=contact_type_id,
+            input_type_id=input_type_id,
+            date_canvassed=date_canvassed,
+            result_code_id=result_code_id,
+            phone=phone,
+        )
+
+    def toggle_volunteer_action(
+        self,
+        id,
+        volunteer_activity_id,
+        action,
+        id_type="vanid",
+        result_code_id=None,
+        contact_type_id=None,
+        input_type_id=None,
+        date_canvassed=None,
+    ):
         """
         Apply or remove a volunteer action to or from a person.
 
         `Args:`
             id: str
                 A valid person id
             id_type: str
@@ -440,17 +539,26 @@
                     "type": "VolunteerActivity"}
 
         logger.info(f'{action} volunteer activity {volunteer_activity_id} to {id_type} {id}')
         self.apply_response(id, response, id_type, contact_type_id, input_type_id, date_canvassed,
                             result_code_id)
         """
 
-    def apply_response(self, id, response, id_type='vanid', contact_type_id=None,
-                       input_type_id=None, date_canvassed=None, result_code_id=None,
-                       omit_contact=False, phone=None):
+    def apply_response(
+        self,
+        id,
+        response,
+        id_type="vanid",
+        contact_type_id=None,
+        input_type_id=None,
+        date_canvassed=None,
+        result_code_id=None,
+        omit_contact=False,
+        phone=None,
+    ):
         """
         Apply responses such as survey questions, activist codes, and volunteer actions
         to a person record. This method allows you apply multiple responses (e.g. two survey
         questions) at the same time. It is a low level method that requires that you
         conform to the VAN API `response object
         format <https://docs.ngpvan.com/reference/canvass-responses>`_.
 
@@ -493,47 +601,51 @@
                          "surveyResponseId": 465468,
                          "action": "SurveyResponse"}
                         ]
             van.apply_response(5222, response)
         """  # noqa: E501,E261
 
         # Set url based on id_type
-        if id_type == 'vanid':
+        if id_type == "vanid":
             url = f"people/{id}/canvassResponses"
         else:
             url = f"people/{id_type}:{id}/canvassResponses"
 
-        json = {"canvassContext": {
-            "contactTypeId": contact_type_id,
-            "inputTypeId": input_type_id,
-            "dateCanvassed": date_canvassed,
-            "omitActivistCodeContactHistory": omit_contact},
-            "resultCodeId": result_code_id}
+        json = {
+            "canvassContext": {
+                "contactTypeId": contact_type_id,
+                "inputTypeId": input_type_id,
+                "dateCanvassed": date_canvassed,
+                "omitActivistCodeContactHistory": omit_contact,
+            },
+            "resultCodeId": result_code_id,
+        }
 
         if contact_type_id == 1 or contact_type_id == 37:
             if phone:
-                json['canvassContext']['phone'] = {
+                json["canvassContext"]["phone"] = {
                     "dialingPrefix": "1",
-                    "phoneNumber": phone
+                    "phoneNumber": phone,
                 }
             else:
-                raise Exception('A phone number must be provided if canvassed via phone or SMS')
+                raise Exception(
+                    "A phone number must be provided if canvassed via phone or SMS"
+                )
 
         if response:
-            json['responses'] = response
+            json["responses"] = response
 
         if result_code_id is not None and response is not None:
             raise ValueError("Both result_code_id and responses cannot be specified.")
 
         if isinstance(response, dict):
             json["responses"] = [response]
 
         if result_code_id is not None and response is not None:
-            raise ValueError(
-                "Both result_code_id and responses cannot be specified.")
+            raise ValueError("Both result_code_id and responses cannot be specified.")
 
         return self.connection.post_request(url, json=json)
 
     def create_relationship(self, vanid_1, vanid_2, relationship_id):
         """
         Create a relationship between two individuals
 
@@ -544,21 +656,20 @@
                 The vanid of the secondary individual; the spoke
             relationship_id : int
                 The relationship id indicating the type of relationship
         `Returns:`
             ``None``
         """
 
-        json = {'relationshipId': relationship_id,
-                'vanId': vanid_2}
+        json = {"relationshipId": relationship_id, "vanId": vanid_2}
 
         self.connection.post_request(f"people/{vanid_1}/relationships", json=json)
-        logger.info(f'Relationship {vanid_1} to {vanid_2} created.')
+        logger.info(f"Relationship {vanid_1} to {vanid_2} created.")
 
-    def apply_person_code(self, id, code_id, id_type='vanid'):
+    def apply_person_code(self, id, code_id, id_type="vanid"):
         """
         Apply a code to a person.
 
         `Args:`
             id: str
                 A valid person id.
             code_id: int
@@ -567,23 +678,23 @@
                 A known person identifier type available on this VAN instance
                 such as ``dwid``
         `Returns:`
             ``None``
         """
 
         # Set url based on id_type
-        if id_type == 'vanid':
+        if id_type == "vanid":
             url = f"people/{id}/codes"
         else:
             url = f"people/{id_type}:{id}/codes"
 
         json = {"codeId": code_id}
 
         self.connection.post_request(url, json=json)
-        logger.info(f'Code {code_id} applied to person id {id}.')
+        logger.info(f"Code {code_id} applied to person id {id}.")
 
     def merge_contacts(self, primary_vanid, source_vanid):
         """
         Merges two contacts in EveryAction. The source contact record will be
         deleted as part of the merge and its data will be moved into the primary
         contact record. In cases where fields conflict between the two contacts
         and we can't keep both values, such as if the contacts have different
@@ -596,12 +707,12 @@
                 The VANID of the primary contact record.
             source_vanid: str
                 The VANID of the source contact record.
         `Returns:`
             The VANID of the primary contact record.
         """
 
-        url = f'people/{source_vanid}/mergeInto'
+        url = f"people/{source_vanid}/mergeInto"
         json = {"vanId": primary_vanid}
 
         r = self.connection.put_request(url, json=json)
         return r
```

### Comparing `parsons-1.0.0/parsons/ngpvan/printed_lists.py` & `parsons-1.1.0/parsons/ngpvan/printed_lists.py`

 * *Files 8% similar despite different names*

```diff
@@ -3,50 +3,60 @@
 from parsons.etl.table import Table
 import logging
 
 logger = logging.getLogger(__name__)
 
 
 class PrintedLists(object):
-
     def __init__(self, van_connection):
 
         self.connection = van_connection
 
-    def get_printed_lists(self, generated_after=None, generated_before=None, created_by=None,
-                          folder_name=None, turf_name=None):
+    def get_printed_lists(
+        self,
+        generated_after=None,
+        generated_before=None,
+        created_by=None,
+        folder_name=None,
+        turf_name=None,
+    ):
         """
         Get printed lists.
 
         `Args:`
             folder_id: int
                 Filter by the id for a VAN folder. If included returns only
                 the saved lists in the folder
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        params = {'generatedAfter': generated_after, 'generatedBefore': generated_before,
-                  'createdBy': created_by, 'folderName': folder_name, 'turfName': turf_name}
+        params = {
+            "generatedAfter": generated_after,
+            "generatedBefore": generated_before,
+            "createdBy": created_by,
+            "folderName": folder_name,
+            "turfName": turf_name,
+        }
 
         params = {key: value for key, value in params.items() if value is not None}
 
-        tbl = Table(self.connection.get_request('printedLists', params=params))
+        tbl = Table(self.connection.get_request("printedLists", params=params))
 
-        logger.info(f'Found {tbl.num_rows} printed lists.')
+        logger.info(f"Found {tbl.num_rows} printed lists.")
         return tbl
 
     def get_printed_list(self, printed_list_number):
         """
         Returns a printed list object.
 
         `Args:`
             printed_list_number: int
                 The printed list number
         `Returns:`
             dict
         """
 
-        r = self.connection.get_request(f'printedLists/{printed_list_number}')
-        logger.info(f'Found printed list {printed_list_number}.')
+        r = self.connection.get_request(f"printedLists/{printed_list_number}")
+        logger.info(f"Found printed list {printed_list_number}.")
         return r
```

### Comparing `parsons-1.0.0/parsons/ngpvan/saved_lists.py` & `parsons-1.1.0/parsons/ngpvan/saved_lists.py`

 * *Files 9% similar despite different names*

```diff
@@ -6,15 +6,14 @@
 import uuid
 from suds.client import Client
 
 logger = logging.getLogger(__name__)
 
 
 class SavedLists(object):
-
     def __init__(self, van_connection):
 
         self.connection = van_connection
 
     def get_saved_lists(self, folder_id=None):
         """
         Get saved lists.
@@ -24,31 +23,33 @@
                 Filter by the id for a VAN folder. If included returns only
                 the saved lists in the folder
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        tbl = Table(self.connection.get_request('savedLists', params={'folderId': folder_id}))
-        logger.info(f'Found {tbl.num_rows} saved lists.')
+        tbl = Table(
+            self.connection.get_request("savedLists", params={"folderId": folder_id})
+        )
+        logger.info(f"Found {tbl.num_rows} saved lists.")
         return tbl
 
     def get_saved_list(self, saved_list_id):
         """
         Returns a saved list object.
 
         `Args:`
             saved_list_id: int
                 The saved list id.
         `Returns:`
             dict
         """
 
-        r = self.connection.get_request(f'savedLists/{saved_list_id}')
-        logger.info(f'Found saved list {saved_list_id}.')
+        r = self.connection.get_request(f"savedLists/{saved_list_id}")
+        logger.info(f"Found saved list {saved_list_id}.")
         return r
 
     def download_saved_list(self, saved_list_id):
         """
         Download the vanids associated with a saved list.
 
         `Args:`
@@ -61,20 +62,32 @@
 
         ej = ExportJobs(self.connection)
         job = ej.export_job_create(saved_list_id)
 
         if isinstance(job, tuple):
             return job
         else:
-            return Table.from_csv(job['downloadUrl'])
+            return Table.from_csv(job["downloadUrl"])
 
-    def upload_saved_list_rest(self, tbl, url_type, folder_id, list_name,
-                               description, callback_url, columns, id_column,
-                               delimiter='csv', header=True, quotes=True,
-                               overwrite=None, **url_kwargs):
+    def upload_saved_list_rest(
+        self,
+        tbl,
+        url_type,
+        folder_id,
+        list_name,
+        description,
+        callback_url,
+        columns,
+        id_column,
+        delimiter="csv",
+        header=True,
+        quotes=True,
+        overwrite=None,
+        **url_kwargs,
+    ):
         """
         Upload a saved list. Invalid or unmatched person id records will be ignored. Your api user
         must be shared on the target folder.
 
         `Args:`
             tbl: parsons.Table
                 A parsons table object containing one column of person ids.
@@ -109,66 +122,87 @@
                 :ref:`Cloud Storage <cloud-storage>` for more details.
         `Returns:`
             dict
                 Upload results information included the number of matched and saved
                 records in your list.
         """
         rando = str(uuid.uuid1())
-        file_name = rando + '.csv'
-        url = cloud_storage.post_file(tbl, url_type, file_path=rando + '.zip', **url_kwargs)
-        logger.info(f'Table uploaded to {url_type}.')
+        file_name = rando + ".csv"
+        url = cloud_storage.post_file(
+            tbl, url_type, file_path=rando + ".zip", **url_kwargs
+        )
+        logger.info(f"Table uploaded to {url_type}.")
 
         # VAN errors for this method are not particularly useful or helpful. For that reason, we
         # will check that the folder exists and if the list already exists.
-        logger.info('Validating folder id and list name.')
-        if folder_id not in [x['folderId'] for x in self.get_folders()]:
+        logger.info("Validating folder id and list name.")
+        if folder_id not in [x["folderId"] for x in self.get_folders()]:
             raise ValueError("Folder does not exist or is not shared with API user.")
 
-        if list_name in [x['name'] for x in self.get_saved_lists(folder_id)] and not overwrite:
-            raise ValueError("Saved list already exists. Set overwrite "
-                             "argument to list ID or change list name.")
+        if (
+            list_name in [x["name"] for x in self.get_saved_lists(folder_id)]
+            and not overwrite
+        ):
+            raise ValueError(
+                "Saved list already exists. Set overwrite "
+                "argument to list ID or change list name."
+            )
 
-        if delimiter not in ['csv', 'tab', 'pipe']:
+        if delimiter not in ["csv", "tab", "pipe"]:
             raise ValueError("Delimiter must be one of 'csv', 'tab' or 'pipe'")
 
-        columns = [{'name': c} for c in columns]
+        columns = [{"name": c} for c in columns]
         delimiter = delimiter.capitalize()
 
-        json = {"description": description,
-                "file": {
-                    "columnDelimiter": delimiter,
-                    "columns": columns,
-                    "fileName": file_name,
-                    "hasHeader": header,
-                    "hasQuotes": quotes,
-                    "sourceUrl": url
-                },
-                "actions": [
-                    {"actionType": "LoadSavedListFile",
-                     "listDescription": description,
-                     "listName": list_name,
-                     "personIdColumn": id_column,
-                     "folderId": folder_id,
-                     "personIdType": "VANID"}],
-                "listeners": [
-                    {"type": "URL",
-                     "value": callback_url}]
+        json = {
+            "description": description,
+            "file": {
+                "columnDelimiter": delimiter,
+                "columns": columns,
+                "fileName": file_name,
+                "hasHeader": header,
+                "hasQuotes": quotes,
+                "sourceUrl": url,
+            },
+            "actions": [
+                {
+                    "actionType": "LoadSavedListFile",
+                    "listDescription": description,
+                    "listName": list_name,
+                    "personIdColumn": id_column,
+                    "folderId": folder_id,
+                    "personIdType": "VANID",
                 }
+            ],
+            "listeners": [{"type": "URL", "value": callback_url}],
+        }
 
         if overwrite:
             json["actions"][0]["overwriteExistingListId"] = overwrite
 
-        file_load_job_response = self.connection.post_request('fileLoadingJobs', json=json)
-        job_id = file_load_job_response['jobId']
-        logger.info(f'Saved list job {job_id} created. Reference '
-                    'callback url to check for job status')
+        file_load_job_response = self.connection.post_request(
+            "fileLoadingJobs", json=json
+        )
+        job_id = file_load_job_response["jobId"]
+        logger.info(
+            f"Saved list job {job_id} created. Reference "
+            "callback url to check for job status"
+        )
         return file_load_job_response
 
-    def upload_saved_list(self, tbl, list_name, folder_id, url_type, id_type='vanid', replace=False,
-                          **url_kwargs):
+    def upload_saved_list(
+        self,
+        tbl,
+        list_name,
+        folder_id,
+        url_type,
+        id_type="vanid",
+        replace=False,
+        **url_kwargs,
+    ):
         """
             .. warning::
                .. deprecated:: 0.X Use :func:`parsons.VAN.upload_saved_list_rest` instead.
 
         Upload a saved list. Invalid or unmatched person id records will be ignored. Your api user
         must be shared on the target folder.
 
@@ -193,66 +227,77 @@
         `Returns:`
             dict
                 Upload results information included the number of matched and saved
                 records in your list.
         """
         # Move to cloud storage
         file_name = str(uuid.uuid1())
-        url = cloud_storage.post_file(tbl, url_type, file_path=file_name + '.zip', **url_kwargs)
-        logger.info(f'Table uploaded to {url_type}.')
+        url = cloud_storage.post_file(
+            tbl, url_type, file_path=file_name + ".zip", **url_kwargs
+        )
+        logger.info(f"Table uploaded to {url_type}.")
 
         # VAN errors for this method are not particularly useful or helpful. For that reason, we
         # will check that the folder exists and if the list already exists.
-        logger.info('Validating folder id and list name.')
-        if folder_id not in [x['folderId'] for x in self.get_folders()]:
+        logger.info("Validating folder id and list name.")
+        if folder_id not in [x["folderId"] for x in self.get_folders()]:
             raise ValueError("Folder does not exist or is not shared with API user.")
 
         if not replace:
-            if list_name in [x['name'] for x in self.get_saved_lists(folder_id)]:
-                raise ValueError("Saved list already exists. Set to replace argument to True or "
-                                 "change list name.")
+            if list_name in [x["name"] for x in self.get_saved_lists(folder_id)]:
+                raise ValueError(
+                    "Saved list already exists. Set to replace argument to True or "
+                    "change list name."
+                )
 
         # i think we dont need this if we have the warning in the funciton description,
         # perhapse a style/standanrds decision
-        if id_type == 'vanid':
-            logger.warning('The NVPVAN SOAP API is deprecated, consider using '
-                           'parsons.VAN.upload_saved_list_rest if you are '
-                           'uploading a list of vanids.')
+        if id_type == "vanid":
+            logger.warning(
+                "The NVPVAN SOAP API is deprecated, consider using "
+                "parsons.VAN.upload_saved_list_rest if you are "
+                "uploading a list of vanids."
+            )
         # Create XML
-        xml = self.connection.soap_client.factory.create('CreateAndStoreSavedListMetaData')
+        xml = self.connection.soap_client.factory.create(
+            "CreateAndStoreSavedListMetaData"
+        )
         xml.SavedList._Name = list_name
         xml.DestinationFolder._ID = folder_id
-        xml.SourceFile.FileName = file_name + '.csv'
+        xml.SourceFile.FileName = file_name + ".csv"
         xml.SourceFile.FileUrl = url
-        xml.SourceFile.FileCompression = 'zip'
+        xml.SourceFile.FileCompression = "zip"
         xml.Options.OverwriteExistingList = replace
 
         # Describe file
-        file_desc = self.connection.soap_client.factory.create('SeparatedFileFormatDescription')
-        file_desc._name = 'csv'
+        file_desc = self.connection.soap_client.factory.create(
+            "SeparatedFileFormatDescription"
+        )
+        file_desc._name = "csv"
         file_desc.HasHeaderRow = True
 
         # Only support single column for now
-        col = self.connection.soap_client.factory.create('Column')
+        col = self.connection.soap_client.factory.create("Column")
         col.Name = id_type
-        col.RefersTo._Path = f"Person[@PersonIDType=\'{id_type}\']"
-        col._Index = '0'
+        col.RefersTo._Path = f"Person[@PersonIDType='{id_type}']"
+        col._Index = "0"
 
         # Assemble request
         file_desc.Columns.Column.append(col)
         xml.SourceFile.Format = file_desc
 
-        r = Client.dict(self.connection.soap_client.service.CreateAndStoreSavedList(xml))
+        r = Client.dict(
+            self.connection.soap_client.service.CreateAndStoreSavedList(xml)
+        )
         if r:
             logger.info(f"Uploaded {r['ListSize']} records to {r['_Name']} saved list.")
         return r
 
 
 class Folders(object):
-
     def __init__(self, van_connection):
 
         # Some sort of test if the van_connection is not present.
 
         self.connection = van_connection
 
     def get_folders(self):
@@ -260,56 +305,56 @@
         Get all folders owned or shared with the API user.
 
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        tbl = Table(self.connection.get_request('folders'))
-        logger.info(f'Found {tbl.num_rows} folders.')
+        tbl = Table(self.connection.get_request("folders"))
+        logger.info(f"Found {tbl.num_rows} folders.")
         return tbl
 
     def get_folder(self, folder_id):
         """
         Get a folder owned by or shared with the API user.
 
         `Args:`
             folder_id: int
                 The folder id.
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        r = self.connection.get_request(f'folders/{folder_id}')
-        logger.info(f'Found folder {folder_id}.')
+        r = self.connection.get_request(f"folders/{folder_id}")
+        logger.info(f"Found folder {folder_id}.")
         return r
 
 
 class ExportJobs(object):
-
     def __init__(self, van_connection):
 
         self.connection = van_connection
 
     def get_export_job_types(self):
         """
         Get export job types
 
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        tbl = Table(self.connection.get_request('exportJobTypes'))
-        logger.info(f'Found {tbl.num_rows} export job types.')
+        tbl = Table(self.connection.get_request("exportJobTypes"))
+        logger.info(f"Found {tbl.num_rows} export job types.")
         return tbl
 
-    def export_job_create(self, list_id, export_type=4,
-                          webhookUrl="https://www.nothing.com"):
+    def export_job_create(
+        self, list_id, export_type=4, webhookUrl="https://www.nothing.com"
+    ):
         """
         Creates an export job
 
         Currently, this is only used for exporting saved lists. It is
         recommended that you use the :meth:`saved_list_download` method
         instead.
 
@@ -321,31 +366,32 @@
             webhookUrl:
                 A webhook to include to notify as to the status of the export
         `Returns:`
             dict
                 The export job object
         """
 
-        json = {"savedListId": str(list_id),
-                "type": str(export_type),
-                "webhookUrl": webhookUrl
-                }
+        json = {
+            "savedListId": str(list_id),
+            "type": str(export_type),
+            "webhookUrl": webhookUrl,
+        }
 
-        r = self.connection.post_request('exportJobs', json=json)
-        logger.info('Retrieved export job.')
+        r = self.connection.post_request("exportJobs", json=json)
+        logger.info("Retrieved export job.")
         return r
 
     def get_export_job(self, export_job_id):
         """
         Get an export job.
 
         `Args:`
             export_job_id: int
                 The xxport job id.
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        r = self.connection.get_request(f'exportJobs/{export_job_id}')
-        logger.info(f'Found export job {export_job_id}.')
+        r = self.connection.get_request(f"exportJobs/{export_job_id}")
+        logger.info(f"Found export job {export_job_id}.")
         return r
```

### Comparing `parsons-1.0.0/parsons/ngpvan/scores.py` & `parsons-1.1.0/parsons/ngpvan/scores.py`

 * *Files 7% similar despite different names*

```diff
@@ -6,45 +6,44 @@
 import logging
 import petl
 
 logger = logging.getLogger(__name__)
 
 
 class Scores(object):
-
     def __init__(self, van_connection):
 
         self.connection = van_connection
 
     def get_scores(self):
         """
         Get all scores.
 
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        tbl = Table(self.connection.get_request('scores'))
-        logger.info(f'Found {tbl.num_rows} scores.')
+        tbl = Table(self.connection.get_request("scores"))
+        logger.info(f"Found {tbl.num_rows} scores.")
         return tbl
 
     def get_score(self, score_id):
         """
         Get an individual score.
 
         `Args:`
             score_id: int
                 The score id
         `Returns:`
             dict
         """
 
-        r = self.connection.get_request(f'scores/{score_id}')
-        logger.info(f'Found score {score_id}.')
+        r = self.connection.get_request(f"scores/{score_id}")
+        logger.info(f"Found score {score_id}.")
         return r
 
     def get_score_updates(self, created_before=None, created_after=None, score_id=None):
         """
         Get score updates.
 
         `Args:`
@@ -55,38 +54,40 @@
                 Filter score updates to those created after date. Use "YYYY-MM-DD"
                 format.
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        params = {'createdBefore': created_before,
-                  'createdAfter': created_after,
-                  'scoreId': score_id}
+        params = {
+            "createdBefore": created_before,
+            "createdAfter": created_after,
+            "scoreId": score_id,
+        }
 
-        tbl = Table(self.connection.get_request('scoreUpdates', params=params))
+        tbl = Table(self.connection.get_request("scoreUpdates", params=params))
         if tbl.num_rows:
-            tbl.unpack_dict('updateStatistics', prepend=False)
-            tbl.unpack_dict('score', prepend=False)
-        logger.info(f'Found {tbl.num_rows} score updates.')
+            tbl.unpack_dict("updateStatistics", prepend=False)
+            tbl.unpack_dict("score", prepend=False)
+        logger.info(f"Found {tbl.num_rows} score updates.")
         return tbl
 
     def get_score_update(self, score_update_id):
         """
         Get a score update object
 
             `Args:`
                 score_update_id : int
                         The score update id
             `Returns:`
                 dict
         """
 
-        r = self.connection.get_request(f'scoreUpdates/{score_update_id}')
-        logger.info(f'Returning score update {score_update_id}.')
+        r = self.connection.get_request(f"scoreUpdates/{score_update_id}")
+        logger.info(f"Returning score update {score_update_id}.")
         return r
 
     def update_score_status(self, score_update_id, status):
         """
         Change the status of a score update object. This end point is used to
         approve a score loading job.
 
@@ -95,34 +96,44 @@
                 The score update id
             status: str
                 One of 'pending approval', 'approved', 'disapproved'
         `Returns:`
             ``None``
         """
 
-        if status not in ['pending approval', 'approved', 'disapproved',
-                          'canceled']:
+        if status not in ["pending approval", "approved", "disapproved", "canceled"]:
 
-            raise ValueError("""Valid inputs for status are, 'pending approval',
-                             'approved','disapproved','canceled'""")
+            raise ValueError(
+                """Valid inputs for status are, 'pending approval',
+                             'approved','disapproved','canceled'"""
+            )
 
         else:
-            if status == 'pending approval':
-                status = 'PendingApproval'
+            if status == "pending approval":
+                status = "PendingApproval"
             else:
                 status = status.capitalize()
 
         json = {"loadStatus": status}
 
-        r = self.connection.patch_request(f'scoreUpdates/{score_update_id}', json=json)
-        logger.info(f'Score {score_update_id} status updated to {status}.')
+        r = self.connection.patch_request(f"scoreUpdates/{score_update_id}", json=json)
+        logger.info(f"Score {score_update_id} status updated to {status}.")
         return r
 
-    def upload_scores(self, tbl, config, url_type, id_type='vanid', email=None, auto_approve=True,
-                      approve_tolerance=.1, **url_kwargs):
+    def upload_scores(
+        self,
+        tbl,
+        config,
+        url_type,
+        id_type="vanid",
+        email=None,
+        auto_approve=True,
+        approve_tolerance=0.1,
+        **url_kwargs,
+    ):
         """
         Upload scores. Use to create or overwrite scores. Multiple score loads
         should be configured in a single call. [1]_
 
         `Args:`
             tbl: object
                 A parsons.Table object. The table must contain the scores and first column in the
@@ -167,63 +178,84 @@
 
         .. [1] NGPVAN asks that you load multiple scores in a single call to reduce the load
            on their servers.
         """
 
         # Move to cloud storage
         file_name = str(uuid.uuid1())
-        url = cloud_storage.post_file(tbl, url_type, file_path=file_name + '.zip', **url_kwargs)
-        logger.info(f'Table uploaded to {url_type}.')
+        url = cloud_storage.post_file(
+            tbl, url_type, file_path=file_name + ".zip", **url_kwargs
+        )
+        logger.info(f"Table uploaded to {url_type}.")
 
         # Generate shell request
-        json = {"description": 'A description',
-                "file": {
-                    "columnDelimiter": 'csv',
-                    "columns": [{'name': c} for c in tbl.columns],
-                    "fileName": file_name + '.csv',
-                    "hasHeader": "True",
-                    "hasQuotes": "False",
-                    "sourceUrl": url},
-                "actions": []
-                }
+        json = {
+            "description": "A description",
+            "file": {
+                "columnDelimiter": "csv",
+                "columns": [{"name": c} for c in tbl.columns],
+                "fileName": file_name + ".csv",
+                "hasHeader": "True",
+                "hasQuotes": "False",
+                "sourceUrl": url,
+            },
+            "actions": [],
+        }
 
         # Configure each score
         for i in config:
-            action = {"actionType": "score",
-                      "personIdColumn": tbl.columns[0],
-                      "personIdType": id_type,
-                      "scoreColumn": i['score_column'],
-                      "scoreId": i['score_id']}
+            action = {
+                "actionType": "score",
+                "personIdColumn": tbl.columns[0],
+                "personIdType": id_type,
+                "scoreColumn": i["score_column"],
+                "scoreId": i["score_id"],
+            }
 
             if auto_approve:
-                average = petl.stats(tbl.table, i['score_column']).mean
-                action['approvalCriteria'] = {"average": average, "tolerance": approve_tolerance}
+                average = petl.stats(tbl.table, i["score_column"]).mean
+                action["approvalCriteria"] = {
+                    "average": average,
+                    "tolerance": approve_tolerance,
+                }
 
-            json['actions'].append(action)
+            json["actions"].append(action)
 
         # Add email listener
         if email:
-            json['listeners'] = [{"type": "EMAIL", 'value': email}]
+            json["listeners"] = [{"type": "EMAIL", "value": email}]
 
         # Upload scores
-        r = self.connection.post_request('fileLoadingJobs', json=json)
+        r = self.connection.post_request("fileLoadingJobs", json=json)
         logger.info(f"Scores job {r['jobId']} created.")
-        return r['jobId']
+        return r["jobId"]
 
 
 class FileLoadingJobs(object):
-
     def __init__(self, van_connection):
 
         self.connection = van_connection
 
-    def create_file_load(self, file_name, file_url, columns, id_column, id_type,
-                         score_id, score_column, delimiter='csv', header=True, quotes=True,
-                         description=None, email=None, auto_average=None,
-                         auto_tolerance=None):
+    def create_file_load(
+        self,
+        file_name,
+        file_url,
+        columns,
+        id_column,
+        id_type,
+        score_id,
+        score_column,
+        delimiter="csv",
+        header=True,
+        quotes=True,
+        description=None,
+        email=None,
+        auto_average=None,
+        auto_tolerance=None,
+    ):
         """
         .. warning::
            .. deprecated:: 0.7 Use :func:`parsons.VAN.upload_scores` instead.
 
         Loads a file. Only used for loading scores at this time. Scores must be
         compressed using `zip`.
 
@@ -253,56 +285,71 @@
                 The tolerance must be less than 10% of the difference between the maximum and
                 minimum possible acceptable values of the score.
         `Returns:`
             dict
                 The file load id
         """
 
-        columns = [{'name': c} for c in columns]
+        columns = [{"name": c} for c in columns]
 
         # To Do: Validate that it is a .zip file. Not entirely sure if this is possible
         # as some urls might not end in ".zip".
 
-        if delimiter not in ['csv', 'tab', 'pipe']:
+        if delimiter not in ["csv", "tab", "pipe"]:
             raise ValueError("Delimiter must be one of 'csv', 'tab' or 'pipe'")
 
         delimiter = delimiter.capitalize()
 
-        json = {"description": 'A description',
-                "file": {
-                    "columnDelimiter": delimiter,
-                    "columns": columns,
-                    "fileName": file_name,
-                    "hasHeader": header,
-                    "hasQuotes": quotes,
-                    "sourceUrl": file_url
-                },
-                "actions": [
-                    {"actionType": "score",
-                     "personIdColumn": id_column,
-                     "personIdType": id_type,
-                     "scoreColumn": score_column,
-                     "scoreId": score_id}],
-                "listeners": [
-                    {"type": "EMAIL",
-                     "value": email}]
+        json = {
+            "description": "A description",
+            "file": {
+                "columnDelimiter": delimiter,
+                "columns": columns,
+                "fileName": file_name,
+                "hasHeader": header,
+                "hasQuotes": quotes,
+                "sourceUrl": file_url,
+            },
+            "actions": [
+                {
+                    "actionType": "score",
+                    "personIdColumn": id_column,
+                    "personIdType": id_type,
+                    "scoreColumn": score_column,
+                    "scoreId": score_id,
                 }
+            ],
+            "listeners": [{"type": "EMAIL", "value": email}],
+        }
 
         if auto_average and auto_tolerance:
 
-            json["actions"]["approvalCriteria"] = {"average": auto_average,
-                                                   "tolerance": auto_tolerance}
+            json["actions"]["approvalCriteria"] = {
+                "average": auto_average,
+                "tolerance": auto_tolerance,
+            }
 
-        r = self.connection.post_request('fileLoadingJobs', json=json)['jobId']
-        logger.info(f'Score loading job {r} created.')
+        r = self.connection.post_request("fileLoadingJobs", json=json)["jobId"]
+        logger.info(f"Score loading job {r} created.")
         return r
 
-    def create_file_load_multi(self, file_name, file_url, columns, id_column, id_type,
-                               score_map, delimiter='csv', header=True, quotes=True,
-                               description=None, email=None):
+    def create_file_load_multi(
+        self,
+        file_name,
+        file_url,
+        columns,
+        id_column,
+        id_type,
+        score_map,
+        delimiter="csv",
+        header=True,
+        quotes=True,
+        description=None,
+        email=None,
+    ):
         """
         .. warning::
            .. deprecated:: 0.7 Use :func:`parsons.VAN.upload_scores` instead.
 
         An iteration of the :meth:`file_load` method that allows you to load multiple scores
         at the same time.
 
@@ -330,53 +377,53 @@
 
             email: str
                 A valid email address in which file loading status will be sent.
         `Returns:`
             The file load job id
         """
 
-        columns = [{'name': c} for c in columns]
+        columns = [{"name": c} for c in columns]
 
         # To Do: Validate that it is a .zip file. Not entirely sure if this is possible
         # as some urls might not end in ".zip".
 
-        if delimiter not in ['csv', 'tab', 'pipe']:
+        if delimiter not in ["csv", "tab", "pipe"]:
             raise ValueError("Delimiter must be one of 'csv', 'tab' or 'pipe'")
 
         delimiter = delimiter.capitalize()
 
-        json = {"description": 'A description',
-                "file": {
-                    "columnDelimiter": delimiter,
-                    "columns": columns,
-                    "fileName": file_name,
-                    "hasHeader": header,
-                    "hasQuotes": quotes,
-                    "sourceUrl": file_url
-                },
-                "listeners": [
-                    {"type": "EMAIL",
-                     "value": email}]
-                }
+        json = {
+            "description": "A description",
+            "file": {
+                "columnDelimiter": delimiter,
+                "columns": columns,
+                "fileName": file_name,
+                "hasHeader": header,
+                "hasQuotes": quotes,
+                "sourceUrl": file_url,
+            },
+            "listeners": [{"type": "EMAIL", "value": email}],
+        }
 
         actions = []
 
         for score in score_map:
 
-            action = {"actionType": "score",
-                      "personIdColumn": id_column,
-                                    "personIdType": id_type,
-                                    "scoreColumn": score['score_column'],
-                                    "scoreId": score['score_id'],
-                                    "approvalCriteria": {
-                                        "average": score['auto_average'],
-                                        "tolerance": score['auto_tolerance']
-                                    }
-                      }
+            action = {
+                "actionType": "score",
+                "personIdColumn": id_column,
+                "personIdType": id_type,
+                "scoreColumn": score["score_column"],
+                "scoreId": score["score_id"],
+                "approvalCriteria": {
+                    "average": score["auto_average"],
+                    "tolerance": score["auto_tolerance"],
+                },
+            }
 
             actions.append(action)
 
-        json['actions'] = actions
+        json["actions"] = actions
 
-        r = self.connection.post_request('fileLoadingJobs', json=json)['jobId']
-        logger.info(f'Score loading job {r} created.')
+        r = self.connection.post_request("fileLoadingJobs", json=json)["jobId"]
+        logger.info(f"Score loading job {r} created.")
         return r
```

### Comparing `parsons-1.0.0/parsons/ngpvan/signups.py` & `parsons-1.1.0/parsons/ngpvan/signups.py`

 * *Files 12% similar despite different names*

```diff
@@ -2,15 +2,14 @@
 from parsons.etl.table import Table
 import logging
 
 logger = logging.getLogger(__name__)
 
 
 class Signups(object):
-
     def __init__(self, van_connection):
 
         self.connection = van_connection
 
     def get_signups_statuses(self, event_id=None, event_type_id=None):
         """
         Get a list of valid signup statuses for a given event type
@@ -24,74 +23,76 @@
                 A valid event type id.
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
         if event_id is None and event_type_id is None:
-            raise ValueError('One of event_id or event_type_id must be populated')
+            raise ValueError("One of event_id or event_type_id must be populated")
 
         if event_id is not None and event_type_id is not None:
-            raise ValueError('Event Id and Event Type ID may not BOTH be populated')
+            raise ValueError("Event Id and Event Type ID may not BOTH be populated")
 
         if event_id:
-            params = {'eventId': event_id}
+            params = {"eventId": event_id}
         if event_type_id:
-            params = {'eventTypeId': event_type_id}
+            params = {"eventTypeId": event_type_id}
 
-        tbl = Table(self.connection.get_request('signups/statuses', params=params))
-        logger.info(f'Found {tbl.num_rows} signups.')
+        tbl = Table(self.connection.get_request("signups/statuses", params=params))
+        logger.info(f"Found {tbl.num_rows} signups.")
         return tbl
 
     def get_person_signups(self, vanid):
         """
         Get the signup history of a person.
 
         `Args:`
             vanid: int
                 A valid vanid associated with a person.
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        tbl = Table(self.connection.get_request('signups', params={'vanID': vanid}))
-        logger.info(f'Found {tbl.num_rows} signups for {vanid}.')
+        tbl = Table(self.connection.get_request("signups", params={"vanID": vanid}))
+        logger.info(f"Found {tbl.num_rows} signups for {vanid}.")
         return self._unpack_signups(tbl)
 
     def get_event_signups(self, event_id):
         """
         Get the signup history of an event.
 
         `Args:`
             event_id: int
                 A valid event_id associated with an event
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        tbl = Table(self.connection.get_request('signups', params={'eventId': event_id}))
-        logger.info(f'Found {tbl.num_rows} signups for event {event_id}.')
+        tbl = Table(
+            self.connection.get_request("signups", params={"eventId": event_id})
+        )
+        logger.info(f"Found {tbl.num_rows} signups for event {event_id}.")
         return self._unpack_signups(tbl)
 
     def get_signup(self, event_signup_id):
         """
         Get a single signup object.
 
         `Args:`
             event_signup_id: int
                 A valid event_signup_id associated with a signup.
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        r = self.connection.get_request(f'signups/{event_signup_id}')
-        logger.info(f'Found sign up {event_signup_id}.')
+        r = self.connection.get_request(f"signups/{event_signup_id}")
+        logger.info(f"Found sign up {event_signup_id}.")
         return r
 
     def create_signup(self, vanid, event_id, shift_id, role_id, status_id, location_id):
         """
         Create a new signup for an event.
 
         `Args:`
@@ -108,28 +109,35 @@
             location_id:
                 A location_id for the event
         `Returns:`
             Int
                 The event signup id
         """
 
-        signup = {'person': {'vanId': vanid},
-                  'event': {'eventId': event_id},
-                  'shift': {'eventShiftId': shift_id},
-                  'role': {'roleId': role_id},
-                  'status': {'statusId': status_id},
-                  'location': {'locationId': location_id}
-                  }
+        signup = {
+            "person": {"vanId": vanid},
+            "event": {"eventId": event_id},
+            "shift": {"eventShiftId": shift_id},
+            "role": {"roleId": role_id},
+            "status": {"statusId": status_id},
+            "location": {"locationId": location_id},
+        }
 
-        r = self.connection.post_request('signups', json=signup)
-        logger.info(f'Signup {r} created.')
+        r = self.connection.post_request("signups", json=signup)
+        logger.info(f"Signup {r} created.")
         return r
 
-    def update_signup(self, event_signup_id, shift_id=None, role_id=None, status_id=None,
-                      location_id=None):
+    def update_signup(
+        self,
+        event_signup_id,
+        shift_id=None,
+        role_id=None,
+        status_id=None,
+        location_id=None,
+    ):
         """
         Update a signup object. All of the kwargs will update the values associated
         with them.
 
         `Args:`
             event_signup_id: int
                 A valid event signup id
@@ -142,47 +150,47 @@
             location_id: int
                 The location_id to update
         `Returns:`
             ``None``
         """
 
         #  Get the signup object
-        signup = self.connection.get_request(f'signups/{event_signup_id}')
+        signup = self.connection.get_request(f"signups/{event_signup_id}")
 
         # Update the signup object
         if shift_id:
-            signup['shift'] = {'eventShiftId': shift_id}
+            signup["shift"] = {"eventShiftId": shift_id}
         if role_id:
-            signup['role'] = {'roleId': role_id}
+            signup["role"] = {"roleId": role_id}
         if status_id:
-            signup['status'] = {'statusId': status_id}
+            signup["status"] = {"statusId": status_id}
         if location_id:
-            signup['location'] = {'locationId': location_id}
+            signup["location"] = {"locationId": location_id}
 
-        return self.connection.put_request(f'signups/{event_signup_id}', json=signup)
+        return self.connection.put_request(f"signups/{event_signup_id}", json=signup)
 
     def delete_signup(self, event_signup_id):
         """
         Delete a signup object
 
         `Args:`
             event_signup_id: int
                 A valid event signup id
         `Returns:`
             ``None``
         """
 
-        r = self.connection.delete_request(f'signups/{event_signup_id}')
-        logger.info(f'Signup {event_signup_id} deleted.')
+        r = self.connection.delete_request(f"signups/{event_signup_id}")
+        logger.info(f"Signup {event_signup_id} deleted.")
         return r
 
     def _unpack_signups(self, table):
 
         # Unpack all of the nested jsons
-        table.unpack_dict('person', prepend=False)
-        table.unpack_dict('status')
-        table.unpack_dict('event')
-        table.unpack_dict('shift')
-        table.unpack_dict('role')
-        table.unpack_dict('location')
+        table.unpack_dict("person", prepend=False)
+        table.unpack_dict("status")
+        table.unpack_dict("event")
+        table.unpack_dict("shift")
+        table.unpack_dict("role")
+        table.unpack_dict("location")
 
         return table
```

### Comparing `parsons-1.0.0/parsons/ngpvan/supporter_groups.py` & `parsons-1.1.0/parsons/ngpvan/supporter_groups.py`

 * *Files 17% similar despite different names*

```diff
@@ -2,45 +2,44 @@
 from parsons.etl.table import Table
 import logging
 
 logger = logging.getLogger(__name__)
 
 
 class SupporterGroups(object):
-
     def __init__(self, van_connection):
 
         self.connection = van_connection
 
     def get_supporter_groups(self):
         """
         Get supporter groups.
 
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        tbl = Table(self.connection.get_request('supporterGroups'))
-        logger.info(f'Found {tbl.num_rows} supporter groups.')
+        tbl = Table(self.connection.get_request("supporterGroups"))
+        logger.info(f"Found {tbl.num_rows} supporter groups.")
         return tbl
 
     def get_supporter_group(self, supporter_group_id):
         """
         Get a supporter group.
 
         `Args:`
             supporter_group_id: int
                 The supporter group id.
         `Returns:`
             dict
         """
 
-        r = self.connection.get_request(f'supporterGroups/{supporter_group_id}')
-        logger.info(f'Found supporter group {supporter_group_id}.')
+        r = self.connection.get_request(f"supporterGroups/{supporter_group_id}")
+        logger.info(f"Found supporter group {supporter_group_id}.")
         return r
 
     def create_supporter_group(self, name, description):
         """
         Create a new supporter group.
 
         `Args:`
@@ -49,31 +48,31 @@
             description: str
                 Optional; A description of the supporter group. 200 character limit.
         `Returns`
             Parsons Table with the newly createed supporter group id, name
             and description
         """
 
-        json = {'name': name, 'description': description}
-        r = self.connection.post_request('supporterGroups', json=json)
+        json = {"name": name, "description": description}
+        r = self.connection.post_request("supporterGroups", json=json)
         return r
 
     def delete_supporter_group(self, supporter_group_id):
         """
         Delete a supporter group.
 
         `Args:`
             supporter_group_id: int
                 The supporter group id
         `Returns:`
             ``None``
         """
 
-        r = self.connection.delete_request(f'supporterGroups/{supporter_group_id}')
-        logger.info(f'Deleted supporter group {supporter_group_id}.')
+        r = self.connection.delete_request(f"supporterGroups/{supporter_group_id}")
+        logger.info(f"Deleted supporter group {supporter_group_id}.")
         return r
 
     def add_person_supporter_group(self, supporter_group_id, vanid):
         """
         Add a person to a supporter group
 
         `Args:`
@@ -81,16 +80,18 @@
                 The supporter group id
             vanid: int
                 The vanid of the person to apply
         `Returns:`
             ``None``
         """
 
-        r = self.connection.put_request(f'supporterGroups/{supporter_group_id}/people/{vanid}')
-        logger.info(f'Added person {vanid} to {supporter_group_id} supporter group.')
+        r = self.connection.put_request(
+            f"supporterGroups/{supporter_group_id}/people/{vanid}"
+        )
+        logger.info(f"Added person {vanid} to {supporter_group_id} supporter group.")
         return r
 
     def delete_person_supporter_group(self, supporter_group_id, vanid):
         """
         Remove a person from a supporter group
 
         `Args:`
@@ -98,10 +99,14 @@
                 The supporter group id
             vanid: int
                 The vanid of the person to remove
         `Returns:`
             ``None``
         """
 
-        r = self.connection.delete_request(f'supporterGroups/{supporter_group_id}/people/{vanid}')
-        logger.info(f'Deleted person {vanid} from {supporter_group_id} supporter group.')
+        r = self.connection.delete_request(
+            f"supporterGroups/{supporter_group_id}/people/{vanid}"
+        )
+        logger.info(
+            f"Deleted person {vanid} from {supporter_group_id} supporter group."
+        )
         return r
```

### Comparing `parsons-1.0.0/parsons/ngpvan/targets.py` & `parsons-1.1.0/parsons/ngpvan/targets.py`

 * *Files 15% similar despite different names*

```diff
@@ -8,15 +8,14 @@
 
 
 class TargetsFailed(Exception):
     pass
 
 
 class Targets(object):
-
     def __init__(self, van_connection):
 
         self.connection = van_connection
 
     def obj_dict(obj):
         return obj.__dict__
 
@@ -25,64 +24,64 @@
         Get targets.
 
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        tbl = Table(self.connection.get_request('targets'))
-        logger.info(f'Found {tbl.num_rows} targets.')
+        tbl = Table(self.connection.get_request("targets"))
+        logger.info(f"Found {tbl.num_rows} targets.")
         return tbl
 
     def get_target(self, target_id):
         """
         Get a single target.
 
         `Args:`
             target_id : int
                 The target id.
         `Returns:`
             dict
                 The target
         """
 
-        r = self.connection.get_request(f'targets/{target_id}')
-        logger.info(f'Found target {target_id}.')
+        r = self.connection.get_request(f"targets/{target_id}")
+        logger.info(f"Found target {target_id}.")
         return r
 
     def get_target_export(self, export_job_id):
         """
         Get specific target export job id's status.
 
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        response = self.connection.get_request(f'targetExportJobs/{export_job_id}')
-        job_status = response.get('jobStatus')
-        if job_status == 'Complete':
-            url = response['file']['downloadUrl']
+        response = self.connection.get_request(f"targetExportJobs/{export_job_id}")
+        job_status = response.get("jobStatus")
+        if job_status == "Complete":
+            url = response["file"]["downloadUrl"]
             return Table(petl.fromcsv(url, encoding="utf-8-sig"))
-        elif job_status == 'Pending' or job_status == 'InProcess':
-            logger.info(f'Target export job is pending or in process for {export_job_id}.')
+        elif job_status == "Pending" or job_status == "InProcess":
+            logger.info(
+                f"Target export job is pending or in process for {export_job_id}."
+            )
         else:
-            raise TargetsFailed(f'Target export failed for {export_job_id}')
+            raise TargetsFailed(f"Target export failed for {export_job_id}")
 
     def create_target_export(self, target_id, webhook_url=None):
         """
         Create new target export job
 
         `Args:`
             target_id : int
                 The target id the export job is creating for.
         `Returns:`
             dict
                 The target export job ID
         """
-        target_export = {
-                        'targetId': target_id
-                        }
+        target_export = {"targetId": target_id}
 
-        r = self.connection.post_request('targetExportJobs', json=target_export)
-        logger.info(f'Created new target export job for {target_id}.')
+        r = self.connection.post_request("targetExportJobs", json=target_export)
+        logger.info(f"Created new target export job for {target_id}.")
         return r
```

### Comparing `parsons-1.0.0/parsons/ngpvan/utilities.py` & `parsons-1.1.0/parsons/ngpvan/utilities.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,26 +1,24 @@
-
-
 def action_parse(action):
     """
     Internal method to parse and validate actions, which are required for some methods
     like toggle_activist_code() and toggle_volunteer_action()
     """
 
     action = action.capitalize()
 
-    if action not in ('Apply', 'Remove'):
+    if action not in ("Apply", "Remove"):
 
         raise ValueError("Action must be either 'Apply' or 'Remove'")
 
     return action
 
 
 def list_to_string(string_arg):
     """
     Some methods arguments parsing of a list to a string.
     """
 
     if string_arg:
-        return '.'.join(string_arg)
+        return ".".join(string_arg)
     else:
         return string_arg
```

### Comparing `parsons-1.0.0/parsons/ngpvan/van.py` & `parsons-1.1.0/parsons/ngpvan/van.py`

 * *Files 13% similar despite different names*

```diff
@@ -17,17 +17,36 @@
 from parsons.ngpvan.custom_fields import CustomFields
 from parsons.ngpvan.targets import Targets
 from parsons.ngpvan.printed_lists import PrintedLists
 
 logger = logging.getLogger(__name__)
 
 
-class VAN(People, Events, SavedLists, PrintedLists, Folders, ExportJobs, ActivistCodes,
-          CanvassResponses, SurveyQuestions, Codes, Scores, FileLoadingJobs, SupporterGroups,
-          Signups, Locations, BulkImport, ChangedEntities, ContactNotes, CustomFields, Targets):
+class VAN(
+    People,
+    Events,
+    SavedLists,
+    PrintedLists,
+    Folders,
+    ExportJobs,
+    ActivistCodes,
+    CanvassResponses,
+    SurveyQuestions,
+    Codes,
+    Scores,
+    FileLoadingJobs,
+    SupporterGroups,
+    Signups,
+    Locations,
+    BulkImport,
+    ChangedEntities,
+    ContactNotes,
+    CustomFields,
+    Targets,
+):
     """
     Returns the VAN class
 
     `Args:`
         api_key : str
             A valid api key Not required if ``VAN_API_KEY`` env variable set.
         auth_name: str
@@ -38,15 +57,17 @@
             Base uri to make api calls.
         raise_for_status: boolean
             Raise excection when encountering a 4XX or 500 error.
     `Returns:`
         VAN object
     """
 
-    def __init__(self, api_key=None, auth_name='default', db=None, raise_for_status=True):
+    def __init__(
+        self, api_key=None, auth_name="default", db=None, raise_for_status=True
+    ):
 
         self.connection = VANConnector(api_key=api_key, db=db)
         self.api_key = api_key
         self.db = db
 
         # The size of each page to return. Currently set to maximum.
         self.page_size = 200
```

### Comparing `parsons-1.0.0/parsons/ngpvan/van_connector.py` & `parsons-1.1.0/parsons/ngpvan/van_connector.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,86 +1,96 @@
 from suds.client import Client
 import logging
 from parsons.utilities import check_env
 from parsons.utilities.api_connector import APIConnector
 
 logger = logging.getLogger(__name__)
 
-URI = 'https://api.securevan.com/v4/'
-SOAP_URI = 'https://api.securevan.com/Services/V3/ListService.asmx?WSDL'
+URI = "https://api.securevan.com/v4/"
+SOAP_URI = "https://api.securevan.com/Services/V3/ListService.asmx?WSDL"
 
 
 class VANConnector(object):
+    def __init__(self, api_key=None, auth_name="default", db=None):
 
-    def __init__(self, api_key=None, auth_name='default', db=None):
+        self.api_key = check_env.check("VAN_API_KEY", api_key)
 
-        self.api_key = check_env.check('VAN_API_KEY', api_key)
-
-        if db == 'MyVoters':
+        if db == "MyVoters":
             self.db_code = 0
-        elif db in ['MyMembers', 'MyCampaign', 'EveryAction']:
+        elif db in ["MyMembers", "MyCampaign", "EveryAction"]:
             self.db_code = 1
         else:
-            raise KeyError('Invalid database type specified. Pick one of:'
-                           ' MyVoters, MyCampaign, MyMembers, EveryAction.')
+            raise KeyError(
+                "Invalid database type specified. Pick one of:"
+                " MyVoters, MyCampaign, MyMembers, EveryAction."
+            )
 
         self.uri = URI
         self.db = db
         self.auth_name = auth_name
-        self.pagination_key = 'nextPageLink'
-        self.auth = (self.auth_name, self.api_key + '|' + str(self.db_code))
-        self.api = APIConnector(self.uri, auth=self.auth, data_key='items',
-                                pagination_key=self.pagination_key)
+        self.pagination_key = "nextPageLink"
+        self.auth = (self.auth_name, self.api_key + "|" + str(self.db_code))
+        self.api = APIConnector(
+            self.uri,
+            auth=self.auth,
+            data_key="items",
+            pagination_key=self.pagination_key,
+        )
 
         # We will not create the SOAP client unless we need to as this triggers checking for
         # valid credentials. As not all API keys are provisioned for SOAP, this keeps it from
         # raising a permission exception when creating the class.
         self._soap_client = None
 
     @property
     def api_key_profile(self):
         """
         Returns the API key profile with includes permissions and other metadata.
         """
 
-        return self.get_request('apiKeyProfiles')[0]
+        return self.get_request("apiKeyProfiles")[0]
 
     @property
     def soap_client(self):
 
         if not self._soap_client:
 
             # Create the SOAP client
-            soap_auth = {'Header': {'DatabaseMode': self.soap_client_db(), 'APIKey': self.api_key}}
+            soap_auth = {
+                "Header": {
+                    "DatabaseMode": self.soap_client_db(),
+                    "APIKey": self.api_key,
+                }
+            }
             self._soap_client = Client(SOAP_URI, soapheaders=soap_auth)
 
         return self._soap_client
 
     def soap_client_db(self):
         """
         Parse the REST database name to the accepted SOAP format
         """
 
-        if self.db == 'MyVoters':
-            return 'MyVoterFile'
-        if self.db == 'EveryAction':
-            return 'MyCampaign'
+        if self.db == "MyVoters":
+            return "MyVoterFile"
+        if self.db == "EveryAction":
+            return "MyCampaign"
         else:
             return self.db
 
     def get_request(self, endpoint, **kwargs):
 
         r = self.api.get_request(self.uri + endpoint, **kwargs)
         data = self.api.data_parse(r)
 
         # Paginate
         while isinstance(r, dict) and self.api.next_page_check_url(r):
-            if endpoint == 'savedLists' and not r['items']:
+            if endpoint == "savedLists" and not r["items"]:
                 break
-            if endpoint == 'printedLists' and not r['items']:
+            if endpoint == "printedLists" and not r["items"]:
                 break
             r = self.api.get_request(r[self.pagination_key], **kwargs)
             data.extend(self.api.data_parse(r))
         return data
 
     def post_request(self, endpoint, **kwargs):
```

### Comparing `parsons-1.0.0/parsons/notifications/gmail.py` & `parsons-1.1.0/parsons/notifications/gmail.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 import base64
 from apiclient import errors
 from googleapiclient.discovery import build
 from httplib2 import Http
 from oauth2client import file, client, tools
 from parsons.notifications.sendmail import SendMail
 
-SCOPES = 'https://www.googleapis.com/auth/gmail.send'
+SCOPES = "https://www.googleapis.com/auth/gmail.send"
 
 
 class Gmail(SendMail):
     """Create a Gmail object, for sending emails.
 
     `Args:`
         creds_path: str
@@ -17,15 +17,15 @@
         token_path: str
             The path to the token.json file.
         user_id: str
             Optional; Sender email address. Defaults to the special value
             "me" which is used to indicate the authenticated user.
     """
 
-    def __init__(self, creds_path=None, token_path=None, user_id='me'):
+    def __init__(self, creds_path=None, token_path=None, user_id="me"):
 
         self.user_id = user_id
 
         if not creds_path:
             raise ValueError("Invalid path to credentials.json.")
 
         if not token_path:
@@ -40,21 +40,21 @@
         if not self.creds or self.creds.invalid:
             flow = client.flow_from_clientsecrets(creds_path, SCOPES)
             self.creds = tools.run_flow(flow, self.store)
 
             # BUG-1
             # self.creds = self.run_flow(flow, self.store, http=http)
 
-        self.service = build('gmail', 'v1', http=self.creds.authorize(Http()))
+        self.service = build("gmail", "v1", http=self.creds.authorize(Http()))
 
         # BUG-1
         # self.service = build('gmail', 'v1', http=self.creds.authorize(http))
 
     def _encode_raw_message(self, message):
-        return {'raw': base64.urlsafe_b64encode(message.as_bytes()).decode()}
+        return {"raw": base64.urlsafe_b64encode(message.as_bytes()).decode()}
 
     def _send_message(self, msg):
         """Send an email message.
 
         `Args:`
             message: dict
                 Message to be sent as a base64url encode object.
@@ -67,19 +67,21 @@
         self.log.info("Sending a message...")
 
         message = self._encode_raw_message(msg)
 
         self.log.debug(message)
 
         try:
-            message = (self.service.users().messages()
-                       .send(userId=self.user_id, body=message).execute())
+            message = (
+                self.service.users()
+                .messages()
+                .send(userId=self.user_id, body=message)
+                .execute()
+            )
         except errors.HttpError:
-            self.log.exception(
-                'An error occurred: while attempting to send a message.')
+            self.log.exception("An error occurred: while attempting to send a message.")
             raise
         else:
             self.log.debug(message)
-            self.log.info(
-                f"Message sent succesfully (Message Id: {message['id']})")
+            self.log.info(f"Message sent succesfully (Message Id: {message['id']})")
 
             return message
```

### Comparing `parsons-1.0.0/parsons/notifications/sendmail.py` & `parsons-1.1.0/parsons/notifications/sendmail.py`

 * *Files 4% similar despite different names*

```diff
@@ -64,22 +64,21 @@
                 The text of the email message.
         `Returns:`
             An object passable to send_message to send
         """
         self.log.info("Creating a simple message...")
 
         message = MIMEText(message_text)
-        message['to'] = to
-        message['from'] = sender
-        message['subject'] = subject
+        message["to"] = to
+        message["from"] = sender
+        message["subject"] = subject
 
         return message
 
-    def _create_message_html(self, sender, to, subject, message_text,
-                             message_html):
+    def _create_message_html(self, sender, to, subject, message_text, message_html):
         """Create an html message for an email.
 
         `Args:`
             sender: str
                 Email address of the sender.
             to: str
                 Email address(es) of the recipient(s).
@@ -90,26 +89,27 @@
             message_html: str
                 The html formatted text of the email message.
         `Returns:`
             An object passable to send_message to send
         """
         self.log.info("Creating an html message...")
 
-        message = MIMEMultipart('alternative')
-        message['subject'] = subject
-        message['from'] = sender
-        message['to'] = to
+        message = MIMEMultipart("alternative")
+        message["subject"] = subject
+        message["from"] = sender
+        message["to"] = to
         if message_text:
-            message.attach(MIMEText(message_text, 'plain'))
-        message.attach(MIMEText(message_html, 'html'))
+            message.attach(MIMEText(message_text, "plain"))
+        message.attach(MIMEText(message_html, "html"))
 
         return message
 
-    def _create_message_attachments(self, sender, to, subject, message_text,
-                                    files, message_html=None):
+    def _create_message_attachments(
+        self, sender, to, subject, message_text, files, message_html=None
+    ):
         """Create a message for an email that includes an attachment.
 
         `Args:`
             sender: str
                 Email address of the sender.
             to: str
                 Email address of the receiver.
@@ -122,75 +122,74 @@
             message_html: str
                 Optional; The html formatted text of the email message.
         `Returns:`
             An object passable to send_message to send
         """
         self.log.info("Creating a message with attachments...")
 
-        message = MIMEMultipart('alternative')
-        message['to'] = to
-        message['from'] = sender
-        message['subject'] = subject
+        message = MIMEMultipart("alternative")
+        message["to"] = to
+        message["from"] = sender
+        message["subject"] = subject
 
-        msg = MIMEText(message_text, 'plain')
+        msg = MIMEText(message_text, "plain")
         message.attach(msg)
 
         if message_html:
-            html = MIMEText(message_html, 'html')
+            html = MIMEText(message_html, "html")
             message.attach(html)
 
         for f in files:
-            filename = getattr(f, 'name', 'file')
-            file_bytes = b''
+            filename = getattr(f, "name", "file")
+            file_bytes = b""
 
             if isinstance(f, io.StringIO):
                 file_bytes = f.getvalue().encode()
             elif isinstance(f, io.BytesIO):
                 file_bytes = f.getvalue()
             else:
                 filename = os.path.basename(f)
-                fp = open(f, 'rb')
+                fp = open(f, "rb")
                 file_bytes = fp.read()
                 fp.close()
 
             content_type, encoding = mimetypes.guess_type(filename)
             self.log.debug(
-                f"(File: {f}, Content-type: {content_type}, "
-                f"Encoding: {encoding})")
+                f"(File: {f}, Content-type: {content_type}, " f"Encoding: {encoding})"
+            )
 
             if content_type is None or encoding is not None:
-                content_type = 'application/octet-stream'
+                content_type = "application/octet-stream"
 
-            main_type, sub_type = content_type.split('/', 1)
+            main_type, sub_type = content_type.split("/", 1)
 
-            if main_type == 'text':
+            if main_type == "text":
                 self.log.info("Added a text file.")
-                msg = MIMEText(file_bytes, _subtype=sub_type, _charset='utf-8')
+                msg = MIMEText(file_bytes, _subtype=sub_type, _charset="utf-8")
 
-            elif main_type == 'image':
+            elif main_type == "image":
                 self.log.info("Added an image file.")
                 msg = MIMEImage(file_bytes, _subtype=sub_type)
-                msg.add_header('Content-ID', f'<{filename}>')
+                msg.add_header("Content-ID", f"<{filename}>")
 
-            elif main_type == 'audio':
+            elif main_type == "audio":
                 self.log.info("Added an audio file.")
                 msg = MIMEAudio(file_bytes, _subtype=sub_type)
 
-            elif main_type == 'application':
+            elif main_type == "application":
                 self.log.info("Added an application file.")
                 msg = MIMEApplication(file_bytes, _subtype=sub_type)
 
             else:
                 self.log.info("Added an unknown-type file.")
                 msg = MIMEBase(main_type, sub_type)
                 msg.set_payload(file_bytes)
                 encode_base64(msg)
 
-            msg.add_header(
-                'Content-Disposition', 'attachment', filename=filename)
+            msg.add_header("Content-Disposition", "attachment", filename=filename)
             message.attach(msg)
 
         return message
 
     def _validate_email_string(self, str):
         self.log.debug(f"Validating email {str}...")
         realname, email_addr = parseaddr(str)
@@ -199,16 +198,17 @@
             raise ValueError("Invalid email address.")
 
         if not validate_email(email_addr):
             raise ValueError("Invalid email address.")
 
         return True
 
-    def send_email(self, sender, to, subject, message_text, message_html=None,
-                   files=None):
+    def send_email(
+        self, sender, to, subject, message_text, message_html=None, files=None
+    ):
         """Send an email message.
 
         `Args:`
             sender: str
                 Email address of the sender.
             to: str or list
                 Email address(es) of the receiver(s). Must be in correct email
@@ -233,38 +233,41 @@
         if isinstance(to, list):
             if len(to) == 0:
                 raise EmptyListError("Must contain at least 1 email.")
 
             for e in to:
                 self._validate_email_string(e)
 
-            to = ', '.join(to)
+            to = ", ".join(to)
 
         elif isinstance(to, str):
             self._validate_email_string(to)
 
         if not message_html and not files:
-            msg_type = 'simple'
+            msg_type = "simple"
             msg = self._create_message_simple(sender, to, subject, message_text)
 
         elif not files:
-            msg_type = 'html'
+            msg_type = "html"
             msg = self._create_message_html(
-                sender, to, subject, message_text, message_html)
+                sender, to, subject, message_text, message_html
+            )
         else:
-            msg_type = 'attachments'
+            msg_type = "attachments"
             if isinstance(files, str):
                 files = [files]
 
             msg = self._create_message_attachments(
-                sender, to, subject, message_text, files, message_html)
+                sender, to, subject, message_text, files, message_html
+            )
 
         self.log.info(f"Sending a(n) {msg_type} email...")
 
         self._send_message(msg)
 
         self.log.info("Email sent succesfully.")
 
 
 class EmptyListError(IndexError):
     """Throw when a list is empty that should contain at least 1 element."""
+
     pass
```

### Comparing `parsons-1.0.0/parsons/notifications/slack.py` & `parsons-1.1.0/parsons/notifications/slack.py`

 * *Files 8% similar despite different names*

```diff
@@ -7,34 +7,36 @@
 from slackclient import SlackClient
 from slackclient.exceptions import SlackClientError
 
 import requests
 
 
 class Slack(object):
-
     def __init__(self, api_key=None):
 
         if api_key is None:
 
             try:
                 self.api_key = os.environ["SLACK_API_TOKEN"]
 
             except KeyError:
-                raise KeyError('Missing api_key. It must be passed as an '
-                               'argument or stored as environmental variable')
+                raise KeyError(
+                    "Missing api_key. It must be passed as an "
+                    "argument or stored as environmental variable"
+                )
 
         else:
 
             self.api_key = api_key
 
         self.client = SlackClient(self.api_key)
 
-    def channels(self, fields=['id', 'name'], exclude_archived=False,
-                 types=['public_channel']):
+    def channels(
+        self, fields=["id", "name"], exclude_archived=False, types=["public_channel"]
+    ):
         """
         Return a list of all channels in a Slack team.
 
         `Args:`
             fields: list
                 A list of the fields to return. By default, only the channel
                 `id` and `name` are returned. See
@@ -48,29 +50,42 @@
                 combination of `public_channel`, `private_channel`,
                 `mpim` (aka group messages), or `im` (aka 1-1 messages).
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
         tbl = self._paginate_request(
-            "conversations.list", "channels", types=types,
-            exclude_archived=exclude_archived)
-
-        tbl.unpack_dict("topic", include_original=False, prepend=True,
-                        prepend_value="topic")
-        tbl.unpack_dict("purpose", include_original=False,
-                        prepend=True, prepend_value="purpose")
+            "conversations.list",
+            "channels",
+            types=types,
+            exclude_archived=exclude_archived,
+        )
+
+        tbl.unpack_dict(
+            "topic", include_original=False, prepend=True, prepend_value="topic"
+        )
+        tbl.unpack_dict(
+            "purpose", include_original=False, prepend=True, prepend_value="purpose"
+        )
 
         rm_cols = [x for x in tbl.columns if x not in fields]
         tbl.remove_column(*rm_cols)
 
         return tbl
 
-    def users(self, fields=['id', 'name', 'deleted', 'profile_real_name_normalized',
-                            'profile_email']):
+    def users(
+        self,
+        fields=[
+            "id",
+            "name",
+            "deleted",
+            "profile_real_name_normalized",
+            "profile_email",
+        ],
+    ):
         """
         Return a list of all users in a Slack team.
 
         `Args:`
             fields: list
                 A list of the fields to return. By default, only the user
                 `id` and `name` and `deleted` status are returned. See
@@ -79,16 +94,17 @@
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
         tbl = self._paginate_request("users.list", "members", include_locale=True)
 
-        tbl.unpack_dict("profile", include_original=False, prepend=True,
-                        prepend_value="profile")
+        tbl.unpack_dict(
+            "profile", include_original=False, prepend=True, prepend_value="profile"
+        )
 
         rm_cols = [x for x in tbl.columns if x not in fields]
         tbl.remove_column(*rm_cols)
 
         return tbl
 
     @classmethod
@@ -104,18 +120,18 @@
                 Text of the message to send.
             webhook: str
                 If you have a webhook url instead of an api_key
                 Looks like: https://hooks.slack.com/services/Txxxxxxx/Bxxxxxx/Dxxxxxxx
             parent_message_id: str
                 The `ts` value of the parent message. If used, this will thread the message.
         """
-        webhook = check('SLACK_API_WEBHOOK', webhook, optional=True)
-        payload = {'channel': channel, 'text': text}
+        webhook = check("SLACK_API_WEBHOOK", webhook, optional=True)
+        payload = {"channel": channel, "text": text}
         if parent_message_id:
-            payload['thread_ts'] = parent_message_id
+            payload["thread_ts"] = parent_message_id
         return requests.post(webhook, json=payload)
 
     def message_channel(self, channel, text, as_user=False, parent_message_id=None):
         """
         Send a message to a Slack channel
 
         `Args:`
@@ -132,32 +148,43 @@
             parent_message_id: str
                 The `ts` value of the parent message. If used, this will thread the message.
         `Returns:`
             `dict`:
                 A response json
         """
         resp = self.client.api_call(
-            "chat.postMessage", channel=channel, text=text,
-            as_user=as_user, thread_ts=parent_message_id)
+            "chat.postMessage",
+            channel=channel,
+            text=text,
+            as_user=as_user,
+            thread_ts=parent_message_id,
+        )
 
-        if not resp['ok']:
+        if not resp["ok"]:
 
-            if resp['error'] == 'ratelimited':
-                time.sleep(int(resp['headers']['Retry-After']))
+            if resp["error"] == "ratelimited":
+                time.sleep(int(resp["headers"]["Retry-After"]))
 
                 resp = self.client.api_call(
-                    "chat.postMessage",
-                    channel=channel, text=text, as_user=as_user)
+                    "chat.postMessage", channel=channel, text=text, as_user=as_user
+                )
 
-            raise SlackClientError(resp['error'])
+            raise SlackClientError(resp["error"])
 
         return resp
 
-    def upload_file(self, channels, filename, filetype=None,
-                    initial_comment=None, title=None, is_binary=False):
+    def upload_file(
+        self,
+        channels,
+        filename,
+        filetype=None,
+        initial_comment=None,
+        title=None,
+        is_binary=False,
+    ):
         """
         Upload a file to Slack channel(s).
 
         `Args:`
             channels: list
                 The list of channel names or IDs where the file will be shared.
             filename: str
@@ -175,57 +202,64 @@
             is_binary: bool
                 If True, open this file in binary mode. This is needed if
                 uploading binary files. Defaults to False.
         `Returns:`
             `dict`:
                 A response json
         """
-        if filetype is None and '.' in filename:
-            filetype = filename.split('.')[-1]
+        if filetype is None and "." in filename:
+            filetype = filename.split(".")[-1]
 
-        mode = 'rb' if is_binary else 'r'
+        mode = "rb" if is_binary else "r"
         with open(filename, mode) as file_content:
             resp = self.client.api_call(
-                "files.upload", channels=channels, file=file_content,
-                filetype=filetype, initial_comment=initial_comment,
-                title=title)
+                "files.upload",
+                channels=channels,
+                file=file_content,
+                filetype=filetype,
+                initial_comment=initial_comment,
+                title=title,
+            )
 
-            if not resp['ok']:
+            if not resp["ok"]:
 
-                if resp['error'] == 'ratelimited':
-                    time.sleep(int(resp['headers']['Retry-After']))
+                if resp["error"] == "ratelimited":
+                    time.sleep(int(resp["headers"]["Retry-After"]))
 
                     resp = self.client.api_call(
-                        "files.upload", channels=channels, file=file_content,
-                        filetype=filetype, initial_comment=initial_comment,
-                        title=title)
+                        "files.upload",
+                        channels=channels,
+                        file=file_content,
+                        filetype=filetype,
+                        initial_comment=initial_comment,
+                        title=title,
+                    )
 
-                raise SlackClientError(resp['error'])
+                raise SlackClientError(resp["error"])
 
         return resp
 
     def _paginate_request(self, endpoint, collection, **kwargs):
         # The max object we're requesting at a time.
         # This is an nternal limit to not overload slack api
         LIMIT = 200
 
         items = []
         next_page = True
         cursor = None
         while next_page:
-            resp = self.client.api_call(
-                endpoint, cursor=cursor, limit=LIMIT, **kwargs)
+            resp = self.client.api_call(endpoint, cursor=cursor, limit=LIMIT, **kwargs)
 
-            if not resp['ok']:
+            if not resp["ok"]:
 
-                if resp['error'] == 'ratelimited':
-                    time.sleep(int(resp['headers']['Retry-After']))
+                if resp["error"] == "ratelimited":
+                    time.sleep(int(resp["headers"]["Retry-After"]))
                     continue
 
-                raise SlackClientError(resp['error'])
+                raise SlackClientError(resp["error"])
 
             items.extend(resp[collection])
 
             if resp["response_metadata"]["next_cursor"]:
                 cursor = resp["response_metadata"]["next_cursor"]
             else:
                 next_page = False
```

### Comparing `parsons-1.0.0/parsons/notifications/smtp.py` & `parsons-1.1.0/parsons/notifications/smtp.py`

 * *Files 9% similar despite different names*

```diff
@@ -17,21 +17,31 @@
         password: str
             The password of the SMTP server login
         tls: bool
             Defaults to True -- pass "0" or "False" to SMTP_TLS to disable
         close_manually: bool
             When set to True, send_message will not close the connection
     """
-    def __init__(self, host=None, port=None, username=None, password=None, tls=None,
-                 close_manually=False):
-        self.host = check('SMTP_HOST', host)
-        self.port = check('SMTP_PORT', port, optional=True) or 587
-        self.username = check('SMTP_USER', username)
-        self.password = check('SMTP_PASSWORD', password)
-        self.tls = not (check('SMTP_TLS', tls, optional=True) in ('false', 'False', '0', False))
+
+    def __init__(
+        self,
+        host=None,
+        port=None,
+        username=None,
+        password=None,
+        tls=None,
+        close_manually=False,
+    ):
+        self.host = check("SMTP_HOST", host)
+        self.port = check("SMTP_PORT", port, optional=True) or 587
+        self.username = check("SMTP_USER", username)
+        self.password = check("SMTP_PASSWORD", password)
+        self.tls = not (
+            check("SMTP_TLS", tls, optional=True) in ("false", "False", "0", False)
+        )
         self.close_manually = close_manually
 
         self.conn = None
 
     def get_connection(self):
         if self.conn is None:
             self.conn = smtplib.SMTP(self.host, self.port)
@@ -49,21 +59,24 @@
                 i.e. the objects created by the create_* instance methods
         `Returns:`
             dict of refused To addresses (otherwise None)
         """
         self.log.info("Sending a message...")
         try:
             conn = self.get_connection()
-            result = conn.sendmail(message['From'],
-                                   [x.strip() for x in message['To'].split(',')],
-                                   message.as_string())
+            result = conn.sendmail(
+                message["From"],
+                [x.strip() for x in message["To"].split(",")],
+                message.as_string(),
+            )
         except Exception:
-            self.log.exception(
-                'An error occurred: while attempting to send a message.')
+            self.log.exception("An error occurred: while attempting to send a message.")
             raise
 
         if result:
-            self.log.warning("Message failed to send to some recipients: " + str(result))
+            self.log.warning(
+                "Message failed to send to some recipients: " + str(result)
+            )
         if not self.close_manually:
             conn.quit()
             self.conn = None
         return result
```

### Comparing `parsons-1.0.0/parsons/pdi/acquisition_types.py` & `parsons-1.1.0/parsons/pdi/acquisition_types.py`

 * *Files 9% similar despite different names*

```diff
@@ -15,16 +15,21 @@
 
         `Returns:`
             parsons.Table
                 A Parsons table of all the data.
         """
         return self._request(self.url_acqtypes, limit=limit)
 
-    def create_acquisition_type(self, acquisition_type: str, acquisition_description: str,
-                                acquisition_method: str, page_default: str = None):
+    def create_acquisition_type(
+        self,
+        acquisition_type: str,
+        acquisition_description: str,
+        acquisition_method: str,
+        page_default: str = None,
+    ):
         """
         Create a new Acquisition Type
         `Args:`
             acquisition_type (string): The acquisition type
             acquisition_description (string): The acquisition description
             acquisition_method (string): The acquisition method
             Options are:
@@ -48,17 +53,17 @@
                 "Import" (Imports)
             }
         """
         payload = {
             "acquisitionType": acquisition_type,
             "acquisitionDescription": acquisition_description,
             "acquisitionMethod": acquisition_method,
-            "pageDefault": page_default
+            "pageDefault": page_default,
         }
-        return self._request(self.url_acqtypes, req_type='POST', post_data=payload)
+        return self._request(self.url_acqtypes, req_type="POST", post_data=payload)
 
     def get_acquisition_type(self, id: str):
         """
         Get a Acquisition Type by id.
         `Args:`
             id: str
                 The Acquisition Type id
@@ -73,16 +78,22 @@
         Delete a Acquisition Type by id.
         `Args:`
             id: str
                 The Acquisition Type id
         """
         return self._request(f"{self.url_acqtypes}/{id}", req_type="DELETE")
 
-    def update_acquisition_type(self, id: str, acquisition_type: str, acquisition_description: str,
-                                acquisition_method: str, page_default: str = None):
+    def update_acquisition_type(
+        self,
+        id: str,
+        acquisition_type: str,
+        acquisition_description: str,
+        acquisition_method: str,
+        page_default: str = None,
+    ):
         """
         Update Acquisition Type
         `Args:`
             acquisition_type (string): The acquisition type
             acquisition_description (string): The acquisition description
             acquisition_method (string): The acquisition method
             Options are:
@@ -106,10 +117,12 @@
                 "Import" (Imports)
             }
         """
         payload = {
             "acquisitionType": acquisition_type,
             "acquisitionDescription": acquisition_description,
             "acquisitionMethod": acquisition_method,
-            "pageDefault": page_default
+            "pageDefault": page_default,
         }
-        return self._request(f"{self.url_acqtypes}/{id}", req_type='PUT', post_data=payload)
+        return self._request(
+            f"{self.url_acqtypes}/{id}", req_type="PUT", post_data=payload
+        )
```

### Comparing `parsons-1.0.0/parsons/pdi/activities.py` & `parsons-1.1.0/parsons/pdi/activities.py`

 * *Files 7% similar despite different names*

```diff
@@ -21,19 +21,16 @@
     def create_activity(self, activity_name: str, canvassing_shift: bool):
         """
         Create a New Activity
         `Args:`
             activity_name str: The activity name
             canvassing_shift bool: The canvassing shift
         """
-        payload = {
-            "activityName": activity_name,
-            "canvassingShift": canvassing_shift
-        }
-        return self._request(self.url_activites, req_type='POST', post_data=payload)
+        payload = {"activityName": activity_name, "canvassingShift": canvassing_shift}
+        return self._request(self.url_activites, req_type="POST", post_data=payload)
 
     def get_activity(self, id: str):
         """
         Get a Activity by id.
         `Args:`
             id: str
                 The Activity id
@@ -47,12 +44,11 @@
         """
         Update an Activity
         `Args:`
             id: Activity id
             activity_name str: The activity name
             canvassing_shift bool: The canvassing shift
         """
-        payload = {
-            "activityName": activity_name,
-            "canvassingShift": canvassing_shift
-        }
-        return self._request(f"{self.url_activites}/{id}", req_type='PUT', post_data=payload)
+        payload = {"activityName": activity_name, "canvassingShift": canvassing_shift}
+        return self._request(
+            f"{self.url_activites}/{id}", req_type="PUT", post_data=payload
+        )
```

### Comparing `parsons-1.0.0/parsons/pdi/contacts.py` & `parsons-1.1.0/parsons/pdi/contacts.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,17 +1,24 @@
 class Contacts:
     """A class to access the contacts PDI API endpoint."""
 
     def __init__(self):
         self.url_contacts = self.base_url + "/contacts"
         super().__init__()
 
-    def get_contacts(self, email: str = None, phone: str = None, first_name: str = None,
-                     last_name: str = None, zip_code: str = None, search_by_email: bool = False,
-                     limit: int = None):
+    def get_contacts(
+        self,
+        email: str = None,
+        phone: str = None,
+        first_name: str = None,
+        last_name: str = None,
+        zip_code: str = None,
+        search_by_email: bool = False,
+        limit: int = None,
+    ):
         """
         Get a list of Contacts.
         `Args:`
             email: str, email address
             phone: str, phone number
             first_name: str, first name
             last_name: str, last name
@@ -25,22 +32,35 @@
         """
         params = {
             "email": email,
             "phone": phone,
             "firstName": first_name,
             "lastName": last_name,
             "zipCode": zip_code,
-            "searchByEmail": search_by_email
+            "searchByEmail": search_by_email,
         }
         return self._request(self.url_contacts, args=params, limit=limit)
 
-    def create_contact(self, name_prefix="", first_name="", last_name="", middle_name="",
-                       name_suffix="", nickname="", occupation="", employer="", volunteer_status="",
-                       donor_status="", member_status="", date_of_birth=None, gender=None,
-                       pdi_id=None):
+    def create_contact(
+        self,
+        name_prefix="",
+        first_name="",
+        last_name="",
+        middle_name="",
+        name_suffix="",
+        nickname="",
+        occupation="",
+        employer="",
+        volunteer_status="",
+        donor_status="",
+        member_status="",
+        date_of_birth=None,
+        gender=None,
+        pdi_id=None,
+    ):
         """
         Create new contact
         `Args:`
             pdiId (string, optional): The pdi identifier. pdiId field is ignored when updating. ,
             namePrefix (string): The name prefix.
             firstName (string): The first name.
             middleName (string): The middle name.
@@ -74,17 +94,17 @@
             "occupation": occupation,
             "employer": employer,
             "volunteerStatus": volunteer_status,
             "donorStatus": donor_status,
             "memberStatus": member_status,
             "dateOfBirth": date_of_birth,
             "gender": gender,
-            "pdiId": pdi_id
+            "pdiId": pdi_id,
         }
-        return self._request(self.url_contacts, req_type='POST', post_data=payload)
+        return self._request(self.url_contacts, req_type="POST", post_data=payload)
 
     def get_contact(self, id: str):
         """
         Get a Contact by id.
 
         `Args:`
             id: str
@@ -92,17 +112,31 @@
         `Returns:`
             parsons.Table
                 A Parsons table of all the data.
         """
         # todo not working quite right
         return self._request(f"{self.url_contacts}/{id}")
 
-    def update_contact(self, id, first_name, last_name, name_prefix="", middle_name="",
-                       name_suffix="", nickname="", occupation="", employer="", volunteer_status="",
-                       donor_status="", member_status="", date_of_birth=None, gender="U"):
+    def update_contact(
+        self,
+        id,
+        first_name,
+        last_name,
+        name_prefix="",
+        middle_name="",
+        name_suffix="",
+        nickname="",
+        occupation="",
+        employer="",
+        volunteer_status="",
+        donor_status="",
+        member_status="",
+        date_of_birth=None,
+        gender="U",
+    ):
         """
         Update Contact
         `Args:`
             namePrefix (string): The name prefix.
             firstName (string): The first name.
             middleName (string): The middle name.
             lastName (string): The last name.
@@ -134,22 +168,30 @@
             "nickname": nickname,
             "occupation": occupation,
             "employer": employer,
             "volunteerStatus": volunteer_status,
             "donorStatus": donor_status,
             "memberStatus": member_status,
             "dateOfBirth": date_of_birth,
-            "gender": gender
+            "gender": gender,
         }
-        res = self._request(f"{self.url_contacts}/{id}", req_type='PUT', post_data=payload)
+        res = self._request(
+            f"{self.url_contacts}/{id}", req_type="PUT", post_data=payload
+        )
         if res["code"] == 201:
             return True
 
-    def add_phone(self, contact_id: int, phone_number: str, phone_type='Mobile', primary=True,
-                  extension=None):
+    def add_phone(
+        self,
+        contact_id: int,
+        phone_number: str,
+        phone_type="Mobile",
+        primary=True,
+        extension=None,
+    ):
         """Add a phone number to a contact
         `Args:`
             contact_id: int
                 Unique ID of the contact you'd like to apply the phone_number to
             phone_number: str
             phone_type: str
                 Options are `Home`, `Work`, `Direct`, `Mobile`, `Fax`, and `Other. Defaults to
@@ -159,24 +201,27 @@
             extension: str
         `Returns:`
             dict
                 Response from PDI
         """
 
         payload = {
-            'phoneNumber': phone_number,
-            'phoneType': phone_type,
-            'isPrimary': primary
+            "phoneNumber": phone_number,
+            "phoneType": phone_type,
+            "isPrimary": primary,
         }
 
         if extension:
-            payload['extension'] = extension
+            payload["extension"] = extension
 
-        response = self._request(self.url_contacts + f'/{str(contact_id)}/phones', req_type='POST',
-                                 post_data=payload)
+        response = self._request(
+            self.url_contacts + f"/{str(contact_id)}/phones",
+            req_type="POST",
+            post_data=payload,
+        )
 
         return response
 
     def add_email(self, contact_id: int, email: str, primary=True):
         """Add an email address to a contact
         `Args:`
             contact_id: int
@@ -185,21 +230,21 @@
             primary: bool
                 True indicates that this email address is the contact's primary email
         `Returns:`
             dict
                 Response from PDI
         """
 
-        payload = {
-            'emailAddress': email,
-            'isPrimary': primary
-        }
+        payload = {"emailAddress": email, "isPrimary": primary}
 
-        response = self._request(self.url_contacts + f'/{str(contact_id)}/emails', req_type='POST',
-                                 post_data=payload)
+        response = self._request(
+            self.url_contacts + f"/{str(contact_id)}/emails",
+            req_type="POST",
+            post_data=payload,
+        )
 
         return response
 
     def delete_contact(self, id: str):
         """
         Delete a Question by id.
         `Args:`
```

### Comparing `parsons-1.0.0/parsons/pdi/events.py` & `parsons-1.1.0/parsons/pdi/events.py`

 * *Files 6% similar despite different names*

```diff
@@ -3,19 +3,19 @@
 logger = logging.getLogger(__name__)
 
 
 class Events:
     """A class for interacting with PDI events via PDIs API"""
 
     def __init__(self):
-        self.events_url = self.base_url + '/events'
-        self.calendars_url = self.base_url + '/calendars'
-        self.eventactivities_url = self.base_url + '/eventActivities'
-        self.activites_url = self.base_url + '/activities'
-        self.activityassignment_url = self.base_url + '/eventActivityAssignements'
+        self.events_url = self.base_url + "/events"
+        self.calendars_url = self.base_url + "/calendars"
+        self.eventactivities_url = self.base_url + "/eventActivities"
+        self.activites_url = self.base_url + "/activities"
+        self.activityassignment_url = self.base_url + "/eventActivityAssignements"
 
         super().__init__()
 
     def get_events(self, first_event_date: str, last_event_date: str, limit=None):
         """Get a table of PDI events in a given time frame
 
         `Args:`
@@ -28,16 +28,16 @@
 
         `Returns:`
             parsons.Table
                 A Parsons table containing all requested events data.
         """
 
         params = {
-            'startDate': first_event_date,
-            'endDate': last_event_date,
+            "startDate": first_event_date,
+            "endDate": last_event_date,
         }
 
         return self._request(self.events_url, args=params, limit=limit)
 
     def get_event_invitations(self, event_id: str, expand=True, limit=None):
         """Get a table of PDI event invitations for a specified event
 
@@ -48,21 +48,35 @@
                 If True returns columns for contact (and all contact info) and event)
 
         `Returns:`
             parsons.Table
                 A Parsons table containing all requested event invitation data.
         """
 
-        params = {'expand': expand}
+        params = {"expand": expand}
 
-        return self._request(f'{self.events_url}/{event_id}/invitations', args=params, limit=limit)
-
-    def create_event(self, calendar_id: str, location_id: str, event_name: str, start_datetime: str,
-                     end_datetime: str, description=None, all_day=False, recurrencetype=None,
-                     recurrence_end_datetime=None, host_phone=None, host_email=None, website=None):
+        return self._request(
+            f"{self.events_url}/{event_id}/invitations", args=params, limit=limit
+        )
+
+    def create_event(
+        self,
+        calendar_id: str,
+        location_id: str,
+        event_name: str,
+        start_datetime: str,
+        end_datetime: str,
+        description=None,
+        all_day=False,
+        recurrencetype=None,
+        recurrence_end_datetime=None,
+        host_phone=None,
+        host_email=None,
+        website=None,
+    ):
         """Create event in a specified calendar
 
         `Args:`
             calendar_id: str
                 The calendar in which you'd like to create an event
             location_id: str
                 The unique ID of the PDI location this event took place/is to take place at
@@ -89,326 +103,406 @@
             website: str
                 An optional website for the event. Defualts to None
 
         `Returns:`
             dict
                 Response from PDI in dictionary object
 
-            """
+        """
 
         payload = {
-          "locationId": location_id,
-          "recurrenceType": recurrencetype,
-          "name": event_name,
-          "description": description,
-          "startDateTimeUtc": start_datetime,
-          "endDateTimeUtc": end_datetime,
-          "isAllDay": str(all_day).lower(),
-          "recurrenceEndDateTimeUtc": recurrence_end_datetime,
-          "phone": host_phone,
-          "contactEmail": host_email,
-          "website": website
+            "locationId": location_id,
+            "recurrenceType": recurrencetype,
+            "name": event_name,
+            "description": description,
+            "startDateTimeUtc": start_datetime,
+            "endDateTimeUtc": end_datetime,
+            "isAllDay": str(all_day).lower(),
+            "recurrenceEndDateTimeUtc": recurrence_end_datetime,
+            "phone": host_phone,
+            "contactEmail": host_email,
+            "website": website,
         }
 
-        response = self._request(self.calendars_url + f'/{calendar_id}' + '/events',
-                                 req_type='POST', post_data=payload)
-        event_id = response['id']
-        logger.info(f'Created event {event_name} (id: {event_id})')
+        response = self._request(
+            self.calendars_url + f"/{calendar_id}" + "/events",
+            req_type="POST",
+            post_data=payload,
+        )
+        event_id = response["id"]
+        logger.info(f"Created event {event_name} (id: {event_id})")
 
         return response
 
-    def create_event_with_activity(self, calendar_id: str, location_id: str, activity_id: str,
-                                   event_name: str, activity_name: str, start_datetime: str,
-                                   end_datetime: str, description=None, all_day=False,
-                                   recurrencetype=None, recurrence_end_datetime=None,
-                                   host_phone=None, host_email=None, website=None,
-                                   signup_goal=None):
+    def create_event_with_activity(
+        self,
+        calendar_id: str,
+        location_id: str,
+        activity_id: str,
+        event_name: str,
+        activity_name: str,
+        start_datetime: str,
+        end_datetime: str,
+        description=None,
+        all_day=False,
+        recurrencetype=None,
+        recurrence_end_datetime=None,
+        host_phone=None,
+        host_email=None,
+        website=None,
+        signup_goal=None,
+    ):
         """Create event in a specified calendar with an associated activity. The activty will
-            be assigned the same start, end time, and recurrance settings as the event.
+        be assigned the same start, end time, and recurrance settings as the event.
 
-                `Args:`
-                    calendar_id: str
-                        The unique ID of the calendar in which you'd like to create an event
-                    location_id: str
-                        The unique ID of the PDI location whek this event took place/is to take
-                        place
-                    activity_id:
-                        The unique ID of the activity type you'd like to add to the event
-                    event_name: str
-                        The name of your event
-                    activity_name: str
-                        The name of your activity. e.g. 'Pictionary!'
-                    description: str
-                        A short description for your event
-                    start_datetime: str
-                        The start datetime of the event in UTC timezone formatted as
-                        yyyy-MM-ddThh:mm:ss.fffZ
-                    end_datetime: str
-                        The end date formatted like start_datetime
-                    is_all_day = bool
-                        set to True if event is an all day event. Defaults to False
-                    recurrencetype: str
-                        Either 'daily', 'weekly', or 'monthly'. Defaults to None
-                    recurrence_end_datetime: str
-                        The end time of the last recurrence of the event formatted as
-                        yyyy-MM-ddThh:mm:ss.fffZ
-                    host_phone: str
-                        An optional contact phone number for the host. Defaults to None
-                    host_email: str
-                        An optional contact email for the host. Defaults to None
-                    website: str
-                        An optional website for the event. Defualts to None
-                    signup_goal: int
-                        The goal of how many people you want to complete the activity
-                `Returns:`
-                    dict
-                        Response from PDI in dictionary object
-                    """
-        event_data = self.create_event(calendar_id, location_id, event_name, start_datetime,
-                                       end_datetime, description, all_day, recurrencetype,
-                                       recurrence_end_datetime, host_phone, host_email, website)
-        event_id = event_data['id']
-        logger.info(f'Created event {event_name} (id: {event_id})')
+            `Args:`
+                calendar_id: str
+                    The unique ID of the calendar in which you'd like to create an event
+                location_id: str
+                    The unique ID of the PDI location whek this event took place/is to take
+                    place
+                activity_id:
+                    The unique ID of the activity type you'd like to add to the event
+                event_name: str
+                    The name of your event
+                activity_name: str
+                    The name of your activity. e.g. 'Pictionary!'
+                description: str
+                    A short description for your event
+                start_datetime: str
+                    The start datetime of the event in UTC timezone formatted as
+                    yyyy-MM-ddThh:mm:ss.fffZ
+                end_datetime: str
+                    The end date formatted like start_datetime
+                is_all_day = bool
+                    set to True if event is an all day event. Defaults to False
+                recurrencetype: str
+                    Either 'daily', 'weekly', or 'monthly'. Defaults to None
+                recurrence_end_datetime: str
+                    The end time of the last recurrence of the event formatted as
+                    yyyy-MM-ddThh:mm:ss.fffZ
+                host_phone: str
+                    An optional contact phone number for the host. Defaults to None
+                host_email: str
+                    An optional contact email for the host. Defaults to None
+                website: str
+                    An optional website for the event. Defualts to None
+                signup_goal: int
+                    The goal of how many people you want to complete the activity
+            `Returns:`
+                dict
+                    Response from PDI in dictionary object
+        """
+        event_data = self.create_event(
+            calendar_id,
+            location_id,
+            event_name,
+            start_datetime,
+            end_datetime,
+            description,
+            all_day,
+            recurrencetype,
+            recurrence_end_datetime,
+            host_phone,
+            host_email,
+            website,
+        )
+        event_id = event_data["id"]
+        logger.info(f"Created event {event_name} (id: {event_id})")
 
         event_activity_payload = {
             "CalendarId": calendar_id,
             "EventId": event_id,
             "ActivityId": activity_id,
             "LocationId": location_id,
             "RecurrenceType": recurrencetype,
             "Name": activity_name,
             "Description": None,
             "StartDateTimeUtc": start_datetime,
             "EndDateTimeUtc": end_datetime,
             "CountGoal": signup_goal,
-            "RecurrenceEndDateTimeUtc": recurrence_end_datetime
+            "RecurrenceEndDateTimeUtc": recurrence_end_datetime,
         }
 
-        response = self._request(self.eventactivities_url, req_type='POST',
-                                 post_data=event_activity_payload)
-        logger.info(f'Created activity {activity_name} for event {event_name} (id: {event_id})')
+        response = self._request(
+            self.eventactivities_url, req_type="POST", post_data=event_activity_payload
+        )
+        logger.info(
+            f"Created activity {activity_name} for event {event_name} (id: {event_id})"
+        )
 
         return response
 
-    def create_event_activity(self, calendar_id: str, event_id: str, activity_id: str,
-                              location_id: str, activity_name: str, start_datetime: str,
-                              end_datetime: str, description=None, recurrencetype=None,
-                              recurrence_end_datetime=None, signup_goal=None):
+    def create_event_activity(
+        self,
+        calendar_id: str,
+        event_id: str,
+        activity_id: str,
+        location_id: str,
+        activity_name: str,
+        start_datetime: str,
+        end_datetime: str,
+        description=None,
+        recurrencetype=None,
+        recurrence_end_datetime=None,
+        signup_goal=None,
+    ):
         """Create event in a specified calendar with an associated activity
 
-                `Args:`
-                    calendar_id: str
-                        The unique ID of the calendar in which you'd like to create an event
-                    event_id: str
-                        The unique ID of the event this activity is to be associated with
-                    activity_id:
-                        The unique ID of the activity type you'd like to add to the event
-                    location_id: str
-                        The unique ID of the PDI location where this event took place/is to take
-                        place
-                    activity_name: str
-                        The name of your activity. e.g. 'Pictionary!'
-                    description: str
-                        A short description for your event activity
-                    start_datetime: str
-                        The start datetime of the event in UTC timezone formatted as
-                        yyyy-MM-ddThh:mm:ss.fffZ
-                    end_datetime: str
-                        The end date formatted like start_datetime
-                    recurrencetype: str
-                        Either 'daily', 'weekly', or 'monthly'. Defaults to None
-                    recurrence_end_datetime: str
-                        The end time of the last recurrence of the event formatted as
-                        yyyy-MM-ddThh:mm:ss.fffZ
-                    signup_goal: int
-                        The goal of how many people you want to complete the activity
-
-
-                `Returns:`
-                    dict
-                        Response from PDI in dictionary object
-                    """
+        `Args:`
+            calendar_id: str
+                The unique ID of the calendar in which you'd like to create an event
+            event_id: str
+                The unique ID of the event this activity is to be associated with
+            activity_id:
+                The unique ID of the activity type you'd like to add to the event
+            location_id: str
+                The unique ID of the PDI location where this event took place/is to take
+                place
+            activity_name: str
+                The name of your activity. e.g. 'Pictionary!'
+            description: str
+                A short description for your event activity
+            start_datetime: str
+                The start datetime of the event in UTC timezone formatted as
+                yyyy-MM-ddThh:mm:ss.fffZ
+            end_datetime: str
+                The end date formatted like start_datetime
+            recurrencetype: str
+                Either 'daily', 'weekly', or 'monthly'. Defaults to None
+            recurrence_end_datetime: str
+                The end time of the last recurrence of the event formatted as
+                yyyy-MM-ddThh:mm:ss.fffZ
+            signup_goal: int
+                The goal of how many people you want to complete the activity
+
+
+        `Returns:`
+            dict
+                Response from PDI in dictionary object
+        """
 
         event_activity_payload = {
             "CalendarId": calendar_id,
             "EventId": event_id,
             "ActivityId": activity_id,
             "LocationId": location_id,
             "RecurrenceType": recurrencetype,
             "Name": activity_name,
             "Description": description,
             "StartDateTimeUtc": start_datetime,
             "EndDateTimeUtc": end_datetime,
             "CountGoal": signup_goal,
-            "RecurrenceEndDateTimeUtc": recurrence_end_datetime
+            "RecurrenceEndDateTimeUtc": recurrence_end_datetime,
         }
 
-        response = self._request(self.eventactivities_url, req_type='POST',
-                                 post_data=event_activity_payload)
-        logger.info(f'Created activity {activity_name} for event {event_id})')
+        response = self._request(
+            self.eventactivities_url, req_type="POST", post_data=event_activity_payload
+        )
+        logger.info(f"Created activity {activity_name} for event {event_id})")
 
         return response
 
-    def create_invitation(self, event_id: str, contact_id: str, status: str, attended: bool,
-                          confirmed=False, specific_occurrence_start=None):
+    def create_invitation(
+        self,
+        event_id: str,
+        contact_id: str,
+        status: str,
+        attended: bool,
+        confirmed=False,
+        specific_occurrence_start=None,
+    ):
         """Create a PDI event invitation indicating a contact has been registered for an event
-            `Args:`
-                event_id: str
-                    The ID of the event to write the RSVP to
-                contact_id: str
-                    The ID of the contact to which the invitation belongs
-                status: str
-                    Options are: "Yes", "No", "Maybe", "Scheduled", "Invited", "Cancelled",
-                    "No-Show", "Completed", and ""
-                attended: boolean
-                    Indicates whether contact attended event
-                confirmed: boolean
-                    Indicates whether invitation confirmed they will attend the event. Defaults to
-                    False
-                specific_occurrence_start: str
-                    If invitation is for a specific occurrence of a recurring event, then the start
-                    datetime of the event in UTC formatted as yyyy-MM-ddTHH:mm:ss.fffZ
-            `Returns:`
-                dict
-                    Response from PDI in dictionary object
+        `Args:`
+            event_id: str
+                The ID of the event to write the RSVP to
+            contact_id: str
+                The ID of the contact to which the invitation belongs
+            status: str
+                Options are: "Yes", "No", "Maybe", "Scheduled", "Invited", "Cancelled",
+                "No-Show", "Completed", and ""
+            attended: boolean
+                Indicates whether contact attended event
+            confirmed: boolean
+                Indicates whether invitation confirmed they will attend the event. Defaults to
+                False
+            specific_occurrence_start: str
+                If invitation is for a specific occurrence of a recurring event, then the start
+                datetime of the event in UTC formatted as yyyy-MM-ddTHH:mm:ss.fffZ
+        `Returns:`
+            dict
+                Response from PDI in dictionary object
         """
 
         event_invitation_payload = {
             "contactId": contact_id,
             "rsvpStatus": status,
             "isConfirmed": confirmed,
-            "attended": attended
+            "attended": attended,
         }
 
         if specific_occurrence_start:
-            event_invitation_payload["specificOcurrenceStartUtc"] = specific_occurrence_start
-
-        response = self._request(self.events_url + f'/{event_id}/invitations',
-                                 req_type='POST', post_data=event_invitation_payload)
+            event_invitation_payload[
+                "specificOcurrenceStartUtc"
+            ] = specific_occurrence_start
+
+        response = self._request(
+            self.events_url + f"/{event_id}/invitations",
+            req_type="POST",
+            post_data=event_invitation_payload,
+        )
         return response
 
-    def update_invitation(self, invitation_id: str, event_id: str, contact_id: str, status=None,
-                          attended=None, confirmed=None, specific_occurrence_start=None):
+    def update_invitation(
+        self,
+        invitation_id: str,
+        event_id: str,
+        contact_id: str,
+        status=None,
+        attended=None,
+        confirmed=None,
+        specific_occurrence_start=None,
+    ):
         """Modify a PDI event invitation
-            `Args:`
-                invitation_id: str
-                    The ID of the event invitation
-                event_id: str
-                    The ID of the event that corresponds to the invitation
-                contact_id: str
-                    The ID of the contact to which the invitation belongs
-                status: str
-                    Options are: "Yes", "No", "Maybe", "Scheduled", "Invited", "Cancelled",
-                    "No-Show", "Completed", and ""
-                attended: boolean
-                    Indicates whether contact attended event
-                confirmed: boolean
-                    Indicates whether invitation confirmed they will attend the event
-                specific_occurrence_start: str
-                    If invitation is for a specific occurrence of a recurring event, then the start
-                    datetime of the event in UTC formatted as yyyy-MM-ddTHH:mm:ss.fffZ
-            `Returns:`
-                dict
-                    Response from PDI in dictionary object
+        `Args:`
+            invitation_id: str
+                The ID of the event invitation
+            event_id: str
+                The ID of the event that corresponds to the invitation
+            contact_id: str
+                The ID of the contact to which the invitation belongs
+            status: str
+                Options are: "Yes", "No", "Maybe", "Scheduled", "Invited", "Cancelled",
+                "No-Show", "Completed", and ""
+            attended: boolean
+                Indicates whether contact attended event
+            confirmed: boolean
+                Indicates whether invitation confirmed they will attend the event
+            specific_occurrence_start: str
+                If invitation is for a specific occurrence of a recurring event, then the start
+                datetime of the event in UTC formatted as yyyy-MM-ddTHH:mm:ss.fffZ
+        `Returns:`
+            dict
+                Response from PDI in dictionary object
         """
 
-        event_invitation_payload = {
-            "contactId": contact_id
-        }
+        event_invitation_payload = {"contactId": contact_id}
 
         if status:
-            event_invitation_payload['rsvpStatus'] = status
+            event_invitation_payload["rsvpStatus"] = status
         if confirmed is not None:
-            event_invitation_payload['isConfirmed'] = confirmed
+            event_invitation_payload["isConfirmed"] = confirmed
         if attended is not None:
-            event_invitation_payload['attended'] = attended
+            event_invitation_payload["attended"] = attended
         if specific_occurrence_start:
-            event_invitation_payload["specificOcurrenceStartUtc"] = specific_occurrence_start
-
-        response = self._request(self.events_url + f'/{event_id}/invitations/{invitation_id}',
-                                 req_type='PUT', post_data=event_invitation_payload)
+            event_invitation_payload[
+                "specificOcurrenceStartUtc"
+            ] = specific_occurrence_start
+
+        response = self._request(
+            self.events_url + f"/{event_id}/invitations/{invitation_id}",
+            req_type="PUT",
+            post_data=event_invitation_payload,
+        )
         return response
 
-    def create_activity_assignment(self, eventactivityid: str, contact_id: str, status: str,
-                                   completed: bool, confirmed=False,
-                                   specific_occurrence_start=None):
+    def create_activity_assignment(
+        self,
+        eventactivityid: str,
+        contact_id: str,
+        status: str,
+        completed: bool,
+        confirmed=False,
+        specific_occurrence_start=None,
+    ):
         """Create an activity assignement
-            `Args:`
-                eventactivityid: str
-                    The ID of the specific event activity you'd like to assign a contact
-                contact_id: str
-                    The ID of the contact to which the assignment belongs
-                status: str
-                    Options are: "Yes", "No", "Maybe", "Scheduled", "Invited", "Cancelled",
-                    "No-Show", "Completed", and ""
-                completed: boolean
-                    Indicates whether contact attended event
-                confirmed: boolean
-                    Indicates whether invitation confirmed they will attend the event
-                specific_occurrence_start: str
-                    If invitation is for a specific occurrence of a recurring event, then the start
-                    datetime of the event in UTC formatted as yyyy-MM-ddTHH:mm:ss.fffZ
-            `Returns:`
-                dict
-                    Response from PDI in dictionary object
+        `Args:`
+            eventactivityid: str
+                The ID of the specific event activity you'd like to assign a contact
+            contact_id: str
+                The ID of the contact to which the assignment belongs
+            status: str
+                Options are: "Yes", "No", "Maybe", "Scheduled", "Invited", "Cancelled",
+                "No-Show", "Completed", and ""
+            completed: boolean
+                Indicates whether contact attended event
+            confirmed: boolean
+                Indicates whether invitation confirmed they will attend the event
+            specific_occurrence_start: str
+                If invitation is for a specific occurrence of a recurring event, then the start
+                datetime of the event in UTC formatted as yyyy-MM-ddTHH:mm:ss.fffZ
+        `Returns:`
+            dict
+                Response from PDI in dictionary object
         """
 
         assignment_payload = {
             "rsvpStatus": status,
             "isConfirmed": confirmed,
             "isShiftWorked": completed,
             "contactId": contact_id,
-            "eventActivityId": eventactivityid
+            "eventActivityId": eventactivityid,
         }
 
         if specific_occurrence_start:
             assignment_payload["specificOcurrenceStartUtc"] = specific_occurrence_start
 
-        response = self._request(self.activityassignment_url, req_type='POST',
-                                 post_data=assignment_payload)
+        response = self._request(
+            self.activityassignment_url, req_type="POST", post_data=assignment_payload
+        )
 
         return response
 
-    def update_activity_assignment(self, activityassignementid: str, eventactivityid: str,
-                                   contact_id: str, status=None, completed=None, confirmed=None,
-                                   specific_occurrence_start=None):
+    def update_activity_assignment(
+        self,
+        activityassignementid: str,
+        eventactivityid: str,
+        contact_id: str,
+        status=None,
+        completed=None,
+        confirmed=None,
+        specific_occurrence_start=None,
+    ):
         """Create an activity assignement
-            `Args:`
-                activityassignementid: str
-                    Id of the specific event activity assignement you want to modify
-                eventactivityid: str
-                    The ID of the specific event activity you'd like to assign a contact
-                contact_id: str
-                    The ID of the contact to which the assignment belongs
-                status: str
-                    Options are: "Yes", "No", "Maybe", "Scheduled", "Invited", "Cancelled",
-                    "No-Show", "Completed", and ""
-                completed: boolean
-                    Indicates whether contact attended event
-                confirmed: boolean
-                    Indicates whether invitation confirmed they will attend the event
-                specific_occurrence_start: str
-                    If invitation is for a specific occurrence of a recurring event, then the start
-                    datetime of the event in UTC formatted as yyyy-MM-ddTHH:mm:ss.fffZ
-            `Returns:`
-                dict
-                    Response from PDI in dictionary object
+        `Args:`
+            activityassignementid: str
+                Id of the specific event activity assignement you want to modify
+            eventactivityid: str
+                The ID of the specific event activity you'd like to assign a contact
+            contact_id: str
+                The ID of the contact to which the assignment belongs
+            status: str
+                Options are: "Yes", "No", "Maybe", "Scheduled", "Invited", "Cancelled",
+                "No-Show", "Completed", and ""
+            completed: boolean
+                Indicates whether contact attended event
+            confirmed: boolean
+                Indicates whether invitation confirmed they will attend the event
+            specific_occurrence_start: str
+                If invitation is for a specific occurrence of a recurring event, then the start
+                datetime of the event in UTC formatted as yyyy-MM-ddTHH:mm:ss.fffZ
+        `Returns:`
+            dict
+                Response from PDI in dictionary object
         """
 
         assignment_payload = {
             "contactId": contact_id,
-            "eventActivityId": eventactivityid
+            "eventActivityId": eventactivityid,
         }
 
         if status:
-            assignment_payload['rsvpStatus'] = status
+            assignment_payload["rsvpStatus"] = status
         if confirmed is not None:
-            assignment_payload['isConfirmed'] = confirmed
+            assignment_payload["isConfirmed"] = confirmed
         if completed is not None:
-            assignment_payload['isShiftWorked'] = completed
+            assignment_payload["isShiftWorked"] = completed
         if specific_occurrence_start:
             assignment_payload["specificOcurrenceStartUtc"] = specific_occurrence_start
 
-        response = self._request(self.activityassignment_url + f'/{activityassignementid}',
-                                 req_type='PUT', post_data=assignment_payload)
+        response = self._request(
+            self.activityassignment_url + f"/{activityassignementid}",
+            req_type="PUT",
+            post_data=assignment_payload,
+        )
 
         return response
```

### Comparing `parsons-1.0.0/parsons/pdi/flag_ids.py` & `parsons-1.1.0/parsons/pdi/flag_ids.py`

 * *Files 11% similar despite different names*

```diff
@@ -31,16 +31,15 @@
 
         `Returns:`
             dict
                 FlagID object.
         """
         return self._request(f"{self.url_flag_ids}/{id}")
 
-    def create_flag_id(self, flag_id, is_default, flag_description=None,
-                       compile=None):
+    def create_flag_id(self, flag_id, is_default, flag_description=None, compile=None):
         """Save a new flag id.
 
         `Args:`
             flag_id: str
                 The flag id type. One of: "AMM", "BNH", "BNM", "DEAD", "DNC",
                 "DNR", "ENDR", "GTD", "HH", "L2VT", "LBO", "LM", "LO", "LS",
                 "LSD", "LSR", "MAYBE", "MOV", "NAH", "NO", "REF", "SO", "SS",
@@ -59,16 +58,15 @@
         """
         payload = {
             "flagId": flag_id,
             "flagIdDescription": flag_description,
             "compile": compile,
             "isDefault": is_default,
         }
-        data = self._request(
-            self.url_flag_ids, req_type="POST", post_data=payload)
+        data = self._request(self.url_flag_ids, req_type="POST", post_data=payload)
 
         return data["id"]
 
     def delete_flag_id(self, id):
         """Delete a flag id.
 
         NOTE: The function returns True (even if the id doesn't exist) unless
@@ -82,16 +80,17 @@
             bool
                 True if the operation is successful.
         """
         self._request(f"{self.url_flag_ids}/{id}", req_type="DELETE")
 
         return True
 
-    def update_flag_id(self, id, flag_id, is_default, flag_description=None,
-                       compile=None):
+    def update_flag_id(
+        self, id, flag_id, is_default, flag_description=None, compile=None
+    ):
         """Update a flag id.
 
         `Args:`
             id: str
                 The flag id identifier.
             flag_id: str
                 The flag id type. One of: "AMM", "BNH", "BNM", "DEAD", "DNC",
@@ -112,10 +111,11 @@
         payload = {
             "flagId": flag_id,
             "flagIdDescription": flag_description,
             "compile": compile,
             "isDefault": is_default,
         }
         data = self._request(
-            f"{self.url_flag_ids}/{id}", req_type="PUT", post_data=payload)
+            f"{self.url_flag_ids}/{id}", req_type="PUT", post_data=payload
+        )
 
         return data["id"]
```

### Comparing `parsons-1.0.0/parsons/pdi/flags.py` & `parsons-1.1.0/parsons/pdi/flags.py`

 * *Files 4% similar despite different names*

```diff
@@ -54,15 +54,16 @@
         """
         if "pdiId" not in list(flag_list[0].keys()):
             raise ValueError("missing required key")
             return {}
         for flag in flag_list:
             try:
                 flag["flagEntryDate"] = str(
-                    datetime.strptime(flag["flagEntryDate"], "%Y-%m-%d").isoformat())
+                    datetime.strptime(flag["flagEntryDate"], "%Y-%m-%d").isoformat()
+                )
             except ValueError:
                 raise ValueError("Invalid date format.")
         print(flag_list)
         return self._request(self.url_flags, post_data=flag_list, req_type="POST")
 
     def delete_flag(self, id: str):
         """
```

### Comparing `parsons-1.0.0/parsons/pdi/locations.py` & `parsons-1.1.0/parsons/pdi/locations.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 class Locations:
     """A class for getting, creating, and editing PDI locations"""
 
     def __init__(self):
-        self.locations_url = self.base_url + '/locations'
+        self.locations_url = self.base_url + "/locations"
 
         super().__init__()
 
     def get_locations(self, limit=None):
         """Get a list of PDI Locations
 
         `Args:`
@@ -26,26 +26,22 @@
             address: str
                A full address including street number, city, state, and zip.
             name: str
                 The name of the location. E.g. "The Overlook Hotel"
         `Returns:`
             dict
                 Response from PDI in dictionary object
-            """
+        """
 
-        payload = {
-            'locationName': name,
-            'locationAddress': address
-        }
-        return self._request(self.locations_url, req_type='POST', post_data=payload)
+        payload = {"locationName": name, "locationAddress": address}
+        return self._request(self.locations_url, req_type="POST", post_data=payload)
 
     def get_location(self, id: str):
         return self._request(f"{self.locations_url}/{id}")
 
     def update_location(self, id: str, location_name: str, address: str):
-        payload = {
-            "locationName": location_name,
-            "locationAddress": address
-        }
-        res = self._request(f"{self.locations_url}/{id}", req_type='PUT', post_data=payload)
+        payload = {"locationName": location_name, "locationAddress": address}
+        res = self._request(
+            f"{self.locations_url}/{id}", req_type="PUT", post_data=payload
+        )
         if res["code"] == 201:
             return True
```

### Comparing `parsons-1.0.0/parsons/pdi/pdi.py` & `parsons-1.1.0/parsons/pdi/pdi.py`

 * *Files 9% similar despite different names*

```diff
@@ -17,19 +17,26 @@
 import logging
 import requests
 
 
 logger = logging.getLogger(__name__)
 
 
-class PDI(FlagIDs, Universes, Questions, AcquisitionTypes, Flags, Events, Locations, Contacts,
-          Activities):
-
-    def __init__(self, username=None, password=None, api_token=None,
-                 qa_url=False):
+class PDI(
+    FlagIDs,
+    Universes,
+    Questions,
+    AcquisitionTypes,
+    Flags,
+    Events,
+    Locations,
+    Contacts,
+    Activities,
+):
+    def __init__(self, username=None, password=None, api_token=None, qa_url=False):
         """
         Instantiate the PDI class
 
         `Args:`
             username: str
                 The username for a PDI account. Can be passed as arguement or
                 can be set as `PDI_USERNAME` environment variable.
@@ -45,35 +52,32 @@
                 token.
         """
         if qa_url:
             self.base_url = "https://apiqa.bluevote.com"
         else:
             self.base_url = "https://api.bluevote.com"
 
-        self.username = check_env.check('PDI_USERNAME', username)
-        self.password = check_env.check('PDI_PASSWORD', password)
-        self.api_token = check_env.check('PDI_API_TOKEN', api_token)
+        self.username = check_env.check("PDI_USERNAME", username)
+        self.password = check_env.check("PDI_PASSWORD", password)
+        self.api_token = check_env.check("PDI_API_TOKEN", api_token)
 
         super().__init__()
 
         self._get_session_token()
 
     def _get_session_token(self):
         headers = {
             "Content-Type": "application/json",
         }
         login = {
             "Username": self.username,
             "Password": self.password,
             "ApiToken": self.api_token,
         }
-        res = requests.post(
-            f"{self.base_url}/sessions",
-            json=login,
-            headers=headers)
+        res = requests.post(f"{self.base_url}/sessions", json=login, headers=headers)
         logger.debug(f"{res.status_code} - {res.url}")
         res.raise_for_status()
         # status_code == 200
         data = res.json()
         self.session_token = data["AccessToken"]
         self.session_exp = parse(data["ExpirationDate"])
 
@@ -82,16 +86,15 @@
             return [self._clean_dict(obj) for obj in dct]
 
         if isinstance(dct, dict):
             return {k: v for k, v in dct.items() if v is not None}
 
         return dct
 
-    def _request(self, url, req_type='GET', post_data=None, args=None,
-                 limit=None):
+    def _request(self, url, req_type="GET", post_data=None, args=None, limit=None):
         # Make sure to have a current token before we make another request
         now = datetime.now(timezone.utc)
         if now > self.session_exp:
             self._get_session_token()
 
         # Based on PDI docs
         # https://api.bluevote.com/docs/index
@@ -130,28 +133,28 @@
             res_json = res.json()
         except JSONDecodeError:
             res_json = None
 
         if "data" not in res_json:
             return res_json
 
-        total_count = (0 if "totalCount" not in res_json
-                       else res_json["totalCount"])
+        total_count = 0 if "totalCount" not in res_json else res_json["totalCount"]
         data = res_json["data"]
 
         if not limit:
             # We don't have a limit, so let's get everything
             # Start a page 2 since we already go page 1
             cursor = 2
             while len(data) < total_count:
                 args = args or {}
                 args["cursor"] = cursor
                 args["limit"] = LIMIT_MAX
                 res = request_fn[req_type](
-                    url, headers=headers, json=post_data, params=args)
+                    url, headers=headers, json=post_data, params=args
+                )
 
                 data.extend(res.json()["data"])
 
                 cursor += 1
 
             return Table(data)
 
@@ -160,14 +163,15 @@
 
             cursor = 2
             while len(data) < total_need:
                 args = args or {}
                 args["cursor"] = cursor
                 args["limit"] = min(LIMIT_MAX, total_need - len(data))
                 res = request_fn[req_type](
-                    url, headers=headers, json=post_data, params=args)
+                    url, headers=headers, json=post_data, params=args
+                )
 
                 data.extend(res.json()["data"])
 
                 cursor += 1
 
             return Table(data)
```

### Comparing `parsons-1.0.0/parsons/pdi/questions.py` & `parsons-1.1.0/parsons/pdi/questions.py`

 * *Files 22% similar despite different names*

```diff
@@ -29,17 +29,26 @@
                 The Question id
         `Returns:`
             parsons.Table
                 A Parsons table of all the data.
         """
         return self._request(f"{self.url_questions}/{id}")
 
-    def create_question(self, question: str, type: str, category: str, answer_options: list,
-                        question_label: str = None, question_description: str = None,
-                        candidate_issue_id: str = None, default: bool = True, *args):
+    def create_question(
+        self,
+        question: str,
+        type: str,
+        category: str,
+        answer_options: list,
+        question_label: str = None,
+        question_description: str = None,
+        candidate_issue_id: str = None,
+        default: bool = True,
+        *args,
+    ):
         """
         answer_options:[
                 {
                 "id": "string",
                 "flagId": "string",
                 "displayDescription": "string",
                 "displayCode": "string"
@@ -50,13 +59,13 @@
             "question": question,
             "questionLabel": question_label,
             "questionDescription": question_description,
             "type": type,
             "category": category,
             "candidateIssueId": candidate_issue_id,
             "default": default,
-            "answerOptions": answer_options
+            "answerOptions": answer_options,
         }
-        return self._request(self.locations_url, req_type='POST', post_data=payload)
+        return self._request(self.locations_url, req_type="POST", post_data=payload)
 
     def delete_question(self, id: str):
         return self._request(f"{self.url_questions}/{id}", req_type="DELETE")
```

### Comparing `parsons-1.0.0/parsons/pdi/universes.py` & `parsons-1.1.0/parsons/pdi/universes.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,7 @@
-
 class Universes:
     """A class to access the Universes PDI API endpoint."""
 
     def __init__(self):
         universes_endpoint = "/universes"
         self.url_universes = self.base_url + universes_endpoint
```

### Comparing `parsons-1.0.0/parsons/phone2action/p2a.py` & `parsons-1.1.0/parsons/phone2action/p2a.py`

 * *Files 3% similar despite different names*

```diff
@@ -17,23 +17,29 @@
             env variable set.
     `Returns:`
         Phone2Action Class
     """
 
     def __init__(self, app_id=None, app_key=None):
         self.capitol_canary = CapitolCanary(app_id, app_key)
-        logger.warning('The Phone2Action class is being deprecated and replaced by CapitalCanary')
+        logger.warning(
+            "The Phone2Action class is being deprecated and replaced by CapitalCanary"
+        )
 
     def __getattr__(self, name):
         try:
             return getattr(self.capitol_canary, name)
         except AttributeError:
-            raise AttributeError(f"{type(self).__name__} object has no attribute {name}")
-
-    def get_advocates(self, state=None, campaign_id=None, updated_since=None, page=None):
+            raise AttributeError(
+                f"{type(self).__name__} object has no attribute {name}"
+            )
+
+    def get_advocates(
+        self, state=None, campaign_id=None, updated_since=None, page=None
+    ):
         """
         Return advocates (person records).
 
         If no page is specified, the method will automatically paginate through the available
         advocates.
 
         `Args:`
@@ -54,18 +60,26 @@
                 * phones
                 * memberships
                 * tags
                 * ids
                 * fields
                 * advocates
         """
-        return self.capitol_canary.get_advocates(state, campaign_id, updated_since, page)
+        return self.capitol_canary.get_advocates(
+            state, campaign_id, updated_since, page
+        )
 
-    def get_campaigns(self, state=None, zip=None, include_generic=False, include_private=False,
-                      include_content=True):
+    def get_campaigns(
+        self,
+        state=None,
+        zip=None,
+        include_generic=False,
+        include_private=False,
+        include_content=True,
+    ):
         """
         Returns a list of campaigns
 
         `Args:`
             state: str
                 Filter by US postal abbreviation for a state or territory e.g., "CA" "NY" or "DC"
             zip: int
@@ -78,33 +92,36 @@
                 If true, include campaign content fields, which may vary. This may cause
                 sync errors.
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        return self.capitol_canary.get_campaigns(state, zip, include_generic, include_private,
-                                                 include_content)
+        return self.capitol_canary.get_campaigns(
+            state, zip, include_generic, include_private, include_content
+        )
 
-    def create_advocate(self,
-                        campaigns,
-                        first_name=None,
-                        last_name=None,
-                        email=None,
-                        phone=None,
-                        address1=None,
-                        address2=None,
-                        city=None,
-                        state=None,
-                        zip5=None,
-                        sms_optin=None,
-                        email_optin=None,
-                        sms_optout=None,
-                        email_optout=None,
-                        **kwargs):
+    def create_advocate(
+        self,
+        campaigns,
+        first_name=None,
+        last_name=None,
+        email=None,
+        phone=None,
+        address1=None,
+        address2=None,
+        city=None,
+        state=None,
+        zip5=None,
+        sms_optin=None,
+        email_optin=None,
+        sms_optout=None,
+        email_optout=None,
+        **kwargs,
+    ):
         """
         Create an advocate.
 
         If you want to opt an advocate into or out of SMS / email campaigns, you must provide
         the email address or phone number (accordingly).
 
         The list of arguments only partially covers the fields that can be set on the advocate.
@@ -165,27 +182,29 @@
             city,
             state,
             zip5,
             sms_optin,
             email_optin,
             sms_optout,
             email_optout,
-            **kwargs
+            **kwargs,
         )
 
-    def update_advocate(self,
-                        advocate_id,
-                        campaigns=None,
-                        email=None,
-                        phone=None,
-                        sms_optin=None,
-                        email_optin=None,
-                        sms_optout=None,
-                        email_optout=None,
-                        **kwargs):
+    def update_advocate(
+        self,
+        advocate_id,
+        campaigns=None,
+        email=None,
+        phone=None,
+        sms_optin=None,
+        email_optin=None,
+        sms_optout=None,
+        email_optout=None,
+        **kwargs,
+    ):
         """
         Update the fields of an advocate.
 
         If you want to opt an advocate into or out of SMS / email campaigns, you must provide
         the email address or phone number along with a list of campaigns.
 
         The list of arguments only partially covers the fields that can be updated on the advocate.
@@ -224,8 +243,9 @@
             campaigns,
             email,
             phone,
             sms_optin,
             email_optin,
             sms_optout,
             email_optout,
-            **kwargs)
+            **kwargs,
+        )
```

### Comparing `parsons-1.0.0/parsons/quickbase/quickbase.py` & `parsons-1.1.0/parsons/quickbase/quickbase.py`

 * *Files 12% similar despite different names*

```diff
@@ -17,67 +17,73 @@
             instance (e.g. demo.quickbase.com).
         user_token: str
             The Quickbase account user token (API key). Not required if
             ``QUICKBASE_USER_TOKEN`` env variable is set.
     `Returns:`
         Quickbase Class
     """
+
     def __init__(self, hostname=None, user_token=None):
-        self.hostname = check_env.check('QUICKBASE_HOSTNAME', hostname)
-        self.user_token = check_env.check('QUICKBASE_USER_TOKEN', user_token)
-        self.api_hostname = 'https://api.quickbase.com/v1'
-        self.client = APIConnector(self.api_hostname,
-                                   headers={'QB-Realm-Hostname': self.hostname,
-                                            'AUTHORIZATION': f'QB-USER-TOKEN {self.user_token}'})
+        self.hostname = check_env.check("QUICKBASE_HOSTNAME", hostname)
+        self.user_token = check_env.check("QUICKBASE_USER_TOKEN", user_token)
+        self.api_hostname = "https://api.quickbase.com/v1"
+        self.client = APIConnector(
+            self.api_hostname,
+            headers={
+                "QB-Realm-Hostname": self.hostname,
+                "AUTHORIZATION": f"QB-USER-TOKEN {self.user_token}",
+            },
+        )
 
     def get_app_tables(self, app_id=None):
         """
         Query records in a Quickbase table. This follows the patterns laid out
         in Quickbase query documentaiton, located here:
         https://help.quickbase.com/api-guide/componentsquery.html
 
         `Args:`
             app_id: str
                 Identifies which Quickbase app from which to fetch tables.
         `Returns:`
             Table Class
         """
-        return Table(self.client.request(
-            f'{self.api_hostname}/tables?appId={app_id}',
-            'GET').json())
+        return Table(
+            self.client.request(
+                f"{self.api_hostname}/tables?appId={app_id}", "GET"
+            ).json()
+        )
 
     def query_records(self, table_from=None):
         """
         Query records in a Quickbase table. This follows the patterns laid out
         in Quickbase query documentaiton, located here:
         https://help.quickbase.com/api-guide/componentsquery.html
 
         `Args:`
             from: str
                 The ID of a Quickbase resource (i.e. a table) to query.
         `Returns:`
             Table Class
         """
-        req_resp = \
-            (self.client.request(f'{self.api_hostname}/records/query',
-                                 'POST',
-                                 json={"from": table_from}).json())
+        req_resp = self.client.request(
+            f"{self.api_hostname}/records/query", "POST", json={"from": table_from}
+        ).json()
 
-        resp_tbl = Table(req_resp['data'])
+        resp_tbl = Table(req_resp["data"])
         cleaned_tbl = Table()
 
         for row in resp_tbl:
             row_dict = {}
             for column in resp_tbl.columns:
-                row_dict[column] = row[column]['value']
+                row_dict[column] = row[column]["value"]
             cleaned_tbl.concat(Table([row_dict]))
             cleaned_tbl.materialize()
 
-        column_resp = req_resp['fields']
+        column_resp = req_resp["fields"]
         column_map = {}
         for entry in column_resp:
-            column_map[str(entry['id'])] = entry['label'].lower().strip()
+            column_map[str(entry["id"])] = entry["label"].lower().strip()
 
         for column in cleaned_tbl.columns:
             cleaned_tbl.rename_column(column, column_map[column])
 
         return cleaned_tbl
```

### Comparing `parsons-1.0.0/parsons/redash/redash.py` & `parsons-1.1.0/parsons/redash/redash.py`

 * *Files 23% similar despite different names*

```diff
@@ -30,49 +30,126 @@
             Specify time between polling for refreshed queries (Defaults to 3 seconds)
         verify: bool
             For https requests, should the certificate be verified (Defaults to True)
     `Returns:`
         Redash Class
     """
 
-    def __init__(self,
-                 base_url=None,
-                 user_api_key=None,
-                 pause_time=3,
-                 timeout=0,  # never timeout
-                 verify=True):
-        self.base_url = check('REDASH_BASE_URL', base_url)
-        self.user_api_key = check('REDASH_USER_API_KEY', user_api_key, optional=True)
-        self.pause = int(check('REDASH_PAUSE_TIME', pause_time, optional=True))
-        self.timeout = int(check('REDASH_TIMEOUT', timeout, optional=True))
+    def __init__(
+        self,
+        base_url=None,
+        user_api_key=None,
+        pause_time=3,
+        timeout=0,  # never timeout
+        verify=True,
+    ):
+        self.base_url = check("REDASH_BASE_URL", base_url)
+        self.user_api_key = check("REDASH_USER_API_KEY", user_api_key, optional=True)
+        self.pause = int(check("REDASH_PAUSE_TIME", pause_time, optional=True))
+        self.timeout = int(check("REDASH_TIMEOUT", timeout, optional=True))
 
         self.verify = verify  # for https requests
         self.session = requests.Session()
         if user_api_key:
-            self.session.headers.update({'Authorization': f'Key {user_api_key}'})
+            self.session.headers.update({"Authorization": f"Key {user_api_key}"})
+
+    def _catch_runtime_error(self, res):
+        if res.status_code != 200:
+            raise RuntimeError(
+                f"Error. Status code: {res.status_code}. Reason: {res.reason}"
+            )
 
     def _poll_job(self, session, job, query_id):
         start_secs = time.time()
-        while job['status'] not in (3, 4):
+        while job["status"] not in (3, 4):
             if self.timeout and start_secs + self.timeout < time.time():
-                raise RedashTimeout(f'Redash timeout: {self.timeout}')
-            poll_url = '{}/api/jobs/{}'.format(self.base_url, job['id'])
+                raise RedashTimeout(f"Redash timeout: {self.timeout}")
+            poll_url = "{}/api/jobs/{}".format(self.base_url, job["id"])
             response = session.get(poll_url, verify=self.verify)
             response_json = response.json()
             job = response_json.get(
-                'job',
-                {'status': 'Error NO JOB IN RESPONSE: {}'.format(json.dumps(response_json))})
-            logger.debug("poll url:%s id:%s status:%s err:%s",
-                         poll_url, query_id, job['status'], job.get('error'))
+                "job",
+                {
+                    "status": "Error NO JOB IN RESPONSE: {}".format(
+                        json.dumps(response_json)
+                    )
+                },
+            )
+            logger.debug(
+                "poll url:%s id:%s status:%s err:%s",
+                poll_url,
+                query_id,
+                job["status"],
+                job.get("error"),
+            )
             time.sleep(self.pause)
 
-        if job['status'] == 3:  # 3 = completed
-            return job['query_result_id']
-        elif job['status'] == 4:  # 3 = ERROR
-            raise RedashQueryFailed('Redash Query {} failed: {}'.format(query_id, job['error']))
+        if job["status"] == 3:  # 3 = completed
+            return job["query_result_id"]
+        elif job["status"] == 4:  # 3 = ERROR
+            raise RedashQueryFailed(
+                "Redash Query {} failed: {}".format(query_id, job["error"])
+            )
+
+    def get_data_source(self, data_source_id):
+        """
+        Get a data source.
+
+        `Args:`
+            data_source_id: int or str
+                ID of data source.
+        `Returns`:
+            Data source json object
+        """
+        res = self.session.get(f"{self.base_url}/api/data_sources/{data_source_id}")
+        self._catch_runtime_error(res)
+        return res.json()
+
+    def update_data_source(
+        self, data_source_id, name, type, dbName, host, password, port, user
+    ):
+        """
+        Update a data source.
+
+        `Args:`
+            data_source_id: str or int
+                ID of data source.
+            name: str
+                Name of data source.
+            type: str
+                Type of data source.
+            dbname: str
+                Database name of data source.
+            host: str
+                Host of data source.
+            password: str
+                Password of data source.
+            port: int or str
+                Port of data source.
+            user: str
+                Username of data source.
+        `Returns:`
+            ``None``
+        """
+        self._catch_runtime_error(
+            self.session.post(
+                f"{self.base_url}/api/data_sources/{data_source_id}",
+                json={
+                    "name": name,
+                    "type": type,
+                    "options": {
+                        "dbname": dbName,
+                        "host": host,
+                        "password": password,
+                        "port": port,
+                        "user": user,
+                    },
+                },
+            )
+        )
 
     def get_fresh_query_results(self, query_id=None, params=None):
         """
         Make a fresh query result and get back the CSV http response object back
         with the CSV string in result.content
 
         `Args:`
@@ -87,38 +164,48 @@
                 in the url, you should just set 'datelimit' in params here.
                 If you set this with REDASH_QUERY_PARAMS environment variable instead of passing
                 the values, then you must include the "p_" prefixes and it should be a single
                 url-encoded string as you would see it in the URL bar.
         `Returns:`
             Table Class
         """
-        query_id = check('REDASH_QUERY_ID', query_id, optional=True)
-        params_from_env = check('REDASH_QUERY_PARAMS', '', optional=True)
-        redash_params = ({'p_%s' % k: str(v).replace("'", "''") for k, v in params.items()}
-                         if params else {})
+        query_id = check("REDASH_QUERY_ID", query_id, optional=True)
+        params_from_env = check("REDASH_QUERY_PARAMS", "", optional=True)
+        redash_params = (
+            {"p_%s" % k: str(v).replace("'", "''") for k, v in params.items()}
+            if params
+            else {}
+        )
 
         response = self.session.post(
-            f'{self.base_url}/api/queries/{query_id}/refresh?{params_from_env}',
+            f"{self.base_url}/api/queries/{query_id}/refresh?{params_from_env}",
             params=redash_params,
-            verify=self.verify)
+            verify=self.verify,
+        )
 
         if response.status_code != 200:
-            raise RedashQueryFailed(f'Refresh failed for query {query_id}. {response.text}')
+            raise RedashQueryFailed(
+                f"Refresh failed for query {query_id}. {response.text}"
+            )
 
-        job = response.json()['job']
+        job = response.json()["job"]
         result_id = self._poll_job(self.session, job, query_id)
         if result_id:
             response = self.session.get(
-                f'{self.base_url}/api/queries/{query_id}/results/{result_id}.csv',
-                verify=self.verify)
+                f"{self.base_url}/api/queries/{query_id}/results/{result_id}.csv",
+                verify=self.verify,
+            )
             if response.status_code != 200:
                 raise RedashQueryFailed(
-                    f'Failed getting results for query {query_id}. {response.text}')
+                    f"Failed getting results for query {query_id}. {response.text}"
+                )
         else:
-            raise RedashQueryFailed(f'Failed getting result {query_id}. {response.text}')
+            raise RedashQueryFailed(
+                f"Failed getting result {query_id}. {response.text}"
+            )
         return Table.from_csv_string(response.text)
 
     def get_cached_query_results(self, query_id=None, query_api_key=None):
         """
         Get the results from a cached query result and get back the CSV http response object back
         with the CSV string in result.content
 
@@ -127,24 +214,28 @@
                 The query id of the query
             query_api_key: str
                 If you did not supply a user_api_key on the Redash object, then you can
                 supply a query_api_key to get cached results back anonymously.
         `Returns:`
             Table Class
         """
-        query_id = check('REDASH_QUERY_ID', query_id)
-        query_api_key = check('REDASH_QUERY_API_KEY', query_api_key, optional=True)
+        query_id = check("REDASH_QUERY_ID", query_id)
+        query_api_key = check("REDASH_QUERY_API_KEY", query_api_key, optional=True)
         params = {}
         if not self.user_api_key and query_api_key:
-            params['api_key'] = query_api_key
-        response = self.session.get(f'{self.base_url}/api/queries/{query_id}/results.csv',
-                                    params=params,
-                                    verify=self.verify)
+            params["api_key"] = query_api_key
+        response = self.session.get(
+            f"{self.base_url}/api/queries/{query_id}/results.csv",
+            params=params,
+            verify=self.verify,
+        )
         if response.status_code != 200:
-            raise RedashQueryFailed(f'Failed getting results for query {query_id}. {response.text}')
+            raise RedashQueryFailed(
+                f"Failed getting results for query {query_id}. {response.text}"
+            )
         return Table.from_csv_string(response.text)
 
     @classmethod
     def load_to_table(cls, refresh=True, **kwargs):
         """
         Fast classmethod makes the appropriate query type (refresh or cached)
         based on which arguments are supplied.
@@ -170,15 +261,21 @@
                 then this is a dict that will pass the parameters in the POST.
                 We add the "p_" prefix for parameters, so if your query had ?p_datelimit=....
                 in the url, you should just set 'datelimit' in params here.
 
         `Returns:`
             Table Class
         """
-        initargs = {a: kwargs.get(a)
-                    for a in ('base_url', 'user_api_key', 'pause_time', 'timeout', 'verify')
-                    if a in kwargs}
+        initargs = {
+            a: kwargs.get(a)
+            for a in ("base_url", "user_api_key", "pause_time", "timeout", "verify")
+            if a in kwargs
+        }
         obj = cls(**initargs)
-        if not refresh or kwargs.get('query_api_key'):
-            return obj.get_cached_query_results(kwargs.get('query_id'), kwargs.get('query_api_key'))
+        if not refresh or kwargs.get("query_api_key"):
+            return obj.get_cached_query_results(
+                kwargs.get("query_id"), kwargs.get("query_api_key")
+            )
         else:
-            return obj.get_fresh_query_results(kwargs.get('query_id'), kwargs.get('params'))
+            return obj.get_fresh_query_results(
+                kwargs.get("query_id"), kwargs.get("params")
+            )
```

### Comparing `parsons-1.0.0/parsons/rockthevote/rtv.py` & `parsons-1.1.0/parsons/rockthevote/rtv.py`

 * *Files 5% similar despite different names*

```diff
@@ -14,28 +14,28 @@
 logger = logging.getLogger(__name__)
 
 
 VALID_REPORT_TYPES = ["extended"]
 TESTING_URI = "https://staging.rocky.rockthevote.com/api/v4"
 PRODUCTION_URI = "https://register.rockthevote.com/api/v4"
 
-DATETIME_FORMAT = '%Y-%m-%d %H:%M:%S UTC'
+DATETIME_FORMAT = "%Y-%m-%d %H:%M:%S UTC"
 """Datetime format for sending date's to the API."""
 
 REQUEST_HEADERS = {
     # For some reason, RTV's firewall REALLY doesn't like the
     # user-agent string that Python's request library gives by default,
     # though it seems fine with the curl user agent
     # For more info on user agents, see:
     # https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/User-Agent
-    'user-agent': 'curl/7.54.0'
+    "user-agent": "curl/7.54.0"
 }
 """Standard request header for sending requests to the API."""
 
-STATUS_URL_PARSE_REGEX = re.compile(r'(\d+)$')
+STATUS_URL_PARSE_REGEX = re.compile(r"(\d+)$")
 """Regex for parsing the report ID from the status URL."""
 
 
 class RTVFailure(Exception):
     """Exception raised when there is an error with the connector."""
 
 
@@ -53,16 +53,16 @@
         testing: bool
             Whether or not to use the staging instance. Defaults to False.
     `Returns`:
         RockTheVote class
     """
 
     def __init__(self, partner_id=None, partner_api_key=None, testing=False):
-        self.partner_id = check_env.check('RTV_PARTNER_ID', partner_id)
-        self.partner_api_key = check_env.check('RTV_PARTNER_API_KEY', partner_api_key)
+        self.partner_id = check_env.check("RTV_PARTNER_ID", partner_id)
+        self.partner_api_key = check_env.check("RTV_PARTNER_API_KEY", partner_api_key)
 
         if testing:
             self.client = APIConnector(TESTING_URI, headers=REQUEST_HEADERS)
         else:
             self.client = APIConnector(PRODUCTION_URI, headers=REQUEST_HEADERS)
 
     def create_registration_report(self, before=None, since=None, report_type=None):
@@ -79,64 +79,72 @@
             report_type: str
                 The type of report to create. If left as None, it creates the default report. The
                 ``extended`` report includes additional fields. Currently only accepts ``extended``.
         `Returns:`
             int
                 The ID of the created report.
         """
-        report_url = 'registrant_reports.json'
+        report_url = "registrant_reports.json"
         # Create the report for the new data
         report_parameters = {
-            'partner_id': self.partner_id,
-            'partner_API_key': self.partner_api_key,
+            "partner_id": self.partner_id,
+            "partner_API_key": self.partner_api_key,
         }
 
         # Declare these here so the logging doesn't error out
         since_date = before_date = None
 
         if report_type:
             if report_type not in VALID_REPORT_TYPES:
-                raise RTVFailure(f"Invalid report type. Must be one of {VALID_REPORT_TYPES}")
+                raise RTVFailure(
+                    f"Invalid report type. Must be one of {VALID_REPORT_TYPES}"
+                )
             report_parameters["report_type"] = report_type
         if since:
             since_date = parse_date(since).strftime(DATETIME_FORMAT)
-            report_parameters['since'] = since_date
+            report_parameters["since"] = since_date
         if before:
             before_date = parse_date(before).strftime(DATETIME_FORMAT)
-            report_parameters['before'] = before_date
+            report_parameters["before"] = before_date
 
         # The report parameters get passed into the request as JSON in the body
         # of the request.
         report_str = f"{report_type} report" if report_type else "report"
         logger.info(
             f"Creating {report_str} for {self.partner_id} "
-            f"for dates: {since_date} to {before_date}...")
-        response = self.client.request(report_url, 'post', json=report_parameters)
+            f"for dates: {since_date} to {before_date}..."
+        )
+        response = self.client.request(report_url, "post", json=report_parameters)
         if response.status_code != requests.codes.ok:
             raise RTVFailure("Couldn't create RTV registrations report")
 
         response_json = response.json()
         # The RTV API says the response should include the report_id, but I have not found
         # that to be the case
-        report_id = response_json.get('report_id')
+        report_id = response_json.get("report_id")
         if report_id:
             logger.info(f"Created report with id {report_id}.")
             return report_id
 
         # If the response didn't include the report_id, then we will parse it out of the URL.
-        status_url = response_json.get('status_url')
+        status_url = response_json.get("status_url")
         url_match = STATUS_URL_PARSE_REGEX.search(status_url)
         if url_match:
             report_id = url_match.group(1)
 
         logger.info(f"Created report with id {report_id}.")
         return report_id
 
-    def get_registration_report(self, report_id, block=False, poll_interval_seconds=60,
-                                report_timeout_seconds=3600):
+    def get_registration_report(
+        self,
+        report_id,
+        block=False,
+        poll_interval_seconds=60,
+        report_timeout_seconds=3600,
+    ):
         """
         Get data from an existing registration report.
 
         `Args:`
             report_id: int
                 The ID of the report to get data from
             block: bool
@@ -147,83 +155,90 @@
                 If blocking, how long to wait for the report before timing out
         `Returns:`
             Parsons Table
                 Parsons table with the report data.
         """
         logger.info(f"Getting report with id {report_id}...")
         credentials = {
-            'partner_id': self.partner_id,
-            'partner_API_key': self.partner_api_key,
+            "partner_id": self.partner_id,
+            "partner_API_key": self.partner_api_key,
         }
-        status_url = f'registrant_reports/{report_id}'
+        status_url = f"registrant_reports/{report_id}"
         download_url = None
 
         # Let's figure out at what time should we just give up because we waited
         # too long
-        end_time = datetime.datetime.now() + datetime.timedelta(seconds=report_timeout_seconds)
+        end_time = datetime.datetime.now() + datetime.timedelta(
+            seconds=report_timeout_seconds
+        )
 
         # If we have a download URL, we can move on and just download the
         # report. Otherwise, as long as we haven't run out of time, we will
         # check the status.
         while not download_url and datetime.datetime.now() < end_time:
             logger.debug(
-                f'Registrations report not ready yet, sleeping {poll_interval_seconds} seconds')
+                f"Registrations report not ready yet, sleeping {poll_interval_seconds} seconds"
+            )
 
             # Check the status again via the status endpoint
-            status_response = self.client.request(status_url, 'get', params=credentials)
+            status_response = self.client.request(status_url, "get", params=credentials)
 
             # Check to make sure the call got a valid response
             if status_response.status_code == requests.codes.ok:
                 status_json = status_response.json()
 
                 # Grab the download_url from the response.
-                download_url = status_json.get('download_url')
+                download_url = status_json.get("download_url")
 
                 if not download_url and not block:
                     return None
             else:
                 raise RTVFailure("Couldn't get report status")
 
             if not download_url:
                 # We just got the status, so we should wait a minute before
                 # we check it again.
                 time.sleep(poll_interval_seconds)
 
         # If we never got a valid download_url, then we timed out waiting for
         # the report to generate. We will log an error and exit.
         if not download_url:
-            raise RTVFailure('Timed out waiting for report')
+            raise RTVFailure("Timed out waiting for report")
 
         # Download the report data
-        download_response = self.client.request(download_url, 'get', params=credentials)
+        download_response = self.client.request(download_url, "get", params=credentials)
 
         # Check to make sure the call got a valid response
         if download_response.status_code == requests.codes.ok:
             report_data = download_response.text
 
             # Load the report data into a Parsons Table
             table = Table.from_csv_string(report_data)
 
             # Transform the data from the report's CSV format to something more
             # Pythonic (snake case)
             normalized_column_names = [
-                re.sub(r'\s', '_', name).lower()
-                for name in table.columns
+                re.sub(r"\s", "_", name).lower() for name in table.columns
             ]
             normalized_column_names = [
-                re.sub(r'[^A-Za-z\d_]', '', name)
-                for name in normalized_column_names
+                re.sub(r"[^A-Za-z\d_]", "", name) for name in normalized_column_names
             ]
             table.table = petl.setheader(table.table, normalized_column_names)
             return table
         else:
-            raise RTVFailure('Unable to download report data')
+            raise RTVFailure("Unable to download report data")
 
-    def run_registration_report(self, before=None, since=None, report_type=None,
-                                poll_interval_seconds=60, report_timeout_seconds=3600):
+    def run_registration_report(
+        self,
+        before=None,
+        since=None,
+        report_type=None,
+        poll_interval_seconds=60,
+        report_timeout_seconds=3600,
+    ):
         """
         Run a new registration report.
 
         This method will block until the report has finished generating, or until the specified
         timeout is reached.
 
         `Args:`
@@ -243,23 +258,29 @@
         `Returns:`
             Parsons.Table
                 The table with the report data.
         """
         report_str = f"{report_type} report" if report_type else "report"
         logger.info(
             f"Running {report_str} for {self.partner_id} "
-            f"for dates: {since} to {before}...")
+            f"for dates: {since} to {before}..."
+        )
         report_id = self.create_registration_report(
-            before=before, since=since, report_type=report_type)
-        return self.get_registration_report(report_id, block=True,
-                                            poll_interval_seconds=poll_interval_seconds,
-                                            report_timeout_seconds=report_timeout_seconds)
-
-    def get_state_requirements(self, lang, home_state_id, home_zip_code,
-                               date_of_birth=None, callback=None):
+            before=before, since=since, report_type=report_type
+        )
+        return self.get_registration_report(
+            report_id,
+            block=True,
+            poll_interval_seconds=poll_interval_seconds,
+            report_timeout_seconds=report_timeout_seconds,
+        )
+
+    def get_state_requirements(
+        self, lang, home_state_id, home_zip_code, date_of_birth=None, callback=None
+    ):
         """
         Checks state eligibility and provides state specific fields information.
         Args:
             lang: str
                 Required. Language. Represented by an abbreviation. 'en', 'es', etc
             home_state_id: str
                 Required. 2-character state abbreviation
@@ -269,33 +290,35 @@
                 Optional. 'mm-dd-yyyy'
             callback: str
                 Optional.  If used, will change the return value from JSON format to jsonp
         Returns:
             Parsons.Table
                 A single row table with the response json
         """
-        requirements_url = 'state_requirements.json'
+        requirements_url = "state_requirements.json"
 
         logger.info(f"Getting the requirements for {home_state_id}...")
 
         params = {
-            'lang': lang,
-            'home_state_id': home_state_id,
-            'home_zip_code': home_zip_code
+            "lang": lang,
+            "home_state_id": home_state_id,
+            "home_zip_code": home_zip_code,
         }
 
         if date_of_birth:
-            params['date_of_birth'] = date_of_birth
+            params["date_of_birth"] = date_of_birth
 
         if callback:
-            params['callback'] = callback
+            params["callback"] = callback
 
-        requirements_response = self.client.request(requirements_url, 'get', params=params)
+        requirements_response = self.client.request(
+            requirements_url, "get", params=params
+        )
 
         if requirements_response.status_code == requests.codes.ok:
             response_json = requirements_response.json()
             table = Table([response_json])
             return table
         else:
             error_json = requirements_response.json()
-            logger.info(f'{error_json}')
+            logger.info(f"{error_json}")
             raise RTVFailure("Could not retrieve state requirements")
```

### Comparing `parsons-1.0.0/parsons/salesforce/salesforce.py` & `parsons-1.1.0/parsons/salesforce/salesforce.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,9 @@
 from simple_salesforce import Salesforce as _Salesforce
 from parsons.utilities import check_env
-from parsons.etl import Table
 import logging
 import json
 
 logger = logging.getLogger(__name__)
 
 
 class Salesforce:
@@ -25,22 +24,26 @@
         test_environment: bool
             If ``True`` the client will connect to a Salesforce sandbox instance. Not required if
             ``SALESFORCE_DOMAIN`` env variable is passed.
     `Returns:`
         Salesforce class
     """
 
-    def __init__(self, username=None, password=None, security_token=None, test_environment=False):
-
-        self.username = check_env.check('SALESFORCE_USERNAME', username)
-        self.password = check_env.check('SALESFORCE_PASSWORD', password)
-        self.security_token = check_env.check('SALESFORCE_SECURITY_TOKEN', security_token)
+    def __init__(
+        self, username=None, password=None, security_token=None, test_environment=False
+    ):
+
+        self.username = check_env.check("SALESFORCE_USERNAME", username)
+        self.password = check_env.check("SALESFORCE_PASSWORD", password)
+        self.security_token = check_env.check(
+            "SALESFORCE_SECURITY_TOKEN", security_token
+        )
 
         if test_environment:
-            self.domain = check_env.check('SALESFORCE_DOMAIN', 'test')
+            self.domain = check_env.check("SALESFORCE_DOMAIN", "test")
         else:
             self.domain = None
 
         self._client = None
 
     def describe_object(self, object):
         """
@@ -60,28 +63,29 @@
             object: str
                 The API name of the type of record on whose fields you want data. Note that custom
                 object names end in `__c`
         `Returns:`
             Dict of all the object's field meta data in Salesforce
         """
 
-        return json.loads(json.dumps(getattr(self.client, object).describe()['fields']))
+        return json.loads(json.dumps(getattr(self.client, object).describe()["fields"]))
 
     def query(self, soql):
         """
         `Args:`
             soql: str
                 The desired query in Salesforce SOQL language (SQL with additional limitations).
                 For reference, see the `Salesforce SOQL documentation <https://developer.salesforce.com/docs/atlas.en-us.soql_sosl.meta/soql_sosl/sforce_api_calls_soql.htm>`_.
         `Returns:`
             list of dicts with Salesforce data
-        """ # noqa: E501,E261
+        """  # noqa: E501,E261
 
-        q = Table(self.client.query_all(soql))
-        logger.info(f'Found {q.num_rows} results')
+        q = self.client.query_all(soql)
+        q = json.loads(json.dumps(q))
+        logger.info(f"Found {q['totalSize']} results")
         return q
 
     def insert_record(self, object, data_table):
         """
         Insert new records of the desired object into Salesforce
 
         `Args:`
@@ -97,17 +101,17 @@
             * success: boolean
             * created: boolean (if new record is created)
             * id: str (id of record created, if successful)
             * errors: list of dicts (with error details)
         """
 
         r = getattr(self.client.bulk, object).insert(data_table.to_dicts())
-        s = [x for x in r if x.get('success') is True]
+        s = [x for x in r if x.get("success") is True]
         logger.info(
-            f'Successfully inserted {len(s)} out of {data_table.num_rows} records to {object}'
+            f"Successfully inserted {len(s)} out of {data_table.num_rows} records to {object}"
         )
         return r
 
     def update_record(self, object, data_table):
         """
         Update existing records of the desired object in Salesforce
 
@@ -124,17 +128,17 @@
                 * success: boolean
                 * created: boolean (if new record is created)
                 * id: str (id of record altered, if successful)
                 * errors: list of dicts (with error details)
         """
 
         r = getattr(self.client.bulk, object).update(data_table.to_dicts())
-        s = [x for x in r if x.get('success') is True]
+        s = [x for x in r if x.get("success") is True]
         logger.info(
-            f'Successfully updated {len(s)} out of {data_table.num_rows} records in {object}'
+            f"Successfully updated {len(s)} out of {data_table.num_rows} records in {object}"
         )
         return r
 
     def upsert_record(self, object, data_table, id_col):
         """
         Insert new records and update existing ones of the desired object in Salesforce
 
@@ -154,17 +158,17 @@
                 * success: boolean
                 * created: boolean (if new record is created)
                 * id: str (id of record created or altered, if successful)
                 * errors: list of dicts (with error details)
         """
 
         r = getattr(self.client.bulk, object).upsert(data_table.to_dicts(), id_col)
-        s = [x for x in r if x.get('success') is True]
+        s = [x for x in r if x.get("success") is True]
         logger.info(
-            f'Successfully upserted {len(s)} out of {data_table.num_rows} records to {object}'
+            f"Successfully upserted {len(s)} out of {data_table.num_rows} records to {object}"
         )
         return r
 
     def delete_record(self, object, id_table, hard_delete=False):
         """
         Delete existing records of the desired object in Salesforce
 
@@ -186,17 +190,17 @@
         """
 
         if hard_delete:
             r = getattr(self.client.bulk, object).hard_delete(id_table.to_dicts())
         else:
             r = getattr(self.client.bulk, object).delete(id_table.to_dicts())
 
-        s = [x for x in r if x.get('success') is True]
+        s = [x for x in r if x.get("success") is True]
         logger.info(
-            f'Successfully deleted {len(s)} out of {id_table.num_rows} records from {object}'
+            f"Successfully deleted {len(s)} out of {id_table.num_rows} records from {object}"
         )
         return r
 
     @property
     def client(self):
         """
         Get the Salesforce client to use for making all calls. For more information, check the
@@ -207,11 +211,11 @@
         """
         if not self._client:
             # Create a Salesforce client to use to make bulk calls
             self._client = _Salesforce(
                 username=self.username,
                 password=self.password,
                 security_token=self.security_token,
-                domain=self.domain
+                domain=self.domain,
             )
 
         return self._client
```

### Comparing `parsons-1.0.0/parsons/scytl/scytl.py` & `parsons-1.1.0/parsons/scytl/scytl.py`

 * *Files 9% similar despite different names*

```diff
@@ -5,30 +5,36 @@
 import typing as t
 from datetime import datetime
 from dateutil.parser import parse as parsedate
 from pytz import timezone
 from io import BytesIO, StringIO
 from dataclasses import dataclass
 
-CLARITY_URL = 'https://results.enr.clarityelections.com/'
+CLARITY_URL = "https://results.enr.clarityelections.com/"
 
-CURRENT_VERSION_URL_TEMPLATE = \
-    CLARITY_URL + '{administrator}/{election_id}/current_ver.txt'
-SUMMARY_CSV_ZIP_URL_TEMPLATE = \
-    CLARITY_URL + '{administrator}/{election_id}/{version_num}/reports/summary.zip'
-DETAIL_XML_ZIP_URL_TEMPLATE = \
-    CLARITY_URL + '{administrator}/{election_id}/{version_num}/reports/detailxml.zip'
-COUNTY_DETAIL_XML_ZIP_URL_TEMPLATE = CLARITY_URL + \
-    '{state}/{county_name}/{county_election_id}/{county_version_num}/reports/detailxml.zip'
-ELECTION_SETTINGS_JSON_URL_TEMPLATE = \
-    CLARITY_URL + '{state}/{election_id}/{version_num}/json/en/electionsettings.json'
+CURRENT_VERSION_URL_TEMPLATE = (
+    CLARITY_URL + "{administrator}/{election_id}/current_ver.txt"
+)
+SUMMARY_CSV_ZIP_URL_TEMPLATE = (
+    CLARITY_URL + "{administrator}/{election_id}/{version_num}/reports/summary.zip"
+)
+DETAIL_XML_ZIP_URL_TEMPLATE = (
+    CLARITY_URL + "{administrator}/{election_id}/{version_num}/reports/detailxml.zip"
+)
+COUNTY_DETAIL_XML_ZIP_URL_TEMPLATE = (
+    CLARITY_URL
+    + "{state}/{county_name}/{county_election_id}/{county_version_num}/reports/detailxml.zip"
+)
+ELECTION_SETTINGS_JSON_URL_TEMPLATE = (
+    CLARITY_URL + "{state}/{election_id}/{version_num}/json/en/electionsettings.json"
+)
 
 BROWSER_HEADERS = {
-    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) ' +
-                  'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'
+    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) "
+    + "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36"
 }
 
 TZ_INFO = {
     "EST": "UTC-5",
     "EDT": "UTC-4",
     "CST": "UTC-6",
     "CDT": "UTC-5",
@@ -47,14 +53,15 @@
 class CountyDetails:
     """
     A class for keeping track of County election details.
 
     A dataclass is decorator that adds special functions including an
     automatic __init__ function. See more here: https://docs.python.org/3/library/dataclasses.html
     """
+
     state: str
     county_name: str
     county_election_id: str
     county_version_num: str
     county_update_date: datetime = None
 
 
@@ -72,17 +79,19 @@
         county: str (optional)
             The name of the county publishing the results.
             ex: Clarke
     """
 
     def __init__(self, state: str, election_id: str, county=""):
         self.state = state
-        self.county = county.replace(' ', '_')
+        self.county = county.replace(" ", "_")
 
-        self.administrator = f"{self.state}/{self.county}" if self.county else self.state
+        self.administrator = (
+            f"{self.state}/{self.county}" if self.county else self.state
+        )
         self.election_id = election_id
 
         self.previous_summary_version_num = None
         self.previous_details_version_num = None
         self.previous_county_details_version_num = None
         self.previous_county_details_list = None
         self.previously_fetched_counties = set([])
@@ -98,15 +107,15 @@
             datetime | None
         """
 
         if input_dt is None:
             return
 
         temp = parsedate(input_dt, tzinfos=TZ_INFO)
-        temp = temp.astimezone(timezone('UTC'))
+        temp = temp.astimezone(timezone("UTC"))
 
         return temp
 
     def _get_version(self, administrator: str, election_id: str) -> str:
         """
         Fetch the latest version of the election results from the Clarity site
 
@@ -174,33 +183,36 @@
 
         county_dict = {}
 
         config_settings_json_url = ELECTION_SETTINGS_JSON_URL_TEMPLATE.format(
             state=state, election_id=election_id, version_num=version_num
         )
 
-        settings_json_res = requests.get(config_settings_json_url, headers=BROWSER_HEADERS)
+        settings_json_res = requests.get(
+            config_settings_json_url, headers=BROWSER_HEADERS
+        )
         settings_json = settings_json_res.json()
 
-        participating_counties = \
-            settings_json['settings']['electiondetails']['participatingcounties']
+        participating_counties = settings_json["settings"]["electiondetails"][
+            "participatingcounties"
+        ]
 
         for county_row in participating_counties:
-            county_info = county_row.split('|')
+            county_info = county_row.split("|")
             source_county_name = county_info[0]
             county_election_id = county_info[1]
             county_version_num = county_info[2]
             county_update_date = self._parse_date_to_utc(county_info[3])
 
             county_details = CountyDetails(
                 state,
                 source_county_name,
                 county_election_id,
                 county_version_num,
-                county_update_date
+                county_update_date,
             )
 
             county_dict[source_county_name] = county_details
 
         return county_dict
 
     def _parse_county_xml_data_to_precincts(
@@ -226,68 +238,70 @@
         precinct_dict = {}
         precinct_votes = []
 
         root = tree
 
         for child in root:
 
-            if child.tag == 'VoterTurnout':
+            if child.tag == "VoterTurnout":
                 precincts = child[0]
 
                 for precinct in precincts:
                     data = precinct.attrib
-                    name = data.get('name')
+                    name = data.get("name")
 
                     precinct_info = {
-                        'total_voters': data.get('totalVoters'),
-                        'ballots_cast': data.get('ballotsCast'),
-                        'voter_turnout':  data.get('voterTurnout'),
-                        'percent_reporting':  data.get('percentReporting')
+                        "total_voters": data.get("totalVoters"),
+                        "ballots_cast": data.get("ballotsCast"),
+                        "voter_turnout": data.get("voterTurnout"),
+                        "percent_reporting": data.get("percentReporting"),
                     }
 
                     precinct_dict[name] = precinct_info
 
-            if child.tag == 'Contest':
+            if child.tag == "Contest":
 
-                office = child.attrib['text']
+                office = child.attrib["text"]
 
                 for choice in child:
                     cand_votes = {}
 
-                    if choice.tag == 'VoteType':
+                    if choice.tag == "VoteType":
                         continue
 
                     source_cand_data = choice.attrib
-                    cand_name = source_cand_data.get('text')
-                    cand_party = source_cand_data.get('party')
+                    cand_name = source_cand_data.get("text")
+                    cand_party = source_cand_data.get("party")
 
                     for vote_type in choice:
-                        vote_type_label = vote_type.attrib['name']
+                        vote_type_label = vote_type.attrib["name"]
 
                         for precinct in vote_type:
-                            precinct_name = precinct.attrib['name']
-                            cand_votes[precinct_name] = int(precinct.attrib['votes'])
+                            precinct_name = precinct.attrib["name"]
+                            cand_votes[precinct_name] = int(precinct.attrib["votes"])
 
                             precinct_turnout = precinct_dict.get(precinct_name, {})
 
                             result = {
-                                'state': county_details.state,
-                                'county_name': county_details.county_name,
-                                'county_id': county_details.county_election_id,
-                                'office': office,
-                                'ballots_cast': precinct_turnout.get('ballots_cast'),
-                                'reg_voters': precinct_turnout.get('total_voters'),
-                                'vote_method': vote_type_label,
-                                'candidate_name': cand_name,
-                                'candidate_party': cand_party,
-                                'precinct_name': precinct_name,
-                                'recorded_votes': cand_votes[precinct_name],
-                                'voter_turnout': precinct_turnout.get('voter_turnout'),
-                                'percent_reporting': precinct_turnout.get('percent_reporting'),
-                                'timestamp_last_updated': county_details.county_update_date
+                                "state": county_details.state,
+                                "county_name": county_details.county_name,
+                                "county_id": county_details.county_election_id,
+                                "office": office,
+                                "ballots_cast": precinct_turnout.get("ballots_cast"),
+                                "reg_voters": precinct_turnout.get("total_voters"),
+                                "vote_method": vote_type_label,
+                                "candidate_name": cand_name,
+                                "candidate_party": cand_party,
+                                "precinct_name": precinct_name,
+                                "recorded_votes": cand_votes[precinct_name],
+                                "voter_turnout": precinct_turnout.get("voter_turnout"),
+                                "percent_reporting": precinct_turnout.get(
+                                    "percent_reporting"
+                                ),
+                                "timestamp_last_updated": county_details.county_update_date,
                             }
 
                             precinct_votes.append(result)
 
         return precinct_votes
 
     def _parse_state_xml_data_to_counties(
@@ -312,70 +326,76 @@
         county_dict = {}
         county_votes = []
 
         timestamp = None
 
         for child in root:
 
-            if child.tag == "Timestamp":  # <Timestamp>1/5/2021 3:22:30 PM EST</Timestamp>
+            if (
+                child.tag == "Timestamp"
+            ):  # <Timestamp>1/5/2021 3:22:30 PM EST</Timestamp>
                 timestamp = self._parse_date_to_utc(child.text)
 
             if child.tag == "ElectionVoterTurnout":
                 counties = child[0]
 
                 for county in counties:
                     data = county.attrib
                     name = data["name"]
 
                     county_dict[name] = data
 
             if child.tag == "Contest":
 
-                office = child.attrib['text']
+                office = child.attrib["text"]
 
                 for choice in child:
                     cand_votes = {}
 
                     if choice.tag == "ParticipatingCounties":
                         continue
 
                     source_cand_data = choice.attrib
-                    cand_name = source_cand_data.get('text')
-                    cand_party = source_cand_data.get('party')
+                    cand_name = source_cand_data.get("text")
+                    cand_party = source_cand_data.get("party")
 
                     for vote_type in choice:
-                        vote_type_label = vote_type.attrib['name']
+                        vote_type_label = vote_type.attrib["name"]
 
                         for county in vote_type:
                             county_name = county.attrib["name"]
                             cand_votes[county_name] = int(county.attrib["votes"])
 
                             county_turnout = county_dict.get(county_name, {})
 
                             result = {
-                                'state': state,
-                                'county_name': county_name,
-                                'office': office,
-                                'ballots_cast': county_turnout.get('ballotsCast'),
-                                'reg_voters': county_turnout.get('totalVoters'),
-                                'precincts_reporting': county_turnout.get('precinctsReported'),
-                                'total_precincts': county_turnout.get('precinctsParticipating'),
-                                'vote_method': vote_type_label,
-                                'candidate_name': cand_name,
-                                'candidate_party': cand_party,
-                                'recorded_votes': cand_votes[county_name],
-                                'timestamp_last_updated': timestamp
+                                "state": state,
+                                "county_name": county_name,
+                                "office": office,
+                                "ballots_cast": county_turnout.get("ballotsCast"),
+                                "reg_voters": county_turnout.get("totalVoters"),
+                                "precincts_reporting": county_turnout.get(
+                                    "precinctsReported"
+                                ),
+                                "total_precincts": county_turnout.get(
+                                    "precinctsParticipating"
+                                ),
+                                "vote_method": vote_type_label,
+                                "candidate_name": cand_name,
+                                "candidate_party": cand_party,
+                                "recorded_votes": cand_votes[county_name],
+                                "timestamp_last_updated": timestamp,
                             }
 
                             county_votes.append(result)
 
         return county_votes
 
     def _fetch_and_parse_summary_results(
-        self, administrator: str, election_id: str, version_num: str, county=''
+        self, administrator: str, election_id: str, version_num: str, county=""
     ) -> t.List[t.Dict]:
         """
         Fetches the summary results CSV file from the Scytl site and parses it
         into a list of election results by candidate.
 
         `Args`:
             administrator: str
@@ -389,36 +409,41 @@
                 The name of the county associated with the summary file
         `Returns`:
             list[dict]
             The list of election results by candidate.
         """
 
         summary_csv_zip_url = SUMMARY_CSV_ZIP_URL_TEMPLATE.format(
-            administrator=administrator, election_id=election_id, version_num=version_num
+            administrator=administrator,
+            election_id=election_id,
+            version_num=version_num,
         )
 
-        zip_bytes = self._parse_file_from_zip_url(summary_csv_zip_url, 'summary.csv')
+        zip_bytes = self._parse_file_from_zip_url(summary_csv_zip_url, "summary.csv")
 
-        string_buffer = StringIO(zip_bytes.decode('latin-1'))
+        string_buffer = StringIO(zip_bytes.decode("latin-1"))
         csv_data = csv.DictReader(string_buffer, delimiter=",")
 
-        data = [{
-            'state': self.state,
-            'county_name': county or self.county,
-            'office': x.get('contest name'),
-            'ballots_cast': x.get('ballots cast'),
-            'reg_voters': x.get('registered voters'),
-            'counties_reporting': x.get('num Area rptg'),
-            'total_counties': x.get('num Area total'),
-            'precincts_reporting': x.get('num Precinct rptg'),
-            'total_precincts': x.get('num Precinct total'),
-            'candidate_name': x.get('choice name'),
-            'candidate_party': x.get('party name'),
-            'recorded_votes': x.get('total votes')
-        } for x in csv_data]
+        data = [
+            {
+                "state": self.state,
+                "county_name": county or self.county,
+                "office": x.get("contest name"),
+                "ballots_cast": x.get("ballots cast"),
+                "reg_voters": x.get("registered voters"),
+                "counties_reporting": x.get("num Area rptg"),
+                "total_counties": x.get("num Area total"),
+                "precincts_reporting": x.get("num Precinct rptg"),
+                "total_precincts": x.get("num Precinct total"),
+                "candidate_name": x.get("choice name"),
+                "candidate_party": x.get("party name"),
+                "recorded_votes": x.get("total votes"),
+            }
+            for x in csv_data
+        ]
 
         return data
 
     def get_summary_results(self, force_update=False) -> t.List[t.Dict]:
         """
         Fetch the latest summary results for the given election, across all contests.
 
@@ -520,41 +545,42 @@
 
         version_num = self._get_version(self.administrator, self.election_id)
 
         if not force_update and version_num == self.previous_details_version_num:
             return
 
         detail_xml_url = DETAIL_XML_ZIP_URL_TEMPLATE.format(
-            administrator=self.administrator, election_id=self.election_id, version_num=version_num
+            administrator=self.administrator,
+            election_id=self.election_id,
+            version_num=version_num,
         )
 
         parsed_data = []
 
-        county_data = self._parse_file_from_zip_url(detail_xml_url, 'detail.xml')
+        county_data = self._parse_file_from_zip_url(detail_xml_url, "detail.xml")
 
         if self.county:
             county_details = CountyDetails(
-                self.state,
-                self.county,
-                self.election_id,
-                version_num
+                self.state, self.county, self.election_id, version_num
             )
 
-            parsed_data = self._parse_county_xml_data_to_precincts(county_data, county_details)
+            parsed_data = self._parse_county_xml_data_to_precincts(
+                county_data, county_details
+            )
         else:
-            parsed_data = self._parse_state_xml_data_to_counties(county_data, self.state)
+            parsed_data = self._parse_state_xml_data_to_counties(
+                county_data, self.state
+            )
 
         self.previous_details_version_num = version_num
 
         return parsed_data
 
     def get_detailed_results_for_participating_counties(
-        self,
-        county_names: t.List[str] = None,
-        force_update=False
+        self, county_names: t.List[str] = None, force_update=False
     ) -> t.Tuple[t.List[str], t.List[t.Dict]]:
         """
         Fetch the latest detailed results for the given election for all participating counties
             with detailed results, across all contests.
 
         Some counties may not have detailed results. If so, this will attempt
             to fetch the summary results for that county. If no results exist for either,
@@ -615,49 +641,55 @@
         fetched_counties = []
         missing_counties = []
 
         for county_name, county_details in county_details_list.items():
             if county_names and county_name not in county_names:
                 continue
 
-            if not force_update and \
-                county_name in self.previously_fetched_counties and \
-                self.previous_county_details_list and \
-                county_details.county_update_date <= \
-                    self.previous_county_details_list[county_name].county_update_date:
+            if (
+                not force_update
+                and county_name in self.previously_fetched_counties
+                and self.previous_county_details_list
+                and county_details.county_update_date
+                <= self.previous_county_details_list[county_name].county_update_date
+            ):
                 continue
 
             detail_xml_url = COUNTY_DETAIL_XML_ZIP_URL_TEMPLATE.format(
                 state=county_details.state,
                 county_name=county_details.county_name,
                 county_election_id=county_details.county_election_id,
-                county_version_num=county_details.county_version_num
+                county_version_num=county_details.county_version_num,
             )
 
             try:
-                county_data = self._parse_file_from_zip_url(detail_xml_url, 'detail.xml')
+                county_data = self._parse_file_from_zip_url(
+                    detail_xml_url, "detail.xml"
+                )
 
             except requests.exceptions.RequestException:
                 try:
                     summary_data = self._fetch_and_parse_summary_results(
                         f"{self.state}/{county_name}",
                         county_details.county_election_id,
                         county_details.county_version_num,
-                        county_name
+                        county_name,
                     )
 
                 except requests.exceptions.RequestException:
                     missing_counties.append(county_name)
 
                 else:
                     if len(summary_data) > 0:
                         parsed_data += summary_data
 
             else:
-                parsed_data += self._parse_county_xml_data_to_precincts(county_data, county_details)
+                parsed_data += self._parse_county_xml_data_to_precincts(
+                    county_data, county_details
+                )
 
                 fetched_counties.append(county_name)
 
         self.previous_county_details_version_num = version_num
         self.previous_county_details_list = county_details_list
         self.previously_fetched_counties = set(fetched_counties)
```

### Comparing `parsons-1.0.0/parsons/sftp/sftp.py` & `parsons-1.1.0/parsons/sftp/sftp.py`

 * *Files 3% similar despite different names*

```diff
@@ -64,15 +64,15 @@
 
         transport.connect(username=self.username, password=self.password, pkey=pkey)
         conn = paramiko.SFTPClient.from_transport(transport)
         yield conn
         conn.close()
         transport.close()
 
-    def list_directory(self, remote_path='.', connection=None):
+    def list_directory(self, remote_path=".", connection=None):
         """
         List the contents of a directory
 
         `Args:`
             remote_path: str
                 The remote path of the directory
             connection: obj
@@ -147,16 +147,22 @@
         else:
             with self.create_connection() as connection:
                 connection.get(remote_path, local_path)
 
         return local_path
 
     @connect
-    def get_files(self, files_to_download=None, remote=None, connection=None, pattern=None,
-                  local_paths=None):
+    def get_files(
+        self,
+        files_to_download=None,
+        remote=None,
+        connection=None,
+        pattern=None,
+        local_paths=None,
+    ):
         """
         Download a list of files, either by providing the list explicitly, providing directories
         that contain files to download, or both.
 
         `Args:`
             files_to_download: list
                 A list of full remote paths (can be relative) to files to download
@@ -172,34 +178,41 @@
                 the same length as the files to be fetched, temporary files are used instead.
         `Returns:`
             list
                 Local paths where the files are saved.
         """
 
         if not (files_to_download or remote):
-            raise ValueError("You must provide either `files_to_download`, `remote`, or both, as "
-                             "an argument to `get_files`.")
+            raise ValueError(
+                "You must provide either `files_to_download`, `remote`, or both, as "
+                "an argument to `get_files`."
+            )
 
         if not files_to_download:
             files_to_download = []
 
         if remote:
             try:  # assume `remote` is a str
                 files_to_download.extend(self.list_files(remote, connection, pattern))
             except TypeError:  # if it's not a str it's a list
                 files_to_download.extend(
-                    f for file_list in [self.list_files(directory, connection, pattern)
-                                        for directory in remote]
+                    f
+                    for file_list in [
+                        self.list_files(directory, connection, pattern)
+                        for directory in remote
+                    ]
                     for f in file_list
                 )
 
         if local_paths and len(local_paths) != len(files_to_download):
-            logger.warning("You provided a list of local paths for your files but it was not "
-                           "the same length as the files you are going to download.\nDefaulting to "
-                           "temporary files.")
+            logger.warning(
+                "You provided a list of local paths for your files but it was not "
+                "the same length as the files you are going to download.\nDefaulting to "
+                "temporary files."
+            )
             local_paths = []
 
         if local_paths:
             return [
                 self.get_file(remote_path, local_path, connection)
                 for local_path, remote_path in zip(local_paths, files_to_download)
             ]
@@ -224,15 +237,15 @@
                 An SFTP connection object
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
         if not file_utilities.valid_table_suffix(remote_path):
-            raise ValueError('File type cannot be converted to a Parsons table.')
+            raise ValueError("File type cannot be converted to a Parsons table.")
 
         return Table.from_csv(self.get_file(remote_path, connection=connection))
 
     def put_file(self, local_path, remote_path, connection=None):
         """
         Put a file on the SFTP server
 
@@ -277,35 +290,39 @@
                 An SFTP connection object
         `Returns:`
             int
                 The file size in MB.
         """
 
         if connection:
-            size = connection.file(remote_path, 'r')._get_size()
+            size = connection.file(remote_path, "r")._get_size()
         else:
             with self.create_connection() as connection:
-                size = connection.file(remote_path, 'r')._get_size()
+                size = connection.file(remote_path, "r")._get_size()
 
         return size / 1024
 
     @staticmethod
     def _list_contents(remote_path, connection, dir_pattern=None, file_pattern=None):
 
         dirs_to_return = []
         files_to_return = []
-        dirs_and_files = [(S_ISDIR, dir_pattern, True, dirs_to_return),
-                          (S_ISREG, file_pattern, False, files_to_return)]
+        dirs_and_files = [
+            (S_ISDIR, dir_pattern, True, dirs_to_return),
+            (S_ISREG, file_pattern, False, files_to_return),
+        ]
 
         try:
             for entry in connection.listdir_attr(remote_path):
                 entry_pathname = remote_path + "/" + entry.filename
                 for method, pattern, do_search_full_path, paths in dirs_and_files:
                     string = entry_pathname if do_search_full_path else entry.filename
-                    if method(entry.st_mode) and (not pattern or re.search(pattern, string)):
+                    if method(entry.st_mode) and (
+                        not pattern or re.search(pattern, string)
+                    ):
                         paths.append(entry_pathname)
         except FileNotFoundError:  # This error is raised when a directory is empty
             pass
         return dirs_to_return, files_to_return
 
     @connect
     def list_subdirectories(self, remote_path, connection=None, pattern=None):
@@ -342,16 +359,23 @@
         `Returns:`
             list
                 The files in `remote_path`.
         """
         return self._list_contents(remote_path, connection, file_pattern=pattern)[1]
 
     @connect
-    def walk_tree(self, remote_path, connection=None, download=False, dir_pattern=None,
-                  file_pattern=None, max_depth=2):
+    def walk_tree(
+        self,
+        remote_path,
+        connection=None,
+        download=False,
+        dir_pattern=None,
+        file_pattern=None,
+        max_depth=2,
+    ):
         """
         Recursively walks a directory, fetching all subdirectories and files (as long as
         they match `dir_pattern` and `file_pattern`, respectively) and the maximum directory
         depth hasn't been exceeded. Optionally downloads discovered files.
 
         `Args:`
             remote_path: str
@@ -372,40 +396,64 @@
         `Returns:`
             tuple
                 A list of directories touched and a list of files.  If the files were downloaded
                 the file list will consist of local paths, if not, remote paths.
         """
 
         if max_depth > 3:
-            logger.warning("Calling `walk_tree` with `max_depth` {}.  "
-                           "Recursively walking a remote directory will be much slower than a "
-                           "similar operation on a local file system.".format(max_depth))
-
-        to_return = self._walk_tree(remote_path, connection, download, dir_pattern, file_pattern,
-                                    max_depth=max_depth)
+            logger.warning(
+                "Calling `walk_tree` with `max_depth` {}.  "
+                "Recursively walking a remote directory will be much slower than a "
+                "similar operation on a local file system.".format(max_depth)
+            )
+
+        to_return = self._walk_tree(
+            remote_path,
+            connection,
+            download,
+            dir_pattern,
+            file_pattern,
+            max_depth=max_depth,
+        )
 
         return to_return
 
-    def _walk_tree(self, remote_path, connection, download=False, dir_pattern=None,
-                   file_pattern=None, depth=0, max_depth=2):
+    def _walk_tree(
+        self,
+        remote_path,
+        connection,
+        download=False,
+        dir_pattern=None,
+        file_pattern=None,
+        depth=0,
+        max_depth=2,
+    ):
 
         dir_list = []
         file_list = []
 
         depth += 1
 
-        dirs, files = self._list_contents(remote_path, connection, dir_pattern, file_pattern)
+        dirs, files = self._list_contents(
+            remote_path, connection, dir_pattern, file_pattern
+        )
 
         if download:
             self.get_files(files_to_download=files)
 
         if depth < max_depth:
             for directory in dirs:
                 deeper_dirs, deeper_files = self._walk_tree(
-                    directory, connection, download, dir_pattern, file_pattern, depth, max_depth
+                    directory,
+                    connection,
+                    download,
+                    dir_pattern,
+                    file_pattern,
+                    depth,
+                    max_depth,
                 )
                 dir_list.extend(deeper_dirs)
                 file_list.extend(deeper_files)
 
         dir_list.extend(dirs)
         file_list.extend(files)
```

### Comparing `parsons-1.0.0/parsons/sftp/utilities.py` & `parsons-1.1.0/parsons/sftp/utilities.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,23 +1,23 @@
 from functools import wraps
 import paramiko
 
 
 def connection_exists(args, kwargs):
     if any([isinstance(arg, paramiko.sftp_client.SFTPClient) for arg in args]):
         return True
-    if 'connection' in kwargs and kwargs['connection']:
+    if "connection" in kwargs and kwargs["connection"]:
         return True
     return False
 
 
 def connect(func):
     @wraps(func)
     def wrapper(*args, **kwargs):
         if not connection_exists(args, kwargs):
             with args[0].create_connection() as connection:
-                kwargs['connection'] = connection
+                kwargs["connection"] = connection
                 return func(*args, **kwargs)
         else:
             return func(*args, **kwargs)
 
     return wrapper
```

### Comparing `parsons-1.0.0/parsons/shopify/shopify.py` & `parsons-1.1.0/parsons/shopify/shopify.py`

 * *Files 14% similar despite different names*

```diff
@@ -5,63 +5,92 @@
 from parsons.utilities import check_env
 from parsons.utilities.api_connector import APIConnector
 
 
 class Shopify(object):
     """
     Instantiate the Shopify class
-
     `Args:`
         subdomain: str
             The Shopify subdomain (e.g. ``myorg`` for myorg.myshopify.com) Not required if
             ``SHOPIFY_SUBDOMAIN`` env variable set.
         password: str
             The Shopify account password. Not required if ``SHOPIFY_PASSWORD`` env
             variable set.
         api_key: str
             The Shopify account API key. Not required if ``SHOPIFY_API_KEY`` env variable
             set.
         api_version: str
             The Shopify API version. Not required if ``SHOPIFY_API_VERSION`` env variable
             set.
+        access_token: str
+            The Shopify access token.  Not required if ``SHOPIFY_ACCESS_TOKEN`` env
+            variable set. If argument or env variable is set, password and api_key
+            are ignored.
     `Returns:`
         Shopify Class
     """
-    def __init__(self, subdomain=None, password=None, api_key=None, api_version=None):
-        self.subdomain = check_env.check('SHOPIFY_SUBDOMAIN', subdomain)
-        self.password = check_env.check('SHOPIFY_PASSWORD', password)
-        self.api_key = check_env.check('SHOPIFY_API_KEY', api_key)
-        self.api_version = check_env.check('SHOPIFY_API_VERSION', api_version)
-        self.base_url = 'https://%s.myshopify.com/admin/api/%s/' % (
-            self.subdomain, self.api_version
+
+    def __init__(
+        self,
+        subdomain=None,
+        password=None,
+        api_key=None,
+        api_version=None,
+        access_token=None,
+    ):
+        self.subdomain = check_env.check("SHOPIFY_SUBDOMAIN", subdomain)
+        self.access_token = check_env.check(
+            "SHOPIFY_ACCESS_TOKEN", access_token, optional=True
         )
-        self.client = APIConnector(self.base_url, auth=(self.api_key, self.password))
+        self.password = check_env.check("SHOPIFY_PASSWORD", password, optional=True)
+        self.api_key = check_env.check("SHOPIFY_API_KEY", api_key, optional=True)
+        self.api_version = check_env.check("SHOPIFY_API_VERSION", api_version)
+        self.base_url = "https://%s.myshopify.com/admin/api/%s/" % (
+            self.subdomain,
+            self.api_version,
+        )
+        if self.access_token is None and (
+            self.password is None or self.api_key is None
+        ):
+            raise KeyError("Must set either access_token or both api_key and password.")
+        if self.access_token is not None:
+            self.client = APIConnector(
+                self.base_url, headers={"X-Shopify-Access-Token": access_token}
+            )
+        else:
+            self.client = APIConnector(
+                self.base_url, auth=(self.api_key, self.password)
+            )
 
     def get_count(self, query_date=None, since_id=None, table_name=None):
         """
         Get the count of rows in a table.
-
         `Args:`
             query_date: str
                 Filter query by a date that rows were created. This filter is ignored if value
                 is None.
             since_id: str
                 Filter query by a minimum ID. This filter is ignored if value is None.
             table_name: str
                 The name of the Shopify table to query.
         `Returns:`
             int
         """
-        return self.client.request(self.get_query_url(query_date, since_id, table_name),
-                                   'GET').json().get("count", 0)
+        return (
+            self.client.request(
+                self.get_query_url(query_date, since_id, table_name), "GET"
+            )
+            .json()
+            .get("count", 0)
+        )
 
     def get_orders(self, query_date=None, since_id=None, completed=True):
         """
         Get Shopify orders.
-
         `Args:`
             query_date: str
                 Filter query by a date that rows were created. Format: yyyy-mm-dd. This filter
                 is ignored if value is None.
             since_id: str
                 Filter query by a minimum ID. This filter is ignored if value is None.
             completed: bool
@@ -71,118 +100,128 @@
         """
         orders = []
 
         def _append_orders(url):
             nonlocal orders
 
             if completed:
-                url += '&financial_status=paid'
+                url += "&financial_status=paid"
 
-            res = self.client.request(url, 'GET')
+            res = self.client.request(url, "GET")
 
             cur_orders = res.json().get("orders", [])
 
             # Flatten orders to non-complex types
             for order in cur_orders:
                 keys_to_add = {}
                 keys_to_delete = []
 
                 for key1 in order:
                     if isinstance(order[key1], dict):
                         for key2 in order[key1]:
-                            keys_to_add[key1 + '_' + key2] = order[key1][key2]
+                            keys_to_add[key1 + "_" + key2] = order[key1][key2]
                         keys_to_delete.append(key1)
-                    elif key1 == 'note_attributes':
+                    elif key1 == "note_attributes":
                         for note in order[key1]:
-                            keys_to_add[key1 + '_' + note['name']] = note['value']
+                            keys_to_add[key1 + "_" + note["name"]] = note["value"]
 
                 order.update(keys_to_add)
                 for key in keys_to_delete:
                     del order[key]
 
             orders += cur_orders
 
             return res
 
         res = _append_orders(self.get_query_url(query_date, since_id, "orders", False))
 
         # Get next page
         while res.headers.get("Link"):
-            link = re.split('; |, ', res.headers.get("Link"))
+            link = re.split("; |, ", res.headers.get("Link"))
             if len(link) and link[len(link) - 1] == 'rel="next"':
                 res = _append_orders(link[len(link) - 2][1:-1])
             else:
                 break
 
         return Table(orders)
 
-    def get_query_url(self, query_date=None, since_id=None, table_name=None, count=True):
+    def get_query_url(
+        self, query_date=None, since_id=None, table_name=None, count=True
+    ):
         """
         Get the URL of a Shopify API request
-
         `Args:`
             query_date: str
                 Filter query by a date that rows were created. Format: yyyy-mm-dd. This filter
                 is ignored if value is None.
             since_id: str
                 Filter query by a minimum ID. This filter is ignored if value is None.
             table_name: str
                 The name of the Shopify table to query.
             count: bool
                 True if refund should be included in Table, False otherwise.
         `Returns:`
             str
         """
-        filters = 'limit=250&status=any'
+        filters = "limit=250&status=any"
 
         if count:
-            table = table_name + '/count.json'
+            table = table_name + "/count.json"
         else:
-            table = table_name + '.json'
+            table = table_name + ".json"
 
         if query_date:
             # Specific date if provided
             query_date = datetime.strptime(query_date, "%Y-%m-%d")
             max_date = query_date + timedelta(days=1)
-            filters += '&created_at_min={}&created_at_max={}'.format(query_date.isoformat(),
-                                                                     max_date.isoformat())
+            filters += "&created_at_min={}&created_at_max={}".format(
+                query_date.isoformat(), max_date.isoformat()
+            )
         elif since_id:
             # Since ID if provided
-            filters += '&since_id=%s' % since_id
+            filters += "&since_id=%s" % since_id
 
-        return self.base_url + '%s?%s' % (table, filters)
+        return self.base_url + "%s?%s" % (table, filters)
 
     def graphql(self, query):
         """
         Make GraphQL request. Reference: https://shopify.dev/api/admin-graphql
-
         `Args:`
             query: str
                 GraphQL query.
         `Returns:`
             dict
         """
-        return self.client.request(
-            self.base_url + 'graphql.json', 'POST', json={"query": query}
-        ).json().get('data')
+        return (
+            self.client.request(
+                self.base_url + "graphql.json", "POST", json={"query": query}
+            )
+            .json()
+            .get("data")
+        )
 
     @classmethod
-    def load_to_table(cls, subdomain=None, password=None, api_key=None, api_version=None,
-                      query_date=None, since_id=None, completed=True):
+    def load_to_table(
+        cls,
+        subdomain=None,
+        password=None,
+        api_key=None,
+        api_version=None,
+        query_date=None,
+        since_id=None,
+        completed=True,
+    ):
         """
         Fast classmethod so you can get the data all at once:
-
             tabledata = Shopify.load_to_table(subdomain='myorg', password='abc123',
                                             api_key='abc123', api_version='2020-10',
                                             query_date='2020-10-20', since_id='8414',
                                             completed=True)
-
         This instantiates the class and makes the appropriate query type to Shopify's orders
         table based on which arguments are supplied.
-
         `Args:`
             subdomain: str
                 The Shopify subdomain (e.g. ``myorg`` for myorg.myshopify.com).
             password: str
                 The Shopify account password.
             api_key: str
                 The Shopify account API key.
@@ -195,9 +234,10 @@
                 Filter query by a minimum ID. This filter is ignored if value is None.
             completed: bool
                 True if only getting completed orders, False otherwise.
                 value as value
         `Returns:`
             Table Class
         """
-        return cls(subdomain, password, api_key, api_version).get_orders(query_date, since_id,
-                                                                         completed)
+        return cls(subdomain, password, api_key, api_version).get_orders(
+            query_date, since_id, completed
+        )
```

### Comparing `parsons-1.0.0/parsons/sisense/sisense.py` & `parsons-1.1.0/parsons/sisense/sisense.py`

 * *Files 14% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 import logging
 
 from parsons.utilities import check_env
 from parsons.utilities.api_connector import APIConnector
 
 logger = logging.getLogger(__name__)
 
-URI = 'https://app.periscopedata.com/api/v1/'
+URI = "https://app.periscopedata.com/api/v1/"
 
 
 class Sisense(object):
     """
     Instantiate the Sisense class.
 
     `Args:`
@@ -21,21 +21,21 @@
             The Sisense API Key. Not required if the ``SISENSE_API_KEY``
             environmental variable is set.
     `Returns:`
         Sisense class
     """
 
     def __init__(self, site_name=None, api_key=None):
-        self.site_name = check_env.check('SISENSE_SITE_NAME', site_name)
-        self.api_key = check_env.check('SISENSE_API_KEY', api_key)
+        self.site_name = check_env.check("SISENSE_SITE_NAME", site_name)
+        self.api_key = check_env.check("SISENSE_API_KEY", api_key)
         self.uri = URI
         self.api = self._api()
 
     def _api(self):
-        headers = {'HTTP-X-PARTNER-AUTH': self.site_name + ":" + self.api_key}
+        headers = {"HTTP-X-PARTNER-AUTH": self.site_name + ":" + self.api_key}
         return APIConnector(uri=self.uri, headers=headers)
 
     def publish_shared_dashboard(self, dashboard_id, chart_id=None, **kwargs):
         """
         This method publishes a dashboard or chart using the provided arguments.
         For available options, see the `API documentation <https://dtdocs.sisense.com/article/embed-api-options>`_. # noqa
 
@@ -45,29 +45,31 @@
             chart_id: str or int
                 The ID of the chart. Only required for publishing individual charts.
             **kwargs:
                 Optional arguments.
         `Returns:`
             Response (dict containing the URL) or an error
         """
-        payload = {'dashboard': dashboard_id, 'chart': chart_id, **kwargs}
-        return self.api.post_request('shared_dashboard/create', data=json.dumps(payload))
+        payload = {"dashboard": dashboard_id, "chart": chart_id, **kwargs}
+        return self.api.post_request(
+            "shared_dashboard/create", data=json.dumps(payload)
+        )
 
     def list_shared_dashboards(self, dashboard_id):
         """
         List all shares of a given dashboard.
 
         `Args:`
             dashboard_id: str or int
                 The ID the dashboard (required).
         `Returns:`
             Response or an error
         """
-        payload = {'dashboard': dashboard_id}
-        return self.api.post_request('shared_dashboard/list', data=json.dumps(payload))
+        payload = {"dashboard": dashboard_id}
+        return self.api.post_request("shared_dashboard/list", data=json.dumps(payload))
 
     def delete_shared_dashboard(self, token):
         """
         To delete a shared dashboard you must provide the token for the shared dashboard.
         The token is the last part of the shared dashboard URL. i.e. if the shared URL is:
 
         https://app.periscopedata.com/shared/9dda9dda-9dda-9dda-9dda-9dda9dda9dda
@@ -76,9 +78,11 @@
 
         `Args:`
             token: str or int
                 The token of the shared dashboard (required).
         `Returns:`
             Response or an error
         """
-        payload = {'token': token}
-        return self.api.post_request('shared_dashboard/delete', data=json.dumps(payload))
+        payload = {"token": token}
+        return self.api.post_request(
+            "shared_dashboard/delete", data=json.dumps(payload)
+        )
```

### Comparing `parsons-1.0.0/parsons/targetsmart/targetsmart_api.py` & `parsons-1.1.0/parsons/targetsmart/targetsmart_api.py`

 * *Files identical despite different names*

### Comparing `parsons-1.0.0/parsons/targetsmart/targetsmart_automation.py` & `parsons-1.1.0/parsons/targetsmart/targetsmart_automation.py`

 * *Files identical despite different names*

### Comparing `parsons-1.0.0/parsons/targetsmart/targetsmart_smartmatch.py` & `parsons-1.1.0/parsons/targetsmart/targetsmart_smartmatch.py`

 * *Files identical despite different names*

### Comparing `parsons-1.0.0/parsons/tools/credential_tools.py` & `parsons-1.1.0/parsons/tools/credential_tools.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 from base64 import b64encode, b64decode
 import click
 import json
 import os
 
-PREFIX = 'PRSNSENV'
+PREFIX = "PRSNSENV"
 
 
 def decode_credential(credential, save_path=None, export=True, echo=False):
     """Decode an encoded credential to a Python object.
 
     `Args:`
         credential: str
@@ -23,21 +23,22 @@
         dict
             The decoded object.
     """
     x = len(PREFIX)
     if credential[:x] != PREFIX:
         raise ValueError("Invalid Parsons variable.")
 
-    decoded_str = b64decode(
-        bytes(credential.replace(PREFIX, ""), "utf-8")).decode("utf-8")
+    decoded_str = b64decode(bytes(credential.replace(PREFIX, ""), "utf-8")).decode(
+        "utf-8"
+    )
 
     decoded_dict = json.loads(decoded_str)
 
     if save_path:
-        with open(save_path, 'w') as f:
+        with open(save_path, "w") as f:
             f.write(json.dumps(decoded_dict))
 
     if export:
         for key, val in decoded_dict.items():
             os.environ[key] = str(val)
 
     if echo:
@@ -70,15 +71,15 @@
     `Args:`
         credential_file: str
             The path to the json file with the credential to be encoded.
     `Returns:`
         str
             The encoded credential.
     """
-    with open(credential_file, 'r') as f:
+    with open(credential_file, "r") as f:
         data = json.load(f)
 
     json_str = json.dumps(data)
     encoded_str = PREFIX + b64encode(bytes(json_str, "utf-8")).decode("utf-8")
 
     return encoded_str
 
@@ -115,31 +116,53 @@
     """
     data_str = json.dumps(credential)
     encoded_str = PREFIX + b64encode(bytes(data_str, "utf-8")).decode("utf-8")
 
     return encoded_str
 
 
-@click.command(options_metavar='[-e [-f] | -d [-xp] [-o <file>]]')
-@click.argument('credential', metavar='credential')
-@click.option('--encode', '-e', 'fn', flag_value='encode',
-              default=True, help="Endcode a credential.")
-@click.option('--decode', '-d', 'fn', flag_value='decode',
-              help='Decode an encoded credential.')
-@click.option('-f', 'is_file', is_flag=True, help=("Treat <credential> as a "
-              "path to a file. Only valid with --encode."))
-@click.option('-o', 'save_path', default='', metavar='<file>',
-              help="The path for where to save the decoded credential.")
-@click.option('-x', 'no_export', is_flag=True, default=False,
-              help=("Do not export the variable to the environment. Only "
-                    "valid with --decode."))
-@click.option('-s', 'suppress', is_flag=True, default=False, help=("Suppress "
-              "the output."))
-def main(credential, fn, is_file=False, save_path="", no_export=False,
-         suppress=False):
+@click.command(options_metavar="[-e [-f] | -d [-xp] [-o <file>]]")
+@click.argument("credential", metavar="credential")
+@click.option(
+    "--encode",
+    "-e",
+    "fn",
+    flag_value="encode",
+    default=True,
+    help="Endcode a credential.",
+)
+@click.option(
+    "--decode", "-d", "fn", flag_value="decode", help="Decode an encoded credential."
+)
+@click.option(
+    "-f",
+    "is_file",
+    is_flag=True,
+    help=("Treat <credential> as a " "path to a file. Only valid with --encode."),
+)
+@click.option(
+    "-o",
+    "save_path",
+    default="",
+    metavar="<file>",
+    help="The path for where to save the decoded credential.",
+)
+@click.option(
+    "-x",
+    "no_export",
+    is_flag=True,
+    default=False,
+    help=(
+        "Do not export the variable to the environment. Only " "valid with --decode."
+    ),
+)
+@click.option(
+    "-s", "suppress", is_flag=True, default=False, help=("Suppress " "the output.")
+)
+def main(credential, fn, is_file=False, save_path="", no_export=False, suppress=False):
     """A command line tool to encode and decode credentials.
 
     Use this tool when the credentials for a service are split into multiple
     parts, and you'd rather deal with them as one variable. For example, to
     connect to a Redshift database, you need 5 parts: username, password,
     database name, host name, and port. With this tool, you could encode those
     5 pieces into one string variable, which can be stored as an environment
@@ -159,27 +182,27 @@
     # Encoding a json file.
     `python env_tools.py -e -f /path/to/credentials.json`
 
     \b
     # Encoding a list currenct environment variables.
     `python env_tools.py -e env_var1,env_var2,env_ var3`
     """
-    if fn == 'encode':
+    if fn == "encode":
         if is_file:
             enc_cred = encode_from_json_file(credential)
         else:
             try:
                 cred = json.loads(credential)
                 enc_cred = encode_from_dict(cred)
             except json.decoder.JSONDecodeError:
-                cred = credential.split(',')
+                cred = credential.split(",")
                 enc_cred = encode_from_env(cred)
         if not suppress:
             print(enc_cred)
-    elif fn == 'decode':
+    elif fn == "decode":
         decode_credential(credential, save_path, not no_export, not suppress)
     else:
         raise ValueError("Invalid function selected. Use --help for help.")
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     main()
```

### Comparing `parsons-1.0.0/parsons/turbovote/turbovote.py` & `parsons-1.1.0/parsons/turbovote/turbovote.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 from parsons.etl import Table
 import requests
 import logging
 from parsons.utilities import check_env
 
 logger = logging.getLogger(__name__)
 
-TURBOVOTE_URI = 'https://turbovote-admin-http-api.prod.democracy.works/'
+TURBOVOTE_URI = "https://turbovote-admin-http-api.prod.democracy.works/"
 
 
 class TurboVote(object):
     """
     Instantiate the TurboVote class
 
     `Args:`
@@ -24,43 +24,42 @@
             required if ``TURBOVOTE_SUBDOMAIN`` env variable set.
     `Returns:`
         class
     """
 
     def __init__(self, username=None, password=None, subdomain=None):
 
-        self.username = check_env.check('TURBOVOTE_USERNAME', username)
-        self.password = check_env.check('TURBOVOTE_PASSWORD', password)
-        self.subdomain = check_env.check('TURBOVOTE_SUBDOMAIN', subdomain)
+        self.username = check_env.check("TURBOVOTE_USERNAME", username)
+        self.password = check_env.check("TURBOVOTE_PASSWORD", password)
+        self.subdomain = check_env.check("TURBOVOTE_SUBDOMAIN", subdomain)
         self.uri = TURBOVOTE_URI
 
     def _get_token(self):
         # Retrieve a temporary bearer token to access API
 
-        url = self.uri + 'login'
-        payload = {'username': self.username,
-                   'password': self.password}
+        url = self.uri + "login"
+        payload = {"username": self.username, "password": self.password}
         r = requests.post(url, data=payload)
         logger.debug(r.url)
         r.raise_for_status()
 
-        return r.json()['id-token']
+        return r.json()["id-token"]
 
     def get_users(self):
         """
         Get users.
 
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        url = self.uri + f'partners/{self.subdomain}.turbovote.org/users'
+        url = self.uri + f"partners/{self.subdomain}.turbovote.org/users"
 
         headers = {"Authorization": f"Bearer {self._get_token()}"}
         r = requests.get(url, headers=headers)
         logger.debug(r)
         r.raise_for_status()
         tbl = Table.from_csv_string(r.text)
-        logger.info(f'{tbl.num_rows} users retrieved.')
+        logger.info(f"{tbl.num_rows} users retrieved.")
 
         return tbl
```

### Comparing `parsons-1.0.0/parsons/twilio/twilio.py` & `parsons-1.1.0/parsons/twilio/twilio.py`

 * *Files 10% similar despite different names*

```diff
@@ -20,23 +20,23 @@
             passed.
     `Returns`:
         Twilio class
     """
 
     def __init__(self, account_sid=None, auth_token=None):
 
-        self.account_sid = check_env.check('TWILIO_ACCOUNT_SID', account_sid)
-        self.auth_token = check_env.check('TWILIO_AUTH_TOKEN', auth_token)
+        self.account_sid = check_env.check("TWILIO_ACCOUNT_SID", account_sid)
+        self.auth_token = check_env.check("TWILIO_AUTH_TOKEN", auth_token)
         self.client = Client(self.account_sid, self.auth_token)
 
     def _table_convert(self, obj):
-        tbl = Table([x.__dict__['_properties'] for x in obj])
+        tbl = Table([x.__dict__["_properties"] for x in obj])
 
-        if 'subresource_uris' in tbl.columns and 'uri' in tbl.columns:
-            tbl.remove_column('subresource_uris', 'uri')
+        if "subresource_uris" in tbl.columns and "uri" in tbl.columns:
+            tbl.remove_column("subresource_uris", "uri")
 
         return tbl
 
     def get_account(self, account_sid):
         """
         Get Twilio account
 
@@ -44,15 +44,15 @@
             account_sid: str
                 The Twilio account sid
         `Returns:`
             dict
         """
 
         r = self.client.api.accounts(account_sid)
-        logger.info(f'Retrieved {account_sid} account.')
+        logger.info(f"Retrieved {account_sid} account.")
         return r.__dict__
 
     def get_accounts(self, name=None, status=None):
         """
         Get Twilio accounts including subaccounts.
 
         `Args:`
@@ -64,19 +64,26 @@
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
         r = self.client.api.accounts.list(friendly_name=name, status=status)
         tbl = self._table_convert(r)
 
-        logger.info(f'Retrieved {tbl.num_rows} accounts.')
+        logger.info(f"Retrieved {tbl.num_rows} accounts.")
         return tbl
 
-    def get_account_usage(self, category=None, start_date=None, end_date=None, time_period=None,
-                          group_by=None, exclude_null=False):
+    def get_account_usage(
+        self,
+        category=None,
+        start_date=None,
+        end_date=None,
+        time_period=None,
+        group_by=None,
+        exclude_null=False,
+    ):
         """
         Get Twilio account usage.
 
         `Args:`
             category: str
                 Filter to a specific type of usage category. The list of possibilities can be found
                 `here <https://www.twilio.com/docs/usage/api/usage-record?code-sample=code-last-months-usage-for-all-usage-categories-4&code-language=Python&code-sdk-version=5.x#usage-all-categories>`_.
@@ -91,51 +98,55 @@
                 The time interval to group usage by. Can be one of ``daily``, ``monthly`` or
                 ``yearly``.
             exclude_null: boolean
                 Exclude rows that have no usage.
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
-        """ # noqa: E501,E261
+        """  # noqa: E501,E261
 
         # Add populated arguments
-        args = {'category': category,
-                'start_date': start_date,
-                'end_date': end_date}
+        args = {"category": category, "start_date": start_date, "end_date": end_date}
         args = json_format.remove_empty_keys(args)
 
         # Parse out the time_periods
-        if time_period == 'today':
+        if time_period == "today":
             r = self.client.usage.records.today.list(**args)
-        elif time_period == 'yesterday':
+        elif time_period == "yesterday":
             r = self.client.usage.records.yesterday.list(**args)
-        elif time_period == 'this_month':
+        elif time_period == "this_month":
             r = self.client.usage.records.this_month.list(**args)
-        elif time_period == 'last_month':
+        elif time_period == "last_month":
             r = self.client.usage.records.last_month.list(**args)
 
         # Parse out the group by
-        elif group_by == 'daily':
+        elif group_by == "daily":
             r = self.client.usage.records.daily.list(**args)
-        elif group_by == 'monthly':
+        elif group_by == "monthly":
             r = self.client.usage.records.monthly.list(**args)
-        elif group_by == 'yearly':
+        elif group_by == "yearly":
             r = self.client.usage.records.yearly.list(**args)
         else:
             r = self.client.usage.records.list(**args)
 
         tbl = self._table_convert(r)
 
         if exclude_null:
-            tbl.remove_null_rows('count', null_value='0')
+            tbl.remove_null_rows("count", null_value="0")
 
         return tbl
 
-    def get_messages(self, to=None, from_=None, date_sent=None, date_sent_before=None,
-                     date_sent_after=None):
+    def get_messages(
+        self,
+        to=None,
+        from_=None,
+        date_sent=None,
+        date_sent_before=None,
+        date_sent_after=None,
+    ):
         """
         Get Twilio messages.
 
         `Args:`
             to: str
                 Filter to messages only sent to the specified phone number.
             from_: str
@@ -147,14 +158,18 @@
             date_sent_after: str
                 Filter to messages only sent after the specified date (ex. ``2019-01-01``).
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        r = self.client.messages.list(to=to, from_=from_, date_sent=date_sent,
-                                      date_sent_before=date_sent_before,
-                                      date_sent_after=date_sent_after)
+        r = self.client.messages.list(
+            to=to,
+            from_=from_,
+            date_sent=date_sent,
+            date_sent_before=date_sent_before,
+            date_sent_after=date_sent_after,
+        )
 
         tbl = self._table_convert(r)
-        logger.info(f'Retrieved {tbl.num_rows} messages.')
+        logger.info(f"Retrieved {tbl.num_rows} messages.")
         return tbl
```

### Comparing `parsons-1.0.0/parsons/utilities/api_connector.py` & `parsons-1.1.0/parsons/utilities/api_connector.py`

 * *Files 2% similar despite different names*

```diff
@@ -28,19 +28,21 @@
         data_key: str
             The name of the key in the response json where the data is contained. Required
             if the data is nested in the response json
     `Returns`:
         APIConnector class
     """
 
-    def __init__(self, uri, headers=None, auth=None, pagination_key=None, data_key=None):
+    def __init__(
+        self, uri, headers=None, auth=None, pagination_key=None, data_key=None
+    ):
 
         # Add a trailing slash if its missing
-        if not uri.endswith('/'):
-            uri = uri + '/'
+        if not uri.endswith("/"):
+            uri = uri + "/"
 
         self.uri = uri
         self.headers = headers
         self.auth = auth
         self.pagination_key = pagination_key
         self.data_key = data_key
 
@@ -68,38 +70,46 @@
                 are looping through data, you might want to ignore individual failures.
 
         `Returns:`
             requests response
         """
         full_url = urllib.parse.urljoin(self.uri, url)
 
-        return _request(req_type, full_url, headers=self.headers, auth=self.auth, json=json,
-                        data=data, params=params)
+        return _request(
+            req_type,
+            full_url,
+            headers=self.headers,
+            auth=self.auth,
+            json=json,
+            data=data,
+            params=params,
+        )
 
     def get_request(self, url, params=None):
         """
         Make a GET request.
 
         Args:
             url: str
                 A complete and valid url for the api request
             params: dict
                 The request parameters
         Returns:
                 A requests response object
         """
 
-        r = self.request(url, 'GET', params=params)
+        r = self.request(url, "GET", params=params)
         self.validate_response(r)
         logger.debug(r.json())
 
         return r.json()
 
-    def post_request(self, url, params=None, data=None, json=None,
-                     success_codes=[200, 201, 202, 204]):
+    def post_request(
+        self, url, params=None, data=None, json=None, success_codes=[200, 201, 202, 204]
+    ):
         """
         Make a POST request.
 
         `Args:`
             url: str
                 A complete and valid url for the api request
             params: dict
@@ -110,15 +120,15 @@
                 A JSON object to post
             success_code: int
                 The expected success code to be returned
         `Returns:`
             A requests response object
         """
 
-        r = self.request(url, 'POST', params=params, data=data, json=json)
+        r = self.request(url, "POST", params=params, data=data, json=json)
 
         # Validate the response and lift up an errors.
         self.validate_response(r)
 
         # Check for a valid success code for the POST. Some APIs return messages with the
         # success code and some do not. Be able to account for both of these types.
         if r.status_code in success_codes:
@@ -138,27 +148,29 @@
                 The request parameters
             success_codes: int
                 The expected success codes to be returned
         Returns:
                 A requests response object or status code
         """
 
-        r = self.request(url, 'DELETE', params=params)
+        r = self.request(url, "DELETE", params=params)
 
         self.validate_response(r)
 
         # Check for a valid success code for the POST. Some APIs return messages with the
         # success code and some do not. Be able to account for both of these types.
         if r.status_code in success_codes:
             if self.json_check(r):
                 return r.json()
             else:
                 return r.status_code
 
-    def put_request(self, url, data=None, json=None, params=None, success_codes=[200, 201, 204]):
+    def put_request(
+        self, url, data=None, json=None, params=None, success_codes=[200, 201, 204]
+    ):
         """
         Make a PUT request.
 
         Args:
             url: str
                 A complete and valid url for the api request
             params: dict
@@ -167,25 +179,27 @@
                 A data object to post
             json: dict
                 A JSON object to post
         Returns:
                 A requests response object
         """
 
-        r = self.request(url, 'PUT', params=params, data=data, json=json)
+        r = self.request(url, "PUT", params=params, data=data, json=json)
 
         self.validate_response(r)
 
         if r.status_code in success_codes:
             if self.json_check(r):
                 return r.json()
             else:
                 return r.status_code
 
-    def patch_request(self, url, params=None, data=None, json=None, success_codes=[200, 201, 204]):
+    def patch_request(
+        self, url, params=None, data=None, json=None, success_codes=[200, 201, 204]
+    ):
         """
         Make a PATCH request.
 
         `Args:`
             url: str
                 A complete and valid url for the api request
             params: dict
@@ -196,15 +210,15 @@
                 A JSON object to post
             success_codes: int
                 The expected success codes to be returned
         `Returns:`
             A requests response object
         """
 
-        r = self.request(url, 'PATCH', params=params, data=data, json=json)
+        r = self.request(url, "PATCH", params=params, data=data, json=json)
 
         self.validate_response(r)
 
         # Check for a valid success code for the POST. Some APIs return messages with the
         # success code and some do not. Be able to account for both of these types.
         if r.status_code in success_codes:
             if self.json_check(r):
@@ -221,21 +235,21 @@
             resp: object
                 A response object
         """
 
         if resp.status_code >= 400:
 
             if resp.reason:
-                message = f'HTTP error occurred ({resp.status_code}): {resp.reason}'
+                message = f"HTTP error occurred ({resp.status_code}): {resp.reason}"
             else:
-                message = f'HTTP error occurred ({resp.status_code})'
+                message = f"HTTP error occurred ({resp.status_code})"
 
             # Some errors return JSONs with useful info about the error. Return it if exists.
             if self.json_check(resp):
-                raise HTTPError(f'{message}, json: {resp.json()}')
+                raise HTTPError(f"{message}, json: {resp.json()}")
             else:
                 raise HTTPError(message)
 
     def data_parse(self, resp):
         """
         Determines if the response json has nested data. If it is nested, it just returns the
         data. This is useful in dealing with requests that might return multiple records, while
```

### Comparing `parsons-1.0.0/parsons/utilities/cloud_storage.py` & `parsons-1.1.0/parsons/utilities/cloud_storage.py`

 * *Files 14% similar despite different names*

```diff
@@ -4,15 +4,17 @@
 This utility method is a generalizable method for moving files to an
 online file storage class. It is used by methods that require access
 to a file via a public url (e.g. VAN). Currently only includes Amazon S3 and
 Google Cloud Storage.
 """
 
 
-def post_file(tbl, type, file_path=None, quoting=csv.QUOTE_MINIMAL, **file_storage_args):
+def post_file(
+    tbl, type, file_path=None, quoting=csv.QUOTE_MINIMAL, **file_storage_args
+):
     """
     This utility method is a generalizable method for moving files to an
     online file storage class. It is used by methods that require access
     to a file via a public url (e.g. VAN).
 
     **S3 is the only option allowed.**
 
@@ -28,23 +30,26 @@
             The type of quoting to use for the csv.
         **kwargs: kwargs
                 Optional arguments specific to the file storage.
     `Returns:`
         ``None``
     """
 
-    if type.upper() == 'S3':
+    if type.upper() == "S3":
 
         # Overwrite the file_path if key is passed
-        if 'key' in file_storage_args:
-            file_storage_args['key'] = file_path
+        if "key" in file_storage_args:
+            file_storage_args["key"] = file_path
 
-        return tbl.to_s3_csv(public_url=True, key=file_path, quoting=quoting, **file_storage_args)
-
-    elif type.upper() == 'GCS':
-
-        return tbl.to_gcs_csv(public_url=True, blob_name=file_path, quoting=quoting,
-                              **file_storage_args)
+        return tbl.to_s3_csv(
+            public_url=True, key=file_path, quoting=quoting, **file_storage_args
+        )
+
+    elif type.upper() == "GCS":
+
+        return tbl.to_gcs_csv(
+            public_url=True, blob_name=file_path, quoting=quoting, **file_storage_args
+        )
 
     else:
 
-        raise ValueError('Type must be S3 or GCS.')
+        raise ValueError("Type must be S3 or GCS.")
```

### Comparing `parsons-1.0.0/parsons/utilities/datetime.py` & `parsons-1.1.0/parsons/utilities/datetime.py`

 * *Files 4% similar despite different names*

```diff
@@ -48,14 +48,15 @@
         parsed = datetime.datetime.fromtimestamp(value, tzinfo)
     elif isinstance(value, datetime.datetime):
         parsed = value
     elif isinstance(value, str):
         parsed = parse(value)
     else:
         raise TypeError(
-            'Unable to parse value; must be one of string or int or datetime, but got type '
-            f'{type(value)}')
+            "Unable to parse value; must be one of string or int or datetime, but got type "
+            f"{type(value)}"
+        )
 
     if not parsed.tzinfo:
         parsed = parsed.replace(tzinfo=tzinfo)
 
     return parsed
```

### Comparing `parsons-1.0.0/parsons/utilities/files.py` & `parsons-1.1.0/parsons/utilities/files.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,21 +1,21 @@
 import errno
 import gzip
 import os
 import shutil
 import tempfile
 
 __all__ = [
-    'create_temp_file',
-    'create_temp_file_for_path',
-    'is_gzip_path',
-    'suffix_for_compression_type',
-    'compression_type_for_path',
-    'string_to_temp_file'
-    ]
+    "create_temp_file",
+    "create_temp_file_for_path",
+    "is_gzip_path",
+    "suffix_for_compression_type",
+    "compression_type_for_path",
+    "string_to_temp_file",
+]
 
 
 # Maximum number of times to try to open a new temp file before giving up.
 TMP_MAX = 1000
 
 
 # This global list keeps track of all temp files created during the runtime of a script.
@@ -73,15 +73,15 @@
         str
             The path of the temp file
     """
 
     # Add the appropriate compression suffix to the file, so other libraries that check the
     # file's extension will know that it is compressed.
     # TODO Make this more robust, maybe even using the entire remote file name as the suffix.
-    suffix = '.gz' if is_gzip_path(path) else None
+    suffix = ".gz" if is_gzip_path(path) else None
     return create_temp_file(suffix=suffix)
 
 
 def close_temp_file(path):
     """
     Force closes a Parsons temp file, which will cause it to be deleted immediately.
 
@@ -149,38 +149,38 @@
     """
     temp_file = TempFile(path)
     _temp_files.append(temp_file)
     return path
 
 
 def is_gzip_path(path):
-    return (path[-3:] == '.gz')
+    return path[-3:] == ".gz"
 
 
 def is_zip_path(path):
-    return (path[-4:] == '.zip')
+    return path[-4:] == ".zip"
 
 
 def is_csv_path(path):
-    return (path[-4:].lower() == '.csv')
+    return path[-4:].lower() == ".csv"
 
 
 def suffix_for_compression_type(compression):
-    if compression == 'gzip':
-        return '.gz'
+    if compression == "gzip":
+        return ".gz"
 
-    return ''
+    return ""
 
 
 def compression_type_for_path(path):
     if is_gzip_path(path):
-        return 'gzip'
+        return "gzip"
 
     if is_zip_path(path):
-        return 'zip'
+        return "zip"
 
     return None
 
 
 def valid_table_suffix(path):
     # Checks if the suffix is valid for conversions to a Parsons table.
 
@@ -200,46 +200,46 @@
     `Returns:`
         str
             The contents of a files.
     """
     compression = compression_type_for_path(path)
 
     open_func = {
-        'gzip': gzip.open,
+        "gzip": gzip.open,
         None: open,
     }
-    with open_func[compression](path, 'r') as fp:
+    with open_func[compression](path, "r") as fp:
         return fp.read()
 
 
 def string_to_temp_file(string, suffix=None):
     """
     Create a temporary file from a string. Currently used for packages
     that require credentials to be stored as a file.
     """
 
     temp_file_path = create_temp_file(suffix=suffix)
 
-    with open(temp_file_path, 'w') as f:
+    with open(temp_file_path, "w") as f:
         f.write(string)
 
     return temp_file_path
 
 
 def zip_check(file_path, compression_type):
     """
     Check if the file suffix or the compression type indicates that it is
     a zip file.
     """
 
     if file_path:
-        if file_path.split('/')[-1].split('.')[-1] == 'zip':
+        if file_path.split("/")[-1].split(".")[-1] == "zip":
             return True
 
-    if compression_type == 'zip':
+    if compression_type == "zip":
         return True
 
     else:
         return False
 
 
 def extract_file_name(file_path=None, include_suffix=True):
@@ -253,17 +253,17 @@
         file name without the suffix (e.g. "myfile.zip" vs. "myfile").
     """
 
     if not file_path:
         return None
 
     if include_suffix:
-        return file_path.split('/')[-1]
+        return file_path.split("/")[-1]
 
-    return file_path.split('/')[-1].split('.')[0]
+    return file_path.split("/")[-1].split(".")[0]
 
 
 def has_data(file_path):
     """
     Check if a file has any data in it.
 
     `Args:`
@@ -299,15 +299,15 @@
     temp_dir = tempfile.gettempdir()
 
     # Try multiple times to create a temp file, just in case (however unlikely) we have some
     # collisions with already existing files.
     for _ in range(TMP_MAX):
         name = next(names)
         if suffix:
-            name = f'{name}{suffix}'
+            name = f"{name}{suffix}"
         path = os.path.join(temp_dir, name)
 
         # Check to see if the path already exists.
         if os.path.exists(path):
             continue
 
         # If we aren't creating it here, then just return the name
@@ -315,23 +315,22 @@
             return path
 
         try:
             # "Touch" the file to ensure that there is a file there, so that if our user tries
             # open it in read mode later, they won't get an error about the file not existing.
             # Also, use mode='x' (exclusive create) to make sure we get an error if the file already
             # exists
-            with open(path, mode='x') as _:
+            with open(path, mode="x") as _:
                 pass
             return path
         # PermissionError can be Windows' way of saying the file exists
         except (FileExistsError, PermissionError):
-            continue    # try again with another filename if we got an error
+            continue  # try again with another filename if we got an error
 
-    raise FileExistsError(errno.EEXIST,
-                          "No usable temporary directory name found")
+    raise FileExistsError(errno.EEXIST, "No usable temporary directory name found")
 
 
 class TempDirectory:
     """
     Class for creating and eventually cleaning up a temporary directory.
 
     Creating the instance of the TempDirectory will create a uniquely named temporary dir. When the
```

### Comparing `parsons-1.0.0/parsons/utilities/json_format.py` & `parsons-1.1.0/parsons/utilities/json_format.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,20 +1,19 @@
-
 def arg_format(arg):
     """
     Many APIs require arguments to formatted like this 'thisTypeConfig' which is not the standard
     for python so this method takes an argument 'this_type_config' and returns it as
     'thisTypeConfig'
     """
 
-    arg_list = arg.split('_')
+    arg_list = arg.split("_")
     arg_list = [a.capitalize() for a in arg_list]
     arg_list[0] = arg_list[0].lower()
 
-    return ''.join(arg_list)
+    return "".join(arg_list)
 
 
 def remove_empty_keys(dirty_dict):
     """
     Remove empty keys from a dictionary. This method is useful when passing jsons
     in which a null field will update the value to null and you don't want that.
     """
@@ -32,19 +31,19 @@
     """
     Flatten nested json to return a dict without nested values. Lists without nested values will
     be ignored, and lists of dicts will only return the first key value pair for each key. Useful
     for passing nested json to validation methods.
     """
     out = {}
 
-    def flatten(x, name=''):
+    def flatten(x, name=""):
         if type(x) is dict:
             for k, v in x.items():
                 flatten(v, k)
         elif type(x) is list:
             for a in x:
                 flatten(a)
-        elif name != '' and name not in out:
+        elif name != "" and name not in out:
             out[name] = x
 
     flatten(json)
     return out
```

### Comparing `parsons-1.0.0/parsons/utilities/oauth_api_connector.py` & `parsons-1.1.0/parsons/utilities/oauth_api_connector.py`

 * *Files 5% similar despite different names*

```diff
@@ -32,28 +32,43 @@
         auto_refresh_url: str
             If provided, the URL for refreshing tokens from the OAuth2 Application
     `Returns`:
         OAuthAPIConnector class
     """
 
     def __init__(
-        self, uri, headers=None, auth=None, pagination_key=None, data_key=None,
-        client_id=None, client_secret=None, token_url=None, auto_refresh_url=None
+        self,
+        uri,
+        headers=None,
+        auth=None,
+        pagination_key=None,
+        data_key=None,
+        client_id=None,
+        client_secret=None,
+        token_url=None,
+        auto_refresh_url=None,
     ):
         super().__init__(
-            uri, headers=headers, auth=auth, pagination_key=pagination_key, data_key=data_key
+            uri,
+            headers=headers,
+            auth=auth,
+            pagination_key=pagination_key,
+            data_key=data_key,
         )
 
         client = BackendApplicationClient(client_id=client_id)
         oauth = OAuth2Session(client=client)
-        self.token = oauth.fetch_token(token_url=token_url,
-                                       client_id=client_id, client_secret=client_secret)
+        self.token = oauth.fetch_token(
+            token_url=token_url, client_id=client_id, client_secret=client_secret
+        )
         self.client = OAuth2Session(
-            client_id, token=self.token, auto_refresh_url=auto_refresh_url,
-            token_updater=self.token_saver
+            client_id,
+            token=self.token,
+            auto_refresh_url=auto_refresh_url,
+            token_updater=self.token_saver,
         )
 
     def request(self, url, req_type, json=None, data=None, params=None):
         """
         Base request using requests libary.
 
         `Args:`
@@ -72,13 +87,18 @@
                 The parameters to append to the url (e.g. http://myapi.com/things?id=1)
 
         `Returns:`
             requests response
         """
         full_url = urllib.parse.urljoin(self.uri, url)
         return self.client.request(
-            req_type, full_url, headers=self.headers, auth=self.auth,
-            json=json, data=data, params=params
+            req_type,
+            full_url,
+            headers=self.headers,
+            auth=self.auth,
+            json=json,
+            data=data,
+            params=params,
         )
 
     def token_saver(self, token):
         self.token = token
```

### Comparing `parsons-1.0.0/parsons/zoom/zoom.py` & `parsons-1.1.0/parsons/zoom/zoom.py`

 * *Files 12% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 from parsons import Table
 import logging
 import jwt
 import datetime
 
 logger = logging.getLogger(__name__)
 
-ZOOM_URI = 'https://api.zoom.us/v2/'
+ZOOM_URI = "https://api.zoom.us/v2/"
 
 
 class Zoom:
     """
     Instantiate the Zoom class.
 
     `Args:`
@@ -21,79 +21,83 @@
         api_secret: str
             A valid Zoom api secret. Not required if ``ZOOM_API_SECRET`` env
             variable set.
     """
 
     def __init__(self, api_key=None, api_secret=None):
 
-        self.api_key = check_env.check('ZOOM_API_KEY', api_key)
-        self.api_secret = check_env.check('ZOOM_API_SECRET', api_secret)
+        self.api_key = check_env.check("ZOOM_API_KEY", api_key)
+        self.api_secret = check_env.check("ZOOM_API_SECRET", api_secret)
         self.client = APIConnector(ZOOM_URI)
 
     def refresh_header_token(self):
         # Generate a token that is valid for 30 seconds and update header. Full documentation
         # on JWT generation using Zoom API: https://marketplace.zoom.us/docs/guides/auth/jwt
 
-        payload = {"iss": self.api_key, "exp": int(datetime.datetime.now().timestamp() + 30)}
-        token = jwt.encode(payload, self.api_secret, algorithm='HS256')
-        self.client.headers = {'authorization': f"Bearer {token}",
-                               'content-type': "application/json"}
+        payload = {
+            "iss": self.api_key,
+            "exp": int(datetime.datetime.now().timestamp() + 30),
+        }
+        token = jwt.encode(payload, self.api_secret, algorithm="HS256")
+        self.client.headers = {
+            "authorization": f"Bearer {token}",
+            "content-type": "application/json",
+        }
 
     def _get_request(self, endpoint, data_key, params=None, **kwargs):
         # To Do: Consider increasing default page size.
 
         self.refresh_header_token()
         r = self.client.get_request(endpoint, params=params, **kwargs)
         self.client.data_key = data_key
         data = self.client.data_parse(r)
 
         if not params:
             params = {}
 
         # Return a dict or table if only one item.
-        if 'page_number' not in r.keys():
+        if "page_number" not in r.keys():
             if isinstance(data, dict):
                 return data
             if isinstance(data, list):
                 return Table(data)
 
         # Else iterate through the pages and return a Table
         else:
-            while r['page_number'] < r['page_count']:
-                params['page_number'] = int(r['page_number']) + 1
+            while r["page_number"] < r["page_count"]:
+                params["page_number"] = int(r["page_number"]) + 1
                 r = self.client.get_request(endpoint, params=params, **kwargs)
                 data.extend(self.client.data_parse(r))
             return Table(data)
 
-    def get_users(self, status='active', role_id=None):
+    def get_users(self, status="active", role_id=None):
         """
         Get users.
 
         `Args:`
             status: str
                 Filter by the user status. Must be one of following: ``active``,
                 ``inactive``, or ``pending``.
             role_id: str
                 Filter by the user role.
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        if status not in ['active', 'inactive', 'pending']:
-            raise ValueError('Invalid status type provided.')
+        if status not in ["active", "inactive", "pending"]:
+            raise ValueError("Invalid status type provided.")
 
-        params = {'status': status,
-                  'role_id': role_id}
+        params = {"status": status, "role_id": role_id}
 
-        tbl = self._get_request('users', 'users', params=params)
-        logger.info(f'Retrieved {tbl.num_rows} users.')
+        tbl = self._get_request("users", "users", params=params)
+        logger.info(f"Retrieved {tbl.num_rows} users.")
         return tbl
 
-    def get_meetings(self, user_id, meeting_type='scheduled'):
+    def get_meetings(self, user_id, meeting_type="scheduled"):
         """
         Get meetings scheduled by a user.
 
         `Args:`
             user_id: str
                 A user id or email address of the meeting host.
             meeting_type: str
@@ -114,106 +118,110 @@
                     * - ``upcoming``
                       - All upcoming meetings including live meetings.
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        tbl = self._get_request(f'users/{user_id}/meetings', 'meetings')
-        logger.info(f'Retrieved {tbl.num_rows} meetings.')
+        tbl = self._get_request(f"users/{user_id}/meetings", "meetings")
+        logger.info(f"Retrieved {tbl.num_rows} meetings.")
         return tbl
 
     def get_past_meeting(self, meeting_uuid):
         """
         Get metadata regarding a past meeting.
 
         `Args:`
             meeting_id: str
                 The meeting id
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        tbl = self._get_request(f'past_meetings/{meeting_uuid}', None)
-        logger.info(f'Retrieved meeting {meeting_uuid}.')
+        tbl = self._get_request(f"past_meetings/{meeting_uuid}", None)
+        logger.info(f"Retrieved meeting {meeting_uuid}.")
         return tbl
 
     def get_past_meeting_participants(self, meeting_id):
         """
         Get past meeting participants.
 
         `Args:`
             meeting_id: str
                 The meeting id
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        tbl = self._get_request(f'report/meetings/{meeting_id}/participants', 'participants')
-        logger.info(f'Retrieved {tbl.num_rows} participants.')
+        tbl = self._get_request(
+            f"report/meetings/{meeting_id}/participants", "participants"
+        )
+        logger.info(f"Retrieved {tbl.num_rows} participants.")
         return tbl
 
     def get_meeting_registrants(self, meeting_id):
         """
         Get meeting registrants.
 
         `Args:`
             meeting_id: str
                 The meeting id
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        tbl = self._get_request(f'meetings/{meeting_id}/registrants', 'registrants')
-        logger.info(f'Retrieved {tbl.num_rows} registrants.')
+        tbl = self._get_request(f"meetings/{meeting_id}/registrants", "registrants")
+        logger.info(f"Retrieved {tbl.num_rows} registrants.")
         return tbl
 
     def get_user_webinars(self, user_id):
         """
         Get meeting registrants.
 
         `Args:`
             user_id: str
                 The user id
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        tbl = self._get_request(f'users/{user_id}/webinars', 'webinars')
-        logger.info(f'Retrieved {tbl.num_rows} webinars.')
+        tbl = self._get_request(f"users/{user_id}/webinars", "webinars")
+        logger.info(f"Retrieved {tbl.num_rows} webinars.")
         return tbl
 
     def get_past_webinar_participants(self, webinar_id):
         """
         Get past meeting participants
 
         `Args:`
             webinar_id: str
                 The webinar id
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        tbl = self._get_request(f'report/webinars/{webinar_id}/participants', 'participants')
-        logger.info(f'Retrieved {tbl.num_rows} webinar participants.')
+        tbl = self._get_request(
+            f"report/webinars/{webinar_id}/participants", "participants"
+        )
+        logger.info(f"Retrieved {tbl.num_rows} webinar participants.")
         return tbl
 
     def get_webinar_registrants(self, webinar_id):
         """
         Get past meeting participants
 
         `Args:`
             webinar_id: str
                 The webinar id
         `Returns:`
             Parsons Table
                 See :ref:`parsons-table` for output options.
         """
 
-        tbl = self._get_request(f'webinars/{webinar_id}/registrants', 'registrants')
-        logger.info(f'Retrieved {tbl.num_rows} webinar registrants.')
+        tbl = self._get_request(f"webinars/{webinar_id}/registrants", "registrants")
+        logger.info(f"Retrieved {tbl.num_rows} webinar registrants.")
         return tbl
```

### Comparing `parsons-1.0.0/parsons.egg-info/SOURCES.txt` & `parsons-1.1.0/parsons.egg-info/SOURCES.txt`

 * *Files identical despite different names*

### Comparing `parsons-1.0.0/parsons.egg-info/requires.txt` & `parsons-1.1.0/parsons.egg-info/requires.txt`

 * *Files 11% similar despite different names*

```diff
@@ -36,14 +36,15 @@
 PyGitHub==1.51
 surveygizmo==1.2.3
 PyJWT==2.4.0
 SQLAlchemy==1.3.23
 requests_oauthlib==1.3.0
 requests-mock==1.5.2
 flake8==4.0.1
+black==22.12.0
 testfixtures==6.18.5
 pytest==7.1.1
 pytest-datadir==1.3.0
 selenium==3.141.0
 jinja2==3.0.2
 
 [all]
```

### Comparing `parsons-1.0.0/setup.py` & `parsons-1.1.0/setup.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,13 +1,12 @@
 import os
 from setuptools import find_packages
 from distutils.core import setup
 
 
-
 def main():
     limited_deps = os.environ.get("PARSONS_LIMITED_DEPENDENCIES", "")
     if limited_deps.strip().upper() in ("1", "YES", "TRUE", "ON"):
         install_requires = [
             "petl",
             "python-dateutil",
             "requests",
@@ -38,49 +37,47 @@
             "newmode": ["newmode"],
             "ngpvan": ["suds-py3"],
             "postgres": ["psycopg2-binary", "SQLAlchemy"],
             "redshift": ["boto3", "psycopg2-binary", "SQLAlchemy"],
             "s3": ["boto3"],
             "salesforce": ["simple-salesforce"],
             "sftp": ["paramiko"],
-            "slack": ["slackclient"],
+            "slack": ["slackclient<2"],
             "smtp": ["validate-email"],
             "targetsmart": ["xmltodict"],
             "twilio": ["twilio"],
             "zoom": ["PyJWT"],
         }
-        extras_require["all"] = sorted({
-            lib
-            for libs in extras_require.values()
-            for lib in libs
-        })
+        extras_require["all"] = sorted(
+            {lib for libs in extras_require.values() for lib in libs}
+        )
     else:
         THIS_DIR = os.path.abspath(os.path.dirname(__file__))
-        with open(os.path.join(THIS_DIR, 'requirements.txt')) as reqs:
-            install_requires = reqs.read().strip().split('\n')
+        with open(os.path.join(THIS_DIR, "requirements.txt")) as reqs:
+            install_requires = reqs.read().strip().split("\n")
         # No op for forward-compatibility
         extras_require = {"all": []}
 
     setup(
         name="parsons",
-        version='1.0.0',
+        version="1.1.0",
         author="The Movement Cooperative",
         author_email="info@movementcooperative.org",
-        url='https://github.com/movementcoop/parsons',
-        keywords=['PROGRESSIVE', 'API', 'ETL'],
+        url="https://github.com/move-coop/parsons",
+        keywords=["PROGRESSIVE", "API", "ETL"],
         packages=find_packages(),
         install_requires=install_requires,
         extras_require=extras_require,
         classifiers=[
-            'Development Status :: 3 - Alpha',
-            'Intended Audience :: Developers',
-            'Programming Language :: Python :: 3.7',
-            'Programming Language :: Python :: 3.8',
-            'Programming Language :: Python :: 3.9',
-            'Programming Language :: Python :: 3.10'
+            "Development Status :: 3 - Alpha",
+            "Intended Audience :: Developers",
+            "Programming Language :: Python :: 3.7",
+            "Programming Language :: Python :: 3.8",
+            "Programming Language :: Python :: 3.9",
+            "Programming Language :: Python :: 3.10",
         ],
         python_requires=">=3.7.0,<3.11.0",
     )
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `parsons-1.0.0/test/fixtures.py` & `parsons-1.1.0/test/fixtures.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,26 +1,26 @@
 import pytest
 from parsons.etl import Table
 
 
-'''
+"""
 Simple Table
 
 The bare minimum Parsons table, and matching files representing that table.
-'''
+"""
 
 
 @pytest.fixture
 def simple_table():
     # Note - If you modify this table, you must also update the related "simple" files.
     # Fortunately Parsons should make that easy to do :)
-    return Table([{'first': 'Bob', 'last': 'Smith'}])
+    return Table([{"first": "Bob", "last": "Smith"}])
 
 
 @pytest.fixture
 def simple_csv_path(shared_datadir):
-    return str(shared_datadir / 'test-simple-table.csv')
+    return str(shared_datadir / "test-simple-table.csv")
 
 
 @pytest.fixture
 def simple_compressed_csv_path(shared_datadir):
-    return str(shared_datadir / 'test-simple-table.csv.gz')
+    return str(shared_datadir / "test-simple-table.csv.gz")
```

### Comparing `parsons-1.0.0/test/test_action_kit.py` & `parsons-1.1.0/test/test_action_kit.py`

 * *Files 10% similar despite different names*

```diff
@@ -3,580 +3,645 @@
 import unittest
 from unittest import mock
 from parsons import ActionKit, Table
 
 from test.utils import assert_matching_tables
 
 ENV_PARAMETERS = {
-    'ACTION_KIT_DOMAIN': 'env_domain',
-    'ACTION_KIT_USERNAME': 'env_username',
-    'ACTION_KIT_PASSWORD': 'env_password'
+    "ACTION_KIT_DOMAIN": "env_domain",
+    "ACTION_KIT_USERNAME": "env_username",
+    "ACTION_KIT_PASSWORD": "env_password",
 }
 
 
 class TestActionKit(unittest.TestCase):
-
     def setUp(self):
         self.actionkit = ActionKit(
-            domain='domain.actionkit.com',
-            username='user',
-            password='password'
+            domain="domain.actionkit.com", username="user", password="password"
         )
         self.actionkit.conn = mock.MagicMock()
 
     def tearDown(self):
         pass
 
     @mock.patch.dict(os.environ, ENV_PARAMETERS)
     def test_from_environ(self):
         actionkit = ActionKit()
-        self.assertEqual(actionkit.domain, 'env_domain')
-        self.assertEqual(actionkit.username, 'env_username')
-        self.assertEqual(actionkit.password, 'env_password')
+        self.assertEqual(actionkit.domain, "env_domain")
+        self.assertEqual(actionkit.username, "env_username")
+        self.assertEqual(actionkit.password, "env_password")
 
     def test_base_endpoint(self):
         # Test the endpoint
-        url = self.actionkit._base_endpoint('user')
-        self.assertEqual(url, 'https://domain.actionkit.com/rest/v1/user/')
+        url = self.actionkit._base_endpoint("user")
+        self.assertEqual(url, "https://domain.actionkit.com/rest/v1/user/")
 
-        url = self.actionkit._base_endpoint('user', 1234)
-        self.assertEqual(url, 'https://domain.actionkit.com/rest/v1/user/1234/')
+        url = self.actionkit._base_endpoint("user", 1234)
+        self.assertEqual(url, "https://domain.actionkit.com/rest/v1/user/1234/")
 
-        url = self.actionkit._base_endpoint('user', '1234')
-        self.assertEqual(url, 'https://domain.actionkit.com/rest/v1/user/1234/')
+        url = self.actionkit._base_endpoint("user", "1234")
+        self.assertEqual(url, "https://domain.actionkit.com/rest/v1/user/1234/")
 
     def test_get_user(self):
         # Test get user
         self.actionkit.get_user(123)
         self.actionkit.conn.get.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/user/123/',
-            params=None
+            "https://domain.actionkit.com/rest/v1/user/123/", params=None
         )
 
     def test_get_user_fields(self):
         self.actionkit.get_user_fields()
         self.actionkit.conn.get.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/user/schema/',
-            params=None
+            "https://domain.actionkit.com/rest/v1/user/schema/", params=None
         )
 
     def test_create_user(self):
         # Test create user
 
         # Mock resp and status code
         resp_mock = mock.MagicMock()
         type(resp_mock.post()).status_code = mock.PropertyMock(return_value=201)
         self.actionkit.conn = resp_mock
 
-        self.actionkit.create_user(email='test')
+        self.actionkit.create_user(email="test")
         self.actionkit.conn.post.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/user/',
-            data=json.dumps({'email': 'test'})
+            "https://domain.actionkit.com/rest/v1/user/",
+            data=json.dumps({"email": "test"}),
         )
 
     def test_update_user(self):
         # Test update user
 
         # Mock resp and status code
         resp_mock = mock.MagicMock()
         type(resp_mock.patch()).status_code = mock.PropertyMock(return_value=202)
         self.actionkit.conn = resp_mock
 
-        self.actionkit.update_user(123, last_name='new name')
+        self.actionkit.update_user(123, last_name="new name")
         self.actionkit.conn.patch.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/user/123/',
-            data=json.dumps({'last_name': 'new name'})
+            "https://domain.actionkit.com/rest/v1/user/123/",
+            data=json.dumps({"last_name": "new name"}),
         )
 
     def test_update_event(self):
         # Test update event
 
         # Mock resp and status code
         resp_mock = mock.MagicMock()
         type(resp_mock.patch()).status_code = mock.PropertyMock(return_value=202)
         self.actionkit.conn = resp_mock
-        self.actionkit.update_event(123, is_approved='test')
+        self.actionkit.update_event(123, is_approved="test")
         self.actionkit.conn.patch.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/event/123/',
-            data=json.dumps({'is_approved': 'test'})
+            "https://domain.actionkit.com/rest/v1/event/123/",
+            data=json.dumps({"is_approved": "test"}),
         )
 
     def test_delete_user(self):
         # Test delete user
 
         # Mock resp and status code
         resp_mock = mock.MagicMock()
         type(resp_mock.patch()).status_code = mock.PropertyMock(return_value=204)
         self.actionkit.conn = resp_mock
 
         self.actionkit.delete_user(123)
         self.actionkit.conn.delete.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/user/123/',
+            "https://domain.actionkit.com/rest/v1/user/123/",
         )
 
     def test_get_campaign(self):
         # Test get campaign
         self.actionkit.get_campaign(123)
         self.actionkit.conn.get.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/campaign/123/',
-            params=None
+            "https://domain.actionkit.com/rest/v1/campaign/123/", params=None
         )
 
     def test_create_campaign(self):
         # Test create campaign
 
         # Mock resp and status code
         resp_mock = mock.MagicMock()
         type(resp_mock.post()).status_code = mock.PropertyMock(return_value=201)
         self.actionkit.conn = resp_mock
 
-        self.actionkit.create_campaign(name='new_campaign', field='field')
+        self.actionkit.create_campaign(name="new_campaign", field="field")
         self.actionkit.conn.post.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/campaign/',
-            data=json.dumps({
-                'name': 'new_campaign',
-                'field': 'field'
-            })
+            "https://domain.actionkit.com/rest/v1/campaign/",
+            data=json.dumps({"name": "new_campaign", "field": "field"}),
         )
 
     def test_get_event(self):
         # Test get event
         self.actionkit.get_event(1)
         self.actionkit.conn.get.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/event/1/',
-            params=None
+            "https://domain.actionkit.com/rest/v1/event/1/", params=None
         )
 
     def test_get_events(self):
         # Test get events
         resp_mock = mock.MagicMock()
         type(resp_mock.get()).status_code = mock.PropertyMock(return_value=201)
         type(resp_mock.get()).json = lambda x: {"meta": {"next": ""}, "objects": []}
         self.actionkit.conn = resp_mock
 
-        self.actionkit.get_events(100, order_by='created_at')
+        self.actionkit.get_events(100, order_by="created_at")
         self.actionkit.conn.get.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/event/',
-            params={'order_by': 'created_at', '_limit': 100}
+            "https://domain.actionkit.com/rest/v1/event/",
+            params={"order_by": "created_at", "_limit": 100},
         )
 
     def test_get_event_create_page(self):
         # Test get event create page
         self.actionkit.get_event_create_page(123)
         self.actionkit.conn.get.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/eventcreatepage/123/',
-            params=None
+            "https://domain.actionkit.com/rest/v1/eventcreatepage/123/", params=None
         )
 
     def test_create_event_create_page(self):
         # Test create event create page
 
         # Mock resp and status code
         resp_mock = mock.MagicMock()
         type(resp_mock.post()).status_code = mock.PropertyMock(return_value=201)
         self.actionkit.conn = resp_mock
 
         self.actionkit.create_event_create_page(
-            name='new_page',
-            campaign_id='123',
-            title='title'
+            name="new_page", campaign_id="123", title="title"
         )
         self.actionkit.conn.post.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/eventcreatepage/',
-            data=json.dumps({
-                'campaign': '/rest/v1/campaign/123/',
-                'name': 'new_page',
-                'title': 'title'
-            })
+            "https://domain.actionkit.com/rest/v1/eventcreatepage/",
+            data=json.dumps(
+                {
+                    "campaign": "/rest/v1/campaign/123/",
+                    "name": "new_page",
+                    "title": "title",
+                }
+            ),
         )
 
     def test_get_event_create_form(self):
         # Test get event create form
         self.actionkit.get_event_create_form(123)
         self.actionkit.conn.get.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/eventcreateform/123/',
-            params=None
+            "https://domain.actionkit.com/rest/v1/eventcreateform/123/", params=None
         )
 
     def test_create_event_create_form(self):
         # Test event create form
 
         # Mock resp and status code
         resp_mock = mock.MagicMock()
         type(resp_mock.post()).status_code = mock.PropertyMock(return_value=201)
         self.actionkit.conn = resp_mock
 
         self.actionkit.create_event_create_form(
-            page_id='123',
-            thank_you_text='thank you'
+            page_id="123", thank_you_text="thank you"
         )
         self.actionkit.conn.post.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/eventcreateform/',
-            data=json.dumps({
-                'page': '/rest/v1/eventcreatepage/123/',
-                'thank_you_text': 'thank you'
-            })
+            "https://domain.actionkit.com/rest/v1/eventcreateform/",
+            data=json.dumps(
+                {"page": "/rest/v1/eventcreatepage/123/", "thank_you_text": "thank you"}
+            ),
         )
 
     def test_get_event_signup_page(self):
         # Test get event signup page
         self.actionkit.get_event_signup_page(123)
         self.actionkit.conn.get.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/eventsignuppage/123/',
-            params=None
+            "https://domain.actionkit.com/rest/v1/eventsignuppage/123/", params=None
         )
 
     def test_create_event_signup_page(self):
         # Test create event signup page
 
         # Mock resp and status code
         resp_mock = mock.MagicMock()
         type(resp_mock.post()).status_code = mock.PropertyMock(return_value=201)
         self.actionkit.conn = resp_mock
 
         self.actionkit.create_event_signup_page(
-            name='new_name',
-            campaign_id='123',
-            title='title'
+            name="new_name", campaign_id="123", title="title"
         )
         self.actionkit.conn.post.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/eventsignuppage/',
-            data=json.dumps({
-                'campaign': '/rest/v1/campaign/123/',
-                'name': 'new_name',
-                'title': 'title'
-            })
+            "https://domain.actionkit.com/rest/v1/eventsignuppage/",
+            data=json.dumps(
+                {
+                    "campaign": "/rest/v1/campaign/123/",
+                    "name": "new_name",
+                    "title": "title",
+                }
+            ),
         )
 
     def test_get_event_signup_form(self):
         # Test get event signup form
         self.actionkit.get_event_signup_form(123)
         self.actionkit.conn.get.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/eventsignupform/123/',
-            params=None
+            "https://domain.actionkit.com/rest/v1/eventsignupform/123/", params=None
         )
 
     def test_create_event_signup_form(self):
         # Test create event signup form
 
         # Mock resp and status code
         resp_mock = mock.MagicMock()
         type(resp_mock.post()).status_code = mock.PropertyMock(return_value=201)
         self.actionkit.conn = resp_mock
 
         self.actionkit.create_event_signup_form(
-            page_id='123',
-            thank_you_text='thank you'
+            page_id="123", thank_you_text="thank you"
         )
         self.actionkit.conn.post.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/eventsignupform/',
-            data=json.dumps({
-                'page': '/rest/v1/page/123/',
-                'thank_you_text': 'thank you'
-            })
+            "https://domain.actionkit.com/rest/v1/eventsignupform/",
+            data=json.dumps(
+                {"page": "/rest/v1/page/123/", "thank_you_text": "thank you"}
+            ),
         )
 
     def test_update_event_signup(self):
         # Test update event signup
 
         # Mock resp and status code
         resp_mock = mock.MagicMock()
         type(resp_mock.patch()).status_code = mock.PropertyMock(return_value=202)
         self.actionkit.conn = resp_mock
-        self.actionkit.update_event_signup(123, email='test')
+        self.actionkit.update_event_signup(123, email="test")
         self.actionkit.conn.patch.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/eventsignup/123/',
-            data=json.dumps({'email': 'test'})
+            "https://domain.actionkit.com/rest/v1/eventsignup/123/",
+            data=json.dumps({"email": "test"}),
         )
 
     def test_get_mailer(self):
         # Test get mailer
         self.actionkit.get_mailer(123)
         self.actionkit.conn.get.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/mailer/123/',
-            params=None
+            "https://domain.actionkit.com/rest/v1/mailer/123/", params=None
         )
 
     def test_create_mailer(self):
         # Test create mailer
 
         # Mock resp and status code
         resp_mock = mock.MagicMock()
         type(resp_mock.post()).status_code = mock.PropertyMock(return_value=201)
         self.actionkit.conn = resp_mock
 
         self.actionkit.create_mailer(
-            fromline='test <test@test.com>', subjects=['test1', 'test2'], html='<p>test</p>'
+            fromline="test <test@test.com>",
+            subjects=["test1", "test2"],
+            html="<p>test</p>",
         )
         self.actionkit.conn.post.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/mailer/',
-            data=json.dumps({
-                'fromline': 'test <test@test.com>', 'subjects': ['test1', 'test2'],
-                'html': '<p>test</p>'
-            })
+            "https://domain.actionkit.com/rest/v1/mailer/",
+            data=json.dumps(
+                {
+                    "fromline": "test <test@test.com>",
+                    "subjects": ["test1", "test2"],
+                    "html": "<p>test</p>",
+                }
+            ),
         )
 
     def test_rebuild_mailer(self):
         # Test rebuild mailer
 
         # Mock resp and status code
         resp_mock = mock.MagicMock()
         type(resp_mock.post()).status_code = mock.PropertyMock(return_value=201)
         self.actionkit.conn = resp_mock
 
         self.actionkit.rebuild_mailer(123)
         self.actionkit.conn.post.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/mailer/123/rebuild/',
-            data=json.dumps({})
+            "https://domain.actionkit.com/rest/v1/mailer/123/rebuild/",
+            data=json.dumps({}),
         )
 
     def test_queue_mailer(self):
         # Test queue mailer
 
         # Mock resp and status code
         resp_mock = mock.MagicMock()
         type(resp_mock.post()).status_code = mock.PropertyMock(return_value=201)
         self.actionkit.conn = resp_mock
 
         self.actionkit.queue_mailer(123)
         self.actionkit.conn.post.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/mailer/123/queue/',
-            data=json.dumps({})
+            "https://domain.actionkit.com/rest/v1/mailer/123/queue/",
+            data=json.dumps({}),
         )
 
     def test_paginated_get(self):
         # Test paginated_get
         resp_mock = mock.MagicMock()
         first_mock = mock.MagicMock()
         second_mock = mock.MagicMock()
         first_mock.status_code = 201
-        first_mock.json = lambda: {"meta": {"next": "/rest/v1/user/abc"},
-                                   "objects": list(map(lambda x: {"value": x}, [*range(100)]))}
+        first_mock.json = lambda: {
+            "meta": {"next": "/rest/v1/user/abc"},
+            "objects": list(map(lambda x: {"value": x}, [*range(100)])),
+        }
         second_mock.status_code = 201
-        second_mock.json = lambda: {"meta": {"next": "/rest/v1/user/def"},
-                                    "objects": list(map(lambda x: {"value": x},
-                                                        [*range(100, 200)]))}
+        second_mock.json = lambda: {
+            "meta": {"next": "/rest/v1/user/def"},
+            "objects": list(map(lambda x: {"value": x}, [*range(100, 200)])),
+        }
         resp_mock.get.side_effect = [first_mock, second_mock]
         self.actionkit.conn = resp_mock
-        results = self.actionkit.paginated_get('user', 150, order_by='created_at')
+        results = self.actionkit.paginated_get("user", 150, order_by="created_at")
         self.assertEqual(results.num_rows, 150)
-        calls = [unittest.mock.call('https://domain.actionkit.com/rest/v1/user/',
-                                    params={'order_by': 'created_at', '_limit': 100}),
-                 unittest.mock.call('https://domain.actionkit.com/rest/v1/user/abc')
-                 ]
+        calls = [
+            unittest.mock.call(
+                "https://domain.actionkit.com/rest/v1/user/",
+                params={"order_by": "created_at", "_limit": 100},
+            ),
+            unittest.mock.call("https://domain.actionkit.com/rest/v1/user/abc"),
+        ]
         self.actionkit.conn.get.assert_has_calls(calls)
 
     def test_paginated_get_custom_limit(self):
         # Test paginated_get
         resp_mock = mock.MagicMock()
         first_mock = mock.MagicMock()
         second_mock = mock.MagicMock()
         first_mock.status_code = 201
-        first_mock.json = lambda: {"meta": {"next": "/rest/v1/user/abc"},
-                                   "objects": list(map(lambda x: {"value": x}, [*range(100)]))}
+        first_mock.json = lambda: {
+            "meta": {"next": "/rest/v1/user/abc"},
+            "objects": list(map(lambda x: {"value": x}, [*range(100)])),
+        }
         second_mock.status_code = 201
-        second_mock.json = lambda: {"meta": {"next": "/rest/v1/user/def"},
-                                    "objects": list(map(lambda x: {"value": x},
-                                                        [*range(100, 200)]))}
+        second_mock.json = lambda: {
+            "meta": {"next": "/rest/v1/user/def"},
+            "objects": list(map(lambda x: {"value": x}, [*range(100, 200)])),
+        }
         resp_mock.get.side_effect = [first_mock, second_mock]
         self.actionkit.conn = resp_mock
-        results = self.actionkit.paginated_get_custom_limit(
-            'user', 150, 'value', 102)
+        results = self.actionkit.paginated_get_custom_limit("user", 150, "value", 102)
         self.assertEqual(results.num_rows, 102)
-        self.assertEqual(results.column_data('value')[0], 0)
-        self.assertEqual(results.column_data('value')[-1], 101)
-        calls = [unittest.mock.call('https://domain.actionkit.com/rest/v1/user/',
-                                    params={'order_by': 'value', '_limit': 100}),
-                 unittest.mock.call('https://domain.actionkit.com/rest/v1/user/abc')
-                 ]
+        self.assertEqual(results.column_data("value")[0], 0)
+        self.assertEqual(results.column_data("value")[-1], 101)
+        calls = [
+            unittest.mock.call(
+                "https://domain.actionkit.com/rest/v1/user/",
+                params={"order_by": "value", "_limit": 100},
+            ),
+            unittest.mock.call("https://domain.actionkit.com/rest/v1/user/abc"),
+        ]
         self.actionkit.conn.get.assert_has_calls(calls)
 
+    def test_get_order(self):
+        # Test get order
+        self.actionkit.get_order(123)
+        self.actionkit.conn.get.assert_called_with(
+            "https://domain.actionkit.com/rest/v1/order/123/", params=None
+        )
+
     def test_update_order(self):
         # Test update order
 
         # Mock resp and status code
         resp_mock = mock.MagicMock()
         type(resp_mock.patch()).status_code = mock.PropertyMock(return_value=202)
         self.actionkit.conn = resp_mock
-        self.actionkit.update_order(123, account='test')
+        self.actionkit.update_order(123, account="test")
         self.actionkit.conn.patch.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/order/123/', data=json.dumps({'account': 'test'})
+            "https://domain.actionkit.com/rest/v1/order/123/",
+            data=json.dumps({"account": "test"}),
+        )
+
+    def test_get_orders(self):
+        # Test get orders
+        resp_mock = mock.MagicMock()
+        type(resp_mock.get()).status_code = mock.PropertyMock(return_value=201)
+        type(resp_mock.get()).json = lambda x: {"meta": {"next": ""}, "objects": []}
+        self.actionkit.conn = resp_mock
+
+        self.actionkit.get_orders(100, order_by="created_at")
+        self.actionkit.conn.get.assert_called_with(
+            "https://domain.actionkit.com/rest/v1/order/",
+            params={"order_by": "created_at", "_limit": 100},
         )
 
     def test_update_paymenttoken(self):
         # Test update payment token
 
         # Mock resp and status code
         resp_mock = mock.MagicMock()
         type(resp_mock.patch()).status_code = mock.PropertyMock(return_value=202)
         self.actionkit.conn = resp_mock
 
-        self.actionkit.update_paymenttoken(1, status='inactive')
+        self.actionkit.update_paymenttoken(1, status="inactive")
         self.actionkit.conn.patch.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/paymenttoken/1/',
-            data=json.dumps({'status': 'inactive'})
+            "https://domain.actionkit.com/rest/v1/paymenttoken/1/",
+            data=json.dumps({"status": "inactive"}),
         )
 
     def test_get_page_followup(self):
         # Test get page followup
         self.actionkit.get_page_followup(123)
         self.actionkit.conn.get.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/pagefollowup/123/',
-            params=None
+            "https://domain.actionkit.com/rest/v1/pagefollowup/123/", params=None
         )
 
     def test_create_page_followup(self):
         # Test create page followup
 
         # Mock resp and status code
         resp_mock = mock.MagicMock()
         type(resp_mock.post()).status_code = mock.PropertyMock(return_value=201)
         self.actionkit.conn = resp_mock
 
-        self.actionkit.create_page_followup(
-            signup_page_id='123',
-            url='url'
-        )
+        self.actionkit.create_page_followup(signup_page_id="123", url="url")
         self.actionkit.conn.post.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/pagefollowup/',
-            data=json.dumps({
-                'page': '/rest/v1/eventsignuppage/123/',
-                'url': 'url'
-            })
+            "https://domain.actionkit.com/rest/v1/pagefollowup/",
+            data=json.dumps({"page": "/rest/v1/eventsignuppage/123/", "url": "url"}),
         )
 
     def test_get_survey_question(self):
         # Test get survey question
         self.actionkit.get_survey_question(123)
         self.actionkit.conn.get.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/surveyquestion/123/', params=None
+            "https://domain.actionkit.com/rest/v1/surveyquestion/123/", params=None
         )
 
     def test_update_survey_question(self):
         # Test update survey question
 
         # Mock resp and status code
         resp_mock = mock.MagicMock()
         type(resp_mock.patch()).status_code = mock.PropertyMock(return_value=202)
         self.actionkit.conn = resp_mock
-        self.actionkit.update_survey_question(123, question_html='test')
+        self.actionkit.update_survey_question(123, question_html="test")
         self.actionkit.conn.patch.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/surveyquestion/123/',
-            data=json.dumps({'question_html': 'test'})
+            "https://domain.actionkit.com/rest/v1/surveyquestion/123/",
+            data=json.dumps({"question_html": "test"}),
         )
 
     def test_cancel_orderrecurring(self):
         # Test cancel recurring order
 
         # Mock resp and status code
         resp_mock = mock.MagicMock()
         type(resp_mock.post()).status_code = mock.PropertyMock(return_value=201)
         self.actionkit.conn = resp_mock
 
         self.actionkit.cancel_orderrecurring(1)
         self.actionkit.conn.post.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/orderrecurring/1/cancel/'
+            "https://domain.actionkit.com/rest/v1/orderrecurring/1/cancel/"
         )
 
     def test_create_transaction(self):
         # Test create transaction
 
         # Mock resp and status code
         resp_mock = mock.MagicMock()
         type(resp_mock.post()).status_code = mock.PropertyMock(return_value=201)
         self.actionkit.conn = resp_mock
 
         self.actionkit.create_transaction(
-            account='Account', amount=1, amount_converted=1, currency='USD', failure_code='',
-            failure_description='', failure_message='', order='/rest/v1/order/1/',
-            status='completed', success=True, test_mode=False, trans_id='abc123', type='sale'
-        )
-        self.actionkit.conn.post.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/transaction/',
-            data=json.dumps({
-                'account': 'Account', 'amount': 1, 'amount_converted': 1, 'currency': 'USD',
-                'failure_code': '', 'failure_description': '', 'failure_message': '',
-                'order': '/rest/v1/order/1/', 'status': 'completed', 'success': True,
-                'test_mode': False, 'trans_id': 'abc123', 'type': 'sale'
-            })
+            account="Account",
+            amount=1,
+            amount_converted=1,
+            currency="USD",
+            failure_code="",
+            failure_description="",
+            failure_message="",
+            order="/rest/v1/order/1/",
+            status="completed",
+            success=True,
+            test_mode=False,
+            trans_id="abc123",
+            type="sale",
+        )
+        self.actionkit.conn.post.assert_called_with(
+            "https://domain.actionkit.com/rest/v1/transaction/",
+            data=json.dumps(
+                {
+                    "account": "Account",
+                    "amount": 1,
+                    "amount_converted": 1,
+                    "currency": "USD",
+                    "failure_code": "",
+                    "failure_description": "",
+                    "failure_message": "",
+                    "order": "/rest/v1/order/1/",
+                    "status": "completed",
+                    "success": True,
+                    "test_mode": False,
+                    "trans_id": "abc123",
+                    "type": "sale",
+                }
+            ),
         )
 
     def test_update_transaction(self):
         # Test update transaction
 
         # Mock resp and status code
         resp_mock = mock.MagicMock()
         type(resp_mock.patch()).status_code = mock.PropertyMock(return_value=202)
         self.actionkit.conn = resp_mock
-        self.actionkit.update_transaction(123, account='test')
+        self.actionkit.update_transaction(123, account="test")
         self.actionkit.conn.patch.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/transaction/123/',
-            data=json.dumps({'account': 'test'})
+            "https://domain.actionkit.com/rest/v1/transaction/123/",
+            data=json.dumps({"account": "test"}),
+        )
+
+    def test_get_transactions(self):
+        # Test get transactions
+        resp_mock = mock.MagicMock()
+        type(resp_mock.get()).status_code = mock.PropertyMock(return_value=201)
+        type(resp_mock.get()).json = lambda x: {"meta": {"next": ""}, "objects": []}
+        self.actionkit.conn = resp_mock
+
+        self.actionkit.get_transactions(100, order_by="created_at")
+        self.actionkit.conn.get.assert_called_with(
+            "https://domain.actionkit.com/rest/v1/transaction/",
+            params={"order_by": "created_at", "_limit": 100},
         )
 
     def test_create_generic_action(self):
         # Test create a generic action
 
         resp_mock = mock.MagicMock()
         type(resp_mock.post()).status_code = mock.PropertyMock(return_value=201)
         self.actionkit.conn = resp_mock
 
-        self.actionkit.create_generic_action(email='bob@bob.com', page='my_action')
+        self.actionkit.create_generic_action(email="bob@bob.com", page="my_action")
 
         self.actionkit.conn.post.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/action/',
-            data=json.dumps({'email': 'bob@bob.com', 'page': 'my_action'}))
+            "https://domain.actionkit.com/rest/v1/action/",
+            data=json.dumps({"email": "bob@bob.com", "page": "my_action"}),
+        )
 
     def test_bulk_upload_table(self):
         resp_mock = mock.MagicMock()
         type(resp_mock.post()).status_code = mock.PropertyMock(return_value=201)
         self.actionkit._conn = lambda self: resp_mock
         self.actionkit.bulk_upload_table(
-            Table([('user_id', 'user_customfield1', 'action_foo'), (5, 'yes', '123 Main St')]),
-            'fake_page')
+            Table(
+                [
+                    ("user_id", "user_customfield1", "action_foo"),
+                    (5, "yes", "123 Main St"),
+                ]
+            ),
+            "fake_page",
+        )
         self.assertEqual(resp_mock.post.call_count, 2)
         name, args, kwargs = resp_mock.method_calls[1]
-        self.assertEqual(kwargs['data'],
-                         {'page': 'fake_page', 'autocreate_user_fields': 0, 'user_fields_only': 0})
-        upload_data = kwargs['files']['upload'].read()
-        self.assertEqual(upload_data.decode(),
-                         'user_id,user_customfield1,action_foo\r\n5,yes,123 Main St\r\n')
+        self.assertEqual(
+            kwargs["data"],
+            {"page": "fake_page", "autocreate_user_fields": 0, "user_fields_only": 0},
+        )
+        upload_data = kwargs["files"]["upload"].read()
+        self.assertEqual(
+            upload_data.decode(),
+            "user_id,user_customfield1,action_foo\r\n5,yes,123 Main St\r\n",
+        )
 
     def test_bulk_upload_table_userfields(self):
         resp_mock = mock.MagicMock()
         type(resp_mock.post()).status_code = mock.PropertyMock(return_value=201)
         self.actionkit._conn = lambda self: resp_mock
         self.actionkit.bulk_upload_table(
-            Table([('user_id', 'user_customfield1'), (5, 'yes')]),
-            'fake_page')
+            Table([("user_id", "user_customfield1"), (5, "yes")]), "fake_page"
+        )
         self.assertEqual(resp_mock.post.call_count, 2)
         name, args, kwargs = resp_mock.method_calls[1]
-        self.assertEqual(kwargs['data'],
-                         {'page': 'fake_page', 'autocreate_user_fields': 0, 'user_fields_only': 1})
-        self.assertEqual(kwargs['files']['upload'].read().decode(),
-                         'user_id,user_customfield1\r\n5,yes\r\n')
+        self.assertEqual(
+            kwargs["data"],
+            {"page": "fake_page", "autocreate_user_fields": 0, "user_fields_only": 1},
+        )
+        self.assertEqual(
+            kwargs["files"]["upload"].read().decode(),
+            "user_id,user_customfield1\r\n5,yes\r\n",
+        )
 
     def test_table_split(self):
-        test1 = Table([('x', 'y', 'z'), ('a', 'b', ''), ('1', '', '3'), ('4', '', '6')])
+        test1 = Table([("x", "y", "z"), ("a", "b", ""), ("1", "", "3"), ("4", "", "6")])
         tables = self.actionkit._split_tables_no_empties(test1, True, [])
         self.assertEqual(len(tables), 2)
-        assert_matching_tables(tables[0], Table([('x', 'y'), ('a', 'b')]))
-        assert_matching_tables(tables[1], Table([('x', 'z'), ('1', '3'), ('4', '6')]))
+        assert_matching_tables(tables[0], Table([("x", "y"), ("a", "b")]))
+        assert_matching_tables(tables[1], Table([("x", "z"), ("1", "3"), ("4", "6")]))
 
-        test2 = Table([('x', 'y', 'z'), ('a', 'b', 'c'), ('1', '2', '3'), ('4', '5', '6')])
+        test2 = Table(
+            [("x", "y", "z"), ("a", "b", "c"), ("1", "2", "3"), ("4", "5", "6")]
+        )
         tables2 = self.actionkit._split_tables_no_empties(test2, True, [])
         self.assertEqual(len(tables2), 1)
         assert_matching_tables(tables2[0], test2)
 
-        test3 = Table([('x', 'y', 'z'), ('a', 'b', ''), ('1', '2', '3'), ('4', '5', '6')])
-        tables3 = self.actionkit._split_tables_no_empties(test3, False, ['z'])
+        test3 = Table(
+            [("x", "y", "z"), ("a", "b", ""), ("1", "2", "3"), ("4", "5", "6")]
+        )
+        tables3 = self.actionkit._split_tables_no_empties(test3, False, ["z"])
         self.assertEqual(len(tables3), 2)
-        assert_matching_tables(tables3[0], Table([('x', 'y'), ('a', 'b')]))
-        assert_matching_tables(tables3[1],
-                               Table([('x', 'y', 'z'), ('1', '2', '3'), ('4', '5', '6')]))
+        assert_matching_tables(tables3[0], Table([("x", "y"), ("a", "b")]))
+        assert_matching_tables(
+            tables3[1], Table([("x", "y", "z"), ("1", "2", "3"), ("4", "5", "6")])
+        )
 
     def test_collect_errors(self):
-        self.actionkit.collect_upload_errors([{'id': '12345'}])
+        self.actionkit.collect_upload_errors([{"id": "12345"}])
         self.actionkit.conn.get.assert_called_with(
-            'https://domain.actionkit.com/rest/v1/uploaderror/',
-            params={'upload': '12345'}
+            "https://domain.actionkit.com/rest/v1/uploaderror/",
+            params={"upload": "12345"},
         )
```

### Comparing `parsons-1.0.0/test/test_auth0.py` & `parsons-1.1.0/test/test_auth0.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,26 +1,31 @@
 from parsons import Auth0, Table
 from test.utils import assert_matching_tables
 import requests_mock
 import unittest
 
-CLIENT_ID = 'abc'
-CLIENT_SECRET = 'def'
-DOMAIN = 'fakedomain.auth0.com'
+CLIENT_ID = "abc"
+CLIENT_SECRET = "def"
+DOMAIN = "fakedomain.auth0.com"
 
 
 class TestAuth0(unittest.TestCase):
     def setUp(self):
         self.auth0 = Auth0(CLIENT_ID, CLIENT_SECRET, DOMAIN)
 
     @requests_mock.Mocker()
     def test_delete_user(self, m):
         user_id = 1
-        m.delete(f'{self.auth0.base_url}/api/v2/users/{user_id}', status_code=204)
+        m.delete(f"{self.auth0.base_url}/api/v2/users/{user_id}", status_code=204)
         self.assertEqual(self.auth0.delete_user(user_id), 204)
 
     @requests_mock.Mocker()
     def test_get_users_by_email(self, m):
-        email = 'fakeemail@fakedomain.com'
-        mock_users = [{'email': 'fake3mail@fakedomain.com', 'id': 2}]
-        m.get(f'{self.auth0.base_url}/api/v2/users-by-email?email={email}', json=mock_users)
-        assert_matching_tables(self.auth0.get_users_by_email(email), Table(mock_users), True)
+        email = "fakeemail@fakedomain.com"
+        mock_users = [{"email": "fake3mail@fakedomain.com", "id": 2}]
+        m.get(
+            f"{self.auth0.base_url}/api/v2/users-by-email?email={email}",
+            json=mock_users,
+        )
+        assert_matching_tables(
+            self.auth0.get_users_by_email(email), Table(mock_users), True
+        )
```

### Comparing `parsons-1.0.0/test/test_aws_async.py` & `parsons-1.1.0/test/test_aws_async.py`

 * *Files 12% similar despite different names*

```diff
@@ -18,94 +18,110 @@
     global count
     global tableargs
     count = count + 1
     tableargs = (table, fakekwargs)
 
 
 class FakeRunner(object):
-
     def __init__(self, init1=None):
         self.init1 = init1
 
     def foobar(self, table, a, x=None, y=None):
         global count
         global tableargs
         count = count + 1
         tableargs = (table, self.init1, a, x, y)
-        if a == 'raise':
-            raise Exception('foo bar')
+        if a == "raise":
+            raise Exception("foo bar")
 
 
 class TestAsync(unittest.TestCase):
-
     def test_task_path_conversion(self):
         # fake_func
         fake_str = get_func_task_path(fake_func)
-        print('fake_str', fake_str)
+        print("fake_str", fake_str)
         fake_renewed = import_and_get_task(fake_str)
         self.assertEqual(fake_renewed(1, 2, 3), (1, 2, 3, 56))
 
         # Table.from_csv_string @classmethod
         csv_str = get_func_task_path(Table.from_csv_string, Table)
-        print('csv_str', csv_str)
+        print("csv_str", csv_str)
         csv_renewed = import_and_get_task(csv_str)
-        assert_matching_tables(csv_renewed('x,y\n1,2'),
-                               Table([('x', 'y'), ('1', '2')]))
+        assert_matching_tables(csv_renewed("x,y\n1,2"), Table([("x", "y"), ("1", "2")]))
 
         # Table.to_dicts (instance)
         dicts_str = get_func_task_path(Table.to_dicts, Table)
-        print('dicts_str', dicts_str)
-        dicts_renewed = import_and_get_task(dicts_str,
-                                            {'lst': [('x', 'y'), (1, 2)]})
-        self.assertEqual(dicts_renewed(),
-                         [{'x': 1, 'y': 2}])
+        print("dicts_str", dicts_str)
+        dicts_renewed = import_and_get_task(dicts_str, {"lst": [("x", "y"), (1, 2)]})
+        self.assertEqual(dicts_renewed(), [{"x": 1, "y": 2}])
 
     def test_distribute_task(self):
         global count
         global tableargs
         datatable = [
-            ('x', 'y'),
-            (1, 2), (3, 4), (5, 6), (7, 8), (9, 0),
-            (11, 12), (13, 14), (15, 16), (17, 18), (19, 10),
+            ("x", "y"),
+            (1, 2),
+            (3, 4),
+            (5, 6),
+            (7, 8),
+            (9, 0),
+            (11, 12),
+            (13, 14),
+            (15, 16),
+            (17, 18),
+            (19, 10),
         ]
         count = 0
         tableargs = None
-        distribute_task(Table(datatable),
-                        fake_table_process,
-                        'foo',  # bucket
-                        group_count=5,
-                        storage='local',
-                        func_kwargs={'x': 1, 'y': [2, 3]})
+        distribute_task(
+            Table(datatable),
+            fake_table_process,
+            "foo",  # bucket
+            group_count=5,
+            storage="local",
+            func_kwargs={"x": 1, "y": [2, 3]},
+        )
         self.assertEqual(count, 2)
         assert_matching_tables(
             tableargs[0],
-            Table([('x', 'y'),
-                   ('11', '12'), ('13', '14'), ('15', '16'), ('17', '18'), ('19', '10')]))
+            Table(
+                [
+                    ("x", "y"),
+                    ("11", "12"),
+                    ("13", "14"),
+                    ("15", "16"),
+                    ("17", "18"),
+                    ("19", "10"),
+                ]
+            ),
+        )
         count = 0
         tableargs = None
-        distribute_task(Table(datatable + [(0, 0)]),
-                        FakeRunner.foobar,
-                        'foo',  # bucket
-                        group_count=5,
-                        storage='local',
-                        func_class=FakeRunner,
-                        func_class_kwargs={'init1': 'initx'},
-                        func_kwargs={'a': 1, 'x': 2, 'y': 3})
+        distribute_task(
+            Table(datatable + [(0, 0)]),
+            FakeRunner.foobar,
+            "foo",  # bucket
+            group_count=5,
+            storage="local",
+            func_class=FakeRunner,
+            func_class_kwargs={"init1": "initx"},
+            func_kwargs={"a": 1, "x": 2, "y": 3},
+        )
         self.assertEqual(count, 3)
-        self.assertEqual(tableargs[1:], ('initx', 1, 2, 3))
-        self.assertEqual(tableargs[1:], ('initx', 1, 2, 3))
-        assert_matching_tables(
-            tableargs[0],
-            Table([('x', 'y'), ('0', '0')]))
+        self.assertEqual(tableargs[1:], ("initx", 1, 2, 3))
+        self.assertEqual(tableargs[1:], ("initx", 1, 2, 3))
+        assert_matching_tables(tableargs[0], Table([("x", "y"), ("0", "0")]))
 
         # 3. catch=True (with throwing)
         count = 0
         tableargs = None
-        distribute_task(Table(datatable[:6]),
-                        FakeRunner.foobar,
-                        'foo',  # bucket
-                        group_count=5,
-                        storage='local',
-                        func_class=FakeRunner,
-                        func_class_kwargs={'init1': 'initx'},
-                        catch=True,
-                        func_kwargs={'a': 'raise', 'x': 2, 'y': 3})
+        distribute_task(
+            Table(datatable[:6]),
+            FakeRunner.foobar,
+            "foo",  # bucket
+            group_count=5,
+            storage="local",
+            func_class=FakeRunner,
+            func_class_kwargs={"init1": "initx"},
+            catch=True,
+            func_kwargs={"a": "raise", "x": 2, "y": 3},
+        )
```

### Comparing `parsons-1.0.0/test/test_bloomerang/test_bloomerang.py` & `parsons-1.1.0/test/test_bloomerang/test_bloomerang.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,138 +1,178 @@
 import os
 import unittest
 import requests_mock
 from unittest import mock
 from test.utils import assert_matching_tables
 from parsons import Bloomerang, Table
 
-from test.test_bloomerang.test_data import ENV_PARAMETERS, ID, TEST_DELETE, \
-    TEST_CREATE_CONSTITUENT, TEST_GET_CONSTITUENT, TEST_GET_CONSTITUENTS, \
-    TEST_CREATE_TRANSACTION, TEST_GET_TRANSACTION, TEST_GET_TRANSACTIONS, \
-    TEST_CREATE_INTERACTION, TEST_GET_INTERACTION, TEST_GET_INTERACTIONS
+from test.test_bloomerang.test_data import (
+    ENV_PARAMETERS,
+    ID,
+    TEST_DELETE,
+    TEST_CREATE_CONSTITUENT,
+    TEST_GET_CONSTITUENT,
+    TEST_GET_CONSTITUENTS,
+    TEST_CREATE_TRANSACTION,
+    TEST_GET_TRANSACTION,
+    TEST_GET_TRANSACTIONS,
+    TEST_CREATE_INTERACTION,
+    TEST_GET_INTERACTION,
+    TEST_GET_INTERACTIONS,
+)
 
 
 class TestBloomerang(unittest.TestCase):
-
     def setUp(self):
-        self.bloomerang = Bloomerang(api_key='test_key')
+        self.bloomerang = Bloomerang(api_key="test_key")
 
     @mock.patch.dict(os.environ, ENV_PARAMETERS)
     def test_init_env(self):
         bloomerang = Bloomerang()
-        self.assertEqual(bloomerang.api_key, 'env_api_key')
-        self.assertEqual(bloomerang.client_id, 'env_client_id')
-        self.assertEqual(bloomerang.client_secret, 'env_client_secret')
+        self.assertEqual(bloomerang.api_key, "env_api_key")
+        self.assertEqual(bloomerang.client_id, "env_client_id")
+        self.assertEqual(bloomerang.client_secret, "env_client_secret")
 
     @requests_mock.Mocker()
     def test_authentication(self, m):
         # API key
-        bloomerang = Bloomerang(api_key='my_key')
-        self.assertEqual(bloomerang.conn.headers['X-API-KEY'], 'my_key')
+        bloomerang = Bloomerang(api_key="my_key")
+        self.assertEqual(bloomerang.conn.headers["X-API-KEY"], "my_key")
 
         # OAuth2
-        m.post(url=bloomerang.uri_auth, json={'code': 'my_auth_code'})
-        m.post(url=bloomerang.uri + 'oauth/token', json={'access_token': 'my_access_token'})
-        bloomerang = Bloomerang(client_id='my_id', client_secret='my_secret')
-        self.assertEqual(bloomerang.authorization_code, 'my_auth_code')
-        self.assertEqual(bloomerang.access_token, 'my_access_token')
-        self.assertEqual(bloomerang.conn.headers['Authorization'], 'Bearer my_access_token')
+        m.post(url=bloomerang.uri_auth, json={"code": "my_auth_code"})
+        m.post(
+            url=bloomerang.uri + "oauth/token", json={"access_token": "my_access_token"}
+        )
+        bloomerang = Bloomerang(client_id="my_id", client_secret="my_secret")
+        self.assertEqual(bloomerang.authorization_code, "my_auth_code")
+        self.assertEqual(bloomerang.access_token, "my_access_token")
+        self.assertEqual(
+            bloomerang.conn.headers["Authorization"], "Bearer my_access_token"
+        )
 
     def test_base_endpoint(self):
-        url = self.bloomerang._base_endpoint('constituent')
-        self.assertEqual(url, 'https://api.bloomerang.co/v2/constituent/')
+        url = self.bloomerang._base_endpoint("constituent")
+        self.assertEqual(url, "https://api.bloomerang.co/v2/constituent/")
 
-        url = self.bloomerang._base_endpoint('constituent', 1234)
-        self.assertEqual(url, 'https://api.bloomerang.co/v2/constituent/1234/')
+        url = self.bloomerang._base_endpoint("constituent", 1234)
+        self.assertEqual(url, "https://api.bloomerang.co/v2/constituent/1234/")
 
-        url = self.bloomerang._base_endpoint('constituent', '1234')
-        self.assertEqual(url, 'https://api.bloomerang.co/v2/constituent/1234/')
+        url = self.bloomerang._base_endpoint("constituent", "1234")
+        self.assertEqual(url, "https://api.bloomerang.co/v2/constituent/1234/")
 
     @requests_mock.Mocker()
     def test_create_constituent(self, m):
-        m.post(f'{self.bloomerang.uri}constituent/', json=TEST_CREATE_CONSTITUENT)
+        m.post(f"{self.bloomerang.uri}constituent/", json=TEST_CREATE_CONSTITUENT)
         self.assertEqual(self.bloomerang.create_constituent(), TEST_CREATE_CONSTITUENT)
 
     @requests_mock.Mocker()
     def test_update_constituent(self, m):
-        m.put(f'{self.bloomerang.uri}constituent/{ID}/', json=TEST_CREATE_CONSTITUENT)
-        self.assertEqual(self.bloomerang.update_constituent(ID), TEST_CREATE_CONSTITUENT)
+        m.put(f"{self.bloomerang.uri}constituent/{ID}/", json=TEST_CREATE_CONSTITUENT)
+        self.assertEqual(
+            self.bloomerang.update_constituent(ID), TEST_CREATE_CONSTITUENT
+        )
 
     @requests_mock.Mocker()
     def test_get_constituent(self, m):
-        m.get(f'{self.bloomerang.uri}constituent/{ID}/', json=TEST_GET_CONSTITUENT)
+        m.get(f"{self.bloomerang.uri}constituent/{ID}/", json=TEST_GET_CONSTITUENT)
         self.assertEqual(self.bloomerang.get_constituent(ID), TEST_GET_CONSTITUENT)
 
     @requests_mock.Mocker()
     def test_delete_constituent(self, m):
-        m.delete(f'{self.bloomerang.uri}constituent/{ID}/', json=TEST_DELETE)
+        m.delete(f"{self.bloomerang.uri}constituent/{ID}/", json=TEST_DELETE)
         self.assertEqual(self.bloomerang.delete_constituent(ID), TEST_DELETE)
 
     @requests_mock.Mocker()
     def test_get_constituents(self, m):
-        m.get(f'{self.bloomerang.uri}constituents/?skip=0&take=50', json=TEST_GET_CONSTITUENTS)
-        assert_matching_tables(self.bloomerang.get_constituents(),
-                               Table(TEST_GET_CONSTITUENTS['Results']))
+        m.get(
+            f"{self.bloomerang.uri}constituents/?skip=0&take=50",
+            json=TEST_GET_CONSTITUENTS,
+        )
+        assert_matching_tables(
+            self.bloomerang.get_constituents(), Table(TEST_GET_CONSTITUENTS["Results"])
+        )
 
     @requests_mock.Mocker()
     def test_create_transaction(self, m):
-        m.post(f'{self.bloomerang.uri}transaction/', json=TEST_CREATE_TRANSACTION)
+        m.post(f"{self.bloomerang.uri}transaction/", json=TEST_CREATE_TRANSACTION)
         self.assertEqual(self.bloomerang.create_transaction(), TEST_CREATE_TRANSACTION)
 
     @requests_mock.Mocker()
     def test_update_transaction(self, m):
-        m.put(f'{self.bloomerang.uri}transaction/{ID}/', json=TEST_CREATE_TRANSACTION)
-        self.assertEqual(self.bloomerang.update_transaction(ID), TEST_CREATE_TRANSACTION)
+        m.put(f"{self.bloomerang.uri}transaction/{ID}/", json=TEST_CREATE_TRANSACTION)
+        self.assertEqual(
+            self.bloomerang.update_transaction(ID), TEST_CREATE_TRANSACTION
+        )
 
     @requests_mock.Mocker()
     def test_get_transaction(self, m):
-        m.get(f'{self.bloomerang.uri}transaction/{ID}/', json=TEST_GET_TRANSACTION)
+        m.get(f"{self.bloomerang.uri}transaction/{ID}/", json=TEST_GET_TRANSACTION)
         self.assertEqual(self.bloomerang.get_transaction(ID), TEST_GET_TRANSACTION)
 
     @requests_mock.Mocker()
     def test_delete_transaction(self, m):
-        m.delete(f'{self.bloomerang.uri}transaction/{ID}/', json=TEST_DELETE)
+        m.delete(f"{self.bloomerang.uri}transaction/{ID}/", json=TEST_DELETE)
         self.assertEqual(self.bloomerang.delete_transaction(ID), TEST_DELETE)
 
     @requests_mock.Mocker()
     def test_get_transactions(self, m):
-        m.get(f'{self.bloomerang.uri}transactions/?skip=0&take=50', json=TEST_GET_TRANSACTIONS)
-        assert_matching_tables(self.bloomerang.get_transactions(),
-                               Table(TEST_GET_TRANSACTIONS['Results']))
+        m.get(
+            f"{self.bloomerang.uri}transactions/?skip=0&take=50",
+            json=TEST_GET_TRANSACTIONS,
+        )
+        assert_matching_tables(
+            self.bloomerang.get_transactions(), Table(TEST_GET_TRANSACTIONS["Results"])
+        )
 
     @requests_mock.Mocker()
     def test_get_transaction_designation(self, m):
-        m.get(f'{self.bloomerang.uri}transaction/designation/{ID}/', json=TEST_GET_TRANSACTION)
-        self.assertEqual(self.bloomerang.get_transaction_designation(ID), TEST_GET_TRANSACTION)
+        m.get(
+            f"{self.bloomerang.uri}transaction/designation/{ID}/",
+            json=TEST_GET_TRANSACTION,
+        )
+        self.assertEqual(
+            self.bloomerang.get_transaction_designation(ID), TEST_GET_TRANSACTION
+        )
 
     @requests_mock.Mocker()
     def test_get_transaction_designations(self, m):
-        m.get(f'{self.bloomerang.uri}transactions/designations/?skip=0&take=50',
-              json=TEST_GET_TRANSACTIONS)
-        assert_matching_tables(self.bloomerang.get_transaction_designations(),
-                               Table(TEST_GET_TRANSACTIONS['Results']))
+        m.get(
+            f"{self.bloomerang.uri}transactions/designations/?skip=0&take=50",
+            json=TEST_GET_TRANSACTIONS,
+        )
+        assert_matching_tables(
+            self.bloomerang.get_transaction_designations(),
+            Table(TEST_GET_TRANSACTIONS["Results"]),
+        )
 
     @requests_mock.Mocker()
     def test_create_interaction(self, m):
-        m.post(f'{self.bloomerang.uri}interaction/', json=TEST_CREATE_INTERACTION)
+        m.post(f"{self.bloomerang.uri}interaction/", json=TEST_CREATE_INTERACTION)
         self.assertEqual(self.bloomerang.create_interaction(), TEST_CREATE_INTERACTION)
 
     @requests_mock.Mocker()
     def test_update_interaction(self, m):
-        m.put(f'{self.bloomerang.uri}interaction/{ID}/', json=TEST_CREATE_INTERACTION)
-        self.assertEqual(self.bloomerang.update_interaction(ID), TEST_CREATE_INTERACTION)
+        m.put(f"{self.bloomerang.uri}interaction/{ID}/", json=TEST_CREATE_INTERACTION)
+        self.assertEqual(
+            self.bloomerang.update_interaction(ID), TEST_CREATE_INTERACTION
+        )
 
     @requests_mock.Mocker()
     def test_get_interaction(self, m):
-        m.get(f'{self.bloomerang.uri}interaction/{ID}/', json=TEST_GET_INTERACTION)
+        m.get(f"{self.bloomerang.uri}interaction/{ID}/", json=TEST_GET_INTERACTION)
         self.assertEqual(self.bloomerang.get_interaction(ID), TEST_GET_INTERACTION)
 
     @requests_mock.Mocker()
     def test_delete_interaction(self, m):
-        m.delete(f'{self.bloomerang.uri}interaction/{ID}/', json=TEST_DELETE)
+        m.delete(f"{self.bloomerang.uri}interaction/{ID}/", json=TEST_DELETE)
         self.assertEqual(self.bloomerang.delete_interaction(ID), TEST_DELETE)
 
     @requests_mock.Mocker()
     def test_get_interactions(self, m):
-        m.get(f'{self.bloomerang.uri}interactions/?skip=0&take=50', json=TEST_GET_INTERACTIONS)
-        assert_matching_tables(self.bloomerang.get_interactions(),
-                               Table(TEST_GET_INTERACTIONS['Results']))
+        m.get(
+            f"{self.bloomerang.uri}interactions/?skip=0&take=50",
+            json=TEST_GET_INTERACTIONS,
+        )
+        assert_matching_tables(
+            self.bloomerang.get_interactions(), Table(TEST_GET_INTERACTIONS["Results"])
+        )
```

### Comparing `parsons-1.0.0/test/test_bloomerang/test_data.py` & `parsons-1.1.0/test/test_bloomerang/test_data.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,20 +1,16 @@
 ENV_PARAMETERS = {
-    'BLOOMERANG_API_KEY': 'env_api_key',
-    'BLOOMERANG_CLIENT_ID': 'env_client_id',
-    'BLOOMERANG_CLIENT_SECRET': 'env_client_secret'
+    "BLOOMERANG_API_KEY": "env_api_key",
+    "BLOOMERANG_CLIENT_ID": "env_client_id",
+    "BLOOMERANG_CLIENT_SECRET": "env_client_secret",
 }
 
 ID = 123
 
-TEST_DELETE = {
-    "Id": 0,
-    "Type": "string",
-    "Deleted": 'true'
-}
+TEST_DELETE = {"Id": 0, "Type": "string", "Deleted": "true"}
 
 TEST_CREATE_CONSTITUENT = {
     "Type": "Individual",
     "Status": "Active",
     "FirstName": "string",
     "LastName": "string",
     "MiddleName": "string",
@@ -35,45 +31,45 @@
     "Birthdate": "2020-08-24",
     "ProfilePictureType": "None",
     "PrimaryEmail": {
         "Id": 0,
         "AccountId": 0,
         "Type": "Home",
         "Value": "user@example.com",
-        "IsPrimary": 'true',
-        "IsBad": 'true'
+        "IsPrimary": "true",
+        "IsBad": "true",
     },
     "PrimaryPhone": {
         "Id": 0,
         "AccountId": 0,
         "Type": "Home",
         "Extension": "string",
         "Number": "string",
-        "IsPrimary": 'true'
+        "IsPrimary": "true",
     },
     "PrimaryAddress": {
         "Id": 0,
         "AccountId": 0,
         "Type": "Home",
         "Street": "string",
         "City": "string",
         "State": "string",
         "PostalCode": "string",
         "Country": "string",
-        "IsPrimary": 'true',
-        "IsBad": 'true'
-    }
+        "IsPrimary": "true",
+        "IsBad": "true",
+    },
 }
 
 TEST_GET_CONSTITUENT = {
     "Id": 0,
     "AccountNumber": 0,
-    "IsInHousehold": 'true',
-    "IsHeadOfHousehold": 'true',
-    "IsFavorite": 'true',
+    "IsInHousehold": "true",
+    "IsHeadOfHousehold": "true",
+    "IsFavorite": "true",
     "FullCustomProfileImageId": 0,
     "FullCustomProfileImageUrl": "string",
     "CroppedCustomProfileImageId": 0,
     "CroppedCustomProfileImageUrl": "string",
     "Type": "Individual",
     "Status": "Active",
     "FirstName": "string",
@@ -96,116 +92,89 @@
     "Birthdate": "2020-09-08",
     "ProfilePictureType": "None",
     "PrimaryEmail": {
         "Id": 0,
         "AccountId": 0,
         "Type": "Home",
         "Value": "user@example.com",
-        "IsPrimary": 'true',
-        "IsBad": 'true'
+        "IsPrimary": "true",
+        "IsBad": "true",
     },
     "PrimaryPhone": {
         "Id": 0,
         "AccountId": 0,
         "Type": "Home",
         "Extension": "string",
         "Number": "string",
-        "IsPrimary": 'true'
+        "IsPrimary": "true",
     },
     "HouseholdId": 0,
     "PreferredCommunicationChannel": "Email",
-    "CommunicationRestrictions": [
-        "DoNotCall"
-    ],
+    "CommunicationRestrictions": ["DoNotCall"],
     "CommunicationRestrictionsUpdateReason": "string",
     "EmailInterestType": "All",
-    "CustomEmailInterests": [
-        {
-            "Id": 0,
-            "Name": "string"
-        }
-    ],
+    "CustomEmailInterests": [{"Id": 0, "Name": "string"}],
     "EmailInterestsUpdateReason": "string",
     "PrimaryAddress": {
         "Id": 0,
         "AccountId": 0,
         "Type": "Home",
         "Street": "string",
         "City": "string",
         "State": "string",
         "PostalCode": "string",
         "Country": "string",
-        "IsPrimary": 'true',
-        "IsBad": 'true',
+        "IsPrimary": "true",
+        "IsBad": "true",
         "StateAbbreviation": "string",
-        "CountryCode": "string"
+        "CountryCode": "string",
     },
     "EngagementScore": "Low",
     "DonorSearchInfo": {
         "Id": 0,
         "GenerosityScore": "Low",
         "AnnualFundLikelihood": "Low",
         "MajorGiftLikelihood": "Low",
         "PlannedGiftLikelihood": "Low",
         "Quality": "Low",
         "LargestGiftMin": 0,
         "LargestGiftMax": 0,
         "WealthAskMin": 0,
         "WealthAskMax": 0,
-        "BusinessExecutive": 'true',
+        "BusinessExecutive": "true",
         "NamesScreened": "string",
-        "DateTimeScreenedUtc": "string"
+        "DateTimeScreenedUtc": "string",
     },
-    "AddressIds": [
-        0
-    ],
-    "EmailIds": [
-        0
-    ],
-    "PhoneIds": [
-        0
-    ],
+    "AddressIds": [0],
+    "EmailIds": [0],
+    "PhoneIds": [0],
     "CustomValues": [
-        {
-            "FieldId": 0,
-            "Value": {
-                "Id": 0,
-                "Value": "string"
-            }
-        },
-        {
-            "FieldId": 0,
-            "Values": [
-                {
-                    "Id": 0,
-                    "Value": "string"
-                }
-            ]
-        }
+        {"FieldId": 0, "Value": {"Id": 0, "Value": "string"}},
+        {"FieldId": 0, "Values": [{"Id": 0, "Value": "string"}]},
     ],
     "AuditTrail": {
         "CreatedDate": "2020-09-08T16:06:59.945Z",
         "CreatedName": "string",
         "LastModifiedDate": "2020-09-08T16:06:59.945Z",
-        "LastModifiedName": "string"
-    }
+        "LastModifiedName": "string",
+    },
 }
 
 TEST_GET_CONSTITUENTS = {
     "Total": 0,
     "TotalFiltered": 0,
     "Start": 0,
     "ResultCount": 0,
     "Results": [
         {
             "Id": 0,
             "AccountNumber": 0,
-            "IsInHousehold": 'true',
-            "IsHeadOfHousehold": 'true',
-            "IsFavorite": 'true',
+            "IsInHousehold": "true",
+            "IsHeadOfHousehold": "true",
+            "IsFavorite": "true",
             "FullCustomProfileImageId": 0,
             "FullCustomProfileImageUrl": "string",
             "CroppedCustomProfileImageId": 0,
             "CroppedCustomProfileImageUrl": "string",
             "Type": "Individual",
             "Status": "Active",
             "FirstName": "string",
@@ -228,90 +197,63 @@
             "Birthdate": "2020-09-05",
             "ProfilePictureType": "None",
             "PrimaryEmail": {
                 "Id": 0,
                 "AccountId": 0,
                 "Type": "Home",
                 "Value": "user@example.com",
-                "IsPrimary": 'true',
-                "IsBad": 'true'
+                "IsPrimary": "true",
+                "IsBad": "true",
             },
             "PrimaryPhone": {
                 "Id": 0,
                 "AccountId": 0,
                 "Type": "Home",
                 "Extension": "string",
                 "Number": "string",
-                "IsPrimary": 'true'
+                "IsPrimary": "true",
             },
             "HouseholdId": 0,
             "PreferredCommunicationChannel": "Email",
-            "CommunicationRestrictions": [
-                "DoNotCall"
-            ],
+            "CommunicationRestrictions": ["DoNotCall"],
             "CommunicationRestrictionsUpdateReason": "string",
             "EmailInterestType": "All",
-            "CustomEmailInterests": [
-                {
-                    "Id": 0,
-                    "Name": "string"
-                }
-            ],
+            "CustomEmailInterests": [{"Id": 0, "Name": "string"}],
             "EmailInterestsUpdateReason": "string",
             "EngagementScore": "Low",
             "DonorSearchInfo": {
                 "Id": 0,
                 "GenerosityScore": "Low",
                 "AnnualFundLikelihood": "Low",
                 "MajorGiftLikelihood": "Low",
                 "PlannedGiftLikelihood": "Low",
                 "Quality": "Low",
                 "LargestGiftMin": 0,
                 "LargestGiftMax": 0,
                 "WealthAskMin": 0,
                 "WealthAskMax": 0,
-                "BusinessExecutive": 'true',
+                "BusinessExecutive": "true",
                 "NamesScreened": "string",
-                "DateTimeScreenedUtc": "string"
+                "DateTimeScreenedUtc": "string",
             },
-            "AddressIds": [
-                0
-            ],
-            "EmailIds": [
-                0
-            ],
-            "PhoneIds": [
-                0
-            ],
+            "AddressIds": [0],
+            "EmailIds": [0],
+            "PhoneIds": [0],
             "CustomValues": [
-                {
-                    "FieldId": 0,
-                    "Value": {
-                        "Id": 0,
-                        "Value": "string"
-                    }
-                },
-                {
-                    "FieldId": 0,
-                    "Values": [
-                        {
-                            "Id": 0,
-                            "Value": "string"
-                        }
-                    ]
-                }
+                {"FieldId": 0, "Value": {"Id": 0, "Value": "string"}},
+                {"FieldId": 0, "Values": [{"Id": 0, "Value": "string"}]},
             ],
             "AuditTrail": {
                 "CreatedDate": "2020-09-05T01:40:43.035Z",
                 "CreatedName": "string",
                 "LastModifiedDate": "2020-09-05T01:40:43.035Z",
-                "LastModifiedName": "string"
-            }
+                "LastModifiedName": "string",
+            },
         }
-    ]
+    ],
 }
 
 TEST_CREATE_TRANSACTION = {
     "AccountId": 0,
     "Date": "2020-09-08",
     "Amount": 0,
     "Method": "None",
@@ -329,171 +271,93 @@
     "InKindMarketValue": 0,
     "WalletItemId": 0,
     "IntegrationUrl": "string",
     "Designations": [
         {
             "Amount": 0,
             "Note": "string",
-            "AcknowledgementStatus": 'true',
-            "AcknowledgementInteractionIds": [
-                0
-            ],
+            "AcknowledgementStatus": "true",
+            "AcknowledgementInteractionIds": [0],
             "Type": "Donation",
             "NonDeductibleAmount": 0,
             "FundId": 0,
             "QuickbooksAccountId": 0,
             "CampaignId": 0,
             "AppealId": 0,
             "TributeId": 0,
-            "SoftCredits": [
-                'null'
-            ],
+            "SoftCredits": ["null"],
             "CustomValues": [
-                {
-                    "FieldId": 0,
-                    "Value": "string"
-                },
-                {
-                    "FieldId": 0,
-                    "ValueId": 0
-                },
-                {
-                    "FieldId": 0,
-                    "ValueIds": [
-                        0
-                    ]
-                }
+                {"FieldId": 0, "Value": "string"},
+                {"FieldId": 0, "ValueId": 0},
+                {"FieldId": 0, "ValueIds": [0]},
             ],
-            "Attachments": [
-                'null',
-                'null'
-            ]
+            "Attachments": ["null", "null"],
         },
         {
             "Amount": 0,
             "Note": "string",
-            "AcknowledgementStatus": 'true',
-            "AcknowledgementInteractionIds": [
-                0
-            ],
+            "AcknowledgementStatus": "true",
+            "AcknowledgementInteractionIds": [0],
             "Type": "PledgePayment",
             "PledgeId": 0,
             "CustomValues": [
-                {
-                    "FieldId": 0,
-                    "Value": "string"
-                },
-                {
-                    "FieldId": 0,
-                    "ValueId": 0
-                },
-                {
-                    "FieldId": 0,
-                    "ValueIds": [
-                        0
-                    ]
-                }
+                {"FieldId": 0, "Value": "string"},
+                {"FieldId": 0, "ValueId": 0},
+                {"FieldId": 0, "ValueIds": [0]},
             ],
-            "Attachments": [
-                'null',
-                'null'
-            ]
+            "Attachments": ["null", "null"],
         },
         {
             "RecurringDonationEndDate": "2020-09-08",
             "Amount": 0,
             "Note": "string",
-            "AcknowledgementStatus": 'true',
-            "AcknowledgementInteractionIds": [
-                0
-            ],
+            "AcknowledgementStatus": "true",
+            "AcknowledgementInteractionIds": [0],
             "RecurringDonationFrequency": "Weekly",
             "RecurringDonationDay1": 0,
             "RecurringDonationDay2": 0,
             "RecurringDonationStartDate": "2020-09-08",
             "Type": "RecurringDonation",
             "FundId": 0,
             "QuickbooksAccountId": 0,
             "CampaignId": 0,
             "AppealId": 0,
             "TributeId": 0,
-            "SoftCredits": [
-                'null'
-            ],
+            "SoftCredits": ["null"],
             "CustomValues": [
-                {
-                    "FieldId": 0,
-                    "Value": "string"
-                },
-                {
-                    "FieldId": 0,
-                    "ValueId": 0
-                },
-                {
-                    "FieldId": 0,
-                    "ValueIds": [
-                        0
-                    ]
-                }
+                {"FieldId": 0, "Value": "string"},
+                {"FieldId": 0, "ValueId": 0},
+                {"FieldId": 0, "ValueIds": [0]},
             ],
-            "Attachments": [
-                'null',
-                'null'
-            ]
+            "Attachments": ["null", "null"],
         },
         {
             "Amount": 0,
             "Note": "string",
-            "AcknowledgementStatus": 'true',
-            "AcknowledgementInteractionIds": [
-                0
-            ],
+            "AcknowledgementStatus": "true",
+            "AcknowledgementInteractionIds": [0],
             "Type": "RecurringDonationPayment",
             "RecurringDonationId": 0,
             "FundId": 0,
             "QuickbooksAccountId": 0,
             "CampaignId": 0,
             "AppealId": 0,
-            "IsExtraPayment": 'true',
+            "IsExtraPayment": "true",
             "CustomValues": [
-                {
-                    "FieldId": 0,
-                    "Value": "string"
-                },
-                {
-                    "FieldId": 0,
-                    "ValueId": 0
-                },
-                {
-                    "FieldId": 0,
-                    "ValueIds": [
-                        0
-                    ]
-                }
+                {"FieldId": 0, "Value": "string"},
+                {"FieldId": 0, "ValueId": 0},
+                {"FieldId": 0, "ValueIds": [0]},
             ],
-            "Attachments": [
-                'null',
-                'null'
-            ]
-        }
+            "Attachments": ["null", "null"],
+        },
     ],
     "Attachments": [
-        {
-            "Guid": "string",
-            "Name": "string",
-            "Extension": "string",
-            "Url": "string"
-        },
-        {
-            "Id": 0,
-            "Name": "string",
-            "Extension": "string",
-            "Url": "string"
-        }
-    ]
+        {"Guid": "string", "Name": "string", "Extension": "string", "Url": "string"},
+        {"Id": 0, "Name": "string", "Extension": "string", "Url": "string"},
+    ],
 }
 
 TEST_GET_TRANSACTION = {
     "Id": 0,
     "TransactionNumber": 0,
     "NonDeductibleAmount": 0,
     "AccountId": 0,
@@ -517,369 +381,181 @@
         {
             "Id": 0,
             "DesignationNumber": 0,
             "TransactionId": 0,
             "Amount": 0,
             "NonDeductibleAmount": 0,
             "Note": "string",
-            "AcknowledgementStatus": 'true',
-            "AcknowledgementInteractionIds": [
-                0
-            ],
-            "Fund": {
-                "Id": 0,
-                "Name": "string"
-            },
-            "QuickbooksAccount": {
-                "Id": 0,
-                "Name": "string"
-            },
-            "Campaign": {
-                "Id": 0,
-                "Name": "string"
-            },
-            "Appeal": {
-                "Id": 0,
-                "Name": "string"
-            },
-            "Tribute": {
-                "Id": 0,
-                "Name": "string"
-            },
+            "AcknowledgementStatus": "true",
+            "AcknowledgementInteractionIds": [0],
+            "Fund": {"Id": 0, "Name": "string"},
+            "QuickbooksAccount": {"Id": 0, "Name": "string"},
+            "Campaign": {"Id": 0, "Name": "string"},
+            "Appeal": {"Id": 0, "Name": "string"},
+            "Tribute": {"Id": 0, "Name": "string"},
             "TributeType": "InHonorOf",
-            "SoftCreditIds": [
-                0
-            ],
-            "AttachmentIds": [
-                0
-            ],
+            "SoftCreditIds": [0],
+            "AttachmentIds": [0],
             "CustomValues": [
-                {
-                    "FieldId": 0,
-                    "Value": {
-                        "Id": 0,
-                        "Value": "string"
-                    }
-                },
-                {
-                    "FieldId": 0,
-                    "Values": [
-                        {
-                            "Id": 0,
-                            "Value": "string"
-                        }
-                    ]
-                }
+                {"FieldId": 0, "Value": {"Id": 0, "Value": "string"}},
+                {"FieldId": 0, "Values": [{"Id": 0, "Value": "string"}]},
             ],
             "AuditTrail": {
                 "CreatedDate": "2020-09-08T14:15:24.293Z",
                 "CreatedName": "string",
                 "LastModifiedDate": "2020-09-08T14:15:24.293Z",
-                "LastModifiedName": "string"
+                "LastModifiedName": "string",
             },
-            "Type": "Donation"
+            "Type": "Donation",
         },
         {
             "Id": 0,
             "DesignationNumber": 0,
             "TransactionId": 0,
             "Amount": 0,
             "NonDeductibleAmount": 0,
             "Note": "string",
-            "AcknowledgementStatus": 'true',
-            "AcknowledgementInteractionIds": [
-                0
-            ],
-            "Fund": {
-                "Id": 0,
-                "Name": "string"
-            },
-            "QuickbooksAccount": {
-                "Id": 0,
-                "Name": "string"
-            },
-            "Campaign": {
-                "Id": 0,
-                "Name": "string"
-            },
-            "Appeal": {
-                "Id": 0,
-                "Name": "string"
-            },
-            "Tribute": {
-                "Id": 0,
-                "Name": "string"
-            },
+            "AcknowledgementStatus": "true",
+            "AcknowledgementInteractionIds": [0],
+            "Fund": {"Id": 0, "Name": "string"},
+            "QuickbooksAccount": {"Id": 0, "Name": "string"},
+            "Campaign": {"Id": 0, "Name": "string"},
+            "Appeal": {"Id": 0, "Name": "string"},
+            "Tribute": {"Id": 0, "Name": "string"},
             "TributeType": "InHonorOf",
-            "SoftCreditIds": [
-                0
-            ],
-            "AttachmentIds": [
-                0
-            ],
+            "SoftCreditIds": [0],
+            "AttachmentIds": [0],
             "CustomValues": [
-                {
-                    "FieldId": 0,
-                    "Value": {
-                        "Id": 0,
-                        "Value": "string"
-                    }
-                },
-                {
-                    "FieldId": 0,
-                    "Values": [
-                        {
-                            "Id": 0,
-                            "Value": "string"
-                        }
-                    ]
-                }
+                {"FieldId": 0, "Value": {"Id": 0, "Value": "string"}},
+                {"FieldId": 0, "Values": [{"Id": 0, "Value": "string"}]},
             ],
             "AuditTrail": {
                 "CreatedDate": "2020-09-08T14:15:24.293Z",
                 "CreatedName": "string",
                 "LastModifiedDate": "2020-09-08T14:15:24.293Z",
-                "LastModifiedName": "string"
+                "LastModifiedName": "string",
             },
             "Type": "Pledge",
-            "PledgePaymentIds": [
-                0
-            ],
+            "PledgePaymentIds": [0],
             "PledgeInstallments": [
-                {
-                    "Id": 0,
-                    "PledgeId": 0,
-                    "Date": "2020-09-08",
-                    "Amount": 0
-                }
+                {"Id": 0, "PledgeId": 0, "Date": "2020-09-08", "Amount": 0}
             ],
             "PledgeBalance": 0,
             "PledgeStatus": "InGoodStanding",
             "PledgeAmountInArrears": 0,
-            "PledgeNextInstallmentDate": "2020-09-08"
+            "PledgeNextInstallmentDate": "2020-09-08",
         },
         {
             "Id": 0,
             "DesignationNumber": 0,
             "TransactionId": 0,
             "Amount": 0,
             "NonDeductibleAmount": 0,
             "Note": "string",
-            "AcknowledgementStatus": 'true',
-            "AcknowledgementInteractionIds": [
-                0
-            ],
-            "Fund": {
-                "Id": 0,
-                "Name": "string"
-            },
-            "QuickbooksAccount": {
-                "Id": 0,
-                "Name": "string"
-            },
-            "Campaign": {
-                "Id": 0,
-                "Name": "string"
-            },
-            "Appeal": {
-                "Id": 0,
-                "Name": "string"
-            },
-            "Tribute": {
-                "Id": 0,
-                "Name": "string"
-            },
+            "AcknowledgementStatus": "true",
+            "AcknowledgementInteractionIds": [0],
+            "Fund": {"Id": 0, "Name": "string"},
+            "QuickbooksAccount": {"Id": 0, "Name": "string"},
+            "Campaign": {"Id": 0, "Name": "string"},
+            "Appeal": {"Id": 0, "Name": "string"},
+            "Tribute": {"Id": 0, "Name": "string"},
             "TributeType": "InHonorOf",
-            "SoftCreditIds": [
-                0
-            ],
-            "AttachmentIds": [
-                0
-            ],
+            "SoftCreditIds": [0],
+            "AttachmentIds": [0],
             "CustomValues": [
-                {
-                    "FieldId": 0,
-                    "Value": {
-                        "Id": 0,
-                        "Value": "string"
-                    }
-                },
-                {
-                    "FieldId": 0,
-                    "Values": [
-                        {
-                            "Id": 0,
-                            "Value": "string"
-                        }
-                    ]
-                }
+                {"FieldId": 0, "Value": {"Id": 0, "Value": "string"}},
+                {"FieldId": 0, "Values": [{"Id": 0, "Value": "string"}]},
             ],
             "AuditTrail": {
                 "CreatedDate": "2020-09-08T14:15:24.293Z",
                 "CreatedName": "string",
                 "LastModifiedDate": "2020-09-08T14:15:24.293Z",
-                "LastModifiedName": "string"
+                "LastModifiedName": "string",
             },
             "Type": "PledgePayment",
-            "PledgeId": 0
+            "PledgeId": 0,
         },
         {
             "Id": 0,
             "DesignationNumber": 0,
             "TransactionId": 0,
             "Amount": 0,
             "NonDeductibleAmount": 0,
             "Note": "string",
-            "AcknowledgementStatus": 'true',
-            "AcknowledgementInteractionIds": [
-                0
-            ],
-            "Fund": {
-                "Id": 0,
-                "Name": "string"
-            },
-            "QuickbooksAccount": {
-                "Id": 0,
-                "Name": "string"
-            },
-            "Campaign": {
-                "Id": 0,
-                "Name": "string"
-            },
-            "Appeal": {
-                "Id": 0,
-                "Name": "string"
-            },
-            "Tribute": {
-                "Id": 0,
-                "Name": "string"
-            },
+            "AcknowledgementStatus": "true",
+            "AcknowledgementInteractionIds": [0],
+            "Fund": {"Id": 0, "Name": "string"},
+            "QuickbooksAccount": {"Id": 0, "Name": "string"},
+            "Campaign": {"Id": 0, "Name": "string"},
+            "Appeal": {"Id": 0, "Name": "string"},
+            "Tribute": {"Id": 0, "Name": "string"},
             "TributeType": "InHonorOf",
-            "SoftCreditIds": [
-                0
-            ],
-            "AttachmentIds": [
-                0
-            ],
+            "SoftCreditIds": [0],
+            "AttachmentIds": [0],
             "CustomValues": [
-                {
-                    "FieldId": 0,
-                    "Value": {
-                        "Id": 0,
-                        "Value": "string"
-                    }
-                },
-                {
-                    "FieldId": 0,
-                    "Values": [
-                        {
-                            "Id": 0,
-                            "Value": "string"
-                        }
-                    ]
-                }
+                {"FieldId": 0, "Value": {"Id": 0, "Value": "string"}},
+                {"FieldId": 0, "Values": [{"Id": 0, "Value": "string"}]},
             ],
             "AuditTrail": {
                 "CreatedDate": "2020-09-08T14:15:24.293Z",
                 "CreatedName": "string",
                 "LastModifiedDate": "2020-09-08T14:15:24.293Z",
-                "LastModifiedName": "string"
+                "LastModifiedName": "string",
             },
             "RecurringDonationEndDate": "2020-09-08",
             "RecurringDonationFrequency": "Weekly",
             "RecurringDonationDay1": 0,
             "RecurringDonationDay2": 0,
             "RecurringDonationStartDate": "2020-09-08",
             "Type": "RecurringDonation",
-            "RecurringDonationPaymentIds": [
-                0
-            ],
+            "RecurringDonationPaymentIds": [0],
             "RecurringDonationNextInstallmentDate": "2020-09-08",
             "RecurringDonationLastPaymentStatus": "AtRisk",
-            "RecurringDonationStatus": "Active"
+            "RecurringDonationStatus": "Active",
         },
         {
             "Id": 0,
             "DesignationNumber": 0,
             "TransactionId": 0,
             "Amount": 0,
             "NonDeductibleAmount": 0,
             "Note": "string",
-            "AcknowledgementStatus": 'true',
-            "AcknowledgementInteractionIds": [
-                0
-            ],
-            "Fund": {
-                "Id": 0,
-                "Name": "string"
-            },
-            "QuickbooksAccount": {
-                "Id": 0,
-                "Name": "string"
-            },
-            "Campaign": {
-                "Id": 0,
-                "Name": "string"
-            },
-            "Appeal": {
-                "Id": 0,
-                "Name": "string"
-            },
-            "Tribute": {
-                "Id": 0,
-                "Name": "string"
-            },
+            "AcknowledgementStatus": "true",
+            "AcknowledgementInteractionIds": [0],
+            "Fund": {"Id": 0, "Name": "string"},
+            "QuickbooksAccount": {"Id": 0, "Name": "string"},
+            "Campaign": {"Id": 0, "Name": "string"},
+            "Appeal": {"Id": 0, "Name": "string"},
+            "Tribute": {"Id": 0, "Name": "string"},
             "TributeType": "InHonorOf",
-            "SoftCreditIds": [
-                0
-            ],
-            "AttachmentIds": [
-                0
-            ],
+            "SoftCreditIds": [0],
+            "AttachmentIds": [0],
             "CustomValues": [
-                {
-                    "FieldId": 0,
-                    "Value": {
-                        "Id": 0,
-                        "Value": "string"
-                    }
-                },
-                {
-                    "FieldId": 0,
-                    "Values": [
-                        {
-                            "Id": 0,
-                            "Value": "string"
-                        }
-                    ]
-                }
+                {"FieldId": 0, "Value": {"Id": 0, "Value": "string"}},
+                {"FieldId": 0, "Values": [{"Id": 0, "Value": "string"}]},
             ],
             "AuditTrail": {
                 "CreatedDate": "2020-09-08T14:15:24.293Z",
                 "CreatedName": "string",
                 "LastModifiedDate": "2020-09-08T14:15:24.293Z",
-                "LastModifiedName": "string"
+                "LastModifiedName": "string",
             },
             "Type": "RecurringDonationPayment",
-            "RecurringDonationId": 0
-        }
-    ],
-    "AttachmentIds": [
-        0
-    ],
-    "IsRefunded": 'true',
-    "RefundIds": [
-        0
+            "RecurringDonationId": 0,
+        },
     ],
+    "AttachmentIds": [0],
+    "IsRefunded": "true",
+    "RefundIds": [0],
     "AuditTrail": {
         "CreatedDate": "2020-09-08T14:15:24.293Z",
         "CreatedName": "string",
         "LastModifiedDate": "2020-09-08T14:15:24.293Z",
-        "LastModifiedName": "string"
-    }
+        "LastModifiedName": "string",
+    },
 }
 
 TEST_GET_TRANSACTIONS = {
     "Total": 0,
     "TotalFiltered": 0,
     "Start": 0,
     "ResultCount": 0,
@@ -901,125 +577,74 @@
             "EftAccountType": "Checking",
             "EftLastFourDigits": "string",
             "EftRoutingNumber": "string",
             "InKindDescription": "string",
             "InKindType": "Goods",
             "InKindMarketValue": 0,
             "IntegrationUrl": "string",
-            "Designations": [
-                'null',
-                'null',
-                'null',
-                'null',
-                'null'
-            ],
-            "AttachmentIds": [
-                0
-            ],
-            "IsRefunded": 'true',
-            "RefundIds": [
-                0
-            ],
+            "Designations": ["null", "null", "null", "null", "null"],
+            "AttachmentIds": [0],
+            "IsRefunded": "true",
+            "RefundIds": [0],
             "AuditTrail": {
                 "CreatedDate": "2020-09-08T16:11:30.821Z",
                 "CreatedName": "string",
                 "LastModifiedDate": "2020-09-08T16:11:30.821Z",
-                "LastModifiedName": "string"
-            }
+                "LastModifiedName": "string",
+            },
         }
-    ]
+    ],
 }
 
 TEST_CREATE_INTERACTION = {
     "AccountId": 0,
     "Date": "2020-09-08",
     "Note": "string",
     "Channel": "Email",
     "Purpose": "Acknowledgement",
     "Subject": "string",
-    "IsInbound": 'true',
+    "IsInbound": "true",
     "CustomValues": [
-        {
-            "FieldId": 0,
-            "Value": "string"
-        },
-        {
-            "FieldId": 0,
-            "ValueId": 0
-        },
-        {
-            "FieldId": 0,
-            "ValueIds": [
-                0
-            ]
-        }
+        {"FieldId": 0, "Value": "string"},
+        {"FieldId": 0, "ValueId": 0},
+        {"FieldId": 0, "ValueIds": [0]},
     ],
     "Attachments": [
-        {
-            "Guid": "string",
-            "Name": "string",
-            "Extension": "string",
-            "Url": "string"
-        },
-        {
-            "Id": 0,
-            "Name": "string",
-            "Extension": "string",
-            "Url": "string"
-        }
-    ]
+        {"Guid": "string", "Name": "string", "Extension": "string", "Url": "string"},
+        {"Id": 0, "Name": "string", "Extension": "string", "Url": "string"},
+    ],
 }
 
 TEST_GET_INTERACTION = {
     "Id": 0,
     "Date": "2020-09-08",
     "Note": "string",
     "Channel": "Email",
     "Purpose": "Acknowledgement",
     "Subject": "string",
-    "IsInbound": 'true',
+    "IsInbound": "true",
     "AccountId": 0,
     "TweetId": "string",
-    "IsBcc": 'true',
+    "IsBcc": "true",
     "EmailAddress": "user@example.com",
-    "AttachmentIds": [
-        0
-    ],
-    "LetterAttachmentIds": [
-        0
-    ],
-    "SurveyLapsedResponses": [
-        "string"
-    ],
+    "AttachmentIds": [0],
+    "LetterAttachmentIds": [0],
+    "SurveyLapsedResponses": ["string"],
     "SurveyEmailInteractionId": 0,
     "SurveyResponseInteractionId": 0,
     "CustomValues": [
-        {
-            "FieldId": 0,
-            "Value": {
-                "Id": 0,
-                "Value": "string"
-            }
-        },
-        {
-            "FieldId": 0,
-            "Values": [
-                {
-                    "Id": 0,
-                    "Value": "string"
-                }
-            ]
-        }
+        {"FieldId": 0, "Value": {"Id": 0, "Value": "string"}},
+        {"FieldId": 0, "Values": [{"Id": 0, "Value": "string"}]},
     ],
     "AuditTrail": {
         "CreatedDate": "2020-09-08T15:27:32.767Z",
         "CreatedName": "string",
         "LastModifiedDate": "2020-09-08T15:27:32.767Z",
-        "LastModifiedName": "string"
-    }
+        "LastModifiedName": "string",
+    },
 }
 
 TEST_GET_INTERACTIONS = {
     "Total": 0,
     "TotalFiltered": 0,
     "Start": 0,
     "ResultCount": 0,
@@ -1027,50 +652,30 @@
         {
             "Id": 0,
             "Date": "2020-09-08",
             "Note": "string",
             "Channel": "Email",
             "Purpose": "Acknowledgement",
             "Subject": "string",
-            "IsInbound": 'true',
+            "IsInbound": "true",
             "AccountId": 0,
             "TweetId": "string",
-            "IsBcc": 'true',
+            "IsBcc": "true",
             "EmailAddress": "user@example.com",
-            "AttachmentIds": [
-                0
-            ],
-            "LetterAttachmentIds": [
-                0
-            ],
-            "SurveyLapsedResponses": [
-                "string"
-            ],
+            "AttachmentIds": [0],
+            "LetterAttachmentIds": [0],
+            "SurveyLapsedResponses": ["string"],
             "SurveyEmailInteractionId": 0,
             "SurveyResponseInteractionId": 0,
             "CustomValues": [
-                {
-                    "FieldId": 0,
-                    "Value": {
-                        "Id": 0,
-                        "Value": "string"
-                    }
-                },
-                {
-                    "FieldId": 0,
-                    "Values": [
-                        {
-                            "Id": 0,
-                            "Value": "string"
-                        }
-                    ]
-                }
+                {"FieldId": 0, "Value": {"Id": 0, "Value": "string"}},
+                {"FieldId": 0, "Values": [{"Id": 0, "Value": "string"}]},
             ],
             "AuditTrail": {
                 "CreatedDate": "2020-09-08T16:10:36.389Z",
                 "CreatedName": "string",
                 "LastModifiedDate": "2020-09-08T16:10:36.389Z",
-                "LastModifiedName": "string"
-            }
+                "LastModifiedName": "string",
+            },
         }
-    ]
+    ],
 }
```

### Comparing `parsons-1.0.0/test/test_braintree/test_braintree.py` & `parsons-1.1.0/test/test_braintree/test_braintree.py`

 * *Files 9% similar despite different names*

```diff
@@ -7,110 +7,147 @@
 
 from parsons import Table, Braintree
 
 _dir = os.path.dirname(__file__)
 
 
 class TestBraintree(unittest.TestCase):
-
     def setUp(self):
-        self.braintree = Braintree(merchant_id='abcd1234abcd1234', public_key='abcd1234abcd1234',
-                                   private_key='abcd1234abcd1234abcd1234abcd1234')
+        self.braintree = Braintree(
+            merchant_id="abcd1234abcd1234",
+            public_key="abcd1234abcd1234",
+            private_key="abcd1234abcd1234abcd1234abcd1234",
+        )
 
     @requests_mock.Mocker()
     def test_dispute_search(self, m):
-        m.post('https://api.braintreegateway.com:443'
-               '/merchants/abcd1234abcd1234/disputes/advanced_search?page=1',
-               text=open(f'{_dir}/test_data/dispute_example.xml').read())
-        table = self.braintree.get_disputes(start_date="2020-01-01", end_date="2020-01-02")
+        m.post(
+            "https://api.braintreegateway.com:443"
+            "/merchants/abcd1234abcd1234/disputes/advanced_search?page=1",
+            text=open(f"{_dir}/test_data/dispute_example.xml").read(),
+        )
+        table = self.braintree.get_disputes(
+            start_date="2020-01-01", end_date="2020-01-02"
+        )
 
         self.assertEqual(len(table.table), 3)
-        self.assertEqual(table[0]['id'], 'abcd1234abcd1234')
-        self.assertEqual(table[1]['id'], 'ghjk6789ghjk6789')
-        self.assertEqual(table[0]['transaction_id'], 'd9f876fg')
-        self.assertEqual(table[1]['transaction_id'], '98df87fg')
-        self.assertEqual(table[0]['reason'], 'transaction_amount_differs')
-        self.assertEqual(table[1]['reason'], 'fraud')
+        self.assertEqual(table[0]["id"], "abcd1234abcd1234")
+        self.assertEqual(table[1]["id"], "ghjk6789ghjk6789")
+        self.assertEqual(table[0]["transaction_id"], "d9f876fg")
+        self.assertEqual(table[1]["transaction_id"], "98df87fg")
+        self.assertEqual(table[0]["reason"], "transaction_amount_differs")
+        self.assertEqual(table[1]["reason"], "fraud")
 
     @requests_mock.Mocker()
     def test_transaction_search(self, m):
-        m.post('https://api.braintreegateway.com:443'
-               '/merchants/abcd1234abcd1234/transactions/advanced_search_ids',
-               text="""
+        m.post(
+            "https://api.braintreegateway.com:443"
+            "/merchants/abcd1234abcd1234/transactions/advanced_search_ids",
+            text="""
                <search-results>
                   <page-size type="integer">50</page-size>
                   <ids type="array"><item>1234abcd</item> <item>0987asdf</item> </ids>
                </search-results>
-        """)
-        table = self.braintree.get_transactions(disbursement_start_date="2020-01-01",
-                                                disbursement_end_date="2020-01-02",
-                                                just_ids=True)
-        assert_matching_tables(table, Table([['id'], ['1234abcd'], ['0987asdf']]))
-        m.post('https://api.braintreegateway.com:443'
-               '/merchants/abcd1234abcd1234/transactions/advanced_search',
-               text=open(f'{_dir}/test_data/transaction_example.xml').read())
-        full_table = self.braintree.get_transactions(disbursement_start_date="2020-01-01",
-                                                     disbursement_end_date="2020-01-02",
-                                                     table_of_ids=table)
+        """,
+        )
+        table = self.braintree.get_transactions(
+            disbursement_start_date="2020-01-01",
+            disbursement_end_date="2020-01-02",
+            just_ids=True,
+        )
+        assert_matching_tables(table, Table([["id"], ["1234abcd"], ["0987asdf"]]))
+        m.post(
+            "https://api.braintreegateway.com:443"
+            "/merchants/abcd1234abcd1234/transactions/advanced_search",
+            text=open(f"{_dir}/test_data/transaction_example.xml").read(),
+        )
+        full_table = self.braintree.get_transactions(
+            disbursement_start_date="2020-01-01",
+            disbursement_end_date="2020-01-02",
+            table_of_ids=table,
+        )
         self.assertEqual(len(table.table), 3)
         self.assertEqual(len(full_table.table), 3)
-        self.assertEqual(table[0]['id'], '1234abcd')
-        self.assertEqual(table[1]['id'], '0987asdf')
+        self.assertEqual(table[0]["id"], "1234abcd")
+        self.assertEqual(table[1]["id"], "0987asdf")
         self.assertEqual(len(table[0].keys()), 1)
         self.assertEqual(len(full_table[0].keys()), 67)
 
-        self.assertEqual(full_table[0]['disbursement_date'], datetime.date(2019, 12, 30))
-        self.assertEqual(full_table[0]['credit_card_bin'], '789234')
-        self.assertEqual(full_table[0]['disbursement_success'], True)
-        self.assertEqual(full_table[0]['amount'], decimal.Decimal('150.00'))
+        self.assertEqual(
+            full_table[0]["disbursement_date"], datetime.date(2019, 12, 30)
+        )
+        self.assertEqual(full_table[0]["credit_card_bin"], "789234")
+        self.assertEqual(full_table[0]["disbursement_success"], True)
+        self.assertEqual(full_table[0]["amount"], decimal.Decimal("150.00"))
 
     @requests_mock.Mocker()
     def test_subscription_search(self, m):
-        m.post('https://api.braintreegateway.com:443'
-               '/merchants/abcd1234abcd1234/subscriptions/advanced_search_ids',
-               text="""
+        m.post(
+            "https://api.braintreegateway.com:443"
+            "/merchants/abcd1234abcd1234/subscriptions/advanced_search_ids",
+            text="""
                <search-results>
                   <page-size type="integer">50</page-size>
                   <ids type="array"><item>aabbcc</item> <item>1a2b3c</item> </ids>
                </search-results>
-        """)
-        table = self.braintree.get_subscriptions(start_date="2022-08-22",
-                                                 end_date="2022-08-23",
-                                                 just_ids=True)
-        assert_matching_tables(table, Table([['id'], ['aabbcc'], ['1a2b3c']]))
-        m.post('https://api.braintreegateway.com:443'
-               '/merchants/abcd1234abcd1234/subscriptions/advanced_search',
-               text=open(f'{_dir}/test_data/subscription_example.xml').read())
-        full_table = self.braintree.get_subscriptions(start_date="2020-01-01",
-                                                      end_date="2020-01-02",
-                                                      table_of_ids=table, include_transactions=True)
+        """,
+        )
+        table = self.braintree.get_subscriptions(
+            start_date="2022-08-22", end_date="2022-08-23", just_ids=True
+        )
+        assert_matching_tables(table, Table([["id"], ["aabbcc"], ["1a2b3c"]]))
+        m.post(
+            "https://api.braintreegateway.com:443"
+            "/merchants/abcd1234abcd1234/subscriptions/advanced_search",
+            text=open(f"{_dir}/test_data/subscription_example.xml").read(),
+        )
+        full_table = self.braintree.get_subscriptions(
+            start_date="2020-01-01",
+            end_date="2020-01-02",
+            table_of_ids=table,
+            include_transactions=True,
+        )
         self.assertEqual(len(table.table), 3)
         self.assertEqual(len(full_table.table), 3)
-        self.assertEqual(table[0]['id'], 'aabbcc')
-        self.assertEqual(table[1]['id'], '1a2b3c')
+        self.assertEqual(table[0]["id"], "aabbcc")
+        self.assertEqual(table[1]["id"], "1a2b3c")
         self.assertEqual(len(table[0].keys()), 1)
         self.assertEqual(len(full_table[0].keys()), 33)
 
-        self.assertEqual(full_table[0]['first_billing_date'], datetime.date(2022, 8, 22))
-        self.assertEqual(full_table[0]['transactions'][0].credit_card_details.bin, '999')
-        self.assertEqual(full_table[0]['never_expires'], True)
-        self.assertEqual(full_table[0]['price'], decimal.Decimal('10.00'))
+        self.assertEqual(
+            full_table[0]["first_billing_date"], datetime.date(2022, 8, 22)
+        )
+        self.assertEqual(
+            full_table[0]["transactions"][0].credit_card_details.bin, "999"
+        )
+        self.assertEqual(full_table[0]["never_expires"], True)
+        self.assertEqual(full_table[0]["price"], decimal.Decimal("10.00"))
 
     def test_query_generation(self):
         query = self.braintree._get_query_objects(
-            'transaction', **{'disbursement_date': {'between': ["2020-01-01", "2020-01-01"]}})
-        self.assertEqual(query[0].name, 'disbursement_date')
-        self.assertEqual(query[0].to_param(), {'min': '2020-01-01', 'max': '2020-01-01'})
+            "transaction",
+            **{"disbursement_date": {"between": ["2020-01-01", "2020-01-01"]}},
+        )
+        self.assertEqual(query[0].name, "disbursement_date")
+        self.assertEqual(
+            query[0].to_param(), {"min": "2020-01-01", "max": "2020-01-01"}
+        )
 
         query = self.braintree._get_query_objects(
-            'transaction', **{'merchant_account_id': {'in_list': ["abc123"]}})
+            "transaction", **{"merchant_account_id": {"in_list": ["abc123"]}}
+        )
 
-        self.assertEqual(query[0].name, 'merchant_account_id')
-        self.assertEqual(query[0].to_param(), ['abc123'])
+        self.assertEqual(query[0].name, "merchant_account_id")
+        self.assertEqual(query[0].to_param(), ["abc123"])
 
         query = self.braintree._get_query_objects(
-            'dispute', **{
-                'merchant_account_id': {'in_list': ["abc123"]},
-                'effective_date': {'between': ["2020-01-01", "2020-01-01"]}})
-        self.assertEqual(query[0].name, 'merchant_account_id')
-        self.assertEqual(query[1].name, 'effective_date')
-        self.assertEqual(query[1].to_param(), {'min': '2020-01-01', 'max': '2020-01-01'})
+            "dispute",
+            **{
+                "merchant_account_id": {"in_list": ["abc123"]},
+                "effective_date": {"between": ["2020-01-01", "2020-01-01"]},
+            },
+        )
+        self.assertEqual(query[0].name, "merchant_account_id")
+        self.assertEqual(query[1].name, "effective_date")
+        self.assertEqual(
+            query[1].to_param(), {"min": "2020-01-01", "max": "2020-01-01"}
+        )
```

### Comparing `parsons-1.0.0/test/test_capitol_canary.py` & `parsons-1.1.0/test/test_capitol_canary.py`

 * *Files 10% similar despite different names*

```diff
@@ -29,21 +29,21 @@
                 "street2": "",
                 "city": "Los Angeles",
                 "state": "CA",
                 "zip5": 96055,
                 "zip4": 9534,
                 "county": "Tehama",
                 "latitude": "50.0632635",
-                "longitude": "-122.09654"
+                "longitude": "-122.09654",
             },
             "districts": {
                 "congressional": "1",
                 "stateSenate": "4",
                 "stateHouse": "3",
-                "cityCouncil": None
+                "cityCouncil": None,
             },
             "ids": [],
             "memberships": [
                 {
                     "id": 15151443,
                     "campaignid": 25373,
                     "name": "20171121 Businesses for Responsible Tax Reform - Contact Congress",
@@ -52,43 +52,32 @@
                 },
                 {
                     "id": 20025582,
                     "campaignid": 32641,
                     "name": "20180524 March for America",
                     "source": None,
                     "created_at": "2018-05-24 21:09:49.000000",
-                }
+                },
             ],
             "fields": [],
             "phones": [
-                {
-                    "id": 10537860,
-                    "address": "+19995206447",
-                    "subscribed": 'false'
-                }
+                {"id": 10537860, "address": "+19995206447", "subscribed": "false"}
             ],
             "emails": [
-                {
-                    "id": 10537871,
-                    "address": "N@k.com",
-                    "subscribed": 'false'
-                },
-                {
-                    "id": 10950446,
-                    "address": "email@me.com",
-                    "subscribed": 'false'
-                }
-            ]
+                {"id": 10537871, "address": "N@k.com", "subscribed": "false"},
+                {"id": 10950446, "address": "email@me.com", "subscribed": "false"},
+            ],
         }
     ],
     "pagination": {
         "count": 1,
         "per_page": 100,
         "current_page": 1,
-        "next_url": "https://api.phone2action.com/2.0/advocates?page=2"}
+        "next_url": "https://api.phone2action.com/2.0/advocates?page=2",
+    },
 }
 
 camp_json = [
     {
         "id": 25373,
         "name": "20171121 Businesses for Responsible Tax Reform - Contact Congress",
         "display_name": "Businesses for Responsible Tax Reform",
@@ -100,198 +89,248 @@
         "restrict_allow": None,
         "content": {
             "summary": "",
             "introduction": "Welcome",
             "call_to_action": "Contact your officials in one click!",
             "thank_you": "<p>Thanks for taking action. Please encourage others to act by "
             "sharing on social media.</p>",
-            "background_image": None
+            "background_image": None,
         },
         "updated_at": {
             "date": "2017-11-21 23:27:11.000000",
             "timezone_type": 3,
-            "timezone": "UTC"
-        }
+            "timezone": "UTC",
+        },
     }
 ]
 
 
 def parse_request_body(m):
-    kvs = m.split('&')
-    return {
-        kv.split('=')[0]: kv.split('=')[1]
-        for kv in kvs
-    }
+    kvs = m.split("&")
+    return {kv.split("=")[0]: kv.split("=")[1] for kv in kvs}
 
 
 class TestP2A(unittest.TestCase):
-
     def setUp(self):
 
-        self.cc = CapitolCanary(app_id='an_id', app_key='app_key')
+        self.cc = CapitolCanary(app_id="an_id", app_key="app_key")
 
     def tearDown(self):
 
         pass
 
     def test_init_args(self):
         # Test initializing class with args
         # Done in the setUp
 
         pass
 
     def test_old_init_envs(self):
         # Test initializing class with old envs
 
-        os.environ['PHONE2ACTION_APP_ID'] = 'id'
-        os.environ['PHONE2ACTION_APP_KEY'] = 'key'
+        os.environ["PHONE2ACTION_APP_ID"] = "id"
+        os.environ["PHONE2ACTION_APP_KEY"] = "key"
 
         cc_envs = CapitolCanary()
-        self.assertEqual(cc_envs.app_id, 'id')
-        self.assertEqual(cc_envs.app_key, 'key')
+        self.assertEqual(cc_envs.app_id, "id")
+        self.assertEqual(cc_envs.app_key, "key")
 
     def test_init_envs(self):
         # Test initializing class with envs
 
-        os.environ['CAPITOLCANARY_APP_ID'] = 'id'
-        os.environ['CAPITOLCANARY_APP_KEY'] = 'key'
+        os.environ["CAPITOLCANARY_APP_ID"] = "id"
+        os.environ["CAPITOLCANARY_APP_KEY"] = "key"
 
         cc_envs = CapitolCanary()
-        self.assertEqual(cc_envs.app_id, 'id')
-        self.assertEqual(cc_envs.app_key, 'key')
+        self.assertEqual(cc_envs.app_id, "id")
+        self.assertEqual(cc_envs.app_key, "key")
 
     @requests_mock.Mocker()
     def test_get_advocates(self, m):
 
-        m.get(self.cc.client.uri + 'advocates', json=adv_json)
-
-        adv_exp = ['id', 'prefix', 'firstname', 'middlename',
-                   'lastname', 'suffix', 'notes', 'stage', 'connections',
-                   'created_at', 'updated_at',
-                   'address_city', 'address_county', 'address_latitude',
-                   'address_longitude', 'address_state', 'address_street1',
-                   'address_street2', 'address_zip4', 'address_zip5',
-                   'districts_cityCouncil', 'districts_congressional',
-                   'districts_stateHouse', 'districts_stateSenate']
-
-        self.assertTrue(validate_list(adv_exp, self.cc.get_advocates()['advocates']))
-        ids_exp = ['advocate_id', 'ids']
-
-        self.assertTrue(validate_list(ids_exp, self.cc.get_advocates()['ids']))
-
-        phone_exp = ['advocate_id', 'phones_address', 'phones_id', 'phones_subscribed']
-        self.assertTrue(validate_list(phone_exp, self.cc.get_advocates()['phones']))
-
-        tags_exp = ['advocate_id', 'tags']
-        self.assertTrue(validate_list(tags_exp, self.cc.get_advocates()['tags']))
-
-        email_exp = ['advocate_id', 'emails_address', 'emails_id', 'emails_subscribed']
-        self.assertTrue(validate_list(email_exp, self.cc.get_advocates()['emails']))
+        m.get(self.cc.client.uri + "advocates", json=adv_json)
 
-        member_exp = ['advocate_id', 'memberships_campaignid', 'memberships_created_at',
-                      'memberships_id', 'memberships_name', 'memberships_source']
-        self.assertTrue(validate_list(member_exp, self.cc.get_advocates()['memberships']))
+        adv_exp = [
+            "id",
+            "prefix",
+            "firstname",
+            "middlename",
+            "lastname",
+            "suffix",
+            "notes",
+            "stage",
+            "connections",
+            "created_at",
+            "updated_at",
+            "address_city",
+            "address_county",
+            "address_latitude",
+            "address_longitude",
+            "address_state",
+            "address_street1",
+            "address_street2",
+            "address_zip4",
+            "address_zip5",
+            "districts_cityCouncil",
+            "districts_congressional",
+            "districts_stateHouse",
+            "districts_stateSenate",
+        ]
+
+        self.assertTrue(validate_list(adv_exp, self.cc.get_advocates()["advocates"]))
+        ids_exp = ["advocate_id", "ids"]
+
+        self.assertTrue(validate_list(ids_exp, self.cc.get_advocates()["ids"]))
+
+        phone_exp = ["advocate_id", "phones_address", "phones_id", "phones_subscribed"]
+        self.assertTrue(validate_list(phone_exp, self.cc.get_advocates()["phones"]))
+
+        tags_exp = ["advocate_id", "tags"]
+        self.assertTrue(validate_list(tags_exp, self.cc.get_advocates()["tags"]))
+
+        email_exp = ["advocate_id", "emails_address", "emails_id", "emails_subscribed"]
+        self.assertTrue(validate_list(email_exp, self.cc.get_advocates()["emails"]))
+
+        member_exp = [
+            "advocate_id",
+            "memberships_campaignid",
+            "memberships_created_at",
+            "memberships_id",
+            "memberships_name",
+            "memberships_source",
+        ]
+        self.assertTrue(
+            validate_list(member_exp, self.cc.get_advocates()["memberships"])
+        )
 
-        fields_exp = ['advocate_id', 'fields']
-        self.assertTrue(validate_list(fields_exp, self.cc.get_advocates()['fields']))
+        fields_exp = ["advocate_id", "fields"]
+        self.assertTrue(validate_list(fields_exp, self.cc.get_advocates()["fields"]))
 
     @requests_mock.Mocker()
     def test_get_advocates__by_page(self, m):
 
         response = copy.deepcopy(adv_json)
         # Make it look like there's more data
-        response['pagination']['count'] = 100
+        response["pagination"]["count"] = 100
 
-        m.get(self.cc.client.uri + 'advocates?page=1', json=adv_json)
-        m.get(self.cc.client.uri + 'advocates?page=2', exc=Exception('Should only call once'))
+        m.get(self.cc.client.uri + "advocates?page=1", json=adv_json)
+        m.get(
+            self.cc.client.uri + "advocates?page=2",
+            exc=Exception("Should only call once"),
+        )
 
         results = self.cc.get_advocates(page=1)
-        self.assertTrue(results['advocates'].num_rows, 1)
+        self.assertTrue(results["advocates"].num_rows, 1)
 
     @requests_mock.Mocker()
     def test_get_advocates__empty(self, m):
 
         response = copy.deepcopy(adv_json)
-        response['data'] = []
+        response["data"] = []
         # Make it look like there's more data
-        response['pagination']['count'] = 0
+        response["pagination"]["count"] = 0
 
-        m.get(self.cc.client.uri + 'advocates', json=adv_json)
+        m.get(self.cc.client.uri + "advocates", json=adv_json)
 
         results = self.cc.get_advocates()
-        self.assertTrue(results['advocates'].num_rows, 0)
+        self.assertTrue(results["advocates"].num_rows, 0)
 
     @requests_mock.Mocker()
     def test_get_campaigns(self, m):
 
-        camp_exp = ['id', 'name', 'display_name', 'subtitle',
-                    'public', 'topic', 'type', 'link', 'restrict_allow',
-                    'updated_at_date', 'updated_at_timezone',
-                    'updated_at_timezone_type', 'content_background_image',
-                    'content_call_to_action', 'content_introduction',
-                    'content_summary', 'content_thank_you']
+        camp_exp = [
+            "id",
+            "name",
+            "display_name",
+            "subtitle",
+            "public",
+            "topic",
+            "type",
+            "link",
+            "restrict_allow",
+            "updated_at_date",
+            "updated_at_timezone",
+            "updated_at_timezone_type",
+            "content_background_image",
+            "content_call_to_action",
+            "content_introduction",
+            "content_summary",
+            "content_thank_you",
+        ]
 
-        m.get(self.cc.client.uri + 'campaigns', json=camp_json)
+        m.get(self.cc.client.uri + "campaigns", json=camp_json)
 
         self.assertTrue(validate_list(camp_exp, self.cc.get_campaigns()))
 
     @requests_mock.Mocker()
     def test_create_advocate(self, m):
 
-        m.post(self.cc.client.uri + 'advocates', json={'advocateid': 1})
+        m.post(self.cc.client.uri + "advocates", json={"advocateid": 1})
 
         # Test arg validation - create requires a phone or an email
-        self.assertRaises(ValueError,
-                          lambda: self.cc.create_advocate(campaigns=[1],
-                                                          firstname='Foo',
-                                                          lastname='bar'))
+        self.assertRaises(
+            ValueError,
+            lambda: self.cc.create_advocate(
+                campaigns=[1], firstname="Foo", lastname="bar"
+            ),
+        )
         # Test arg validation - sms opt in requires a phone
-        self.assertRaises(ValueError,
-                          lambda: self.cc.create_advocate(campaigns=[1],
-                                                          email='foo@bar.com',
-                                                          sms_optin=True))
+        self.assertRaises(
+            ValueError,
+            lambda: self.cc.create_advocate(
+                campaigns=[1], email="foo@bar.com", sms_optin=True
+            ),
+        )
 
         # Test arg validation - email opt in requires a email
-        self.assertRaises(ValueError,
-                          lambda: self.cc.create_advocate(campaigns=[1],
-                                                          phone='1234567890',
-                                                          email_optin=True))
+        self.assertRaises(
+            ValueError,
+            lambda: self.cc.create_advocate(
+                campaigns=[1], phone="1234567890", email_optin=True
+            ),
+        )
 
         # Test a successful call
-        advocateid = self.cc.create_advocate(campaigns=[1],
-                                             email='foo@bar.com',
-                                             email_optin=True,
-                                             firstname='Test')
+        advocateid = self.cc.create_advocate(
+            campaigns=[1], email="foo@bar.com", email_optin=True, firstname="Test"
+        )
         self.assertTrue(m.called)
         self.assertEqual(advocateid, 1)
 
         # Check that the properties were mapped
         data = parse_request_body(m.last_request.text)
-        self.assertEqual(data['firstname'], 'Test')
-        self.assertNotIn('lastname', data)
-        self.assertEqual(data['emailOptin'], '1')
-        self.assertEqual(data['email'], 'foo%40bar.com')
+        self.assertEqual(data["firstname"], "Test")
+        self.assertNotIn("lastname", data)
+        self.assertEqual(data["emailOptin"], "1")
+        self.assertEqual(data["email"], "foo%40bar.com")
 
     @requests_mock.Mocker()
     def test_update_advocate(self, m):
 
-        m.post(self.cc.client.uri + 'advocates')
+        m.post(self.cc.client.uri + "advocates")
 
         # Test arg validation - sms opt in requires a phone
-        self.assertRaises(ValueError,
-                          lambda: self.cc.update_advocate(advocate_id=1, sms_optin=True))
+        self.assertRaises(
+            ValueError, lambda: self.cc.update_advocate(advocate_id=1, sms_optin=True)
+        )
 
         # Test arg validation - email opt in requires a email
-        self.assertRaises(ValueError,
-                          lambda: self.cc.update_advocate(advocate_id=1, email_optin=True))
+        self.assertRaises(
+            ValueError, lambda: self.cc.update_advocate(advocate_id=1, email_optin=True)
+        )
 
         # Test a successful call
-        self.cc.update_advocate(advocate_id=1, campaigns=[1], email='foo@bar.com',
-                                email_optin=True, firstname='Test')
+        self.cc.update_advocate(
+            advocate_id=1,
+            campaigns=[1],
+            email="foo@bar.com",
+            email_optin=True,
+            firstname="Test",
+        )
         self.assertTrue(m.called)
 
         # Check that the properties were mapped
         data = parse_request_body(m.last_request.text)
-        self.assertEqual(data['firstname'], 'Test')
-        self.assertNotIn('lastname', data)
-        self.assertEqual(data['emailOptin'], '1')
-        self.assertEqual(data['email'], 'foo%40bar.com')
+        self.assertEqual(data["firstname"], "Test")
+        self.assertNotIn("lastname", data)
+        self.assertEqual(data["emailOptin"], "1")
+        self.assertEqual(data["email"], "foo%40bar.com")
```

### Comparing `parsons-1.0.0/test/test_civis.py` & `parsons-1.1.0/test/test_civis.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,28 +1,29 @@
 import unittest
 import os
 from parsons import CivisClient, Table
 
 # from . import scratch_creds
 
 
-@unittest.skipIf(not os.environ.get('LIVE_TEST'), 'Skipping because not running live test')
+@unittest.skipIf(
+    not os.environ.get("LIVE_TEST"), "Skipping because not running live test"
+)
 class TestCivisClient(unittest.TestCase):
-
     def setUp(self):
 
         self.civis = CivisClient()
 
         # Create a schema, create a table, create a view
         setup_sql = """
                     drop schema if exists test_parsons cascade;
                     create schema test_parsons;
                     """
 
-        self.lst_dicts = [{'first': 'Bob', 'last': 'Smith'}]
+        self.lst_dicts = [{"first": "Bob", "last": "Smith"}]
         self.tbl = Table(self.lst_dicts)
 
         self.civis.query(setup_sql)
 
     def tearDown(self):
 
         # Drop the view, the table and the schema
@@ -31,20 +32,20 @@
                        """
 
         self.civis.query(teardown_sql)
 
     def test_table_import_query(self):
 
         # Test that a good table imports correctly
-        self.civis.table_import(self.tbl, 'test_parsons.test_table')
+        self.civis.table_import(self.tbl, "test_parsons.test_table")
 
     def test_query(self):
 
         # Test that queries match
-        self.civis.table_import(self.tbl, 'test_parsons.test_table')
+        self.civis.table_import(self.tbl, "test_parsons.test_table")
         tbl = self.civis.query("SELECT COUNT(*) FROM test_parsons.test_table")
-        self.assertEqual(tbl[0]['count'], '1')
+        self.assertEqual(tbl[0]["count"], "1")
 
     def test_to_civis(self):
 
         # Test that the to_civis() method works too
-        self.tbl.to_civis('test_parsons.test_table')
+        self.tbl.to_civis("test_parsons.test_table")
```

### Comparing `parsons-1.0.0/test/test_credential_tools.py` & `parsons-1.1.0/test/test_credential_tools.py`

 * *Files 21% similar despite different names*

```diff
@@ -2,80 +2,76 @@
 import json
 import os
 import shutil
 import unittest
 
 
 class TestCredentialTool(unittest.TestCase):
-
     def setUp(self):
 
-        os.environ['TES_VAR1'] = 'variable1'
-        os.environ['TES_VAR2'] = 'variable2'
+        os.environ["TES_VAR1"] = "variable1"
+        os.environ["TES_VAR2"] = "variable2"
 
         self.tmp_folder = "tmp"
         self.json_file = "credentials.json"
         os.mkdir(self.tmp_folder)
 
-        with open(f"{self.tmp_folder}/{self.json_file}", 'w') as f:
-            f.write(json.dumps({
-                "json": "file"
-            }))
+        with open(f"{self.tmp_folder}/{self.json_file}", "w") as f:
+            f.write(json.dumps({"json": "file"}))
 
     def tearDown(self):
 
         # Delete tmp folder and files
         shutil.rmtree(self.tmp_folder)
 
     def test_decode_credential(self):
-        encoded_cred = ("PRSNSENVeyJFTkNfVkFSMSI6ICJlbmNvZGVkLXZhcmlhYmxl"
-                        "LTEiLCAiRU5DX1ZBUjIiOiAiZW5jLXZhci0yIn0=")
+        encoded_cred = (
+            "PRSNSENVeyJFTkNfVkFSMSI6ICJlbmNvZGVkLXZhcmlhYmxl"
+            "LTEiLCAiRU5DX1ZBUjIiOiAiZW5jLXZhci0yIn0="
+        )
 
-        expected = {
-            "ENC_VAR1": "encoded-variable-1",
-            "ENC_VAR2": "enc-var-2"}
+        expected = {"ENC_VAR1": "encoded-variable-1", "ENC_VAR2": "enc-var-2"}
 
-        self.assertDictEqual(
-            ct.decode_credential(encoded_cred, export=False), expected)
+        self.assertDictEqual(ct.decode_credential(encoded_cred, export=False), expected)
 
     def test_decode_credential_export(self):
-        encoded_cred = ("PRSNSENVeyJFTkNfVkFSMSI6ICJlbmNvZGVkLXZhcmlhYmxl"
-                        "LTEiLCAiRU5DX1ZBUjIiOiAiZW5jLXZhci0yIn0=")
+        encoded_cred = (
+            "PRSNSENVeyJFTkNfVkFSMSI6ICJlbmNvZGVkLXZhcmlhYmxl"
+            "LTEiLCAiRU5DX1ZBUjIiOiAiZW5jLXZhci0yIn0="
+        )
 
-        expected = {
-            "ENC_VAR1": "encoded-variable-1",
-            "ENC_VAR2": "enc-var-2"}
+        expected = {"ENC_VAR1": "encoded-variable-1", "ENC_VAR2": "enc-var-2"}
 
         self.assertNotIn("ENC_VAR1", os.environ)
         self.assertNotIn("ENC_VAR2", os.environ)
 
         ct.decode_credential(encoded_cred)
 
         self.assertIn("ENC_VAR1", os.environ)
         self.assertIn("ENC_VAR2", os.environ)
 
-        self.assertEqual(os.environ['ENC_VAR1'], expected['ENC_VAR1'])
-        self.assertEqual(os.environ['ENC_VAR2'], expected['ENC_VAR2'])
+        self.assertEqual(os.environ["ENC_VAR1"], expected["ENC_VAR1"])
+        self.assertEqual(os.environ["ENC_VAR2"], expected["ENC_VAR2"])
 
     def test_decode_credential_save(self):
-        encoded_cred = ("PRSNSENVeyJFTkNfVkFSMSI6ICJlbmNvZGVkLXZhcmlhYmxl"
-                        "LTEiLCAiRU5DX1ZBUjIiOiAiZW5jLXZhci0yIn0=")
+        encoded_cred = (
+            "PRSNSENVeyJFTkNfVkFSMSI6ICJlbmNvZGVkLXZhcmlhYmxl"
+            "LTEiLCAiRU5DX1ZBUjIiOiAiZW5jLXZhci0yIn0="
+        )
 
-        expected = {
-            "ENC_VAR1": "encoded-variable-1",
-            "ENC_VAR2": "enc-var-2"}
+        expected = {"ENC_VAR1": "encoded-variable-1", "ENC_VAR2": "enc-var-2"}
 
         file_path = f"{self.tmp_folder}/saved_credentials.json"
         self.assertFalse(os.path.isfile(file_path))
 
         ct.decode_credential(encoded_cred, export=False, save_path=file_path)
 
         self.assertTrue(os.path.isfile(file_path))
 
-        with open(file_path, 'r') as f:
+        with open(file_path, "r") as f:
             cred = json.load(f)
 
         self.assertDictEqual(cred, expected)
 
     def test_decode_credential_error(self):
         non_json = "non-json string"
 
@@ -87,21 +83,22 @@
 
         self.assertEqual(ct.encode_from_json_str(json_str), expected)
 
     def test_encode_from_json_file(self):
         json_path = f"{self.tmp_folder}/{self.json_file}"
         expected = "PRSNSENVeyJqc29uIjogImZpbGUifQ=="
 
-        self.assertEqual(
-            ct.encode_from_json_file(json_path), expected)
+        self.assertEqual(ct.encode_from_json_file(json_path), expected)
 
     def testencode_from_env(self):
-        lst = ['TES_VAR1', 'TES_VAR2']
-        expected = ("PRSNSENVeyJURVNfVkFSMSI6ICJ2YXJpYWJsZTEiLCAiVEVTX1ZBU"
-                    "jIiOiAidmFyaWFibGUyIn0=")
+        lst = ["TES_VAR1", "TES_VAR2"]
+        expected = (
+            "PRSNSENVeyJURVNfVkFSMSI6ICJ2YXJpYWJsZTEiLCAiVEVTX1ZBU"
+            "jIiOiAidmFyaWFibGUyIn0="
+        )
 
         self.assertEqual(ct.encode_from_env(lst), expected)
 
     def test_encode_from_dict(self):
         dct = {"dict": "variable"}
         expected = "PRSNSENVeyJkaWN0IjogInZhcmlhYmxlIn0="
```

### Comparing `parsons-1.0.0/test/test_databases/fakes.py` & `parsons-1.1.0/test/test_databases/fakes.py`

 * *Files 5% similar despite different names*

```diff
@@ -7,44 +7,46 @@
 class FakeDatabase:
     def __init__(self):
         self.table_map = {}
         self.copy_call_args = []
 
     def setup_table(self, table_name, data, failures=0):
         self.table_map[table_name] = {
-            'failures': failures,
-            'table': FakeTable(table_name, data)
+            "failures": failures,
+            "table": FakeTable(table_name, data),
         }
-        return self.table_map[table_name]['table']
+        return self.table_map[table_name]["table"]
 
     def table(self, table_name):
         if table_name not in self.table_map:
             self.setup_table(table_name, None)
 
-        return self.table_map[table_name]['table']
+        return self.table_map[table_name]["table"]
 
     def copy(self, data, table_name, **kwargs):
-        logger.info('Copying %s rows', data.num_rows)
+        logger.info("Copying %s rows", data.num_rows)
         if table_name not in self.table_map:
             self.setup_table(table_name, Table())
 
-        if self.table_map[table_name]['table'].data is None:
-            self.table_map[table_name]['table'].data = Table()
+        if self.table_map[table_name]["table"].data is None:
+            self.table_map[table_name]["table"].data = Table()
 
-        if self.table_map[table_name]['failures'] > 0:
-            self.table_map[table_name]['failures'] -= 1
-            raise ValueError('Canned error')
-
-        self.copy_call_args.append({
-            'data': data,
-            'table_name': table_name,
-            'kwargs': kwargs,
-        })
+        if self.table_map[table_name]["failures"] > 0:
+            self.table_map[table_name]["failures"] -= 1
+            raise ValueError("Canned error")
+
+        self.copy_call_args.append(
+            {
+                "data": data,
+                "table_name": table_name,
+                "kwargs": kwargs,
+            }
+        )
 
-        tbl = self.table_map[table_name]['table']
+        tbl = self.table_map[table_name]["table"]
 
         tbl.data.concat(data)
 
     def get_table_object(self, table_name):
 
         pass
 
@@ -92,18 +94,18 @@
 
     def get_rows(self, offset=0, chunk_size=None, order_by=None):
         data = self.data.cut(*self.data.columns)
 
         if order_by:
             data.sort(order_by)
 
-        return Table(data[offset:chunk_size + offset])
+        return Table(data[offset : chunk_size + offset])
 
     def get_new_rows_count(self, primary_key_col, start_value=None):
         data = self.data.select_rows(lambda row: row[primary_key_col] > start_value)
         return data.num_rows
 
     def get_new_rows(self, primary_key, cutoff_value, offset=0, chunk_size=None):
         data = self.data.select_rows(lambda row: row[primary_key] > cutoff_value)
         data.sort(primary_key)
 
-        return Table(data[offset:chunk_size + offset])
+        return Table(data[offset : chunk_size + offset])
```

### Comparing `parsons-1.0.0/test/test_databases/test_database.py` & `parsons-1.1.0/test/test_databases/test_database.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,9 +1,16 @@
 from parsons.databases.database.constants import (
-    SMALLINT, MEDIUMINT, INT, BIGINT, FLOAT, BOOL, VARCHAR)
+    SMALLINT,
+    MEDIUMINT,
+    INT,
+    BIGINT,
+    FLOAT,
+    BOOL,
+    VARCHAR,
+)
 
 from parsons.databases.database.database import DatabaseCreateStatement
 
 import pytest
 
 
 @pytest.fixture
@@ -17,141 +24,154 @@
     db = DatabaseCreateStatement()
     db.DO_PARSE_BOOLS = True
     return db
 
 
 @pytest.mark.parametrize(
     ("int1", "int2", "higher"),
-    ((SMALLINT, SMALLINT, SMALLINT),
-     (SMALLINT, MEDIUMINT, MEDIUMINT),
-     (SMALLINT, INT, INT),
-     (SMALLINT, BIGINT, BIGINT),
-     (MEDIUMINT, SMALLINT, MEDIUMINT),
-     (MEDIUMINT, MEDIUMINT, MEDIUMINT),
-     (MEDIUMINT, INT, INT),
-     (MEDIUMINT, BIGINT, BIGINT),
-     (INT, SMALLINT, INT),
-     (INT, MEDIUMINT, INT),
-     (INT, INT, INT),
-     (INT, BIGINT, BIGINT),
-     (BIGINT, SMALLINT, BIGINT),
-     (BIGINT, MEDIUMINT, BIGINT),
-     (BIGINT, INT, BIGINT),
-     (BIGINT, BIGINT, BIGINT),
-     (None, BIGINT, BIGINT),
-     (INT, None, INT),
-     ))
+    (
+        (SMALLINT, SMALLINT, SMALLINT),
+        (SMALLINT, MEDIUMINT, MEDIUMINT),
+        (SMALLINT, INT, INT),
+        (SMALLINT, BIGINT, BIGINT),
+        (MEDIUMINT, SMALLINT, MEDIUMINT),
+        (MEDIUMINT, MEDIUMINT, MEDIUMINT),
+        (MEDIUMINT, INT, INT),
+        (MEDIUMINT, BIGINT, BIGINT),
+        (INT, SMALLINT, INT),
+        (INT, MEDIUMINT, INT),
+        (INT, INT, INT),
+        (INT, BIGINT, BIGINT),
+        (BIGINT, SMALLINT, BIGINT),
+        (BIGINT, MEDIUMINT, BIGINT),
+        (BIGINT, INT, BIGINT),
+        (BIGINT, BIGINT, BIGINT),
+        (None, BIGINT, BIGINT),
+        (INT, None, INT),
+    ),
+)
 def test_get_bigger_int(dcs, int1, int2, higher):
     assert dcs.get_bigger_int(int1, int2) == higher
 
 
 @pytest.mark.parametrize(
     ("val", "is_valid"),
-    ((10, True),
-     (1_0, True),
-     (+10, True),
-     (+1_0, True),
-     (1.2, True),
-     (1.0_0, True),
-     (1., True),
-     (1_0., True),
-     (+1.2, True),
-     (+1., True),
-     (+1.0_0, True),
-     (0, True),
-     (0.0, True),
-     ("10", True),
-     ("1_0", False),
-     ("+10", True),
-     ("+1_0", False),
-     ("1.2", True),
-     ("1.0_0", False),
-     ("1.", True),
-     ("1_0.", False),
-     ("+1.2", True),
-     ("+1.", True),
-     ("+1.0_0", False),
-     ("0", True),
-     ("0.0", True),
-     (True, False),
-     ("True", False),
-     ("a string", False),
-     ({}, False),
-     ([], False),
-     ([], False),
-     (None, False),
-     ))
+    (
+        (10, True),
+        (1_0, True),
+        (+10, True),
+        (+1_0, True),
+        (1.2, True),
+        (1.0_0, True),
+        (1.0, True),
+        (1_0.0, True),
+        (+1.2, True),
+        (+1.0, True),
+        (+1.0_0, True),
+        (0, True),
+        (0.0, True),
+        ("10", True),
+        ("1_0", False),
+        ("+10", True),
+        ("+1_0", False),
+        ("1.2", True),
+        ("1.0_0", False),
+        ("1.", True),
+        ("1_0.", False),
+        ("+1.2", True),
+        ("+1.", True),
+        ("+1.0_0", False),
+        ("0", True),
+        ("0.0", True),
+        (True, False),
+        ("True", False),
+        ("a string", False),
+        ({}, False),
+        ([], False),
+        ([], False),
+        (None, False),
+    ),
+)
 def test_is_valid_sql_num(dcs, val, is_valid):
     assert dcs.is_valid_sql_num(val) == is_valid
 
 
 @pytest.mark.parametrize(
     ("val", "cmp_type", "detected_type"),
-    ((1, None, SMALLINT),
-     (1, "", SMALLINT),
-     (1, MEDIUMINT, MEDIUMINT),
-     (32769, None, MEDIUMINT),
-     (32769, BIGINT, BIGINT),
-     (2147483648, None, BIGINT),
-     (2147483648, FLOAT, FLOAT),
-     (5.001, None, FLOAT),
-     (5.001, "", FLOAT),
-     ("FALSE", VARCHAR, VARCHAR),
-     ("word", "", VARCHAR),
-     ("word", INT, VARCHAR),
-     ("1_2", BIGINT, VARCHAR),
-     ("01", FLOAT, VARCHAR),
-     ("00001", None, VARCHAR),
-     ("word", None, VARCHAR),
-     ("1_2", None, VARCHAR),
-     ("01", None, VARCHAR),
-     ("{}", None, VARCHAR),
-     ))
+    (
+        (1, None, SMALLINT),
+        (1, "", SMALLINT),
+        (1, MEDIUMINT, MEDIUMINT),
+        (32769, None, MEDIUMINT),
+        (32769, BIGINT, BIGINT),
+        (2147483648, None, BIGINT),
+        (2147483648, FLOAT, FLOAT),
+        (5.001, None, FLOAT),
+        (5.001, "", FLOAT),
+        ("FALSE", VARCHAR, VARCHAR),
+        ("word", "", VARCHAR),
+        ("word", INT, VARCHAR),
+        ("1_2", BIGINT, VARCHAR),
+        ("01", FLOAT, VARCHAR),
+        ("00001", None, VARCHAR),
+        ("word", None, VARCHAR),
+        ("1_2", None, VARCHAR),
+        ("01", None, VARCHAR),
+        ("{}", None, VARCHAR),
+    ),
+)
 def test_detect_data_type(dcs, val, cmp_type, detected_type):
     assert dcs.detect_data_type(val, cmp_type) == detected_type
 
 
 @pytest.mark.parametrize(
     ("val", "cmp_type", "detected_type"),
-    ((2, None, SMALLINT),
-     (2, "", SMALLINT),
-     (1, MEDIUMINT, MEDIUMINT),
-     (2, BOOL, SMALLINT),
-     (True, None, BOOL),
-     (0, None, BOOL),
-     (1, None, BOOL),
-     (1, BOOL, BOOL),
-     ("F", None, BOOL),
-     ("FALSE", None, BOOL),
-     ("Yes", None, BOOL)
-     ))
+    (
+        (2, None, SMALLINT),
+        (2, "", SMALLINT),
+        (1, MEDIUMINT, MEDIUMINT),
+        (2, BOOL, SMALLINT),
+        (True, None, BOOL),
+        (0, None, BOOL),
+        (1, None, BOOL),
+        (1, BOOL, BOOL),
+        ("F", None, BOOL),
+        ("FALSE", None, BOOL),
+        ("Yes", None, BOOL),
+    ),
+)
 def test_detect_data_type_bools(dcs_bool, val, cmp_type, detected_type):
     assert dcs_bool.detect_data_type(val, cmp_type) == detected_type
 
 
 @pytest.mark.parametrize(
     ("col", "renamed"),
-    (("a", "a"),
-     ("A", "a"),
-     ("", "_"),
-     ("SELECT", "select_"),
-     ("two words", "two_words"),
-     ("   trailing space   ", "trailing_space"),
-     ("1234567890", "x_1234567890"),
-     ("0word", "x_0word"),
-
-     # create a really long column name
-     # len("asdfghjkla" * 13) == 130
-     # len("asdfghjkla" * 10) == 100
-     ("asdfghjkla" * 13, "asdfghjkla" * 10),
-     ))
+    (
+        ("a", "a"),
+        ("A", "a"),
+        ("", "_"),
+        ("SELECT", "select_"),
+        ("two words", "two_words"),
+        ("   trailing space   ", "trailing_space"),
+        ("1234567890", "x_1234567890"),
+        ("0word", "x_0word"),
+        # create a really long column name
+        # len("asdfghjkla" * 13) == 130
+        # len("asdfghjkla" * 10) == 100
+        ("asdfghjkla" * 13, "asdfghjkla" * 10),
+    ),
+)
 def test_default_format_column(dcs, col, renamed):
     assert dcs.format_column(col) == renamed
 
 
 @pytest.mark.parametrize(
     ("cols", "cols_formatted"),
-    ((["a", "A", "b", "   b   ", "col name", "col_name"],
-      ["a", "a_1", "b", "b_3", "col_name", "col_name_5"]),
-     ))
+    (
+        (
+            ["a", "A", "b", "   b   ", "col name", "col_name"],
+            ["a", "a_1", "b", "b_3", "col_name", "col_name_5"],
+        ),
+    ),
+)
 def test_default_format_columns(dcs, cols, cols_formatted):
     assert dcs.format_columns(cols) == cols_formatted
```

### Comparing `parsons-1.0.0/test/test_databases/test_dbsync.py` & `parsons-1.1.0/test/test_databases/test_dbsync.py`

 * *Files 14% similar despite different names*

```diff
@@ -2,40 +2,41 @@
 from test.test_databases.fakes import FakeDatabase
 from test.utils import assert_matching_tables
 import unittest
 import os
 
 _dir = os.path.dirname(__file__)
 
-TEMP_SCHEMA = 'parsons_test'
+TEMP_SCHEMA = "parsons_test"
 
 
 # These tests interact directly with the Postgres database. In order to run, set the
 # env to LIVE_TEST='TRUE'.
-@unittest.skipIf(not os.environ.get('LIVE_TEST'), 'Skipping because not running live test')
+@unittest.skipIf(
+    not os.environ.get("LIVE_TEST"), "Skipping because not running live test"
+)
 class TestPostgresDBSync(unittest.TestCase):
-
     def setUp(self):
 
         self.temp_schema = TEMP_SCHEMA
         self.db = Postgres()
 
         # Create a schema.
         setup_sql = f"""
                      DROP SCHEMA IF EXISTS {self.temp_schema} CASCADE;
                      CREATE SCHEMA {self.temp_schema};
                      """
         self.db.query(setup_sql)
 
         # Load dummy data to parsons tables
-        self.table1 = Table.from_csv(f'{_dir}/test_data/sample_table_1.csv')
-        self.table2 = Table.from_csv(f'{_dir}/test_data/sample_table_2.csv')
+        self.table1 = Table.from_csv(f"{_dir}/test_data/sample_table_1.csv")
+        self.table2 = Table.from_csv(f"{_dir}/test_data/sample_table_2.csv")
 
         # Create source table
-        self.db.copy(self.table1, f'{self.temp_schema}.source')
+        self.db.copy(self.table1, f"{self.temp_schema}.source")
 
         # Create DB Sync object
         self.db_sync = DBSync(self.db, self.db)
 
     def tearDown(self):
         # Drop the view, the table and the schema
 
@@ -43,270 +44,305 @@
                        DROP SCHEMA IF EXISTS {self.temp_schema} CASCADE;
                        """
         self.db.query(teardown_sql)
 
     def test_table_sync_full_drop(self):
         # Test a db sync with drop.
 
-        self.db_sync.table_sync_full(f'{self.temp_schema}.source',
-                                     f'{self.temp_schema}.destination')
+        self.db_sync.table_sync_full(
+            f"{self.temp_schema}.source", f"{self.temp_schema}.destination"
+        )
 
         source = self.db.query(f"SELECT * FROM {self.temp_schema}.source")
         destination = self.db.query(f"SELECT * FROM {self.temp_schema}.destination")
         assert_matching_tables(source, destination)
 
     def test_table_sync_full_truncate(self):
         # Test a db sync with truncate.
 
         self.db_sync.table_sync_full(
-            f'{self.temp_schema}.source', f'{self.temp_schema}.destination', if_exists='truncate')
+            f"{self.temp_schema}.source",
+            f"{self.temp_schema}.destination",
+            if_exists="truncate",
+        )
         source = self.db.query(f"SELECT * FROM {self.temp_schema}.source")
         destination = self.db.query(f"SELECT * FROM {self.temp_schema}.destination")
         assert_matching_tables(source, destination)
 
     def test_table_sync_full_empty_table(self):
         # Test a full sync of a table when the source table is empty.
 
         # Empty the source table
         self.db.query(f"TRUNCATE {self.temp_schema}.source")
 
         # Attempt to sync.
         self.db_sync.table_sync_full(
-            f'{self.temp_schema}.source', f'{self.temp_schema}.destination')
+            f"{self.temp_schema}.source", f"{self.temp_schema}.destination"
+        )
 
     def test_table_sync_full_chunk(self):
         # Test chunking in full sync.
 
         self.db_sync.chunk_size = 10
-        self.db_sync.table_sync_full(f'{self.temp_schema}.source',
-                                     f'{self.temp_schema}.destination')
+        self.db_sync.table_sync_full(
+            f"{self.temp_schema}.source", f"{self.temp_schema}.destination"
+        )
 
         source = self.db.query(f"SELECT * FROM {self.temp_schema}.source")
         destination = self.db.query(f"SELECT * FROM {self.temp_schema}.destination")
         assert_matching_tables(source, destination)
 
     def test_table_sync_incremental(self):
         # Test that incremental sync
 
-        self.db.copy(self.table1, f'{self.temp_schema}.destination')
-        self.db.copy(self.table2, f'{self.temp_schema}.source', if_exists='append')
-        self.db_sync.table_sync_incremental(f'{self.temp_schema}.source',
-                                            f'{self.temp_schema}.destination',
-                                            'pk')
+        self.db.copy(self.table1, f"{self.temp_schema}.destination")
+        self.db.copy(self.table2, f"{self.temp_schema}.source", if_exists="append")
+        self.db_sync.table_sync_incremental(
+            f"{self.temp_schema}.source", f"{self.temp_schema}.destination", "pk"
+        )
 
         count1 = self.db.query(f"SELECT * FROM {self.temp_schema}.source")
         count2 = self.db.query(f"SELECT * FROM {self.temp_schema}.destination")
         assert_matching_tables(count1, count2)
 
     def test_table_sync_incremental_chunk(self):
         # Test chunking of incremental sync.
 
         self.db_sync.chunk_size = 10
-        self.db.copy(self.table1, f'{self.temp_schema}.destination')
-        self.db.copy(self.table2, f'{self.temp_schema}.source', if_exists='append')
-        self.db_sync.table_sync_incremental(f'{self.temp_schema}.source',
-                                            f'{self.temp_schema}.destination',
-                                            'pk')
+        self.db.copy(self.table1, f"{self.temp_schema}.destination")
+        self.db.copy(self.table2, f"{self.temp_schema}.source", if_exists="append")
+        self.db_sync.table_sync_incremental(
+            f"{self.temp_schema}.source", f"{self.temp_schema}.destination", "pk"
+        )
 
         count1 = self.db.query(f"SELECT * FROM {self.temp_schema}.source")
         count2 = self.db.query(f"SELECT * FROM {self.temp_schema}.destination")
         assert_matching_tables(count1, count2)
 
     def test_table_sync_incremental_create_destination_table(self):
         # Test that an incremental sync works if the destination table does not exist.
 
-        self.db_sync.table_sync_incremental(f'{self.temp_schema}.source',
-                                            f'{self.temp_schema}.destination',
-                                            'pk')
+        self.db_sync.table_sync_incremental(
+            f"{self.temp_schema}.source", f"{self.temp_schema}.destination", "pk"
+        )
 
         count1 = self.db.query(f"SELECT * FROM {self.temp_schema}.source")
         count2 = self.db.query(f"SELECT * FROM {self.temp_schema}.destination")
         assert_matching_tables(count1, count2)
 
     def test_table_sync_incremental_empty_table(self):
         # Test an incremental sync of a table when the source table is empty.
 
         # Empty the source table
         self.db.query(f"TRUNCATE {self.temp_schema}.source")
 
         # Attempt to sync.
-        self.db_sync.table_sync_incremental(f'{self.temp_schema}.source',
-                                            f'{self.temp_schema}.destination',
-                                            'pk')
+        self.db_sync.table_sync_incremental(
+            f"{self.temp_schema}.source", f"{self.temp_schema}.destination", "pk"
+        )
 
 
 # These tests interact directly with the Postgres database. In order to run, set the
 # env to LIVE_TEST='TRUE'.
-@unittest.skipIf(not os.environ.get('LIVE_TEST'), 'Skipping because not running live test')
+@unittest.skipIf(
+    not os.environ.get("LIVE_TEST"), "Skipping because not running live test"
+)
 class TestRedshiftDBSync(TestPostgresDBSync):
-    """ This test inherits all of the tests from the Postgres test. """
+    """This test inherits all of the tests from the Postgres test."""
 
     def setUp(self):
 
         self.temp_schema = TEMP_SCHEMA
         self.db = Redshift()
 
         # Create a schema.
         setup_sql = f"""
                      DROP SCHEMA IF EXISTS {self.temp_schema} CASCADE;
                      CREATE SCHEMA {self.temp_schema};
                      """
         self.db.query(setup_sql)
 
         # Load dummy data to parsons tables
-        self.table1 = Table.from_csv(f'{_dir}/test_data/sample_table_1.csv')
-        self.table2 = Table.from_csv(f'{_dir}/test_data/sample_table_2.csv')
+        self.table1 = Table.from_csv(f"{_dir}/test_data/sample_table_1.csv")
+        self.table2 = Table.from_csv(f"{_dir}/test_data/sample_table_2.csv")
 
         # Create source table
-        self.db.copy(self.table1, f'{self.temp_schema}.source')
+        self.db.copy(self.table1, f"{self.temp_schema}.source")
 
         # Create DB Sync object
         self.db_sync = DBSync(self.db, self.db)
 
 
 class TestFakeDBSync(unittest.TestCase):
-
     def setUp(self):
         self.fake_source = FakeDatabase()
         self.fake_destination = FakeDatabase()
 
     def test_table_sync_full(self):
         dbsync = DBSync(self.fake_source, self.fake_destination)
-        source_data = Table([
-            {'id': 1, 'value': 11},
-            {'id': 2, 'value': 121142},
-            {'id': 3, 'value': 111},
-            {'id': 4, 'value': 12211},
-            {'id': 5, 'value': 1231},
-        ])
-        self.fake_source.setup_table('source', source_data)
+        source_data = Table(
+            [
+                {"id": 1, "value": 11},
+                {"id": 2, "value": 121142},
+                {"id": 3, "value": 111},
+                {"id": 4, "value": 12211},
+                {"id": 5, "value": 1231},
+            ]
+        )
+        self.fake_source.setup_table("source", source_data)
 
-        dbsync.table_sync_full('source', 'destination')
+        dbsync.table_sync_full("source", "destination")
 
-        destination = self.fake_destination.table('destination')
+        destination = self.fake_destination.table("destination")
 
         # Make sure the data came through
         assert_matching_tables(source_data, destination.data)
 
     def test_table_sync_incremental(self):
         dbsync = DBSync(self.fake_source, self.fake_destination)
-        source_data = Table([
-            {'id': 1, 'value': 11},
-            {'id': 2, 'value': 121142},
-            {'id': 3, 'value': 111},
-            {'id': 4, 'value': 12211},
-            {'id': 5, 'value': 1231},
-        ])
-        self.fake_source.setup_table('source', source_data)
+        source_data = Table(
+            [
+                {"id": 1, "value": 11},
+                {"id": 2, "value": 121142},
+                {"id": 3, "value": 111},
+                {"id": 4, "value": 12211},
+                {"id": 5, "value": 1231},
+            ]
+        )
+        self.fake_source.setup_table("source", source_data)
 
         # Start with one row
-        destination_data = Table([
-            {'id': 1, 'value': 11},
-        ])
-        self.fake_destination.setup_table('destination', destination_data)
+        destination_data = Table(
+            [
+                {"id": 1, "value": 11},
+            ]
+        )
+        self.fake_destination.setup_table("destination", destination_data)
 
-        dbsync.table_sync_incremental('source', 'destination', 'id')
+        dbsync.table_sync_incremental("source", "destination", "id")
 
-        destination = self.fake_destination.table('destination')
+        destination = self.fake_destination.table("destination")
 
         # Make sure the rest of the data came through
         assert_matching_tables(source_data, destination.data)
 
     def test_table_sync_full_with_retry(self):
         # Setup the dbsync with two retries
         dbsync = DBSync(self.fake_source, self.fake_destination, retries=2)
-        source_data = Table([
-            {'id': 1, 'value': 11},
-            {'id': 2, 'value': 121142},
-        ])
-        self.fake_source.setup_table('source', source_data)
+        source_data = Table(
+            [
+                {"id": 1, "value": 11},
+                {"id": 2, "value": 121142},
+            ]
+        )
+        self.fake_source.setup_table("source", source_data)
 
         # Have the copy fail twice
-        self.fake_destination.setup_table('destination', Table(), failures=2)
+        self.fake_destination.setup_table("destination", Table(), failures=2)
 
-        dbsync.table_sync_full('source', 'destination')
+        dbsync.table_sync_full("source", "destination")
 
-        destination = self.fake_destination.table('destination')
+        destination = self.fake_destination.table("destination")
 
         # Make sure all of the data still came through
         assert_matching_tables(source_data, destination.data)
 
     def test_table_sync_full_without_retry(self):
         # Setup the dbsync with no retries
         dbsync = DBSync(self.fake_source, self.fake_destination, retries=0)
-        source_data = Table([
-            {'id': 1, 'value': 11},
-            {'id': 2, 'value': 121142},
-        ])
-        self.fake_source.setup_table('source', source_data)
+        source_data = Table(
+            [
+                {"id": 1, "value": 11},
+                {"id": 2, "value": 121142},
+            ]
+        )
+        self.fake_source.setup_table("source", source_data)
 
         # Have the copy fail once
-        self.fake_destination.setup_table('destination', Table(), failures=1)
+        self.fake_destination.setup_table("destination", Table(), failures=1)
 
         # Make sure the sync results in an exception
-        self.assertRaises(ValueError, lambda: dbsync.table_sync_full('source', 'destination'))
+        self.assertRaises(
+            ValueError, lambda: dbsync.table_sync_full("source", "destination")
+        )
 
     def test_table_sync_full_order_by(self):
         dbsync = DBSync(self.fake_source, self.fake_destination)
-        source_data = Table([
-            {'id': 1, 'value': 21},
-            {'id': 2, 'value': 121142},
-            {'id': 3, 'value': 1},
-        ])
-        self.fake_source.setup_table('source', source_data)
-        self.fake_destination.setup_table('destination', Table())
+        source_data = Table(
+            [
+                {"id": 1, "value": 21},
+                {"id": 2, "value": 121142},
+                {"id": 3, "value": 1},
+            ]
+        )
+        self.fake_source.setup_table("source", source_data)
+        self.fake_destination.setup_table("destination", Table())
 
-        dbsync.table_sync_full('source', 'destination', order_by='value')
+        dbsync.table_sync_full("source", "destination", order_by="value")
 
-        destination = self.fake_destination.table('destination')
+        destination = self.fake_destination.table("destination")
 
         # Check that the rows were inserted in the expected order
-        self.assertEqual(destination.data[0]['id'], 3)
-        self.assertEqual(destination.data[1]['id'], 1)
-        self.assertEqual(destination.data[2]['id'], 2)
+        self.assertEqual(destination.data[0]["id"], 3)
+        self.assertEqual(destination.data[1]["id"], 1)
+        self.assertEqual(destination.data[2]["id"], 2)
 
     def test_table_sync_full_read_chunk(self):
         dbsync = DBSync(self.fake_source, self.fake_destination, read_chunk_size=2)
-        source_data = Table([
-            {'id': 1, 'value': 11},
-            {'id': 2, 'value': 121142},
-            {'id': 3, 'value': 111},
-            {'id': 4, 'value': 12211},
-            {'id': 5, 'value': 1231},
-        ])
-        self.fake_source.setup_table('source', source_data)
+        source_data = Table(
+            [
+                {"id": 1, "value": 11},
+                {"id": 2, "value": 121142},
+                {"id": 3, "value": 111},
+                {"id": 4, "value": 12211},
+                {"id": 5, "value": 1231},
+            ]
+        )
+        self.fake_source.setup_table("source", source_data)
 
-        dbsync.table_sync_full('source', 'destination')
+        dbsync.table_sync_full("source", "destination")
 
-        destination = self.fake_destination.table('destination')
+        destination = self.fake_destination.table("destination")
 
         # Make sure the data came through
         assert_matching_tables(source_data, destination.data)
 
         # Make sure copy was called the expected number of times
         # read chunks of 2, 5 rows to write.. should be 3 copy calls
-        self.assertEqual(len(self.fake_destination.copy_call_args), 3,
-                         self.fake_destination.copy_call_args)
+        self.assertEqual(
+            len(self.fake_destination.copy_call_args),
+            3,
+            self.fake_destination.copy_call_args,
+        )
 
     def test_table_sync_full_write_chunk(self):
-        dbsync = DBSync(self.fake_source, self.fake_destination, read_chunk_size=1,
-                        write_chunk_size=3)
-        source_data = Table([
-            {'id': 1, 'value': 11},
-            {'id': 2, 'value': 121142},
-            {'id': 3, 'value': 111},
-            {'id': 4, 'value': 12211},
-            {'id': 5, 'value': 1231},
-        ])
-        self.fake_source.setup_table('source', source_data)
+        dbsync = DBSync(
+            self.fake_source,
+            self.fake_destination,
+            read_chunk_size=1,
+            write_chunk_size=3,
+        )
+        source_data = Table(
+            [
+                {"id": 1, "value": 11},
+                {"id": 2, "value": 121142},
+                {"id": 3, "value": 111},
+                {"id": 4, "value": 12211},
+                {"id": 5, "value": 1231},
+            ]
+        )
+        self.fake_source.setup_table("source", source_data)
 
-        dbsync.table_sync_full('source', 'destination')
+        dbsync.table_sync_full("source", "destination")
 
-        destination = self.fake_destination.table('destination')
+        destination = self.fake_destination.table("destination")
 
         # Make sure the data came through
         assert_matching_tables(source_data, destination.data)
 
         # Make sure copy was called the expected number of times
         # write chunks of 3, 5 rows to write.. should be 2 copy calls
-        self.assertEqual(len(self.fake_destination.copy_call_args), 2,
-                         self.fake_destination.copy_call_args)
+        self.assertEqual(
+            len(self.fake_destination.copy_call_args),
+            2,
+            self.fake_destination.copy_call_args,
+        )
```

### Comparing `parsons-1.0.0/test/test_databases/test_mysql.py` & `parsons-1.1.0/test/test_databases/test_mysql.py`

 * *Files 10% similar despite different names*

```diff
@@ -2,17 +2,18 @@
 from parsons.databases.mysql.create_table import MySQLCreateTable
 from test.utils import assert_matching_tables
 import unittest
 import os
 
 
 # These tests interact directly with the MySQL database. To run, set env variable "LIVE_TEST=True"
-@unittest.skipIf(not os.environ.get('LIVE_TEST'), 'Skipping because not running live test')
+@unittest.skipIf(
+    not os.environ.get("LIVE_TEST"), "Skipping because not running live test"
+)
 class TestMySQLLive(unittest.TestCase):
-
     def setUp(self):
 
         self.mysql = MySQL()
 
     def tearDown(self):
 
         # Drop the view, the table and the schema
@@ -38,146 +39,158 @@
         self.mysql.query(sql)
 
         sql = "INSERT INTO test (name, user_name) VALUES ('me', 'myuser');"
         self.mysql.query(sql)
 
         r = self.mysql.query("select * from test")
 
-        assert_matching_tables(Table([{'name': 'me', 'user_name': 'myuser'}]), r)
+        assert_matching_tables(Table([{"name": "me", "user_name": "myuser"}]), r)
 
 
 # These tests interact directly with the MySQL database. To run, set env variable "LIVE_TEST=True"
-@unittest.skipIf(not os.environ.get('LIVE_TEST'), 'Skipping because not running live test')
+@unittest.skipIf(
+    not os.environ.get("LIVE_TEST"), "Skipping because not running live test"
+)
 class TestMySQL(unittest.TestCase):
-
     def setUp(self):
 
         self.mysql = MySQL()
 
         # Create tables
         self.mysql.query(
-            "CREATE TABLE IF NOT EXISTS test (name VARCHAR(255), user_name VARCHAR(255), id INT)")
-        self.mysql.query("""
+            "CREATE TABLE IF NOT EXISTS test (name VARCHAR(255), user_name VARCHAR(255), id INT)"
+        )
+        self.mysql.query(
+            """
                          INSERT INTO test (name, user_name, id)
                          VALUES ('me', 'myuser', '1'),
                                 ('you', 'hey', '2'),
                                 ('you', 'hey', '3')
-                         """)
+                         """
+        )
 
-        self.tbl = MySQLCreateTable(self.mysql, 'test')
+        self.tbl = MySQLCreateTable(self.mysql, "test")
 
     def tearDown(self):
 
         self.mysql.query("DROP TABLE IF EXISTS test;")
 
     def test_num_rows(self):
 
         self.assertEqual(self.tbl.num_rows, 3)
 
     def test_max_primary_key(self):
 
-        self.assertEqual(self.tbl.max_primary_key('id'), 3)
+        self.assertEqual(self.tbl.max_primary_key("id"), 3)
 
     def test_distinct_primary_key(self):
 
-        self.assertTrue(self.tbl.distinct_primary_key('id'))
-        self.assertFalse(self.tbl.distinct_primary_key('user_name'))
+        self.assertTrue(self.tbl.distinct_primary_key("id"))
+        self.assertFalse(self.tbl.distinct_primary_key("user_name"))
 
     def test_columns(self):
 
-        self.assertEqual(self.tbl.columns, ['name', 'user_name', 'id'])
+        self.assertEqual(self.tbl.columns, ["name", "user_name", "id"])
 
     def test_exists(self):
 
         self.assertTrue(self.tbl.exists)
 
-        tbl_bad = MySQLCreateTable(self.mysql, 'bad_test')
+        tbl_bad = MySQLCreateTable(self.mysql, "bad_test")
         self.assertFalse(tbl_bad.exists)
 
     def test_drop(self):
 
         self.tbl.drop()
         self.assertFalse(self.tbl.exists)
 
     def test_truncate(self):
 
         self.tbl.truncate()
         self.assertEqual(self.tbl.num_rows, 0)
 
     def test_get_rows(self):
 
-        data = [['name', 'user_name', 'id'],
-                ['me', 'myuser', '1'],
-                ['you', 'hey', '2'],
-                ['you', 'hey', '3']]
+        data = [
+            ["name", "user_name", "id"],
+            ["me", "myuser", "1"],
+            ["you", "hey", "2"],
+            ["you", "hey", "3"],
+        ]
         tbl = Table(data)
 
         assert_matching_tables(self.tbl.get_rows(), tbl)
 
     def test_get_new_rows(self):
 
-        data = [['name', 'user_name', 'id'],
-                ['you', 'hey', '2'],
-                ['you', 'hey', '3']]
+        data = [["name", "user_name", "id"], ["you", "hey", "2"], ["you", "hey", "3"]]
         tbl = Table(data)
 
         # Basic
-        assert_matching_tables(self.tbl.get_new_rows('id', 1), tbl)
+        assert_matching_tables(self.tbl.get_new_rows("id", 1), tbl)
 
         # Chunking
-        assert_matching_tables(self.tbl.get_new_rows('id', 1, chunk_size=1), tbl)
+        assert_matching_tables(self.tbl.get_new_rows("id", 1, chunk_size=1), tbl)
 
     def test_get_new_rows_count(self):
 
-        self.assertEqual(self.tbl.get_new_rows_count('id', 1), 2)
+        self.assertEqual(self.tbl.get_new_rows_count("id", 1), 2)
+
 
 # TODO: figure out why there are 2 of these
 class TestMySQL(unittest.TestCase):  # noqa
-
     def setUp(self):
 
-        self.mysql = MySQL(username='test', password='test', host='test', db='test', port=123)
-
-        self.tbl = Table([['ID', 'Name', 'Score'],
-                          [1, 'Jim', 1.9],
-                          [2, 'John', -0.5],
-                          [3, 'Sarah', .0004]])
+        self.mysql = MySQL(
+            username="test", password="test", host="test", db="test", port=123
+        )
+
+        self.tbl = Table(
+            [
+                ["ID", "Name", "Score"],
+                [1, "Jim", 1.9],
+                [2, "John", -0.5],
+                [3, "Sarah", 0.0004],
+            ]
+        )
 
     def test_data_type(self):
 
         # Test bool
         self.mysql.DO_PARSE_BOOLS = True
-        self.assertEqual(self.mysql.data_type(1, ''), 'bool')
-        self.assertEqual(self.mysql.data_type(False, ''), 'bool')
+        self.assertEqual(self.mysql.data_type(1, ""), "bool")
+        self.assertEqual(self.mysql.data_type(False, ""), "bool")
 
         self.mysql.DO_PARSE_BOOLS = False
         # Test smallint
-        self.assertEqual(self.mysql.data_type(1, ''), 'smallint')
-        self.assertEqual(self.mysql.data_type(2, ''), 'smallint')
+        self.assertEqual(self.mysql.data_type(1, ""), "smallint")
+        self.assertEqual(self.mysql.data_type(2, ""), "smallint")
         # Test int
-        self.assertEqual(self.mysql.data_type(32769, ''), 'mediumint')
+        self.assertEqual(self.mysql.data_type(32769, ""), "mediumint")
         # Test bigint
-        self.assertEqual(self.mysql.data_type(2147483648, ''), 'bigint')
+        self.assertEqual(self.mysql.data_type(2147483648, ""), "bigint")
         # Test varchar that looks like an int
-        self.assertEqual(self.mysql.data_type('00001', ''), 'varchar')
+        self.assertEqual(self.mysql.data_type("00001", ""), "varchar")
         # Test varchar that looks like a bool
-        self.assertEqual(self.mysql.data_type(False, ''), 'varchar')
+        self.assertEqual(self.mysql.data_type(False, ""), "varchar")
         # Test a float as a decimal
-        self.assertEqual(self.mysql.data_type(5.001, ''), 'float')
+        self.assertEqual(self.mysql.data_type(5.001, ""), "float")
         # Test varchar
-        self.assertEqual(self.mysql.data_type('word', ''), 'varchar')
+        self.assertEqual(self.mysql.data_type("word", ""), "varchar")
         # Test int with underscore
-        self.assertEqual(self.mysql.data_type('1_2', ''), 'varchar')
+        self.assertEqual(self.mysql.data_type("1_2", ""), "varchar")
         # Test int with leading zero
-        self.assertEqual(self.mysql.data_type('01', ''), 'varchar')
+        self.assertEqual(self.mysql.data_type("01", ""), "varchar")
 
     def test_evaluate_table(self):
 
-        table_map = [{'name': 'ID', 'type': 'smallint', 'width': 0},
-                     {'name': 'Name', 'type': 'varchar', 'width': 8},
-                     {'name': 'Score', 'type': 'float', 'width': 0}]
+        table_map = [
+            {"name": "ID", "type": "smallint", "width": 0},
+            {"name": "Name", "type": "varchar", "width": 8},
+            {"name": "Score", "type": "float", "width": 0},
+        ]
         self.assertEqual(self.mysql.evaluate_table(self.tbl), table_map)
 
     def test_create_statement(self):
 
         stmt = "CREATE TABLE test_table ( \n id smallint \n,name varchar(10) \n,score float \n);"
-        self.assertEqual(self.mysql.create_statement(self.tbl, 'test_table'), stmt)
+        self.assertEqual(self.mysql.create_statement(self.tbl, "test_table"), stmt)
```

### Comparing `parsons-1.0.0/test/test_databases/test_postgres.py` & `parsons-1.1.0/test/test_databases/test_postgres.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,165 +1,186 @@
 from parsons import Postgres, Table
 from test.utils import assert_matching_tables
 import unittest
 import os
 
 # The name of the schema and will be temporarily created for the tests
-TEMP_SCHEMA = 'parsons_test'
+TEMP_SCHEMA = "parsons_test"
 
 # These tests do not interact with the Postgres Database directly, and don't need real credentials
 
 
 class TestPostgresCreateStatement(unittest.TestCase):
-
     def setUp(self):
 
-        self.pg = Postgres(username='test', password='test', host='test', db='test', port=123)
-
-        self.tbl = Table([['ID', 'Name'],
-                          [1, 'Jim'],
-                          [2, 'John'],
-                          [3, 'Sarah']])
-
-        self.tbl2 = Table([
-            ["c1", "c2", "c3", "c4", "c5", "c6", "c7"],
-            ["a", "", 1, "NA", 1.4, 1, 2],
-            ["b", "", 2, "NA", 1.4, 1, 2],
-            ["c", "", 3.4, "NA", "", "", "a"],
-            ["d", "", 5, "NA", 1.4, 1, 2],
-            ["e", "", 6, "NA", 1.4, 1, 2],
-            ["f", "", 7.8, "NA", 1.4, 1, 2],
-            ["g", "", 9, "NA", 1.4, 1, 2],
-        ])
+        self.pg = Postgres(
+            username="test", password="test", host="test", db="test", port=123
+        )
+
+        self.tbl = Table([["ID", "Name"], [1, "Jim"], [2, "John"], [3, "Sarah"]])
+
+        self.tbl2 = Table(
+            [
+                ["c1", "c2", "c3", "c4", "c5", "c6", "c7"],
+                ["a", "", 1, "NA", 1.4, 1, 2],
+                ["b", "", 2, "NA", 1.4, 1, 2],
+                ["c", "", 3.4, "NA", "", "", "a"],
+                ["d", "", 5, "NA", 1.4, 1, 2],
+                ["e", "", 6, "NA", 1.4, 1, 2],
+                ["f", "", 7.8, "NA", 1.4, 1, 2],
+                ["g", "", 9, "NA", 1.4, 1, 2],
+            ]
+        )
 
         self.mapping = self.pg.generate_data_types(self.tbl)
         self.mapping2 = self.pg.generate_data_types(self.tbl2)
         self.pg.DO_PARSE_BOOLS = True
         self.mapping3 = self.pg.generate_data_types(self.tbl2)
 
     def test_connection(self):
 
         # Test connection with kwargs passed
-        Postgres(username='test', password='test', host='test', db='test')
+        Postgres(username="test", password="test", host="test", db="test")
 
         # Test connection with env variables
-        os.environ['PGUSER'] = 'user_env'
-        os.environ['PGPASSWORD'] = 'pass_env'
-        os.environ['PGHOST'] = 'host_env'
-        os.environ['PGDATABASE'] = 'db_env'
-        os.environ['PGPORT'] = '5432'
+        os.environ["PGUSER"] = "user_env"
+        os.environ["PGPASSWORD"] = "pass_env"
+        os.environ["PGHOST"] = "host_env"
+        os.environ["PGDATABASE"] = "db_env"
+        os.environ["PGPORT"] = "5432"
         pg_env = Postgres()
 
-        self.assertEqual(pg_env.username, 'user_env')
-        self.assertEqual(pg_env.password, 'pass_env')
-        self.assertEqual(pg_env.host, 'host_env')
-        self.assertEqual(pg_env.db, 'db_env')
+        self.assertEqual(pg_env.username, "user_env")
+        self.assertEqual(pg_env.password, "pass_env")
+        self.assertEqual(pg_env.host, "host_env")
+        self.assertEqual(pg_env.db, "db_env")
         self.assertEqual(pg_env.port, 5432)
 
     def test_data_type(self):
         self.pg.DO_PARSE_BOOLS = False
         # Test smallint
-        self.assertEqual(self.pg.data_type(1, ''), 'smallint')
-        self.assertEqual(self.pg.data_type(2, ''), 'smallint')
+        self.assertEqual(self.pg.data_type(1, ""), "smallint")
+        self.assertEqual(self.pg.data_type(2, ""), "smallint")
         # Test int
-        self.assertEqual(self.pg.data_type(32769, ''), 'int')
+        self.assertEqual(self.pg.data_type(32769, ""), "int")
         # Test bigint
-        self.assertEqual(self.pg.data_type(2147483648, ''), 'bigint')
+        self.assertEqual(self.pg.data_type(2147483648, ""), "bigint")
         # Test varchar that looks like an int
-        self.assertEqual(self.pg.data_type('00001', ''), 'varchar')
+        self.assertEqual(self.pg.data_type("00001", ""), "varchar")
         # Test varchar that looks like a bool
-        self.assertEqual(self.pg.data_type(True, ''), 'varchar')
+        self.assertEqual(self.pg.data_type(True, ""), "varchar")
         # Test a float as a decimal
-        self.assertEqual(self.pg.data_type(5.001, ''), 'decimal')
+        self.assertEqual(self.pg.data_type(5.001, ""), "decimal")
         # Test varchar
-        self.assertEqual(self.pg.data_type('word', ''), 'varchar')
+        self.assertEqual(self.pg.data_type("word", ""), "varchar")
         # Test int with underscore
-        self.assertEqual(self.pg.data_type('1_2', ''), 'varchar')
+        self.assertEqual(self.pg.data_type("1_2", ""), "varchar")
         # Test int with leading zero
-        self.assertEqual(self.pg.data_type('01', ''), 'varchar')
+        self.assertEqual(self.pg.data_type("01", ""), "varchar")
 
         # Test bool
         self.pg.DO_PARSE_BOOLS = True
-        self.assertEqual(self.pg.data_type(1, ''), 'bool')
-        self.assertEqual(self.pg.data_type(True, ''), 'bool')
+        self.assertEqual(self.pg.data_type(1, ""), "bool")
+        self.assertEqual(self.pg.data_type(True, ""), "bool")
 
     def test_generate_data_types(self):
 
         # Test correct header labels
-        self.assertEqual(self.mapping['headers'], ['ID', 'Name'])
+        self.assertEqual(self.mapping["headers"], ["ID", "Name"])
         # Test correct data types
-        self.assertEqual(self.mapping['type_list'], ['smallint', 'varchar'])
+        self.assertEqual(self.mapping["type_list"], ["smallint", "varchar"])
         self.assertEqual(
-            self.mapping2['type_list'],
-            ['varchar', 'varchar', 'decimal', 'varchar', "decimal", "smallint", "varchar"])
+            self.mapping2["type_list"],
+            [
+                "varchar",
+                "varchar",
+                "decimal",
+                "varchar",
+                "decimal",
+                "smallint",
+                "varchar",
+            ],
+        )
         self.assertEqual(
-            self.mapping3['type_list'],
-            ['varchar', 'varchar', 'decimal', 'varchar', "decimal", "bool", "varchar"])
+            self.mapping3["type_list"],
+            ["varchar", "varchar", "decimal", "varchar", "decimal", "bool", "varchar"],
+        )
         # Test correct lengths
-        self.assertEqual(self.mapping['longest'], [1, 5])
+        self.assertEqual(self.mapping["longest"], [1, 5])
 
     def test_vc_padding(self):
 
         # Test padding calculated correctly
-        self.assertEqual(self.pg.vc_padding(self.mapping, .2), [1, 6])
+        self.assertEqual(self.pg.vc_padding(self.mapping, 0.2), [1, 6])
 
     def test_vc_max(self):
 
         # Test max sets it to the max
-        self.assertEqual(self.pg.vc_max(self.mapping, ['Name']), [1, 65535])
+        self.assertEqual(self.pg.vc_max(self.mapping, ["Name"]), [1, 65535])
 
         # Test raises when can't find column
         # To Do
 
     def test_vc_validate(self):
 
         # Test that a column with a width of 0 is set to 1
-        self.mapping['longest'][0] = 0
+        self.mapping["longest"][0] = 0
         self.mapping = self.pg.vc_validate(self.mapping)
         self.assertEqual(self.mapping, [1, 5])
 
     def test_create_sql(self):
 
         # Test the the statement is expected
-        sql = self.pg.create_sql('tmc.test', self.mapping, distkey='ID')
+        sql = self.pg.create_sql("tmc.test", self.mapping, distkey="ID")
         exp_sql = "create table tmc.test (\n  id smallint,\n  name varchar(5)) \ndistkey(ID) ;"
         self.assertEqual(sql, exp_sql)
 
     def test_column_validate(self):
 
-        bad_cols = ['a', 'a', '', 'SELECT', 'asdfjkasjdfklasjdfklajskdfljaskldfjaklsdfjlaksdfjklasjdfklasjdkfljaskldfljkasjdkfasjlkdfjklasdfjklakjsfasjkdfljaslkdfjklasdfjklasjkldfakljsdfjalsdkfjklasjdfklasjdfklasdkljf']  # noqa: E501
-        fixed_cols = ['a', 'a_1', 'col_2', 'col_3', 'asdfjkasjdfklasjdfklajskdfljaskldfjaklsdfjlaksdfjklasjdfklasjdkfljaskldfljkasjdkfasjlkdfjklasdfjklakjsfasjkdfljaslkdfjkl']  # noqa: E501
+        bad_cols = [
+            "a",
+            "a",
+            "",
+            "SELECT",
+            "asdfjkasjdfklasjdfklajskdfljaskldfjaklsdfjlaksdfjklasjdfklasjdkfljaskldfljkasjdkfasjlkdfjklasdfjklakjsfasjkdfljaslkdfjklasdfjklasjkldfakljsdfjalsdkfjklasjdfklasjdfklasdkljf",  # noqa: E501
+        ]
+        fixed_cols = [
+            "a",
+            "a_1",
+            "col_2",
+            "col_3",
+            "asdfjkasjdfklasjdfklajskdfljaskldfjaklsdfjlaksdfjklasjdfklasjdkfljaskldfljkasjdkfasjlkdfjklasdfjklakjsfasjkdfljaslkdfjkl",  # noqa: E501
+        ]
         self.assertEqual(self.pg.column_name_validate(bad_cols), fixed_cols)
 
     def test_create_statement(self):
 
         # Assert that copy statement is expected
-        sql = self.pg.create_statement(self.tbl, 'tmc.test', distkey='ID')
+        sql = self.pg.create_statement(self.tbl, "tmc.test", distkey="ID")
         exp_sql = """create table tmc.test (\n  "id" smallint,\n  "name" varchar(5)) \ndistkey(ID) ;"""  # noqa: E501
         self.assertEqual(sql, exp_sql)
 
         # Assert that an error is raised by an empty table
-        empty_table = Table([['Col_1', 'Col_2']])
-        self.assertRaises(ValueError, self.pg.create_statement, empty_table, 'tmc.test')
+        empty_table = Table([["Col_1", "Col_2"]])
+        self.assertRaises(ValueError, self.pg.create_statement, empty_table, "tmc.test")
+
 
 # These tests interact directly with the Postgres database
 
 
-@unittest.skipIf(not os.environ.get('LIVE_TEST'), 'Skipping because not running live test')
+@unittest.skipIf(
+    not os.environ.get("LIVE_TEST"), "Skipping because not running live test"
+)
 class TestPostgresDB(unittest.TestCase):
-
     def setUp(self):
 
         self.temp_schema = TEMP_SCHEMA
         self.pg = Postgres()
 
-        self.tbl = Table([['ID', 'Name'],
-                          [1, 'Jim'],
-                          [2, 'John'],
-                          [3, 'Sarah']])
+        self.tbl = Table([["ID", "Name"], [1, "Jim"], [2, "John"], [3, "Sarah"]])
 
         # Create a schema, create a table, create a view
         setup_sql = f"""
                     drop schema if exists {self.temp_schema} cascade;
                     create schema {self.temp_schema};
                     """
 
@@ -179,73 +200,81 @@
                        drop schema if exists {self.temp_schema} cascade;
                        """
         self.pg.query(teardown_sql)
 
     def test_query(self):
 
         # Check that query sending back expected result
-        r = self.pg.query('select 1')
-        self.assertEqual(r[0]['?column?'], 1)
+        r = self.pg.query("select 1")
+        self.assertEqual(r[0]["?column?"], 1)
 
     def test_query_with_parameters(self):
         table_name = f"{self.temp_schema}.test"
-        self.pg.copy(self.tbl, f"{self.temp_schema}.test", if_exists='append')
+        self.pg.copy(self.tbl, f"{self.temp_schema}.test", if_exists="append")
 
         sql = f"select * from {table_name} where name = %s"
-        name = 'Sarah'
+        name = "Sarah"
         r = self.pg.query(sql, parameters=[name])
-        self.assertEqual(r[0]['name'], name)
+        self.assertEqual(r[0]["name"], name)
 
         sql = f"select * from {table_name} where name in (%s, %s)"
-        names = ['Sarah', 'John']
+        names = ["Sarah", "John"]
         r = self.pg.query(sql, parameters=names)
         self.assertEqual(r.num_rows, 2)
 
     def test_copy(self):
 
         # Copy a table and ensure table exists
-        self.pg.copy(self.tbl, f'{self.temp_schema}.test_copy', if_exists='drop')
-        r = self.pg.query(f"select * from {self.temp_schema}.test_copy where name='Jim'")
-        self.assertEqual(r[0]['id'], 1)
+        self.pg.copy(self.tbl, f"{self.temp_schema}.test_copy", if_exists="drop")
+        r = self.pg.query(
+            f"select * from {self.temp_schema}.test_copy where name='Jim'"
+        )
+        self.assertEqual(r[0]["id"], 1)
 
         # Copy table and ensure truncate works.
-        self.pg.copy(self.tbl, f'{self.temp_schema}.test_copy', if_exists='truncate')
+        self.pg.copy(self.tbl, f"{self.temp_schema}.test_copy", if_exists="truncate")
         tbl = self.pg.query(f"select count(*) from {self.temp_schema}.test_copy")
         self.assertEqual(tbl.first, 3)
 
         # Copy table and ensure that drop works.
-        self.pg.copy(self.tbl, f'{self.temp_schema}.test_copy', if_exists='drop')
+        self.pg.copy(self.tbl, f"{self.temp_schema}.test_copy", if_exists="drop")
         tbl = self.pg.query(f"select count(*) from {self.temp_schema}.test_copy")
         self.assertEqual(tbl.first, 3)
 
         # Copy table and ensure that append works.
-        self.pg.copy(self.tbl, f'{self.temp_schema}.test_copy', if_exists='append')
+        self.pg.copy(self.tbl, f"{self.temp_schema}.test_copy", if_exists="append")
         tbl = self.pg.query(f"select count(*) from {self.temp_schema}.test_copy")
         self.assertEqual(tbl.first, 6)
 
         # Try to copy the table and ensure that default fail works.
-        self.assertRaises(ValueError, self.pg.copy, self.tbl, f'{self.temp_schema}.test_copy')
+        self.assertRaises(
+            ValueError, self.pg.copy, self.tbl, f"{self.temp_schema}.test_copy"
+        )
 
         # Try to copy the table and ensure that explicit fail works.
         self.assertRaises(
-            ValueError, self.pg.copy, self.tbl, f'{self.temp_schema}.test_copy', if_exists='fail')
+            ValueError,
+            self.pg.copy,
+            self.tbl,
+            f"{self.temp_schema}.test_copy",
+            if_exists="fail",
+        )
 
     def test_to_postgres(self):
 
-        self.tbl.to_postgres(f'{self.temp_schema}.test_copy')
-        r = self.pg.query(f"select * from {self.temp_schema}.test_copy where name='Jim'")
-        self.assertEqual(r[0]['id'], 1)
+        self.tbl.to_postgres(f"{self.temp_schema}.test_copy")
+        r = self.pg.query(
+            f"select * from {self.temp_schema}.test_copy where name='Jim'"
+        )
+        self.assertEqual(r[0]["id"], 1)
 
     def test_from_postgres(self):
 
-        tbl = Table([['id', 'name'],
-                     [1, 'Jim'],
-                     [2, 'John'],
-                     [3, 'Sarah']])
+        tbl = Table([["id", "name"], [1, "Jim"], [2, "John"], [3, "Sarah"]])
 
-        self.pg.copy(self.tbl, f'{self.temp_schema}.test_copy', if_exists='drop')
+        self.pg.copy(self.tbl, f"{self.temp_schema}.test_copy", if_exists="drop")
         out_tbl = self.tbl.from_postgres(f"SELECT * FROM {self.temp_schema}.test_copy")
         assert_matching_tables(out_tbl, tbl)
 
 
 if __name__ == "__main__":
     unittest.main()
```

### Comparing `parsons-1.0.0/test/test_etl.py` & `parsons-1.1.0/test/test_etl.py`

 * *Files 18% similar despite different names*

```diff
@@ -10,51 +10,52 @@
 
 # Notes :
 # - The `Table.to_postgres()` test is housed in the Postgres tests
 # - The `Table.from_postgres()` test is housed in the Postgres test
 
 
 class TestParsonsTable(unittest.TestCase):
-
     def setUp(self):
 
         # Create Table object
-        self.lst = [{'a': 1, 'b': 2, 'c': 3},
-                    {'a': 4, 'b': 5, 'c': 6},
-                    {'a': 7, 'b': 8, 'c': 9},
-                    {'a': 10, 'b': 11, 'c': 12},
-                    {'a': 13, 'b': 14, 'c': 15}]
-        self.lst_dicts = [{'first': 'Bob', 'last': 'Smith'}]
+        self.lst = [
+            {"a": 1, "b": 2, "c": 3},
+            {"a": 4, "b": 5, "c": 6},
+            {"a": 7, "b": 8, "c": 9},
+            {"a": 10, "b": 11, "c": 12},
+            {"a": 13, "b": 14, "c": 15},
+        ]
+        self.lst_dicts = [{"first": "Bob", "last": "Smith"}]
         self.tbl = Table(self.lst_dicts)
 
         # Create a tmp dir
-        os.mkdir('tmp')
+        os.mkdir("tmp")
 
     def tearDown(self):
 
         # Delete tmp folder and files
-        shutil.rmtree('tmp')
+        shutil.rmtree("tmp")
 
     def test_from_list_of_dicts(self):
 
         tbl = Table(self.lst)
 
         # Test Iterate and is list like
-        self.assertEqual(tbl[0], {'a': 1, 'b': 2, 'c': 3})
+        self.assertEqual(tbl[0], {"a": 1, "b": 2, "c": 3})
 
     def test_from_list_of_lists(self):
 
         list_of_lists = [
-            ['a', 'b', 'c'],
+            ["a", "b", "c"],
             [1, 2, 3],
             [4, 5, 6],
         ]
         tbl = Table(list_of_lists)
 
-        self.assertEqual(tbl[0], {'a': 1, 'b': 2, 'c': 3})
+        self.assertEqual(tbl[0], {"a": 1, "b": 2, "c": 3})
 
     def test_from_petl(self):
 
         nrows = 10
         ptbl = petl.dummytable(numrows=nrows)
         tbl = Table(ptbl)
         self.assertEqual(tbl.num_rows, nrows)
@@ -87,27 +88,27 @@
         _ = tbl_materialized.materialize_to_file()
 
         assert_matching_tables(self.tbl, tbl_materialized)
 
     def test_empty_column(self):
         # Test that returns True on an empty column and False on a populated one.
 
-        tbl = Table([['a', 'b'], ['1', None], ['2', None]])
+        tbl = Table([["a", "b"], ["1", None], ["2", None]])
 
-        self.assertTrue(tbl.empty_column('b'))
-        self.assertFalse(tbl.empty_column('a'))
+        self.assertTrue(tbl.empty_column("b"))
+        self.assertFalse(tbl.empty_column("a"))
 
     def test_from_columns(self):
 
-        header = ['col1', 'col2']
+        header = ["col1", "col2"]
         col1 = [1, 2, 3]
-        col2 = ['a', 'b', 'c']
+        col2 = ["a", "b", "c"]
         tbl = Table.from_columns([col1, col2], header=header)
 
-        self.assertEqual(tbl[0], {'col1': 1, 'col2': 'a'})
+        self.assertEqual(tbl[0], {"col1": 1, "col2": "a"})
 
     # Removing this test since it is an optional dependency.
     """
     def test_from_datafame(self):
 
         import pandas
 
@@ -126,475 +127,496 @@
     def test_to_petl(self):
 
         # Is a petl table
         self.assertIsInstance(self.tbl.to_petl(), petl.io.json.DictsView)
 
     def test_to_html(self):
 
-        html_file = 'tmp/test.html'
+        html_file = "tmp/test.html"
 
         # Test writing file
         self.tbl.to_html(html_file)
 
         # Test written correctly
-        html = ("<table class='petl'>\n"
-                "<thead>\n"
-                "<tr>\n"
-                "<th>first</th>\n"
-                "<th>last</th>\n"
-                "</tr>\n"
-                "</thead>\n"
-                "<tbody>\n"
-                "<tr>\n"
-                "<td>Bob</td>\n"
-                "<td>Smith</td>\n"
-                "</tr>\n"
-                "</tbody>\n"
-                "</table>\n")
-        with open(html_file, 'r') as f:
+        html = (
+            "<table class='petl'>\n"
+            "<thead>\n"
+            "<tr>\n"
+            "<th>first</th>\n"
+            "<th>last</th>\n"
+            "</tr>\n"
+            "</thead>\n"
+            "<tbody>\n"
+            "<tr>\n"
+            "<td>Bob</td>\n"
+            "<td>Smith</td>\n"
+            "</tr>\n"
+            "</tbody>\n"
+            "</table>\n"
+        )
+        with open(html_file, "r") as f:
             self.assertEqual(f.read(), html)
 
     def test_to_temp_html(self):
 
         # Test write to object
         path = self.tbl.to_html()
 
         # Written correctly
-        html = ("<table class='petl'>\n"
-                "<thead>\n"
-                "<tr>\n"
-                "<th>first</th>\n"
-                "<th>last</th>\n"
-                "</tr>\n"
-                "</thead>\n"
-                "<tbody>\n"
-                "<tr>\n"
-                "<td>Bob</td>\n"
-                "<td>Smith</td>\n"
-                "</tr>\n"
-                "</tbody>\n"
-                "</table>\n")
-        with open(path, 'r') as f:
+        html = (
+            "<table class='petl'>\n"
+            "<thead>\n"
+            "<tr>\n"
+            "<th>first</th>\n"
+            "<th>last</th>\n"
+            "</tr>\n"
+            "</thead>\n"
+            "<tbody>\n"
+            "<tr>\n"
+            "<td>Bob</td>\n"
+            "<td>Smith</td>\n"
+            "</tr>\n"
+            "</tbody>\n"
+            "</table>\n"
+        )
+        with open(path, "r") as f:
             self.assertEqual(f.read(), html)
 
     def _assert_expected_csv(self, path, orig_tbl):
         result_tbl = Table.from_csv(path)
         assert_matching_tables(orig_tbl, result_tbl)
 
     def test_to_from_csv(self):
-        path = 'tmp/test.csv'
+        path = "tmp/test.csv"
         self.tbl.to_csv(path)
         self._assert_expected_csv(path, self.tbl)
         os.remove(path)
 
     def test_to_from_csv_compressed(self):
-        path = 'tmp/test.csv.gz'
+        path = "tmp/test.csv.gz"
         self.tbl.to_csv(path)
         self._assert_expected_csv(path, self.tbl)
         os.remove(path)
 
     def test_to_from_temp_csv(self):
         path = self.tbl.to_csv()
         self._assert_expected_csv(path, self.tbl)
 
     def test_to_from_temp_csv_compressed(self):
-        path = self.tbl.to_csv(temp_file_compression='gzip')
+        path = self.tbl.to_csv(temp_file_compression="gzip")
         self._assert_expected_csv(path, self.tbl)
 
     def test_from_csv_string(self):
         path = self.tbl.to_csv()
         # Pull the file into a string
-        with open(path, 'r') as f:
+        with open(path, "r") as f:
             str = f.read()
 
         result_tbl = Table.from_csv_string(str)
         assert_matching_tables(self.tbl, result_tbl)
 
     def test_append_csv_compressed(self):
-        path = self.tbl.to_csv(temp_file_compression='gzip')
-        append_tbl = Table([{'first': 'Mary', 'last': 'Nichols'}])
+        path = self.tbl.to_csv(temp_file_compression="gzip")
+        append_tbl = Table([{"first": "Mary", "last": "Nichols"}])
         append_tbl.append_csv(path)
 
         result_tbl = Table.from_csv(path)
         # Combine tables, so we can check the resulting file
         self.tbl.concat(append_tbl)
         assert_matching_tables(self.tbl, result_tbl)
 
     def test_from_csv_raises_on_empty_file(self):
         # Create empty file
-        path = 'tmp/empty.csv'
-        open(path, 'a').close()
+        path = "tmp/empty.csv"
+        open(path, "a").close()
 
         self.assertRaises(ValueError, Table.from_csv, path)
 
     def test_to_csv_zip(self):
 
         try:
             # Test using the to_csv() method
-            self.tbl.to_csv('myzip.zip')
-            tmp = zip_archive.unzip_archive('myzip.zip')
+            self.tbl.to_csv("myzip.zip")
+            tmp = zip_archive.unzip_archive("myzip.zip")
             assert_matching_tables(self.tbl, Table.from_csv(tmp))
 
             # Test using the to_csv_zip() method
-            self.tbl.to_zip_csv('myzip.zip')
-            tmp = zip_archive.unzip_archive('myzip.zip')
+            self.tbl.to_zip_csv("myzip.zip")
+            tmp = zip_archive.unzip_archive("myzip.zip")
             assert_matching_tables(self.tbl, Table.from_csv(tmp))
         finally:
-            os.unlink('myzip.zip')
+            os.unlink("myzip.zip")
 
     def test_to_civis(self):
 
         # Not really sure the best way to do this at the moment.
         pass
 
     def test_to_from_json(self):
-        path = 'tmp/test.json'
+        path = "tmp/test.json"
         self.tbl.to_json(path)
 
         result_tbl = Table.from_json(path)
         assert_matching_tables(self.tbl, result_tbl)
         os.remove(path)
 
     def test_to_from_json_compressed(self):
-        path = 'tmp/test.json.gz'
+        path = "tmp/test.json.gz"
         self.tbl.to_json(path)
 
         result_tbl = Table.from_json(path)
         assert_matching_tables(self.tbl, result_tbl)
         os.remove(path)
 
     def test_to_from_temp_json(self):
         path = self.tbl.to_json()
         result_tbl = Table.from_json(path)
         assert_matching_tables(self.tbl, result_tbl)
 
     def test_to_from_temp_json_compressed(self):
-        path = self.tbl.to_json(temp_file_compression='gzip')
+        path = self.tbl.to_json(temp_file_compression="gzip")
         result_tbl = Table.from_json(path)
         assert_matching_tables(self.tbl, result_tbl)
 
     def test_to_from_json_line_delimited(self):
-        path = 'tmp/test.json'
+        path = "tmp/test.json"
         self.tbl.to_json(path, line_delimited=True)
 
         result_tbl = Table.from_json(path, line_delimited=True)
         assert_matching_tables(self.tbl, result_tbl)
         os.remove(path)
 
     def test_to_from_json_line_delimited_compressed(self):
-        path = 'tmp/test.json.gz'
+        path = "tmp/test.json.gz"
         self.tbl.to_json(path, line_delimited=True)
 
         result_tbl = Table.from_json(path, line_delimited=True)
         assert_matching_tables(self.tbl, result_tbl)
         os.remove(path)
 
     def test_columns(self):
         # Test that columns are listed correctly
-        self.assertEqual(self.tbl.columns, ['first', 'last'])
+        self.assertEqual(self.tbl.columns, ["first", "last"])
 
     def test_add_column(self):
         # Test that a new column is added correctly
-        self.tbl.add_column('middle', index=1)
-        self.assertEqual(self.tbl.columns[1], 'middle')
+        self.tbl.add_column("middle", index=1)
+        self.assertEqual(self.tbl.columns[1], "middle")
 
     def test_column_add_dupe(self):
         # Test that we can't add an existing column name
-        self.assertRaises(ValueError, self.tbl.add_column, 'first')
+        self.assertRaises(ValueError, self.tbl.add_column, "first")
 
     def test_remove_column(self):
         # Test that column is removed correctly
-        self.tbl.remove_column('first')
-        self.assertNotEqual(self.tbl.data[0], 'first')
+        self.tbl.remove_column("first")
+        self.assertNotEqual(self.tbl.data[0], "first")
 
     def test_rename_column(self):
         # Test that you can rename a column
-        self.tbl.rename_column('first', 'f')
-        self.assertEqual(self.tbl.columns[0], 'f')
+        self.tbl.rename_column("first", "f")
+        self.assertEqual(self.tbl.columns[0], "f")
 
     def test_column_rename_dupe(self):
         # Test that we can't rename to a column that already exists
-        self.assertRaises(ValueError, self.tbl.rename_column, 'last', 'first')
+        self.assertRaises(ValueError, self.tbl.rename_column, "last", "first")
 
     def test_fill_column(self):
         # Test that the column is filled
         tbl = Table(self.lst)
 
         # Fixed Value
-        tbl.fill_column('c', 0)
-        self.assertEqual(list(tbl.table['c']), [0] * tbl.num_rows)
+        tbl.fill_column("c", 0)
+        self.assertEqual(list(tbl.table["c"]), [0] * tbl.num_rows)
 
         # Calculated Value
-        tbl.fill_column('c', lambda x: x['b'] * 2)
-        self.assertEqual(list(tbl.table['c']), [x['b'] * 2 for x in self.lst])
+        tbl.fill_column("c", lambda x: x["b"] * 2)
+        self.assertEqual(list(tbl.table["c"]), [x["b"] * 2 for x in self.lst])
 
     def test_fillna_column(self):
         # Test that None values in the column are filled
 
-        self.lst = [{'a': 1, 'b': 2, 'c': 3},
-                    {'a': 4, 'b': 5, 'c': None},
-                    {'a': 7, 'b': 8, 'c': 9},
-                    {'a': 10, 'b': 11, 'c': None},
-                    {'a': 13, 'b': 14, 'c': 15}]
+        self.lst = [
+            {"a": 1, "b": 2, "c": 3},
+            {"a": 4, "b": 5, "c": None},
+            {"a": 7, "b": 8, "c": 9},
+            {"a": 10, "b": 11, "c": None},
+            {"a": 13, "b": 14, "c": 15},
+        ]
 
         # Fixed Value only
         tbl = Table(self.lst)
-        tbl.fillna_column('c', 0)
-        self.assertEqual(list(tbl.table['c']), [3, 0, 9, 0, 15])
+        tbl.fillna_column("c", 0)
+        self.assertEqual(list(tbl.table["c"]), [3, 0, 9, 0, 15])
 
     def test_move_column(self):
         # Test moving a column from end to front
-        self.tbl.move_column('last', 0)
-        self.assertEqual(self.tbl.columns[0], 'last')
+        self.tbl.move_column("last", 0)
+        self.assertEqual(self.tbl.columns[0], "last")
 
     def test_convert_column(self):
         # Test that column updates
-        self.tbl.convert_column('first', 'upper')
-        self.assertEqual(self.tbl[0], {'first': 'BOB', 'last': 'Smith'})
+        self.tbl.convert_column("first", "upper")
+        self.assertEqual(self.tbl[0], {"first": "BOB", "last": "Smith"})
 
     def test_convert_columns_to_str(self):
         # Test that all columns are string
         mixed_raw = [
-            {'col1': 1, 'col2': 2, 'col3': 3},
-            {'col1': 'one', 'col2': 2, 'col3': [3, 'three', 3.0]},
-            {'col1': {'one': 1, "two": 2.0}, 'col2': None, "col3": 'three'}
+            {"col1": 1, "col2": 2, "col3": 3},
+            {"col1": "one", "col2": 2, "col3": [3, "three", 3.0]},
+            {"col1": {"one": 1, "two": 2.0}, "col2": None, "col3": "three"},
         ]
         tbl = Table(mixed_raw)
         tbl.convert_columns_to_str()
 
         cols = tbl.get_columns_type_stats()
-        type_set = {i for x in cols for i in x['type']}
-        self.assertTrue('str' in type_set and len(type_set) == 1)
+        type_set = {i for x in cols for i in x["type"]}
+        self.assertTrue("str" in type_set and len(type_set) == 1)
 
     def test_convert_table(self):
         # Test that the table updates
-        self.tbl.convert_table('upper')
-        self.assertEqual(self.tbl[0], {'first': 'BOB', 'last': 'SMITH'})
+        self.tbl.convert_table("upper")
+        self.assertEqual(self.tbl[0], {"first": "BOB", "last": "SMITH"})
 
     def test_coalesce_columns(self):
         # Test coalescing into an existing column
         test_raw = [
-            {'first': 'Bob', 'last': 'Smith', 'lastname': None},
-            {'first': 'Jane', 'last': '', 'lastname': 'Doe'},
-            {'first': 'Mary', 'last': 'Simpson', 'lastname': 'Peters'},
+            {"first": "Bob", "last": "Smith", "lastname": None},
+            {"first": "Jane", "last": "", "lastname": "Doe"},
+            {"first": "Mary", "last": "Simpson", "lastname": "Peters"},
         ]
         tbl = Table(test_raw)
-        tbl.coalesce_columns('last', ['last', 'lastname'])
+        tbl.coalesce_columns("last", ["last", "lastname"])
 
-        expected = Table([
-            {'first': 'Bob', 'last': 'Smith'},
-            {'first': 'Jane', 'last': 'Doe'},
-            {'first': 'Mary', 'last': 'Simpson'},
-        ])
+        expected = Table(
+            [
+                {"first": "Bob", "last": "Smith"},
+                {"first": "Jane", "last": "Doe"},
+                {"first": "Mary", "last": "Simpson"},
+            ]
+        )
         assert_matching_tables(tbl, expected)
 
         # Test coalescing into a new column
         tbl = Table(test_raw)
-        tbl.coalesce_columns('new_last', ['last', 'lastname'])
-        expected = Table([
-            {'first': 'Bob', 'new_last': 'Smith'},
-            {'first': 'Jane', 'new_last': 'Doe'},
-            {'first': 'Mary', 'new_last': 'Simpson'},
-        ])
+        tbl.coalesce_columns("new_last", ["last", "lastname"])
+        expected = Table(
+            [
+                {"first": "Bob", "new_last": "Smith"},
+                {"first": "Jane", "new_last": "Doe"},
+                {"first": "Mary", "new_last": "Simpson"},
+            ]
+        )
         assert_matching_tables(tbl, expected)
 
     def test_unpack_dict(self):
 
-        test_dict = [{'a': 1, 'b': {'nest1': 1, 'nest2': 2}}]
+        test_dict = [{"a": 1, "b": {"nest1": 1, "nest2": 2}}]
         test_table = Table(test_dict)
 
         # Test that dict at the top level
-        test_table.unpack_dict('b', prepend=False)
-        self.assertEqual(test_table.columns, ['a', 'nest1', 'nest2'])
+        test_table.unpack_dict("b", prepend=False)
+        self.assertEqual(test_table.columns, ["a", "nest1", "nest2"])
 
     def test_unpack_list(self):
 
-        test_table = Table([{'a': 1, 'b': [1, 2, 3]}])
+        test_table = Table([{"a": 1, "b": [1, 2, 3]}])
 
         # Test that list at the top level
-        test_table.unpack_list('b', replace=True)
-        self.assertEqual(['a', 'b_0', 'b_1', 'b_2'], test_table.columns)
+        test_table.unpack_list("b", replace=True)
+        self.assertEqual(["a", "b_0", "b_1", "b_2"], test_table.columns)
 
     def test_unpack_list_with_mixed_col(self):
 
         # Test unpacking column with non-list items
-        mixed_tbl = Table([{'id': 1, 'tag': [1, 2, None, 4]}, {'id': 2, 'tag': None}])
-        tbl_unpacked = Table(mixed_tbl.unpack_list('tag'))
+        mixed_tbl = Table([{"id": 1, "tag": [1, 2, None, 4]}, {"id": 2, "tag": None}])
+        tbl_unpacked = Table(mixed_tbl.unpack_list("tag"))
 
         # Make sure result has the right number of columns
         self.assertEqual(len(tbl_unpacked.columns), 5)
 
-        result_table = Table([
-            {'id': 1, 'tag_0': 1, 'tag_1': 2, 'tag_2': None, 'tag_3': 4},
-            {'id': 2, 'tag_0': None, 'tag_1': None, 'tag_2': None, 'tag_3': None}])
+        result_table = Table(
+            [
+                {"id": 1, "tag_0": 1, "tag_1": 2, "tag_2": None, "tag_3": 4},
+                {"id": 2, "tag_0": None, "tag_1": None, "tag_2": None, "tag_3": None},
+            ]
+        )
 
         # Check that the values for both rows are distributed correctly
         self.assertEqual(
             result_table.data[0] + result_table.data[1],
-            tbl_unpacked.data[0] + tbl_unpacked.data[1])
+            tbl_unpacked.data[0] + tbl_unpacked.data[1],
+        )
 
     def test_unpack_nested_columns_as_rows(self):
 
         # A Table with mixed content
-        test_table = Table([
-                {'id': 1, 'nested': {'A': 1, 'B': 2, 'C': 3}, 'extra': 'hi'},
-                {'id': 2, 'nested': {'A': 4, 'B': 5, 'I': 6}, 'extra': 'hi'},
-                {'id': 3, 'nested': 'string!', 'extra': 'hi'},
-                {'id': 4, 'nested': None, 'extra': 'hi'},
-                {'id': 5, 'nested': ['this!', 'is!', 'a!', 'list!'], 'extra': 'hi'}
-            ])
+        test_table = Table(
+            [
+                {"id": 1, "nested": {"A": 1, "B": 2, "C": 3}, "extra": "hi"},
+                {"id": 2, "nested": {"A": 4, "B": 5, "I": 6}, "extra": "hi"},
+                {"id": 3, "nested": "string!", "extra": "hi"},
+                {"id": 4, "nested": None, "extra": "hi"},
+                {"id": 5, "nested": ["this!", "is!", "a!", "list!"], "extra": "hi"},
+            ]
+        )
 
-        standalone = test_table.unpack_nested_columns_as_rows('nested')
+        standalone = test_table.unpack_nested_columns_as_rows("nested")
 
         # Check that the columns are as expected
-        self.assertEqual(['uid', 'id', 'nested', 'value'], standalone.columns)
+        self.assertEqual(["uid", "id", "nested", "value"], standalone.columns)
 
         # Check that the row count is as expected
         self.assertEqual(standalone.num_rows, 11)
 
         # Check that the uids are unique, indicating that each row is unique
-        self.assertEqual(len({row['uid'] for row in standalone}), 11)
+        self.assertEqual(len({row["uid"] for row in standalone}), 11)
 
     def test_unpack_nested_columns_as_rows_expanded(self):
 
-        test_table = Table([
-                {'id': 1, 'nested': {'A': 1, 'B': 2, 'C': 3}, 'extra': 'hi'},
-                {'id': 2, 'nested': {'A': 4, 'B': 5, 'I': 6}, 'extra': 'hi'},
-                {'id': 3, 'nested': 'string!', 'extra': 'hi'},
-                {'id': 4, 'nested': None, 'extra': 'hi'},
-                {'id': 5, 'nested': ['this!', 'is!', 'a!', 'list!'], 'extra': 'hi'}
-            ])
-
-        expanded = test_table.unpack_nested_columns_as_rows('nested', expand_original=True)
+        test_table = Table(
+            [
+                {"id": 1, "nested": {"A": 1, "B": 2, "C": 3}, "extra": "hi"},
+                {"id": 2, "nested": {"A": 4, "B": 5, "I": 6}, "extra": "hi"},
+                {"id": 3, "nested": "string!", "extra": "hi"},
+                {"id": 4, "nested": None, "extra": "hi"},
+                {"id": 5, "nested": ["this!", "is!", "a!", "list!"], "extra": "hi"},
+            ]
+        )
+
+        expanded = test_table.unpack_nested_columns_as_rows(
+            "nested", expand_original=True
+        )
 
         # Check that the columns are as expected
-        self.assertEqual(['uid', 'id', 'extra', 'nested', 'nested_value'], expanded.columns)
+        self.assertEqual(
+            ["uid", "id", "extra", "nested", "nested_value"], expanded.columns
+        )
 
         # Check that the row count is as expected
         self.assertEqual(expanded.num_rows, 12)
 
         # Check that the uids are unique, indicating that each row is unique
-        self.assertEqual(len({row['uid'] for row in expanded}), 12)
+        self.assertEqual(len({row["uid"] for row in expanded}), 12)
 
     def test_cut(self):
 
         # Test that the cut works correctly
-        cut_tbl = self.tbl.cut('first')
-        self.assertEqual(cut_tbl.columns, ['first'])
+        cut_tbl = self.tbl.cut("first")
+        self.assertEqual(cut_tbl.columns, ["first"])
 
     def test_row_select(self):
 
-        tbl = Table([['foo', 'bar', 'baz'],
-                     ['c', 4, 9.3],
-                     ['a', 2, 88.2],
-                     ['b', 1, 23.3]])
-        expected = Table([{'foo': 'a', 'bar': 2, 'baz': 88.2}])
+        tbl = Table(
+            [["foo", "bar", "baz"], ["c", 4, 9.3], ["a", 2, 88.2], ["b", 1, 23.3]]
+        )
+        expected = Table([{"foo": "a", "bar": 2, "baz": 88.2}])
 
         # Try with this method
         select_tbl = tbl.select_rows("{foo} == 'a' and {baz} > 88.1")
         self.assertEqual(select_tbl.data[0], expected.data[0])
 
         # And try with this method
-        select_tbl2 = tbl.select_rows(lambda row: row.foo == 'a' and row.baz > 88.1)
+        select_tbl2 = tbl.select_rows(lambda row: row.foo == "a" and row.baz > 88.1)
         self.assertEqual(select_tbl2.data[0], expected.data[0])
 
     def test_remove_null_rows(self):
 
         # Test that null rows are removed from a single column
-        null_table = Table([{'a': 1, 'b': 2}, {'a': 1, 'b': None}])
-        self.assertEqual(null_table.remove_null_rows('b').num_rows, 1)
+        null_table = Table([{"a": 1, "b": 2}, {"a": 1, "b": None}])
+        self.assertEqual(null_table.remove_null_rows("b").num_rows, 1)
 
         # Teest that null rows are removed from multiple columns
-        null_table = Table([{'a': 1, 'b': 2, 'c': 3}, {'a': 1, 'b': None, 'c': 3}])
-        self.assertEqual(null_table.remove_null_rows(['b', 'c']).num_rows, 1)
+        null_table = Table([{"a": 1, "b": 2, "c": 3}, {"a": 1, "b": None, "c": 3}])
+        self.assertEqual(null_table.remove_null_rows(["b", "c"]).num_rows, 1)
 
     def test_long_table(self):
 
         # Create a long table, that is 4 rows long
-        tbl = Table([{'id': 1, 'tag': [1, 2, 3, 4]}])
-        self.assertEqual(tbl.long_table(['id'], 'tag').num_rows, 4)
+        tbl = Table([{"id": 1, "tag": [1, 2, 3, 4]}])
+        self.assertEqual(tbl.long_table(["id"], "tag").num_rows, 4)
 
         # Assert that column has been dropped
-        self.assertEqual(tbl.columns, ['id'])
+        self.assertEqual(tbl.columns, ["id"])
 
         # Assert that column has been retained
-        tbl_keep = Table([{'id': 1, 'tag': [1, 2, 3, 4]}])
-        tbl_keep.long_table(['id'], 'tag', retain_original=True)
-        self.assertEqual(tbl_keep.columns, ['id', 'tag'])
+        tbl_keep = Table([{"id": 1, "tag": [1, 2, 3, 4]}])
+        tbl_keep.long_table(["id"], "tag", retain_original=True)
+        self.assertEqual(tbl_keep.columns, ["id", "tag"])
 
     def test_long_table_with_na(self):
 
         # Create a long table that is 4 rows long
-        tbl = Table([{'id': 1, 'tag': [1, 2, 3, 4]}, {'id': 2, 'tag': None}])
-        self.assertEqual(tbl.long_table(['id'], 'tag').num_rows, 4)
+        tbl = Table([{"id": 1, "tag": [1, 2, 3, 4]}, {"id": 2, "tag": None}])
+        self.assertEqual(tbl.long_table(["id"], "tag").num_rows, 4)
 
         # Assert that column has been dropped
-        self.assertEqual(tbl.columns, ['id'])
+        self.assertEqual(tbl.columns, ["id"])
 
         # Assert that column has been retained
-        tbl_keep = Table([{'id': 1, 'tag': [1, 2, 3, 4]}, {'id': 2, 'tag': None}])
-        tbl_keep.long_table(['id'], 'tag', retain_original=True)
-        self.assertEqual(tbl_keep.columns, ['id', 'tag'])
+        tbl_keep = Table([{"id": 1, "tag": [1, 2, 3, 4]}, {"id": 2, "tag": None}])
+        tbl_keep.long_table(["id"], "tag", retain_original=True)
+        self.assertEqual(tbl_keep.columns, ["id", "tag"])
 
     def test_rows(self):
         # Test that there is only one row in the table
         self.assertEqual(self.tbl.num_rows, 1)
 
     def test_first(self):
         # Test that the first value in the table is returned.
-        self.assertEqual(self.tbl.first, 'Bob')
+        self.assertEqual(self.tbl.first, "Bob")
 
         # Test empty value returns None
         empty_tbl = Table([[1], [], [3]])
         self.assertIsNone(empty_tbl.first)
 
     def test_get_item(self):
         # Test indexing on table
 
         # Test a valid column
         tbl = Table(self.lst)
         lst = [1, 4, 7, 10, 13]
-        self.assertEqual(tbl['a'], lst)
+        self.assertEqual(tbl["a"], lst)
 
         # Test a valid row
-        row = {'a': 4, 'b': 5, 'c': 6}
+        row = {"a": 4, "b": 5, "c": 6}
         self.assertEqual(tbl[1], row)
 
     def test_column_data(self):
         # Test that that the data in the column is returned as a list
 
         # Test a valid column
         tbl = Table(self.lst)
         lst = [1, 4, 7, 10, 13]
-        self.assertEqual(tbl.column_data('a'), lst)
+        self.assertEqual(tbl.column_data("a"), lst)
 
         # Test an invalid column
-        self.assertRaises(TypeError, tbl['c'])
+        self.assertRaises(TypeError, tbl["c"])
 
     def test_row_data(self):
 
         # Test a valid column
         tbl = Table(self.lst)
-        row = {'a': 4, 'b': 5, 'c': 6}
+        row = {"a": 4, "b": 5, "c": 6}
         self.assertEqual(tbl.row_data(1), row)
 
     def test_stack(self):
         tbl1 = self.tbl.select_rows(lambda x: x)
-        tbl2 = Table([{'first': 'Mary', 'last': 'Nichols'}])
+        tbl2 = Table([{"first": "Mary", "last": "Nichols"}])
         # Different column names shouldn't matter for stack()
-        tbl3 = Table([{'f': 'Lucy', 'l': 'Peterson'}])
+        tbl3 = Table([{"f": "Lucy", "l": "Peterson"}])
         tbl1.stack(tbl2, tbl3)
 
         expected_tbl = Table(petl.stack(self.tbl.table, tbl2.table, tbl3.table))
         assert_matching_tables(expected_tbl, tbl1)
 
     def test_concat(self):
         tbl1 = self.tbl.select_rows(lambda x: x)
-        tbl2 = Table([{'first': 'Mary', 'last': 'Nichols'}])
-        tbl3 = Table([{'first': 'Lucy', 'last': 'Peterson'}])
+        tbl2 = Table([{"first": "Mary", "last": "Nichols"}])
+        tbl3 = Table([{"first": "Lucy", "last": "Peterson"}])
         tbl1.concat(tbl2, tbl3)
 
         expected_tbl = Table(petl.cat(self.tbl.table, tbl2.table, tbl3.table))
         assert_matching_tables(expected_tbl, tbl1)
 
     def test_chunk(self):
 
@@ -606,223 +628,271 @@
             self.assertEqual(100, c.num_rows)
 
         # Assert last table is 99
         self.assertEqual(99, chunks[4].num_rows)
 
     def test_match_columns(self):
         raw = [
-            {'first name': 'Mary', 'LASTNAME': 'Nichols', 'Middle__Name': 'D'},
-            {'first name': 'Lucy', 'LASTNAME': 'Peterson', 'Middle__Name': 'S'},
+            {"first name": "Mary", "LASTNAME": "Nichols", "Middle__Name": "D"},
+            {"first name": "Lucy", "LASTNAME": "Peterson", "Middle__Name": "S"},
         ]
         tbl = Table(raw)
 
         desired_raw = [
-            {'first_name': 'Mary', 'middle_name': 'D', 'last_name': 'Nichols'},
-            {'first_name': 'Lucy', 'middle_name': 'S', 'last_name': 'Peterson'},
+            {"first_name": "Mary", "middle_name": "D", "last_name": "Nichols"},
+            {"first_name": "Lucy", "middle_name": "S", "last_name": "Peterson"},
         ]
         desired_tbl = Table(desired_raw)
 
         # Test with fuzzy matching
         tbl.match_columns(desired_tbl.columns)
         assert_matching_tables(desired_tbl, tbl)
 
         # Test disable fuzzy matching, and fail due due to the missing cols
         self.assertRaises(
             TypeError,
             Table(raw).match_columns,
             desired_tbl.columns,
             fuzzy_match=False,
-            if_missing_columns='fail')
+            if_missing_columns="fail",
+        )
 
         # Test disable fuzzy matching, and fail due to the extra cols
         self.assertRaises(
             TypeError,
             Table(raw).match_columns,
             desired_tbl.columns,
             fuzzy_match=False,
-            if_extra_columns='fail')
+            if_extra_columns="fail",
+        )
 
         # Test table that already has the right columns, shouldn't need fuzzy match
         tbl = Table(desired_raw)
         tbl.match_columns(
             desired_tbl.columns,
             fuzzy_match=False,
-            if_missing_columns='fail',
-            if_extra_columns='fail')
+            if_missing_columns="fail",
+            if_extra_columns="fail",
+        )
         assert_matching_tables(desired_tbl, tbl)
 
         # Test table with missing col, verify the missing col gets added by default
-        tbl = Table([
-            {'first name': 'Mary', 'LASTNAME': 'Nichols'},
-            {'first name': 'Lucy', 'LASTNAME': 'Peterson'},
-        ])
+        tbl = Table(
+            [
+                {"first name": "Mary", "LASTNAME": "Nichols"},
+                {"first name": "Lucy", "LASTNAME": "Peterson"},
+            ]
+        )
         tbl.match_columns(desired_tbl.columns)
         desired_tbl = (
-            Table(desired_raw).remove_column('middle_name').add_column('middle_name', index=1))
+            Table(desired_raw)
+            .remove_column("middle_name")
+            .add_column("middle_name", index=1)
+        )
         assert_matching_tables(desired_tbl, tbl)
 
         # Test table with extra col, verify the extra col gets removed by default
-        tbl = Table([
-            {'first name': 'Mary', 'LASTNAME': 'Nichols', 'Age': 32, 'Middle__Name': 'D'},
-            {'first name': 'Lucy', 'LASTNAME': 'Peterson', 'Age': 26, 'Middle__Name': 'S'},
-        ])
+        tbl = Table(
+            [
+                {
+                    "first name": "Mary",
+                    "LASTNAME": "Nichols",
+                    "Age": 32,
+                    "Middle__Name": "D",
+                },
+                {
+                    "first name": "Lucy",
+                    "LASTNAME": "Peterson",
+                    "Age": 26,
+                    "Middle__Name": "S",
+                },
+            ]
+        )
         desired_tbl = Table(desired_raw)
         tbl.match_columns(desired_tbl.columns)
         assert_matching_tables(desired_tbl, tbl)
 
         # Test table with two columns that normalize the same and aren't in desired cols, verify
         # they both get removed.
-        tbl = Table([
-            {
-                'first name': 'Mary', 'LASTNAME': 'Nichols',
-                'Age': 32, 'Middle__Name': 'D', 'AGE': None
-            },
-            {
-                'first name': 'Lucy', 'LASTNAME': 'Peterson',
-                'Age': 26, 'Middle__Name': 'S', 'AGE': None
-            },
-        ])
+        tbl = Table(
+            [
+                {
+                    "first name": "Mary",
+                    "LASTNAME": "Nichols",
+                    "Age": 32,
+                    "Middle__Name": "D",
+                    "AGE": None,
+                },
+                {
+                    "first name": "Lucy",
+                    "LASTNAME": "Peterson",
+                    "Age": 26,
+                    "Middle__Name": "S",
+                    "AGE": None,
+                },
+            ]
+        )
         tbl.match_columns(desired_tbl.columns)
         assert_matching_tables(desired_tbl, tbl)
 
         # Test table with two columns that match desired cols, verify only the first gets kept.
-        tbl = Table([
-            {'first name': 'Mary', 'LASTNAME': 'Nichols', 'First Name': None, 'Middle__Name': 'D'},
-            {'first name': 'Lucy', 'LASTNAME': 'Peterson', 'First Name': None, 'Middle__Name': 'S'},
-        ])
+        tbl = Table(
+            [
+                {
+                    "first name": "Mary",
+                    "LASTNAME": "Nichols",
+                    "First Name": None,
+                    "Middle__Name": "D",
+                },
+                {
+                    "first name": "Lucy",
+                    "LASTNAME": "Peterson",
+                    "First Name": None,
+                    "Middle__Name": "S",
+                },
+            ]
+        )
         tbl.match_columns(desired_tbl.columns)
         assert_matching_tables(desired_tbl, tbl)
 
     def test_to_dicts(self):
         self.assertEqual(self.lst, Table(self.lst).to_dicts())
         self.assertEqual(self.lst_dicts, self.tbl.to_dicts())
 
     def test_reduce_rows(self):
         table = [
-            ['foo', 'bar'],
-            ['a', 3],
-            ['a', 7],
-            ['b', 2],
-            ['b', 1],
-            ['b', 9],
-            ['c', 4]]
+            ["foo", "bar"],
+            ["a", 3],
+            ["a", 7],
+            ["b", 2],
+            ["b", 1],
+            ["b", 9],
+            ["c", 4],
+        ]
         expected = [
             {"foo": "a", "barsum": 10},
             {"foo": "b", "barsum": 12},
-            {"foo": "c", "barsum": 4}]
+            {"foo": "c", "barsum": 4},
+        ]
 
         ptable = Table(table)
 
         ptable.reduce_rows(
-            'foo',
+            "foo",
             lambda key, rows: [key, sum(row[1] for row in rows)],
-            ['foo', 'barsum'])
+            ["foo", "barsum"],
+        )
 
         self.assertEqual(expected, ptable.to_dicts())
 
     def test_map_columns_exact(self):
 
-        input_tbl = Table([['fn', 'ln', 'MID'], ['J', 'B', 'H']])
+        input_tbl = Table([["fn", "ln", "MID"], ["J", "B", "H"]])
 
-        column_map = {'first_name': ['fn', 'first'],
-                      'last_name': ['last', 'ln'],
-                      'middle_name': ['mi']}
+        column_map = {
+            "first_name": ["fn", "first"],
+            "last_name": ["last", "ln"],
+            "middle_name": ["mi"],
+        }
 
-        exact_tbl = Table([['first_name', 'last_name', 'MID'],
-                           ['J', 'B', 'H']])
+        exact_tbl = Table([["first_name", "last_name", "MID"], ["J", "B", "H"]])
         input_tbl.map_columns(column_map)
         assert_matching_tables(input_tbl, exact_tbl)
 
     def test_map_columns_fuzzy(self):
 
-        input_tbl = Table([['fn', 'ln', 'Mi_'], ['J', 'B', 'H']])
+        input_tbl = Table([["fn", "ln", "Mi_"], ["J", "B", "H"]])
 
-        column_map = {'first_name': ['fn', 'first'],
-                      'last_name': ['last', 'ln'],
-                      'middle_name': ['mi']}
+        column_map = {
+            "first_name": ["fn", "first"],
+            "last_name": ["last", "ln"],
+            "middle_name": ["mi"],
+        }
 
-        fuzzy_tbl = Table([['first_name', 'last_name', 'middle_name'],
-                           ['J', 'B', 'H']])
+        fuzzy_tbl = Table([["first_name", "last_name", "middle_name"], ["J", "B", "H"]])
         input_tbl.map_columns(column_map, exact_match=False)
         assert_matching_tables(input_tbl, fuzzy_tbl)
 
     def test_get_column_max_with(self):
 
-        tbl = Table([
-            ['a', 'b', 'c'],
-            ['wide_text', False, 'slightly longer text'],
-            ['text', 2, 'byte_text']
-        ])
+        tbl = Table(
+            [
+                ["a", "b", "c"],
+                ["wide_text", False, "slightly longer text"],
+                ["text", 2, "byte_text"],
+            ]
+        )
 
         # Basic test
-        self.assertEqual(tbl.get_column_max_width('a'), 9)
+        self.assertEqual(tbl.get_column_max_width("a"), 9)
 
         # Doesn't break for non-strings
-        self.assertEqual(tbl.get_column_max_width('b'), 5)
+        self.assertEqual(tbl.get_column_max_width("b"), 5)
 
         # Evaluates based on byte length rather than char length
-        self.assertEqual(tbl.get_column_max_width('c'), 33)
+        self.assertEqual(tbl.get_column_max_width("c"), 33)
 
     def test_sort(self):
 
         # Test basic sort
-        unsorted_tbl = Table([['a', 'b'], [3, 1], [2, 2], [1, 3]])
+        unsorted_tbl = Table([["a", "b"], [3, 1], [2, 2], [1, 3]])
         sorted_tbl = unsorted_tbl.sort()
-        self.assertEqual(sorted_tbl[0], {'a': 1, 'b': 3})
+        self.assertEqual(sorted_tbl[0], {"a": 1, "b": 3})
 
         # Test column sort
-        unsorted_tbl = Table([['a', 'b'], [3, 1], [2, 2], [1, 3]])
-        sorted_tbl = unsorted_tbl.sort('b')
-        self.assertEqual(sorted_tbl[0], {'a': 3, 'b': 1})
+        unsorted_tbl = Table([["a", "b"], [3, 1], [2, 2], [1, 3]])
+        sorted_tbl = unsorted_tbl.sort("b")
+        self.assertEqual(sorted_tbl[0], {"a": 3, "b": 1})
 
         # Test reverse sort
-        unsorted_tbl = Table([['a', 'b'], [3, 1], [2, 2], [1, 3]])
+        unsorted_tbl = Table([["a", "b"], [3, 1], [2, 2], [1, 3]])
         sorted_tbl = unsorted_tbl.sort(reverse=True)
-        self.assertEqual(sorted_tbl[0], {'a': 3, 'b': 1})
+        self.assertEqual(sorted_tbl[0], {"a": 3, "b": 1})
 
     def test_set_header(self):
 
         # Rename columns
-        tbl = Table([['one', 'two'], [1, 2], [3, 4]])
-        new_tbl = tbl.set_header(['oneone', 'twotwo'])
+        tbl = Table([["one", "two"], [1, 2], [3, 4]])
+        new_tbl = tbl.set_header(["oneone", "twotwo"])
 
-        self.assertEqual(new_tbl[0], {'oneone': 1, 'twotwo': 2})
+        self.assertEqual(new_tbl[0], {"oneone": 1, "twotwo": 2})
 
         # Change number of columns
-        tbl = Table([['one', 'two'], [1, 2], [3, 4]])
-        new_tbl = tbl.set_header(['one'])
+        tbl = Table([["one", "two"], [1, 2], [3, 4]])
+        new_tbl = tbl.set_header(["one"])
 
-        self.assertEqual(new_tbl[0], {'one': 1})
+        self.assertEqual(new_tbl[0], {"one": 1})
 
     def test_bool(self):
         empty = Table()
-        not_empty = Table([{'one': 1, 'two': 2}])
+        not_empty = Table([{"one": 1, "two": 2}])
 
         self.assertEqual(not empty, True)
         self.assertEqual(not not_empty, False)
 
     def test_use_petl(self):
         # confirm that this method doesn't exist for parsons.Table
-        self.assertRaises(AttributeError, getattr, Table, 'skipcomments')
+        self.assertRaises(AttributeError, getattr, Table, "skipcomments")
 
-        tbl = Table([
-            ['col1', 'col2'],
-            ['# this is a comment row', ],
-            ['a', 1],
-            ['#this is another comment', 'this is also ignored'],
-            ['b', 2]
-        ])
-        tbl_expected = Table([
-            ['col1', 'col2'],
-            ['a', 1],
-            ['b', 2]
-        ])
+        tbl = Table(
+            [
+                ["col1", "col2"],
+                [
+                    "# this is a comment row",
+                ],
+                ["a", 1],
+                ["#this is another comment", "this is also ignored"],
+                ["b", 2],
+            ]
+        )
+        tbl_expected = Table([["col1", "col2"], ["a", 1], ["b", 2]])
 
-        tbl_after = tbl.use_petl('skipcomments', '#')
+        tbl_after = tbl.use_petl("skipcomments", "#")
         assert_matching_tables(tbl_expected, tbl_after)
 
-        tbl.use_petl('skipcomments', '#', update_table=True)
+        tbl.use_petl("skipcomments", "#", update_table=True)
         assert_matching_tables(tbl_expected, tbl)
 
         from petl.util.base import Table as PetlTable
-        tbl_petl = tbl.use_petl('skipcomments', '#', to_petl=True)
+
+        tbl_petl = tbl.use_petl("skipcomments", "#", to_petl=True)
         self.assertIsInstance(tbl_petl, PetlTable)
```

### Comparing `parsons-1.0.0/test/test_facebook_ads.py` & `parsons-1.1.0/test/test_facebook_ads.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,83 +1,111 @@
 import unittest
 import os
 
 from parsons import FacebookAds, Table
 
 
-users_table = Table([
-    {"first": "Bob", "middle": "J", "last": "Smith", "phone": "1234567890", "cell": None,
-     "vb_voterbase_dob": "19820413"},
-    {"first": "Sue", "middle": "Lucy", "last": "Doe", "phone": None, "cell": "2345678901",
-     "vb_voterbase_dob": None},
-])
-
-
-@unittest.skipIf(not os.environ.get('LIVE_TEST'), 'Skipping because not running live test')
+users_table = Table(
+    [
+        {
+            "first": "Bob",
+            "middle": "J",
+            "last": "Smith",
+            "phone": "1234567890",
+            "cell": None,
+            "vb_voterbase_dob": "19820413",
+        },
+        {
+            "first": "Sue",
+            "middle": "Lucy",
+            "last": "Doe",
+            "phone": None,
+            "cell": "2345678901",
+            "vb_voterbase_dob": None,
+        },
+    ]
+)
+
+
+@unittest.skipIf(
+    not os.environ.get("LIVE_TEST"), "Skipping because not running live test"
+)
 class TestFacebookAdsIntegration(unittest.TestCase):
-
     def setUp(self):
 
         self.fb_ads = FacebookAds()
 
         self.audience_id = self.fb_ads.create_custom_audience(
-            name="Test Audience",
-            data_source="USER_PROVIDED_ONLY"
-            )
+            name="Test Audience", data_source="USER_PROVIDED_ONLY"
+        )
 
     def tearDown(self):
         self.fb_ads.delete_custom_audience(self.audience_id)
 
     def test_create_custom_audience(self):
         # Audience created in setUp
         self.assertIsNotNone(self.audience_id)
 
     def test_create_custom_audience_bad_data_source(self):
         self.assertRaises(
             KeyError,
-            self.fb_ads.create_custom_audience, name="Something", data_source="INVALID"
-            )
+            self.fb_ads.create_custom_audience,
+            name="Something",
+            data_source="INVALID",
+        )
 
     def test_add_users_to_custom_audience(self):
         # Note we don't actually check the results of adding these users, eg. how many were
         # matched by FB. This test just ensures the method doesn't error.
         # TODO See if we can do a more effective test.
         self.fb_ads.add_users_to_custom_audience(self.audience_id, users_table)
 
     def test_add_users_to_custom_audience_no_valid_columns(self):
         # We don't yet support full names for matching, so this shouldn't work
-        tbl = Table([
-            {"full name": "Bob Smith"},
-            ])
+        tbl = Table(
+            [
+                {"full name": "Bob Smith"},
+            ]
+        )
         self.assertRaises(
-            KeyError,
-            self.fb_ads.add_users_to_custom_audience, self.audience_id, tbl
-            )
+            KeyError, self.fb_ads.add_users_to_custom_audience, self.audience_id, tbl
+        )
 
 
 class TestFacebookAdsUtilities(unittest.TestCase):
-
     def test_get_match_key_for_column(self):
         # Test just a few of the mappings
-        self.assertEqual('EMAIL', FacebookAds._get_match_key_for_column('email'))
-        self.assertEqual('EMAIL', FacebookAds._get_match_key_for_column('voterbase_email'))
-        self.assertEqual('FN', FacebookAds._get_match_key_for_column('first name'))
-        self.assertEqual('FN', FacebookAds._get_match_key_for_column('FIRST-NAME '))
-        self.assertEqual('FN', FacebookAds._get_match_key_for_column('vb_tsmart_first_name'))
-        self.assertEqual('LN', FacebookAds._get_match_key_for_column('Last Name!'))
-        self.assertEqual('ST', FacebookAds._get_match_key_for_column('state code'))
-        self.assertEqual('ST', FacebookAds._get_match_key_for_column('vb_vf_source_state'))
-        self.assertEqual('GEN', FacebookAds._get_match_key_for_column('vb_voterbase_gender'))
+        self.assertEqual("EMAIL", FacebookAds._get_match_key_for_column("email"))
+        self.assertEqual(
+            "EMAIL", FacebookAds._get_match_key_for_column("voterbase_email")
+        )
+        self.assertEqual("FN", FacebookAds._get_match_key_for_column("first name"))
+        self.assertEqual("FN", FacebookAds._get_match_key_for_column("FIRST-NAME "))
+        self.assertEqual(
+            "FN", FacebookAds._get_match_key_for_column("vb_tsmart_first_name")
+        )
+        self.assertEqual("LN", FacebookAds._get_match_key_for_column("Last Name!"))
+        self.assertEqual("ST", FacebookAds._get_match_key_for_column("state code"))
+        self.assertEqual(
+            "ST", FacebookAds._get_match_key_for_column("vb_vf_source_state")
+        )
+        self.assertEqual(
+            "GEN", FacebookAds._get_match_key_for_column("vb_voterbase_gender")
+        )
         self.assertEqual(
-            'PHONE', FacebookAds._get_match_key_for_column('vb_voterbase_phone_wireless'))
-        self.assertIsNone(FacebookAds._get_match_key_for_column('invalid'))
+            "PHONE",
+            FacebookAds._get_match_key_for_column("vb_voterbase_phone_wireless"),
+        )
+        self.assertIsNone(FacebookAds._get_match_key_for_column("invalid"))
 
     def test_get_preprocess_key_for_column(self):
         self.assertEqual(
-            'DOB YYYYMMDD', FacebookAds._get_preprocess_key_for_column('vb_voterbase_dob'))
+            "DOB YYYYMMDD",
+            FacebookAds._get_preprocess_key_for_column("vb_voterbase_dob"),
+        )
 
     def test_get_match_table_for_users_table(self):
         # This tests basic column matching, as well as the more complex cases like:
         # - Where multiple columns map to the same FB key ("PHONE").
         # - Parsing of a YYYYMMDD birth date column.
         t = FacebookAds.get_match_table_for_users_table(users_table)
 
@@ -95,15 +123,17 @@
         self.assertEqual("Doe", row1["LN"])
         self.assertEqual("2345678901", row1["PHONE"])
         self.assertEqual("", row1["DOBY"])
         self.assertEqual("", row1["DOBM"])
         self.assertEqual("", row1["DOBD"])
 
     def test_get_match_schema_and_data(self):
-        match_table = Table([
-            {"FN": "Bob", "LN": "Smith"},
-            {"FN": "Sue", "LN": "Doe"},
-        ])
+        match_table = Table(
+            [
+                {"FN": "Bob", "LN": "Smith"},
+                {"FN": "Sue", "LN": "Doe"},
+            ]
+        )
         (schema, data) = FacebookAds._get_match_schema_and_data(match_table)
         self.assertEqual(["FN", "LN"], schema)
         self.assertEqual(("Bob", "Smith"), data[0])
         self.assertEqual(("Sue", "Doe"), data[1])
```

### Comparing `parsons-1.0.0/test/test_freshdesk/test_freshdesk.py` & `parsons-1.1.0/test/test_freshdesk/test_freshdesk.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,42 +1,41 @@
 from parsons import Freshdesk
 import unittest
 import requests_mock
 from test.test_freshdesk import expected_json
 
-DOMAIN = 'myorg'
-API_KEY = 'mykey'
+DOMAIN = "myorg"
+API_KEY = "mykey"
 
 
 class TestFreshdesk(unittest.TestCase):
-
     def setUp(self):
 
         self.fd = Freshdesk(DOMAIN, API_KEY)
 
     @requests_mock.Mocker()
     def test_get_agents(self, m):
 
         # Test that agents are returned correctly.
-        m.get(self.fd.uri + 'agents', json=expected_json.test_agent)
+        m.get(self.fd.uri + "agents", json=expected_json.test_agent)
         self.fd.get_agents()
 
     @requests_mock.Mocker()
     def test_get_tickets(self, m):
 
         # Test that tickets are returned correctly.
-        m.get(self.fd.uri + 'tickets', json=expected_json.test_ticket)
+        m.get(self.fd.uri + "tickets", json=expected_json.test_ticket)
         self.fd.get_tickets()
 
     @requests_mock.Mocker()
     def test_get_companies(self, m):
 
         # Test that tickets are returned correctly.
-        m.get(self.fd.uri + 'companies', json=expected_json.test_company)
+        m.get(self.fd.uri + "companies", json=expected_json.test_company)
         self.fd.get_companies()
 
     @requests_mock.Mocker()
     def test_get_contacts(self, m):
 
         # Test that tickets are returned correctly.
-        m.get(self.fd.uri + 'contacts', json=expected_json.test_contact)
+        m.get(self.fd.uri + "contacts", json=expected_json.test_contact)
         self.fd.get_contacts()
```

### Comparing `parsons-1.0.0/test/test_github/test_github.py` & `parsons-1.1.0/test/test_github/test_github.py`

 * *Files 17% similar despite different names*

```diff
@@ -8,53 +8,59 @@
 from parsons.github.github import ParsonsGitHubError
 
 
 _dir = os.path.dirname(__file__)
 
 
 class TestGitHub(unittest.TestCase):
-
     def setUp(self):
-        self.github = GitHub(access_token='token')
+        self.github = GitHub(access_token="token")
 
     @requests_mock.Mocker()
     def test_wrap_github_404(self, m):
         with patch("github.Github.get_repo") as get_repo_mock:
             get_repo_mock.side_effect = UnknownObjectException("", "")
             with self.assertRaises(ParsonsGitHubError):
                 self.github.get_repo("octocat/Hello-World")
 
     @requests_mock.Mocker()
     def test_get_repo(self, m):
-        with open(os.path.join(_dir, 'test_data', 'test_get_repo.json'), 'r') as f:
+        with open(os.path.join(_dir, "test_data", "test_get_repo.json"), "r") as f:
             m.get(requests_mock.ANY, text=f.read())
-        repo = self.github.get_repo('octocat/Hello-World')
-        self.assertEqual(repo['id'], 1296269)
-        self.assertEqual(repo['name'], 'Hello-World')
+        repo = self.github.get_repo("octocat/Hello-World")
+        self.assertEqual(repo["id"], 1296269)
+        self.assertEqual(repo["name"], "Hello-World")
 
     @requests_mock.Mocker()
     def test_list_repo_issues(self, m):
-        with open(os.path.join(_dir, 'test_data', 'test_get_repo.json'), 'r') as f:
-            m.get('https://api.github.com:443/repos/octocat/Hello-World', text=f.read())
-        with open(os.path.join(_dir, 'test_data', 'test_list_repo_issues.json'), 'r') as f:
-            m.get('https://api.github.com:443/repos/octocat/Hello-World/issues', text=f.read())
-        issues_table = self.github.list_repo_issues('octocat/Hello-World')
+        with open(os.path.join(_dir, "test_data", "test_get_repo.json"), "r") as f:
+            m.get("https://api.github.com:443/repos/octocat/Hello-World", text=f.read())
+        with open(
+            os.path.join(_dir, "test_data", "test_list_repo_issues.json"), "r"
+        ) as f:
+            m.get(
+                "https://api.github.com:443/repos/octocat/Hello-World/issues",
+                text=f.read(),
+            )
+        issues_table = self.github.list_repo_issues("octocat/Hello-World")
         self.assertIsInstance(issues_table, Table)
         self.assertEqual(len(issues_table.table), 2)
-        self.assertEqual(issues_table[0]['id'], 1)
-        self.assertEqual(issues_table[0]['title'], 'Found a bug')
+        self.assertEqual(issues_table[0]["id"], 1)
+        self.assertEqual(issues_table[0]["title"], "Found a bug")
 
     @requests_mock.Mocker()
     def test_download_file(self, m):
-        with open(os.path.join(_dir, 'test_data', 'test_get_repo.json'), 'r') as f:
-            m.get('https://api.github.com:443/repos/octocat/Hello-World', text=f.read())
-        with open(os.path.join(_dir, 'test_data', 'test_download_file.csv'), 'r') as f:
+        with open(os.path.join(_dir, "test_data", "test_get_repo.json"), "r") as f:
+            m.get("https://api.github.com:443/repos/octocat/Hello-World", text=f.read())
+        with open(os.path.join(_dir, "test_data", "test_download_file.csv"), "r") as f:
             m.get(
-                'https://raw.githubusercontent.com/octocat/Hello-World/testing/data.csv',
-                text=f.read()
+                "https://raw.githubusercontent.com/octocat/Hello-World/testing/data.csv",
+                text=f.read(),
             )
 
-        file_path = self.github.download_file('octocat/Hello-World', 'data.csv', branch='testing')
-        with open(file_path, 'r') as f:
+        file_path = self.github.download_file(
+            "octocat/Hello-World", "data.csv", branch="testing"
+        )
+        with open(file_path, "r") as f:
             file_contents = f.read()
 
         self.assertEqual(file_contents, "header\ndata\n")
```

### Comparing `parsons-1.0.0/test/test_gmail/test_gmail.py` & `parsons-1.1.0/test/test_gmail/test_gmail.py`

 * *Files 9% similar despite different names*

```diff
@@ -8,87 +8,98 @@
 import email
 
 
 _dir = os.path.dirname(__file__)
 
 
 class TestGmail(unittest.TestCase):
-
     @requests_mock.Mocker()
     def setUp(self, m):
         self.tmp_folder = "tmp/"
         self.credentials_file = f"{self.tmp_folder}/credentials.json"
         self.token_file = f"{self.tmp_folder}/token.json"
 
         os.mkdir(self.tmp_folder)
 
-        with open(self.credentials_file, 'w') as f:
-            f.write(json.dumps({
-              "installed": {
-                "client_id": "someclientid.apps.googleusercontent.com",
-                "project_id": "some-project-id-12345",
-                "auth_uri": "https://accounts.google.com/o/oauth2/auth",
-                "token_uri": "https://www.googleapis.com/oauth2/v3/token",
-                "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
-                "client_secret": "someclientsecret",
-                "redirect_uris": ["urn:ietf:wg:oauth:2.0:oob", "http://localhost"]
-              }
-            }))
-
-        with open(self.token_file, 'w') as f:
-            f.write(json.dumps({
-              "access_token": "someaccesstoken",
-              "client_id": "some-client-id.apps.googleusercontent.com",
-              "client_secret": "someclientsecret",
-              "refresh_token": "1/refreshrate",
-              "token_expiry": "2030-02-20T23:28:09Z",
-              "token_uri": "https://www.googleapis.com/oauth2/v3/token",
-              "user_agent": None,
-              "revoke_uri": "https://oauth2.googleapis.com/revoke",
-              "id_token": None,
-              "id_token_jwt": None,
-              "token_response": {
-                "access_token": "someaccesstoken",
-                "expires_in": 3600000,
-                "scope": "https://www.googleapis.com/auth/gmail.send",
-                "token_type": "Bearer"
-              },
-              "scopes": ["https://www.googleapis.com/auth/gmail.send"],
-              "token_info_uri": "https://oauth2.googleapis.com/tokeninfo",
-              "invalid": False,
-              "_class": "OAuth2Credentials",
-              "_module": "oauth2client.client"
-            }))
+        with open(self.credentials_file, "w") as f:
+            f.write(
+                json.dumps(
+                    {
+                        "installed": {
+                            "client_id": "someclientid.apps.googleusercontent.com",
+                            "project_id": "some-project-id-12345",
+                            "auth_uri": "https://accounts.google.com/o/oauth2/auth",
+                            "token_uri": "https://www.googleapis.com/oauth2/v3/token",
+                            "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",  # noqa: E501
+                            "client_secret": "someclientsecret",
+                            "redirect_uris": [
+                                "urn:ietf:wg:oauth:2.0:oob",
+                                "http://localhost",
+                            ],
+                        }
+                    }
+                )
+            )
+
+        with open(self.token_file, "w") as f:
+            f.write(
+                json.dumps(
+                    {
+                        "access_token": "someaccesstoken",
+                        "client_id": "some-client-id.apps.googleusercontent.com",
+                        "client_secret": "someclientsecret",
+                        "refresh_token": "1/refreshrate",
+                        "token_expiry": "2030-02-20T23:28:09Z",
+                        "token_uri": "https://www.googleapis.com/oauth2/v3/token",
+                        "user_agent": None,
+                        "revoke_uri": "https://oauth2.googleapis.com/revoke",
+                        "id_token": None,
+                        "id_token_jwt": None,
+                        "token_response": {
+                            "access_token": "someaccesstoken",
+                            "expires_in": 3600000,
+                            "scope": "https://www.googleapis.com/auth/gmail.send",
+                            "token_type": "Bearer",
+                        },
+                        "scopes": ["https://www.googleapis.com/auth/gmail.send"],
+                        "token_info_uri": "https://oauth2.googleapis.com/tokeninfo",
+                        "invalid": False,
+                        "_class": "OAuth2Credentials",
+                        "_module": "oauth2client.client",
+                    }
+                )
+            )
 
         self.gmail = Gmail(self.credentials_file, self.token_file)
 
     def tearDown(self):
         # Delete tmp folder and files
         shutil.rmtree(self.tmp_folder)
 
     def test_create_message_simple(self):
         sender = "Sender <sender@email.com>"
         to = "Recepient <recepient@email.com>"
         subject = "This is a test email"
         message_text = "The is the message text of the email"
 
-        msg = self.gmail._create_message_simple(
-            sender, to, subject, message_text)
+        msg = self.gmail._create_message_simple(sender, to, subject, message_text)
         raw = self.gmail._encode_raw_message(msg)
 
         decoded = email.message_from_bytes(
-            base64.urlsafe_b64decode(bytes(raw['raw'], 'utf-8')))
+            base64.urlsafe_b64decode(bytes(raw["raw"], "utf-8"))
+        )
 
         expected_items = [
-            ('Content-Type', 'text/plain; charset="us-ascii"'),
-            ('MIME-Version', '1.0'),
-            ('Content-Transfer-Encoding', '7bit'),
-            ('to', to),
-            ('from', sender),
-            ('subject', subject)]
+            ("Content-Type", 'text/plain; charset="us-ascii"'),
+            ("MIME-Version", "1.0"),
+            ("Content-Transfer-Encoding", "7bit"),
+            ("to", to),
+            ("from", sender),
+            ("subject", subject),
+        ]
 
         # Check the metadata
         self.assertListEqual(decoded.items(), expected_items)
 
         # Check the message
         self.assertEqual(decoded.get_payload(), message_text)
 
@@ -100,34 +111,40 @@
         sender = "Sender <sender@email.com>"
         to = "Recepient <recepient@email.com>"
         subject = "This is a test html email"
         message_text = "The is the message text of the email"
         message_html = "<p>This is the html message part of the email</p>"
 
         msg = self.gmail._create_message_html(
-            sender, to, subject, message_text, message_html)
+            sender, to, subject, message_text, message_html
+        )
         raw = self.gmail._encode_raw_message(msg)
 
         decoded = email.message_from_bytes(
-            base64.urlsafe_b64decode(bytes(raw['raw'], 'utf-8')))
+            base64.urlsafe_b64decode(bytes(raw["raw"], "utf-8"))
+        )
 
         expected_items = [
-            ('Content-Type', 'multipart/alternative;\n boundary='),
-            ('MIME-Version', '1.0'),
-            ('subject', subject),
-            ('from', sender),
-            ('to', to)]
+            ("Content-Type", "multipart/alternative;\n boundary="),
+            ("MIME-Version", "1.0"),
+            ("subject", subject),
+            ("from", sender),
+            ("to", to),
+        ]
 
         # The boundary id changes everytime. Replace it with the beginnig to
         # avoid failures
         updated_items = []
         for i in decoded.items():
-            if 'Content-Type' in i[0] and 'multipart/alternative;\n boundary=' in i[1]:  # noqa
+            if (
+                "Content-Type" in i[0] and "multipart/alternative;\n boundary=" in i[1]
+            ):  # noqa
                 updated_items.append(
-                    ('Content-Type', 'multipart/alternative;\n boundary='))
+                    ("Content-Type", "multipart/alternative;\n boundary=")
+                )
             else:
                 updated_items.append((i[0], i[1]))
 
         # Check the metadata
         self.assertListEqual(updated_items, expected_items)
 
         # Check the message
@@ -143,35 +160,39 @@
 
     def test_create_message_html_no_text(self):
         sender = "Sender <sender@email.com>"
         to = "Recepient <recepient@email.com>"
         subject = "This is a test html email"
         message_html = "<p>This is the html message part of the email</p>"
 
-        msg = self.gmail._create_message_html(
-            sender, to, subject, '', message_html)
+        msg = self.gmail._create_message_html(sender, to, subject, "", message_html)
         raw = self.gmail._encode_raw_message(msg)
 
         decoded = email.message_from_bytes(
-            base64.urlsafe_b64decode(bytes(raw['raw'], 'utf-8')))
+            base64.urlsafe_b64decode(bytes(raw["raw"], "utf-8"))
+        )
 
         expected_items = [
-            ('Content-Type', 'multipart/alternative;\n boundary='),
-            ('MIME-Version', '1.0'),
-            ('subject', subject),
-            ('from', sender),
-            ('to', to)]
+            ("Content-Type", "multipart/alternative;\n boundary="),
+            ("MIME-Version", "1.0"),
+            ("subject", subject),
+            ("from", sender),
+            ("to", to),
+        ]
 
         # The boundary id changes everytime. Replace it with the beginnig to
         # avoid failures
         updated_items = []
         for i in decoded.items():
-            if 'Content-Type' in i[0] and 'multipart/alternative;\n boundary=' in i[1]:  # noqa
+            if (
+                "Content-Type" in i[0] and "multipart/alternative;\n boundary=" in i[1]
+            ):  # noqa
                 updated_items.append(
-                    ('Content-Type', 'multipart/alternative;\n boundary='))
+                    ("Content-Type", "multipart/alternative;\n boundary=")
+                )
             else:
                 updated_items.append((i[0], i[1]))
 
         # Check the metadata
         self.assertListEqual(updated_items, expected_items)
 
         # Check the message
@@ -185,357 +206,394 @@
         self.assertEqual(sum([1 for i in decoded.walk()]), expected_parts)
 
     def test_create_message_attachments(self):
         sender = "Sender <sender@email.com>"
         to = "Recepient <recepient@email.com>"
         subject = "This is a test email with attachements"
         message_text = "The is the message text of the email with attachments"
-        message_html = ("<p>This is the html message part of the email "
-                        "with attachments</p>")
-        attachments = [f'{_dir}/assets/loremipsum.txt']
+        message_html = (
+            "<p>This is the html message part of the email " "with attachments</p>"
+        )
+        attachments = [f"{_dir}/assets/loremipsum.txt"]
 
         msg = self.gmail._create_message_attachments(
-            sender, to, subject, message_text, attachments,
-            message_html=message_html)
+            sender, to, subject, message_text, attachments, message_html=message_html
+        )
         raw = self.gmail._encode_raw_message(msg)
 
         decoded = email.message_from_bytes(
-            base64.urlsafe_b64decode(bytes(raw['raw'], 'utf-8')))
+            base64.urlsafe_b64decode(bytes(raw["raw"], "utf-8"))
+        )
 
         expected_items = [
-            ('Content-Type', 'multipart/alternative;\n boundary='),
-            ('MIME-Version', '1.0'),
-            ('to', to),
-            ('from', sender),
-            ('subject', subject)]
+            ("Content-Type", "multipart/alternative;\n boundary="),
+            ("MIME-Version", "1.0"),
+            ("to", to),
+            ("from", sender),
+            ("subject", subject),
+        ]
 
         # The boundary id changes everytime. Replace it with the beginnig to
         # avoid failures
         updated_items = []
         for i in decoded.items():
-            if 'Content-Type' in i[0] and 'multipart/alternative;\n boundary=' in i[1]:  # noqa
+            if (
+                "Content-Type" in i[0] and "multipart/alternative;\n boundary=" in i[1]
+            ):  # noqa
                 updated_items.append(
-                    ('Content-Type', 'multipart/alternative;\n boundary='))
+                    ("Content-Type", "multipart/alternative;\n boundary=")
+                )
             else:
                 updated_items.append((i[0], i[1]))
 
         # Check the metadata
         self.assertListEqual(updated_items, expected_items)
 
         # Check the message
         # The first part is just a container for the text and html parts
         parts = decoded.get_payload()
 
         self.assertEqual(parts[0].get_payload(), message_text)
         self.assertEqual(parts[1].get_payload(), message_html)
 
-        if os.linesep == '\r\n':
-            file = f'{_dir}/assets/loremipsum_b64_win_txt.txt'
+        if os.linesep == "\r\n":
+            file = f"{_dir}/assets/loremipsum_b64_win_txt.txt"
         else:
-            file = f'{_dir}/assets/loremipsum_b64_txt.txt'
+            file = f"{_dir}/assets/loremipsum_b64_txt.txt"
 
-        with open(file, 'r') as f:
+        with open(file, "r") as f:
             b64_txt = f.read()
         self.assertEqual(parts[2].get_payload(), b64_txt)
 
-        self.assertEqual(parts[2].get_content_type(), 'text/plain')
+        self.assertEqual(parts[2].get_content_type(), "text/plain")
 
         # Check the number of parts
         expected_parts = 4
         self.assertEqual(sum([1 for i in decoded.walk()]), expected_parts)
 
     def test_create_message_attachments_jpeg(self):
         sender = "Sender <sender@email.com>"
         to = "Recepient <recepient@email.com>"
         subject = "This is a test email with attachements"
         message_text = "The is the message text of the email with attachments"
-        message_html = ("<p>This is the html message part of the email "
-                        "with attachments</p>")
-        attachments = [f'{_dir}/assets/loremipsum.jpeg']
+        message_html = (
+            "<p>This is the html message part of the email " "with attachments</p>"
+        )
+        attachments = [f"{_dir}/assets/loremipsum.jpeg"]
 
         msg = self.gmail._create_message_attachments(
-            sender, to, subject, message_text, attachments,
-            message_html=message_html)
+            sender, to, subject, message_text, attachments, message_html=message_html
+        )
         raw = self.gmail._encode_raw_message(msg)
 
         decoded = email.message_from_bytes(
-            base64.urlsafe_b64decode(bytes(raw['raw'], 'utf-8')))
+            base64.urlsafe_b64decode(bytes(raw["raw"], "utf-8"))
+        )
 
         expected_items = [
-            ('Content-Type', 'multipart/alternative;\n boundary='),
-            ('MIME-Version', '1.0'),
-            ('to', to),
-            ('from', sender),
-            ('subject', subject)]
+            ("Content-Type", "multipart/alternative;\n boundary="),
+            ("MIME-Version", "1.0"),
+            ("to", to),
+            ("from", sender),
+            ("subject", subject),
+        ]
 
         # The boundary id changes everytime. Replace it with the beginnig to
         # avoid failures
         updated_items = []
         for i in decoded.items():
-            if 'Content-Type' in i[0] and 'multipart/alternative;\n boundary=' in i[1]:  # noqa
+            if (
+                "Content-Type" in i[0] and "multipart/alternative;\n boundary=" in i[1]
+            ):  # noqa
                 updated_items.append(
-                    ('Content-Type', 'multipart/alternative;\n boundary='))
+                    ("Content-Type", "multipart/alternative;\n boundary=")
+                )
             else:
                 updated_items.append((i[0], i[1]))
 
         # Check the metadata
         self.assertListEqual(updated_items, expected_items)
 
         # Check the message
         # The first part is just a container for the text and html parts
         parts = decoded.get_payload()
 
         self.assertEqual(parts[0].get_payload(), message_text)
         self.assertEqual(parts[1].get_payload(), message_html)
 
-        with open(f'{_dir}/assets/loremipsum_b64_jpeg.txt', 'r') as f:
+        with open(f"{_dir}/assets/loremipsum_b64_jpeg.txt", "r") as f:
             b64_txt = f.read()
         self.assertEqual(parts[2].get_payload(), b64_txt)
 
         expected_id = f"<{attachments[0].split('/')[-1]}>"
-        self.assertEqual(parts[2].get('Content-ID'), expected_id)
-        self.assertEqual(parts[2].get_content_type(), 'image/jpeg')
+        self.assertEqual(parts[2].get("Content-ID"), expected_id)
+        self.assertEqual(parts[2].get_content_type(), "image/jpeg")
 
         # Check the number of parts
         expected_parts = 4
         self.assertEqual(sum([1 for i in decoded.walk()]), expected_parts)
 
     def test_create_message_attachments_m4a(self):
         sender = "Sender <sender@email.com>"
         to = "Recepient <recepient@email.com>"
         subject = "This is a test email with attachements"
         message_text = "The is the message text of the email with attachments"
-        message_html = ("<p>This is the html message part of the email "
-                        "with attachments</p>")
-        attachments = [f'{_dir}/assets/loremipsum.m4a']
+        message_html = (
+            "<p>This is the html message part of the email " "with attachments</p>"
+        )
+        attachments = [f"{_dir}/assets/loremipsum.m4a"]
 
         msg = self.gmail._create_message_attachments(
-            sender, to, subject, message_text, attachments,
-            message_html=message_html)
+            sender, to, subject, message_text, attachments, message_html=message_html
+        )
         raw = self.gmail._encode_raw_message(msg)
 
         decoded = email.message_from_bytes(
-            base64.urlsafe_b64decode(bytes(raw['raw'], 'utf-8')))
+            base64.urlsafe_b64decode(bytes(raw["raw"], "utf-8"))
+        )
 
         expected_items = [
-            ('Content-Type', 'multipart/alternative;\n boundary='),
-            ('MIME-Version', '1.0'),
-            ('to', to),
-            ('from', sender),
-            ('subject', subject)]
+            ("Content-Type", "multipart/alternative;\n boundary="),
+            ("MIME-Version", "1.0"),
+            ("to", to),
+            ("from", sender),
+            ("subject", subject),
+        ]
 
         # The boundary id changes everytime. Replace it with the beginnig to
         # avoid failures
         updated_items = []
         for i in decoded.items():
-            if 'Content-Type' in i[0] and 'multipart/alternative;\n boundary=' in i[1]:  # noqa
+            if (
+                "Content-Type" in i[0] and "multipart/alternative;\n boundary=" in i[1]
+            ):  # noqa
                 updated_items.append(
-                    ('Content-Type', 'multipart/alternative;\n boundary='))
+                    ("Content-Type", "multipart/alternative;\n boundary=")
+                )
             else:
                 updated_items.append((i[0], i[1]))
 
         # Check the metadata
         self.assertListEqual(updated_items, expected_items)
 
         # Check the message
         # The first part is just a container for the text and html parts
         parts = decoded.get_payload()
 
         self.assertEqual(parts[0].get_payload(), message_text)
         self.assertEqual(parts[1].get_payload(), message_html)
 
-        with open(f'{_dir}/assets/loremipsum_b64_m4a.txt', 'r') as f:
+        with open(f"{_dir}/assets/loremipsum_b64_m4a.txt", "r") as f:
             b64_txt = f.read()
         self.assertEqual(parts[2].get_payload(), b64_txt)
 
-        self.assertEqual(parts[2].get_content_maintype(), 'audio')
+        self.assertEqual(parts[2].get_content_maintype(), "audio")
 
         # Check the number of parts
         expected_parts = 4
         self.assertEqual(sum([1 for i in decoded.walk()]), expected_parts)
 
     def test_create_message_attachments_mp3(self):
         sender = "Sender <sender@email.com>"
         to = "Recepient <recepient@email.com>"
         subject = "This is a test email with attachements"
         message_text = "The is the message text of the email with attachments"
-        message_html = ("<p>This is the html message part of the email "
-                        "with attachments</p>")
-        attachments = [f'{_dir}/assets/loremipsum.mp3']
+        message_html = (
+            "<p>This is the html message part of the email " "with attachments</p>"
+        )
+        attachments = [f"{_dir}/assets/loremipsum.mp3"]
 
         msg = self.gmail._create_message_attachments(
-            sender, to, subject, message_text, attachments,
-            message_html=message_html)
+            sender, to, subject, message_text, attachments, message_html=message_html
+        )
         raw = self.gmail._encode_raw_message(msg)
 
         decoded = email.message_from_bytes(
-            base64.urlsafe_b64decode(bytes(raw['raw'], 'utf-8')))
+            base64.urlsafe_b64decode(bytes(raw["raw"], "utf-8"))
+        )
 
         expected_items = [
-            ('Content-Type', 'multipart/alternative;\n boundary='),
-            ('MIME-Version', '1.0'),
-            ('to', to),
-            ('from', sender),
-            ('subject', subject)]
+            ("Content-Type", "multipart/alternative;\n boundary="),
+            ("MIME-Version", "1.0"),
+            ("to", to),
+            ("from", sender),
+            ("subject", subject),
+        ]
 
         # The boundary id changes everytime. Replace it with the beginnig to
         # avoid failures
         updated_items = []
         for i in decoded.items():
-            if 'Content-Type' in i[0] and 'multipart/alternative;\n boundary=' in i[1]:  # noqa
+            if (
+                "Content-Type" in i[0] and "multipart/alternative;\n boundary=" in i[1]
+            ):  # noqa
                 updated_items.append(
-                    ('Content-Type', 'multipart/alternative;\n boundary='))
+                    ("Content-Type", "multipart/alternative;\n boundary=")
+                )
             else:
                 updated_items.append((i[0], i[1]))
 
         # Check the metadata
         self.assertListEqual(updated_items, expected_items)
 
         # Check the message
         # The first part is just a container for the text and html parts
         parts = decoded.get_payload()
 
         self.assertEqual(parts[0].get_payload(), message_text)
         self.assertEqual(parts[1].get_payload(), message_html)
 
-        with open(f'{_dir}/assets/loremipsum_b64_mp3.txt', 'r') as f:
+        with open(f"{_dir}/assets/loremipsum_b64_mp3.txt", "r") as f:
             b64_txt = f.read()
         self.assertEqual(parts[2].get_payload(), b64_txt)
 
-        self.assertEqual(parts[2].get_content_type(), 'audio/mpeg')
+        self.assertEqual(parts[2].get_content_type(), "audio/mpeg")
 
         # Check the number of parts
         expected_parts = 4
         self.assertEqual(sum([1 for i in decoded.walk()]), expected_parts)
 
     def test_create_message_attachments_mp4(self):
         sender = "Sender <sender@email.com>"
         to = "Recepient <recepient@email.com>"
         subject = "This is a test email with attachements"
         message_text = "The is the message text of the email with attachments"
-        message_html = ("<p>This is the html message part of the email "
-                        "with attachments</p>")
-        attachments = [f'{_dir}/assets/loremipsum.mp4']
+        message_html = (
+            "<p>This is the html message part of the email " "with attachments</p>"
+        )
+        attachments = [f"{_dir}/assets/loremipsum.mp4"]
 
         msg = self.gmail._create_message_attachments(
-            sender, to, subject, message_text, attachments,
-            message_html=message_html)
+            sender, to, subject, message_text, attachments, message_html=message_html
+        )
         raw = self.gmail._encode_raw_message(msg)
 
         decoded = email.message_from_bytes(
-            base64.urlsafe_b64decode(bytes(raw['raw'], 'utf-8')))
+            base64.urlsafe_b64decode(bytes(raw["raw"], "utf-8"))
+        )
 
         expected_items = [
-            ('Content-Type', 'multipart/alternative;\n boundary='),
-            ('MIME-Version', '1.0'),
-            ('to', to),
-            ('from', sender),
-            ('subject', subject)]
+            ("Content-Type", "multipart/alternative;\n boundary="),
+            ("MIME-Version", "1.0"),
+            ("to", to),
+            ("from", sender),
+            ("subject", subject),
+        ]
 
         # The boundary id changes everytime. Replace it with the beginnig to
         # avoid failures
         updated_items = []
         for i in decoded.items():
-            if 'Content-Type' in i[0] and 'multipart/alternative;\n boundary=' in i[1]:  # noqa
+            if (
+                "Content-Type" in i[0] and "multipart/alternative;\n boundary=" in i[1]
+            ):  # noqa
                 updated_items.append(
-                    ('Content-Type', 'multipart/alternative;\n boundary='))
+                    ("Content-Type", "multipart/alternative;\n boundary=")
+                )
             else:
                 updated_items.append((i[0], i[1]))
 
         # Check the metadata
         self.assertListEqual(updated_items, expected_items)
 
         # Check the message
         # The first part is just a container for the text and html parts
         parts = decoded.get_payload()
 
         self.assertEqual(parts[0].get_payload(), message_text)
         self.assertEqual(parts[1].get_payload(), message_html)
 
-        with open(f'{_dir}/assets/loremipsum_b64_mp4.txt', 'r') as f:
+        with open(f"{_dir}/assets/loremipsum_b64_mp4.txt", "r") as f:
             b64_txt = f.read()
         self.assertEqual(parts[2].get_payload(), b64_txt)
 
-        self.assertEqual(parts[2].get_content_type(), 'video/mp4')
+        self.assertEqual(parts[2].get_content_type(), "video/mp4")
 
         # Check the number of parts
         expected_parts = 4
         self.assertEqual(sum([1 for i in decoded.walk()]), expected_parts)
 
     def test_create_message_attachments_pdf(self):
         sender = "Sender <sender@email.com>"
         to = "Recepient <recepient@email.com>"
         subject = "This is a test email with attachements"
         message_text = "The is the message text of the email with attachments"
-        message_html = ("<p>This is the html message part of the email "
-                        "with attachments</p>")
-        attachments = [f'{_dir}/assets/loremipsum.pdf']
+        message_html = (
+            "<p>This is the html message part of the email " "with attachments</p>"
+        )
+        attachments = [f"{_dir}/assets/loremipsum.pdf"]
 
         msg = self.gmail._create_message_attachments(
-            sender, to, subject, message_text, attachments,
-            message_html=message_html)
+            sender, to, subject, message_text, attachments, message_html=message_html
+        )
 
         raw = self.gmail._encode_raw_message(msg)
 
         decoded = email.message_from_bytes(
-            base64.urlsafe_b64decode(bytes(raw['raw'], 'utf-8')))
+            base64.urlsafe_b64decode(bytes(raw["raw"], "utf-8"))
+        )
 
         expected_items = [
-            ('Content-Type', 'multipart/alternative;\n boundary='),
-            ('MIME-Version', '1.0'),
-            ('to', to),
-            ('from', sender),
-            ('subject', subject)]
+            ("Content-Type", "multipart/alternative;\n boundary="),
+            ("MIME-Version", "1.0"),
+            ("to", to),
+            ("from", sender),
+            ("subject", subject),
+        ]
 
         # The boundary id changes everytime. Replace it with the beginnig to
         # avoid failures
         updated_items = []
         for i in decoded.items():
-            if 'Content-Type' in i[0] and 'multipart/alternative;\n boundary=' in i[1]:  # noqa
+            if (
+                "Content-Type" in i[0] and "multipart/alternative;\n boundary=" in i[1]
+            ):  # noqa
                 updated_items.append(
-                    ('Content-Type', 'multipart/alternative;\n boundary='))
+                    ("Content-Type", "multipart/alternative;\n boundary=")
+                )
             else:
                 updated_items.append((i[0], i[1]))
 
         # Check the metadata
         self.assertListEqual(updated_items, expected_items)
 
         # Check the message
         # The first part is just a container for the text and html parts
         parts = decoded.get_payload()
 
         self.assertEqual(parts[0].get_payload(), message_text)
         self.assertEqual(parts[1].get_payload(), message_html)
 
-        with open(f'{_dir}/assets/loremipsum_b64_pdf.txt', 'r') as f:
+        with open(f"{_dir}/assets/loremipsum_b64_pdf.txt", "r") as f:
             b64_txt = f.read()
         self.assertEqual(parts[2].get_payload(), b64_txt)
 
-        self.assertEqual(parts[2].get_content_type(), 'application/pdf')
+        self.assertEqual(parts[2].get_content_type(), "application/pdf")
 
         # Check the number of parts
         expected_parts = 4
         self.assertEqual(sum([1 for i in decoded.walk()]), expected_parts)
 
     def test__validate_email_string(self):
         emails = [
             {"email": "Sender <sender@email.com>", "expected": True},
             {"email": "sender@email.com", "expected": True},
             {"email": "<sender@email.com>", "expected": True},
             {"email": "Sender sender@email.com", "expected": False},
             {"email": "Sender <sender2email.com>", "expected": False},
             {"email": "Sender <sender@email,com>", "expected": True},
-            {"email": "Sender <sender+alias@email,com>", "expected": True}
+            {"email": "Sender <sender+alias@email,com>", "expected": True},
         ]
 
         for e in emails:
-            if e['expected']:
-                self.assertTrue(self.gmail._validate_email_string(e['email']))
+            if e["expected"]:
+                self.assertTrue(self.gmail._validate_email_string(e["email"]))
             else:
                 self.assertRaises(
-                    ValueError, self.gmail._validate_email_string, e['email'])
+                    ValueError, self.gmail._validate_email_string, e["email"]
+                )
 
     # TODO test sending emails
 
 
 if __name__ == "__main__":
     unittest.main()
```

### Comparing `parsons-1.0.0/test/test_mailchimp/expected_json.py` & `parsons-1.1.0/test/test_mailchimp/expected_json.py`

 * *Files 8% similar despite different names*

```diff
@@ -14,15 +14,15 @@
             "needs_block_refresh": False,
             "resendable": True,
             "recipients": {
                 "list_id": "zyx",
                 "list_is_active": True,
                 "list_name": "Support Our Candidate List 1",
                 "segment_text": "",
-                "recipient_count": 145
+                "recipient_count": 145,
             },
             "settings": {
                 "subject_line": "Sample Campaign 1",
                 "preview_text": "This is a sample campaign.",
                 "title": "Sample Campaign Donation Ask",
                 "from_name": "Our Candidate",
                 "reply_to": "our_candidate@example.com",
@@ -32,42 +32,36 @@
                 "authenticate": True,
                 "auto_footer": False,
                 "inline_css": False,
                 "auto_tweet": False,
                 "fb_comments": True,
                 "timewarp": False,
                 "template_id": 12345,
-                "drag_and_drop": True
+                "drag_and_drop": True,
             },
             "tracking": {
                 "opens": True,
                 "html_clicks": True,
                 "text_clicks": False,
                 "goal_tracking": False,
                 "ecomm360": False,
                 "google_analytics": "",
-                "clicktale": ""
+                "clicktale": "",
             },
             "report_summary": {
                 "opens": 48,
                 "unique_opens": 34,
                 "open_rate": 0.23776223776223776,
                 "clicks": 1,
                 "subscriber_clicks": 1,
                 "click_rate": 0.006993006993006993,
-                "ecommerce": {
-                    "total_orders": 0,
-                    "total_spent": 0,
-                    "total_revenue": 0
-                }
+                "ecommerce": {"total_orders": 0, "total_spent": 0, "total_revenue": 0},
             },
-            "delivery_status": {
-                "enabled": False
-            },
-            "_links": []
+            "delivery_status": {"enabled": False},
+            "_links": [],
         },
         {
             "id": "def",
             "web_id": 456,
             "type": "regular",
             "create_time": "2019-05-29T11:46:41+00:00",
             "archive_url": "http://example.com/sample-campaign-2",
@@ -79,15 +73,15 @@
             "needs_block_refresh": False,
             "resendable": True,
             "recipients": {
                 "list_id": "wvu",
                 "list_is_active": True,
                 "list_name": "Support Our Candidate List 2",
                 "segment_text": "",
-                "recipient_count": 87
+                "recipient_count": 87,
             },
             "settings": {
                 "subject_line": "Sample Campaign 2",
                 "preview_text": "This is another sample campaign.",
                 "title": "Sample Campaign 2 Donation Ask",
                 "from_name": "Our Candidate",
                 "reply_to": "our_candidate@example.com",
@@ -97,43 +91,39 @@
                 "authenticate": True,
                 "auto_footer": False,
                 "inline_css": False,
                 "auto_tweet": False,
                 "fb_comments": True,
                 "timewarp": False,
                 "template_id": 67890,
-                "drag_and_drop": True
+                "drag_and_drop": True,
             },
             "tracking": {
                 "opens": True,
                 "html_clicks": True,
                 "text_clicks": False,
                 "goal_tracking": False,
                 "ecomm360": False,
                 "google_analytics": "",
-                "clicktale": ""
+                "clicktale": "",
             },
             "report_summary": {
                 "opens": 108,
                 "unique_opens": 48,
                 "open_rate": 0.5647058823529412,
                 "clicks": 25,
                 "subscriber_clicks": 14,
                 "click_rate": 0.16470588235294117,
-                "ecommerce": {
-                    "total_orders": 0,
-                    "total_spent": 0,
-                    "total_revenue": 0
-                }
-            },
-            "delivery_status": {
-                "enabled": False
+                "ecommerce": {"total_orders": 0, "total_spent": 0, "total_revenue": 0},
             },
-            "_links": []
-        }]}
+            "delivery_status": {"enabled": False},
+            "_links": [],
+        },
+    ]
+}
 
 test_lists = {
     "lists": [
         {
             "id": "zyx",
             "web_id": 98765,
             "name": "Support Our Candidate List 1",
@@ -141,25 +131,26 @@
                 "company": "Support Our Candidate",
                 "address1": "123 Main Street",
                 "address2": "",
                 "city": "Townsville",
                 "state": "OH",
                 "zip": "43358",
                 "country": "US",
-                "phone": ""
+                "phone": "",
             },
             "permission_reminder": (
                 "You are receiving this email because you signed up at an event, while being "
-                "canvassed, or on our website."),
+                "canvassed, or on our website."
+            ),
             "use_archive_bar": True,
             "campaign_defaults": {
                 "from_name": "Our Candidate",
                 "from_email": "our_candidate@example.com",
                 "subject": "",
-                "language": "en"
+                "language": "en",
             },
             "notify_on_subscribe": "",
             "notify_on_unsubscribe": "",
             "date_created": "2019-03-25T22:55:44+00:00",
             "list_rating": 3,
             "email_type_option": False,
             "subscribe_url_short": "http://example.com/sample-subscribe_url_2",
@@ -182,41 +173,42 @@
                 "merge_field_count": 5,
                 "avg_sub_rate": 0,
                 "avg_unsub_rate": 1,
                 "target_sub_rate": 3,
                 "open_rate": 38.40236686390532,
                 "click_rate": 4.016786570743405,
                 "last_sub_date": "2019-09-24T01:07:56+00:00",
-                "last_unsub_date": "2020-01-06T01:55:02+00:00"
+                "last_unsub_date": "2020-01-06T01:55:02+00:00",
             },
-            "_links": []
+            "_links": [],
         },
         {
             "id": "xvu",
             "web_id": 43210,
             "name": "Support Our Candidate List 2",
             "contact": {
                 "company": "Support Our Candidate",
                 "address1": "123 Main Street",
                 "address2": "",
                 "city": "Townsville",
                 "state": "OH",
                 "zip": "43358",
                 "country": "US",
-                "phone": ""
+                "phone": "",
             },
             "permission_reminder": (
                 "You are receiving this email because you signed up at an event, while being "
-                "canvassed, or on our website."),
+                "canvassed, or on our website."
+            ),
             "use_archive_bar": True,
             "campaign_defaults": {
                 "from_name": "Our Candidate",
                 "from_email": "our_candidate@example.com",
                 "subject": "",
-                "language": "en"
+                "language": "en",
             },
             "notify_on_subscribe": "",
             "notify_on_unsubscribe": "",
             "date_created": "2018-09-15T22:15:21+00:00",
             "list_rating": 3,
             "email_type_option": False,
             "subscribe_url_short": "http://example.com/sample-subscribe_url_1",
@@ -239,147 +231,137 @@
                 "merge_field_count": 5,
                 "avg_sub_rate": 0,
                 "avg_unsub_rate": 1,
                 "target_sub_rate": 3,
                 "open_rate": 64.19236186394533,
                 "click_rate": 3.746759370417411,
                 "last_sub_date": "2020-01-01T00:19:46+00:00",
-                "last_unsub_date": "2019-12-23T11:44:31+00:00"
+                "last_unsub_date": "2019-12-23T11:44:31+00:00",
             },
-            "_links": []
+            "_links": [],
         },
     ],
     "total_items": 1,
     "constraints": {
         "may_create": False,
         "max_instances": 1,
-        "current_total_instances": 1
+        "current_total_instances": 1,
     },
-    "_links": []
+    "_links": [],
 }
 
 test_members = {
-  "members": [
-    {
-      "id": "9eb69db8d0371811aa18803a1ae21584",
-      "email_address": "member_1@example.com",
-      "unique_email_id": "c82a25d939",
-      "web_id": 24816326,
-      "email_type": "html",
-      "status": "subscribed",
-      "merge_fields": {
-        "FNAME": "Member",
-        "LNAME": "One",
-        "ADDRESS": {
-          "addr1": "",
-          "addr2": "",
-          "city": "",
-          "state": "",
-          "zip": "",
-          "country": "US"
-        },
-        "PHONE": "",
-        "BIRTHDAY": ""
-      },
-      "stats": {
-        "avg_open_rate": 0.3571,
-        "avg_click_rate": 0
-      },
-      "ip_signup": "",
-      "timestamp_signup": "",
-      "ip_opt": "174.59.50.35",
-      "timestamp_opt": "2019-03-25T22:55:44+00:00",
-      "member_rating": 4,
-      "last_changed": "2019-03-25T22:55:44+00:00",
-      "language": "en",
-      "vip": False,
-      "email_client": "Gmail",
-      "location": {
-        "latitude": 40.0293,
-        "longitude": -76.2656,
-        "gmtoff": 0,
-        "dstoff": 0,
-        "country_code": "US",
-        "timezone": "717/223"
-      },
-      "source": "Unknown",
-      "tags_count": 0,
-      "tags": [],
-      "list_id": "67fdf4b1f4",
-      "_links": []
-    },
-    {
-      "id": "4f315641dbad7b74acc0f4a5d3741ac6",
-      "email_address": "member_2@example.com",
-      "unique_email_id": "8d308d69d3",
-      "web_id": 12233445,
-      "email_type": "html",
-      "status": "subscribed",
-      "merge_fields": {
-        "FNAME": "Member",
-        "LNAME": "Two",
-        "ADDRESS": "",
-        "PHONE": "",
-        "BIRTHDAY": ""
-      },
-      "stats": {
-        "avg_open_rate": 0.5,
-        "avg_click_rate": 0
-      },
-      "ip_signup": "",
-      "timestamp_signup": "",
-      "ip_opt": "174.59.50.35",
-      "timestamp_opt": "2019-03-25T23:04:46+00:00",
-      "member_rating": 4,
-      "last_changed": "2019-03-25T23:04:46+00:00",
-      "language": "",
-      "vip": False,
-      "email_client": "iPhone",
-      "location": {
-        "latitude": 40.0459,
-        "longitude": -76.3542,
-        "gmtoff": 0,
-        "dstoff": 0,
-        "country_code": "US",
-        "timezone": "717/223"
-      },
-      "source": "Import",
-      "tags_count": 2,
-      "tags": [
+    "members": [
         {
-          "id": 17493,
-          "name": "canvass"
+            "id": "9eb69db8d0371811aa18803a1ae21584",
+            "email_address": "member_1@example.com",
+            "unique_email_id": "c82a25d939",
+            "web_id": 24816326,
+            "email_type": "html",
+            "status": "subscribed",
+            "merge_fields": {
+                "FNAME": "Member",
+                "LNAME": "One",
+                "ADDRESS": {
+                    "addr1": "",
+                    "addr2": "",
+                    "city": "",
+                    "state": "",
+                    "zip": "",
+                    "country": "US",
+                },
+                "PHONE": "",
+                "BIRTHDAY": "",
+            },
+            "stats": {"avg_open_rate": 0.3571, "avg_click_rate": 0},
+            "ip_signup": "",
+            "timestamp_signup": "",
+            "ip_opt": "174.59.50.35",
+            "timestamp_opt": "2019-03-25T22:55:44+00:00",
+            "member_rating": 4,
+            "last_changed": "2019-03-25T22:55:44+00:00",
+            "language": "en",
+            "vip": False,
+            "email_client": "Gmail",
+            "location": {
+                "latitude": 40.0293,
+                "longitude": -76.2656,
+                "gmtoff": 0,
+                "dstoff": 0,
+                "country_code": "US",
+                "timezone": "717/223",
+            },
+            "source": "Unknown",
+            "tags_count": 0,
+            "tags": [],
+            "list_id": "67fdf4b1f4",
+            "_links": [],
         },
         {
-          "id": 17497,
-          "name": "canvass-03-17-2019"
-        }
-      ],
-      "list_id": "67fdf4b1f4",
-      "_links": []
-    }]}
+            "id": "4f315641dbad7b74acc0f4a5d3741ac6",
+            "email_address": "member_2@example.com",
+            "unique_email_id": "8d308d69d3",
+            "web_id": 12233445,
+            "email_type": "html",
+            "status": "subscribed",
+            "merge_fields": {
+                "FNAME": "Member",
+                "LNAME": "Two",
+                "ADDRESS": "",
+                "PHONE": "",
+                "BIRTHDAY": "",
+            },
+            "stats": {"avg_open_rate": 0.5, "avg_click_rate": 0},
+            "ip_signup": "",
+            "timestamp_signup": "",
+            "ip_opt": "174.59.50.35",
+            "timestamp_opt": "2019-03-25T23:04:46+00:00",
+            "member_rating": 4,
+            "last_changed": "2019-03-25T23:04:46+00:00",
+            "language": "",
+            "vip": False,
+            "email_client": "iPhone",
+            "location": {
+                "latitude": 40.0459,
+                "longitude": -76.3542,
+                "gmtoff": 0,
+                "dstoff": 0,
+                "country_code": "US",
+                "timezone": "717/223",
+            },
+            "source": "Import",
+            "tags_count": 2,
+            "tags": [
+                {"id": 17493, "name": "canvass"},
+                {"id": 17497, "name": "canvass-03-17-2019"},
+            ],
+            "list_id": "67fdf4b1f4",
+            "_links": [],
+        },
+    ]
+}
 
 test_unsubscribes = {
-  "unsubscribes": [
-    {
-      "email_id": "e542e5cd7b414e5ff8409ff57cf154be",
-      "email_address": "unsubscribe_1@exmaple.com",
-      "merge_fields": {
-        "FNAME": "Unsubscriber",
-        "LNAME": "One",
-        "ADDRESS": "",
-        "PHONE": "5558754307",
-        "BIRTHDAY": ""
-      },
-      "vip": False,
-      "timestamp": "2019-12-09T21:18:06+00:00",
-      "reason": "None given",
-      "campaign_id": "abc",
-      "list_id": "zyx",
-      "list_is_active": True,
-      "_links": []
-    }
-  ],
-  "campaign_id": "abc",
-  "total_items": 1,
-  "_links": []
+    "unsubscribes": [
+        {
+            "email_id": "e542e5cd7b414e5ff8409ff57cf154be",
+            "email_address": "unsubscribe_1@exmaple.com",
+            "merge_fields": {
+                "FNAME": "Unsubscriber",
+                "LNAME": "One",
+                "ADDRESS": "",
+                "PHONE": "5558754307",
+                "BIRTHDAY": "",
+            },
+            "vip": False,
+            "timestamp": "2019-12-09T21:18:06+00:00",
+            "reason": "None given",
+            "campaign_id": "abc",
+            "list_id": "zyx",
+            "list_is_active": True,
+            "_links": [],
+        }
+    ],
+    "campaign_id": "abc",
+    "total_items": 1,
+    "_links": [],
 }
```

### Comparing `parsons-1.0.0/test/test_mailchimp/test_mailchimp.py` & `parsons-1.1.0/test/test_mailchimp/test_mailchimp.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,49 +1,51 @@
 from parsons import Mailchimp
 import unittest
 import requests_mock
 from test.test_mailchimp import expected_json
 
-API_KEY = 'mykey-us00'
+API_KEY = "mykey-us00"
 
 
 class TestMailchimp(unittest.TestCase):
-
     def setUp(self):
 
         self.mc = Mailchimp(API_KEY)
 
     @requests_mock.Mocker()
     def test_get_campaigns(self, m):
 
         # Test that campaigns are returned correctly.
-        m.get(self.mc.uri + 'campaigns', json=expected_json.test_campaigns)
+        m.get(self.mc.uri + "campaigns", json=expected_json.test_campaigns)
         tbl = self.mc.get_campaigns()
 
         self.assertEqual(tbl.num_rows, 2)
 
     @requests_mock.Mocker()
     def test_get_lists(self, m):
 
         # Test that lists are returned correctly.
-        m.get(self.mc.uri + 'lists', json=expected_json.test_lists)
+        m.get(self.mc.uri + "lists", json=expected_json.test_lists)
         tbl = self.mc.get_lists()
 
         self.assertEqual(tbl.num_rows, 2)
 
     @requests_mock.Mocker()
     def test_get_members(self, m):
 
         # Test that list members are returned correctly.
-        m.get(self.mc.uri + 'lists/zyx/members', json=expected_json.test_members)
-        tbl = self.mc.get_members(list_id='zyx')
+        m.get(self.mc.uri + "lists/zyx/members", json=expected_json.test_members)
+        tbl = self.mc.get_members(list_id="zyx")
 
         self.assertEqual(tbl.num_rows, 2)
 
     @requests_mock.Mocker()
     def test_get_unsubscribes(self, m):
 
         # Test that campaign unsubscribes are returned correctly.
-        m.get(self.mc.uri + 'reports/abc/unsubscribed', json=expected_json.test_unsubscribes)
-        tbl = self.mc.get_unsubscribes(campaign_id='abc')
+        m.get(
+            self.mc.uri + "reports/abc/unsubscribed",
+            json=expected_json.test_unsubscribes,
+        )
+        tbl = self.mc.get_unsubscribes(campaign_id="abc")
 
         self.assertEqual(tbl.num_rows, 1)
```

### Comparing `parsons-1.0.0/test/test_mobilize/test_mobilize_json.py` & `parsons-1.1.0/test/test_mobilize/test_mobilize_json.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,87 +1,92 @@
 import json
 
 GET_ORGANIZATIONS_JSON = {
     "count": 38,
     "next": None,
     "previous": (
-        "https://events.mobilizeamerica.io/api/v1/organizations?updated_since=1543644000"),
+        "https://events.mobilizeamerica.io/api/v1/organizations?updated_since=1543644000"
+    ),
     "data": [
-                {
-                    "id": 1251,
-                    "name": "Mike Blake for New York City",
-                    "slug": "mikefornyc",
-                    "is_coordinated": 'True',
-                    "is_independent": 'True',
-                    "is_primary_campaign": 'False',
-                    "state": "",
-                    "district": "",
-                    "candidate_name": "",
-                    "race_type": "OTHER_LOCAL",
-                    "event_feed_url": "https://events.mobilizeamerica.io/mikefornyc/",
-                    "created_date": 1545885434,
-                    "modified_date": 1546132256
-                }
-    ]
+        {
+            "id": 1251,
+            "name": "Mike Blake for New York City",
+            "slug": "mikefornyc",
+            "is_coordinated": "True",
+            "is_independent": "True",
+            "is_primary_campaign": "False",
+            "state": "",
+            "district": "",
+            "candidate_name": "",
+            "race_type": "OTHER_LOCAL",
+            "event_feed_url": "https://events.mobilizeamerica.io/mikefornyc/",
+            "created_date": 1545885434,
+            "modified_date": 1546132256,
+        }
+    ],
 }
 
 GET_EVENTS_JSON = {
-    'count': 1, 'next': None, 'previous': None,
-    'data': [
+    "count": 1,
+    "next": None,
+    "previous": None,
+    "data": [
         {
-            'id': 86738,
-            'description': (
-                'Join our team of volunteers and learn how to engage students in local '
-                'high schools, communicate our mission, and register young voters.'),
-            'timezone': 'America/Chicago',
-            'title': 'Student Voter Initiative Training',
-            'summary': '',
-            'featured_image_url': (
-                'https://mobilizeamerica.imgix.net/uploads/event/'
-                '40667432145_6188839fe3_o_20190102224312253645.jpeg'),
-            'sponsor': {
-                'id': 1076,
-                'name': 'Battleground Texas',
-                'slug': 'battlegroundtexas',
-                'is_coordinated': True,
-                'is_independent': False,
-                'is_primary_campaign': False,
-                'state': '',
-                'district': '',
-                'candidate_name': '',
-                'race_type': None,
-                'event_feed_url': 'https://events.mobilizeamerica.io/battlegroundtexas/',
-                'created_date': 1538590930,
-                'modified_date': 1546468308
+            "id": 86738,
+            "description": (
+                "Join our team of volunteers and learn how to engage students in local "
+                "high schools, communicate our mission, and register young voters."
+            ),
+            "timezone": "America/Chicago",
+            "title": "Student Voter Initiative Training",
+            "summary": "",
+            "featured_image_url": (
+                "https://mobilizeamerica.imgix.net/uploads/event/"
+                "40667432145_6188839fe3_o_20190102224312253645.jpeg"
+            ),
+            "sponsor": {
+                "id": 1076,
+                "name": "Battleground Texas",
+                "slug": "battlegroundtexas",
+                "is_coordinated": True,
+                "is_independent": False,
+                "is_primary_campaign": False,
+                "state": "",
+                "district": "",
+                "candidate_name": "",
+                "race_type": None,
+                "event_feed_url": "https://events.mobilizeamerica.io/battlegroundtexas/",
+                "created_date": 1538590930,
+                "modified_date": 1546468308,
             },
-            'timeslots': [{
-                'id': 526226,
-                'start_date': 1547330400,
-                'end_date': 1547335800}],
-            'location': {
-                'venue': 'Harris County Democratic Party HQ',
-                'address_lines': ['4619 Lyons Ave', ''],
-                'locality': 'Houston',
-                'region': 'TX',
-                'postal_code': '77020',
-                'location': {'latitude': 29.776446, 'longitude': -95.323037},
-                'congressional_district': '18',
-                'state_leg_district': '142',
-                'state_senate_district': None
+            "timeslots": [
+                {"id": 526226, "start_date": 1547330400, "end_date": 1547335800}
+            ],
+            "location": {
+                "venue": "Harris County Democratic Party HQ",
+                "address_lines": ["4619 Lyons Ave", ""],
+                "locality": "Houston",
+                "region": "TX",
+                "postal_code": "77020",
+                "location": {"latitude": 29.776446, "longitude": -95.323037},
+                "congressional_district": "18",
+                "state_leg_district": "142",
+                "state_senate_district": None,
             },
-            'event_type': 'TRAINING',
-            'created_date': 1546469706,
-            'modified_date': 1547335800,
-            'browser_url': (
-                'https://events.mobilizeamerica.io/battlegroundtexas/event/86738/'),
-            'high_priority': None,
-            'contact': None,
-            'visibility': 'PUBLIC'
+            "event_type": "TRAINING",
+            "created_date": 1546469706,
+            "modified_date": 1547335800,
+            "browser_url": (
+                "https://events.mobilizeamerica.io/battlegroundtexas/event/86738/"
+            ),
+            "high_priority": None,
+            "contact": None,
+            "visibility": "PUBLIC",
         }
-    ]
+    ],
 }
 
 GET_EVENTS_ORGANIZATION_JSON = json.loads(
     """
     {
         "count": 2,
         "next": null,
@@ -242,21 +247,15 @@
             "build_commit": "abcd",
             "page_title": null
         }
     }"""
 )
 
 GET_EVENTS_DELETED_JSON = {
-    'count': 2,
-    'next': None,
-    'previous': None,
-    'data': [
-        {
-            'id': 86765,
-            'deleted_date': 1546705971
-        },
-        {
-            'id': 86782,
-            'deleted_date': 1546912779
-        }
-    ]
+    "count": 2,
+    "next": None,
+    "previous": None,
+    "data": [
+        {"id": 86765, "deleted_date": 1546705971},
+        {"id": 86782, "deleted_date": 1546912779},
+    ],
 }
```

### Comparing `parsons-1.0.0/test/test_p2a.py` & `parsons-1.1.0/test/test_p2a.py`

 * *Files 20% similar despite different names*

```diff
@@ -29,21 +29,21 @@
                 "street2": "",
                 "city": "Los Angeles",
                 "state": "CA",
                 "zip5": 96055,
                 "zip4": 9534,
                 "county": "Tehama",
                 "latitude": "50.0632635",
-                "longitude": "-122.09654"
+                "longitude": "-122.09654",
             },
             "districts": {
                 "congressional": "1",
                 "stateSenate": "4",
                 "stateHouse": "3",
-                "cityCouncil": None
+                "cityCouncil": None,
             },
             "ids": [],
             "memberships": [
                 {
                     "id": 15151443,
                     "campaignid": 25373,
                     "name": "20171121 Businesses for Responsible Tax Reform - Contact Congress",
@@ -52,43 +52,32 @@
                 },
                 {
                     "id": 20025582,
                     "campaignid": 32641,
                     "name": "20180524 March for America",
                     "source": None,
                     "created_at": "2018-05-24 21:09:49.000000",
-                }
+                },
             ],
             "fields": [],
             "phones": [
-                {
-                    "id": 10537860,
-                    "address": "+19995206447",
-                    "subscribed": 'false'
-                }
+                {"id": 10537860, "address": "+19995206447", "subscribed": "false"}
             ],
             "emails": [
-                {
-                    "id": 10537871,
-                    "address": "N@k.com",
-                    "subscribed": 'false'
-                },
-                {
-                    "id": 10950446,
-                    "address": "email@me.com",
-                    "subscribed": 'false'
-                }
-            ]
+                {"id": 10537871, "address": "N@k.com", "subscribed": "false"},
+                {"id": 10950446, "address": "email@me.com", "subscribed": "false"},
+            ],
         }
     ],
     "pagination": {
         "count": 1,
         "per_page": 100,
         "current_page": 1,
-        "next_url": "https://api.phone2action.com/2.0/advocates?page=2"}
+        "next_url": "https://api.phone2action.com/2.0/advocates?page=2",
+    },
 }
 
 camp_json = [
     {
         "id": 25373,
         "name": "20171121 Businesses for Responsible Tax Reform - Contact Congress",
         "display_name": "Businesses for Responsible Tax Reform",
@@ -100,188 +89,239 @@
         "restrict_allow": None,
         "content": {
             "summary": "",
             "introduction": "Welcome",
             "call_to_action": "Contact your officials in one click!",
             "thank_you": "<p>Thanks for taking action. Please encourage others to act by "
             "sharing on social media.</p>",
-            "background_image": None
+            "background_image": None,
         },
         "updated_at": {
             "date": "2017-11-21 23:27:11.000000",
             "timezone_type": 3,
-            "timezone": "UTC"
-        }
+            "timezone": "UTC",
+        },
     }
 ]
 
 
 def parse_request_body(m):
-    kvs = m.split('&')
-    return {
-        kv.split('=')[0]: kv.split('=')[1]
-        for kv in kvs
-    }
+    kvs = m.split("&")
+    return {kv.split("=")[0]: kv.split("=")[1] for kv in kvs}
 
 
 class TestP2A(unittest.TestCase):
-
     def setUp(self):
 
-        self.p2a = Phone2Action(app_id='an_id', app_key='app_key')
+        self.p2a = Phone2Action(app_id="an_id", app_key="app_key")
 
     def tearDown(self):
 
         pass
 
     def test_init_args(self):
         # Test initializing class with args
         # Done in the setUp
 
         pass
 
     def test_init_envs(self):
         # Test initilizing class with envs
 
-        os.environ['PHONE2ACTION_APP_ID'] = 'id'
-        os.environ['PHONE2ACTION_APP_KEY'] = 'key'
+        os.environ["PHONE2ACTION_APP_ID"] = "id"
+        os.environ["PHONE2ACTION_APP_KEY"] = "key"
 
         p2a_envs = Phone2Action()
-        self.assertEqual(p2a_envs.app_id, 'id')
-        self.assertEqual(p2a_envs.app_key, 'key')
+        self.assertEqual(p2a_envs.app_id, "id")
+        self.assertEqual(p2a_envs.app_key, "key")
 
     @requests_mock.Mocker()
     def test_get_advocates(self, m):
 
-        m.get(self.p2a.client.uri + 'advocates', json=adv_json)
-
-        adv_exp = ['id', 'prefix', 'firstname', 'middlename',
-                   'lastname', 'suffix', 'notes', 'stage', 'connections',
-                   'created_at', 'updated_at',
-                   'address_city', 'address_county', 'address_latitude',
-                   'address_longitude', 'address_state', 'address_street1',
-                   'address_street2', 'address_zip4', 'address_zip5',
-                   'districts_cityCouncil', 'districts_congressional',
-                   'districts_stateHouse', 'districts_stateSenate']
-
-        self.assertTrue(validate_list(adv_exp, self.p2a.get_advocates()['advocates']))
-        ids_exp = ['advocate_id', 'ids']
-
-        self.assertTrue(validate_list(ids_exp, self.p2a.get_advocates()['ids']))
-
-        phone_exp = ['advocate_id', 'phones_address', 'phones_id', 'phones_subscribed']
-        self.assertTrue(validate_list(phone_exp, self.p2a.get_advocates()['phones']))
-
-        tags_exp = ['advocate_id', 'tags']
-        self.assertTrue(validate_list(tags_exp, self.p2a.get_advocates()['tags']))
-
-        email_exp = ['advocate_id', 'emails_address', 'emails_id', 'emails_subscribed']
-        self.assertTrue(validate_list(email_exp, self.p2a.get_advocates()['emails']))
+        m.get(self.p2a.client.uri + "advocates", json=adv_json)
 
-        member_exp = ['advocate_id', 'memberships_campaignid', 'memberships_created_at',
-                      'memberships_id', 'memberships_name', 'memberships_source']
-        self.assertTrue(validate_list(member_exp, self.p2a.get_advocates()['memberships']))
+        adv_exp = [
+            "id",
+            "prefix",
+            "firstname",
+            "middlename",
+            "lastname",
+            "suffix",
+            "notes",
+            "stage",
+            "connections",
+            "created_at",
+            "updated_at",
+            "address_city",
+            "address_county",
+            "address_latitude",
+            "address_longitude",
+            "address_state",
+            "address_street1",
+            "address_street2",
+            "address_zip4",
+            "address_zip5",
+            "districts_cityCouncil",
+            "districts_congressional",
+            "districts_stateHouse",
+            "districts_stateSenate",
+        ]
+
+        self.assertTrue(validate_list(adv_exp, self.p2a.get_advocates()["advocates"]))
+        ids_exp = ["advocate_id", "ids"]
+
+        self.assertTrue(validate_list(ids_exp, self.p2a.get_advocates()["ids"]))
+
+        phone_exp = ["advocate_id", "phones_address", "phones_id", "phones_subscribed"]
+        self.assertTrue(validate_list(phone_exp, self.p2a.get_advocates()["phones"]))
+
+        tags_exp = ["advocate_id", "tags"]
+        self.assertTrue(validate_list(tags_exp, self.p2a.get_advocates()["tags"]))
+
+        email_exp = ["advocate_id", "emails_address", "emails_id", "emails_subscribed"]
+        self.assertTrue(validate_list(email_exp, self.p2a.get_advocates()["emails"]))
+
+        member_exp = [
+            "advocate_id",
+            "memberships_campaignid",
+            "memberships_created_at",
+            "memberships_id",
+            "memberships_name",
+            "memberships_source",
+        ]
+        self.assertTrue(
+            validate_list(member_exp, self.p2a.get_advocates()["memberships"])
+        )
 
-        fields_exp = ['advocate_id', 'fields']
-        self.assertTrue(validate_list(fields_exp, self.p2a.get_advocates()['fields']))
+        fields_exp = ["advocate_id", "fields"]
+        self.assertTrue(validate_list(fields_exp, self.p2a.get_advocates()["fields"]))
 
     @requests_mock.Mocker()
     def test_get_advocates__by_page(self, m):
 
         response = copy.deepcopy(adv_json)
         # Make it look like there's more data
-        response['pagination']['count'] = 100
+        response["pagination"]["count"] = 100
 
-        m.get(self.p2a.client.uri + 'advocates?page=1', json=adv_json)
-        m.get(self.p2a.client.uri + 'advocates?page=2', exc=Exception('Should only call once'))
+        m.get(self.p2a.client.uri + "advocates?page=1", json=adv_json)
+        m.get(
+            self.p2a.client.uri + "advocates?page=2",
+            exc=Exception("Should only call once"),
+        )
 
         results = self.p2a.get_advocates(page=1)
-        self.assertTrue(results['advocates'].num_rows, 1)
+        self.assertTrue(results["advocates"].num_rows, 1)
 
     @requests_mock.Mocker()
     def test_get_advocates__empty(self, m):
 
         response = copy.deepcopy(adv_json)
-        response['data'] = []
+        response["data"] = []
         # Make it look like there's more data
-        response['pagination']['count'] = 0
+        response["pagination"]["count"] = 0
 
-        m.get(self.p2a.client.uri + 'advocates', json=adv_json)
+        m.get(self.p2a.client.uri + "advocates", json=adv_json)
 
         results = self.p2a.get_advocates()
-        self.assertTrue(results['advocates'].num_rows, 0)
+        self.assertTrue(results["advocates"].num_rows, 0)
 
     @requests_mock.Mocker()
     def test_get_campaigns(self, m):
 
-        camp_exp = ['id', 'name', 'display_name', 'subtitle',
-                    'public', 'topic', 'type', 'link', 'restrict_allow',
-                    'updated_at_date', 'updated_at_timezone',
-                    'updated_at_timezone_type', 'content_background_image',
-                    'content_call_to_action', 'content_introduction',
-                    'content_summary', 'content_thank_you']
+        camp_exp = [
+            "id",
+            "name",
+            "display_name",
+            "subtitle",
+            "public",
+            "topic",
+            "type",
+            "link",
+            "restrict_allow",
+            "updated_at_date",
+            "updated_at_timezone",
+            "updated_at_timezone_type",
+            "content_background_image",
+            "content_call_to_action",
+            "content_introduction",
+            "content_summary",
+            "content_thank_you",
+        ]
 
-        m.get(self.p2a.client.uri + 'campaigns', json=camp_json)
+        m.get(self.p2a.client.uri + "campaigns", json=camp_json)
 
         self.assertTrue(validate_list(camp_exp, self.p2a.get_campaigns()))
 
     @requests_mock.Mocker()
     def test_create_advocate(self, m):
 
-        m.post(self.p2a.client.uri + 'advocates', json={'advocateid': 1})
+        m.post(self.p2a.client.uri + "advocates", json={"advocateid": 1})
 
         # Test arg validation - create requires a phone or an email
-        self.assertRaises(ValueError,
-                          lambda: self.p2a.create_advocate(campaigns=[1],
-                                                           firstname='Foo',
-                                                           lastname='bar'))
+        self.assertRaises(
+            ValueError,
+            lambda: self.p2a.create_advocate(
+                campaigns=[1], firstname="Foo", lastname="bar"
+            ),
+        )
         # Test arg validation - sms opt in requires a phone
-        self.assertRaises(ValueError,
-                          lambda: self.p2a.create_advocate(campaigns=[1],
-                                                           email='foo@bar.com',
-                                                           sms_optin=True))
+        self.assertRaises(
+            ValueError,
+            lambda: self.p2a.create_advocate(
+                campaigns=[1], email="foo@bar.com", sms_optin=True
+            ),
+        )
 
         # Test arg validation - email opt in requires a email
-        self.assertRaises(ValueError,
-                          lambda: self.p2a.create_advocate(campaigns=[1],
-                                                           phone='1234567890',
-                                                           email_optin=True))
+        self.assertRaises(
+            ValueError,
+            lambda: self.p2a.create_advocate(
+                campaigns=[1], phone="1234567890", email_optin=True
+            ),
+        )
 
         # Test a successful call
-        advocateid = self.p2a.create_advocate(campaigns=[1],
-                                              email='foo@bar.com',
-                                              email_optin=True,
-                                              firstname='Test')
+        advocateid = self.p2a.create_advocate(
+            campaigns=[1], email="foo@bar.com", email_optin=True, firstname="Test"
+        )
         self.assertTrue(m.called)
         self.assertEqual(advocateid, 1)
 
         # Check that the properties were mapped
         data = parse_request_body(m.last_request.text)
-        self.assertEqual(data['firstname'], 'Test')
-        self.assertNotIn('lastname', data)
-        self.assertEqual(data['emailOptin'], '1')
-        self.assertEqual(data['email'], 'foo%40bar.com')
+        self.assertEqual(data["firstname"], "Test")
+        self.assertNotIn("lastname", data)
+        self.assertEqual(data["emailOptin"], "1")
+        self.assertEqual(data["email"], "foo%40bar.com")
 
     @requests_mock.Mocker()
     def test_update_advocate(self, m):
 
-        m.post(self.p2a.client.uri + 'advocates')
+        m.post(self.p2a.client.uri + "advocates")
 
         # Test arg validation - sms opt in requires a phone
-        self.assertRaises(ValueError,
-                          lambda: self.p2a.update_advocate(advocate_id=1, sms_optin=True))
+        self.assertRaises(
+            ValueError, lambda: self.p2a.update_advocate(advocate_id=1, sms_optin=True)
+        )
 
         # Test arg validation - email opt in requires a email
-        self.assertRaises(ValueError,
-                          lambda: self.p2a.update_advocate(advocate_id=1, email_optin=True))
+        self.assertRaises(
+            ValueError,
+            lambda: self.p2a.update_advocate(advocate_id=1, email_optin=True),
+        )
 
         # Test a successful call
-        self.p2a.update_advocate(advocate_id=1, campaigns=[1], email='foo@bar.com',
-                                 email_optin=True, firstname='Test')
+        self.p2a.update_advocate(
+            advocate_id=1,
+            campaigns=[1],
+            email="foo@bar.com",
+            email_optin=True,
+            firstname="Test",
+        )
         self.assertTrue(m.called)
 
         # Check that the properties were mapped
         data = parse_request_body(m.last_request.text)
-        self.assertEqual(data['firstname'], 'Test')
-        self.assertNotIn('lastname', data)
-        self.assertEqual(data['emailOptin'], '1')
-        self.assertEqual(data['email'], 'foo%40bar.com')
+        self.assertEqual(data["firstname"], "Test")
+        self.assertNotIn("lastname", data)
+        self.assertEqual(data["emailOptin"], "1")
+        self.assertEqual(data["email"], "foo%40bar.com")
```

### Comparing `parsons-1.0.0/test/test_pdi/conftest.py` & `parsons-1.1.0/test/test_pdi/conftest.py`

 * *Files 10% similar despite different names*

```diff
@@ -4,32 +4,34 @@
 import pytest
 
 
 @pytest.fixture
 def live_pdi():
     # Generate a live PDI connection based on these env vars
 
-    username = os.environ['PDI_USERNAME']
-    password = os.environ['PDI_PASSWORD']
-    api_token = os.environ['PDI_API_TOKEN']
+    username = os.environ["PDI_USERNAME"]
+    password = os.environ["PDI_PASSWORD"]
+    api_token = os.environ["PDI_API_TOKEN"]
 
     pdi = PDI(username, password, api_token, qa_url=True)
 
     return pdi
 
 
 @pytest.fixture
 def mock_pdi(requests_mock):
     # Not meant to hit live api servers
 
     requests_mock.post(
-        "https://apiqa.bluevote.com/sessions", json={
+        "https://apiqa.bluevote.com/sessions",
+        json={
             "AccessToken": "AccessToken",
             "ExpirationDate": "2100-01-01",
-        })
+        },
+    )
 
     username = "PDI_USERNAME"
     password = "PDI_PASSWORD"
     api_token = "PDI_API_TOKEN"
 
     pdi = PDI(username, password, api_token, qa_url=True)
```

### Comparing `parsons-1.0.0/test/test_pdi/test_flag_ids.py` & `parsons-1.1.0/test/test_pdi/test_flag_ids.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 from test.utils import mark_live_test
 
 from parsons import Table
 
 from contextlib import contextmanager
 from requests.exceptions import HTTPError
+
 # import json
 import pytest
 
 #
 # Fixtures and constants
 #
 
@@ -54,83 +55,90 @@
 
 
 @mark_live_test
 @pytest.mark.parametrize("limit", [None, 5, 15])
 def test_get_flag_ids(live_pdi, limit):
     flag_ids = live_pdi.get_flag_ids(limit=limit)
 
-    expected_columns = [
-        "id", "flagId", "flagIdDescription", "compile", "isDefault"]
+    expected_columns = ["id", "flagId", "flagIdDescription", "compile", "isDefault"]
     expected_num_rows = limit or QA_NUM_FLAG_IDS
 
     assert isinstance(flag_ids, Table)
     assert flag_ids.columns == expected_columns
     assert flag_ids.num_rows == expected_num_rows
 
 
 @mark_live_test
 @pytest.mark.parametrize(
     "id",
-    [pytest.param(QA_REAL_FLAG_ID),
-     pytest.param(QA_INVALID_FLAG_ID, marks=[xfail_http_error]),
-     ])
+    [
+        pytest.param(QA_REAL_FLAG_ID),
+        pytest.param(QA_INVALID_FLAG_ID, marks=[xfail_http_error]),
+    ],
+)
 def test_get_flag_id(live_pdi, id):
     flag_id = live_pdi.get_flag_id(id)
 
-    expected_keys = [
-        "id", "flagId", "flagIdDescription", "compile", "isDefault"]
+    expected_keys = ["id", "flagId", "flagIdDescription", "compile", "isDefault"]
 
     assert isinstance(flag_id, dict)
     assert list(flag_id.keys()) == expected_keys
 
 
 @mark_live_test
 @pytest.mark.parametrize(
     ["flag_id", "is_default"],
-    [pytest.param(None, True, marks=[xfail_http_error]),
-     pytest.param("amm", None, marks=[xfail_http_error]),
-     pytest.param("amm", True),
-     ])
+    [
+        pytest.param(None, True, marks=[xfail_http_error]),
+        pytest.param("amm", None, marks=[xfail_http_error]),
+        pytest.param("amm", True),
+    ],
+)
 def test_create_flag_id(live_pdi, cleanup_flag_id, flag_id, is_default):
     flag_id = live_pdi.create_flag_id(flag_id, is_default)
 
     cleanup_flag_id(live_pdi, flag_id)
 
 
 @mark_live_test
 @pytest.mark.parametrize(
     ["my_flag_id"],
-    [pytest.param(None),
-     pytest.param(QA_INVALID_FLAG_ID),
-     pytest.param(QA_MALFORMED_FLAG_ID, marks=[xfail_http_error]),
-     ])
+    [
+        pytest.param(None),
+        pytest.param(QA_INVALID_FLAG_ID),
+        pytest.param(QA_MALFORMED_FLAG_ID, marks=[xfail_http_error]),
+    ],
+)
 def test_delete_flag_id(live_pdi, create_temp_flag_id, my_flag_id):
     with create_temp_flag_id(live_pdi, my_flag_id) as flag_id:
         did_delete = live_pdi.delete_flag_id(flag_id)
 
     assert did_delete
 
 
 @mark_live_test
 @pytest.mark.parametrize(
     ["my_flag_id"],
-    [pytest.param(None),
-     pytest.param(QA_INVALID_FLAG_ID, marks=[xfail_http_error]),
-     pytest.param(QA_MALFORMED_FLAG_ID, marks=[xfail_http_error]),
-     ])
+    [
+        pytest.param(None),
+        pytest.param(QA_INVALID_FLAG_ID, marks=[xfail_http_error]),
+        pytest.param(QA_MALFORMED_FLAG_ID, marks=[xfail_http_error]),
+    ],
+)
 def test_update_flag_id(live_pdi, create_temp_flag_id, my_flag_id):
     with create_temp_flag_id(live_pdi, my_flag_id) as flag_id:
         # flag initial state:
         # {"id":flag_id,"flagId":"amm","flagIdDescription":null,"compile":"","isDefault":false}  # noqa
         id = live_pdi.update_flag_id(flag_id, "bnh", True)
         assert id == flag_id
 
         expected_dict = {
             "id": flag_id,
             "flagId": "bnh",
             "flagIdDescription": None,
             "compile": "",
-            "isDefault": True}
+            "isDefault": True,
+        }
 
         flag_id_dict = live_pdi.get_flag_id(flag_id)
 
         assert expected_dict == flag_id_dict
```

### Comparing `parsons-1.0.0/test/test_pdi/test_pdi.py` & `parsons-1.1.0/test/test_pdi/test_pdi.py`

 * *Files 14% similar despite different names*

```diff
@@ -25,27 +25,33 @@
 @mark_live_test
 def test_connection():
     PDI(qa_url=True)
 
 
 @pytest.mark.parametrize(
     ["username", "password", "api_token"],
-    [(None, None,  None),
-     (None, "pass",  "token"),
-     ("user", None, "token"),
-     ("user", "pass", None),
-     ])
+    [
+        (None, None, None),
+        (None, "pass", "token"),
+        ("user", None, "token"),
+        ("user", "pass", None),
+    ],
+)
 def test_init_error(username, password, api_token):
     remove_from_env("PDI_USERNAME", "PDI_PASSWORD", "PDI_API_TOKEN")
     with pytest.raises(KeyError):
         PDI(username, password, api_token)
 
 
 @pytest.mark.parametrize(
     ["obj", "exp_obj"],
-    [({"a": "a", "b": None, "c": "c"}, {"a": "a", "c": "c"}),
-     ([{"a": "a", "b": None, "c": "c"}, {"a": "a", "c": None}],
-      [{"a": "a", "c": "c"}, {"a": "a"}]),
-     ("string", "string"),
-     ])
+    [
+        ({"a": "a", "b": None, "c": "c"}, {"a": "a", "c": "c"}),
+        (
+            [{"a": "a", "b": None, "c": "c"}, {"a": "a", "c": None}],
+            [{"a": "a", "c": "c"}, {"a": "a"}],
+        ),
+        ("string", "string"),
+    ],
+)
 def test_clean_dict(mock_pdi, obj, exp_obj):
     assert mock_pdi._clean_dict(obj) == exp_obj
```

### Comparing `parsons-1.0.0/test/test_redshift.py` & `parsons-1.1.0/test/test_redshift.py`

 * *Files 16% similar despite different names*

```diff
@@ -3,319 +3,423 @@
 import unittest
 import os
 import re
 from test.utils import validate_list
 from testfixtures import LogCapture
 
 # The name of the schema and will be temporarily created for the tests
-TEMP_SCHEMA = 'parsons_test2'
+TEMP_SCHEMA = "parsons_test2"
 
 # These tests do not interact with the Redshift Database directly, and don't need real credentials
 
 
 class TestRedshift(unittest.TestCase):
-
     def setUp(self):
 
-        self.rs = Redshift(username='test', password='test', host='test', db='test', port=123)
-
-        self.tbl = Table([['ID', 'Name'],
-                          [1, 'Jim'],
-                          [2, 'John'],
-                          [3, 'Sarah']])
-
-        self.tbl2 = Table([
-            ["c1", "c2", "c3", "c4", "c5", "c6", "c7"],
-            ["a", "", 1, "NA", 1.4, 1, 2],
-            ["b", "", 2, "NA", 1.4, 1, 2],
-            ["c", "", 3.4, "NA", "", "", "a"],
-            ["d", "", 5, "NA", 1.4, 1, 2],
-            ["e", "", 6, "NA", 1.4, 1, 2],
-            ["f", "", 7.8, "NA", 1.4, 1, 2],
-            ["g", "", 9, "NA", 1.4, 1, 2],
-        ])
+        self.rs = Redshift(
+            username="test", password="test", host="test", db="test", port=123
+        )
+
+        self.tbl = Table([["ID", "Name"], [1, "Jim"], [2, "John"], [3, "Sarah"]])
+
+        self.tbl2 = Table(
+            [
+                ["c1", "c2", "c3", "c4", "c5", "c6", "c7"],
+                ["a", "", 1, "NA", 1.4, 1, 2],
+                ["b", "", 2, "NA", 1.4, 1, 2],
+                ["c", "", 3.4, "NA", "", "", "a"],
+                ["d", "", 5, "NA", 1.4, 1, 2],
+                ["e", "", 6, "NA", 1.4, 1, 2],
+                ["f", "", 7.8, "NA", 1.4, 1, 2],
+                ["g", "", 9, "NA", 1.4, 1, 2],
+            ]
+        )
 
         self.mapping = self.rs.generate_data_types(self.tbl)
         self.rs.DO_PARSE_BOOLS = True
         self.mapping2 = self.rs.generate_data_types(self.tbl2)
         self.rs.DO_PARSE_BOOLS = False
         self.mapping3 = self.rs.generate_data_types(self.tbl2)
 
     def test_split_full_table_name(self):
-        schema, table = Redshift.split_full_table_name('some_schema.some_table')
-        self.assertEqual(schema, 'some_schema')
-        self.assertEqual(table, 'some_table')
+        schema, table = Redshift.split_full_table_name("some_schema.some_table")
+        self.assertEqual(schema, "some_schema")
+        self.assertEqual(table, "some_table")
 
         # When missing the schema
-        schema, table = Redshift.split_full_table_name('some_table')
-        self.assertEqual(schema, 'public')
-        self.assertEqual(table, 'some_table')
+        schema, table = Redshift.split_full_table_name("some_table")
+        self.assertEqual(schema, "public")
+        self.assertEqual(table, "some_table")
 
         # When there are too many parts
-        self.assertRaises(ValueError, Redshift.split_full_table_name, 'a.b.c')
+        self.assertRaises(ValueError, Redshift.split_full_table_name, "a.b.c")
 
     def test_combine_schema_and_table_name(self):
-        full_table_name = Redshift.combine_schema_and_table_name('some_schema', 'some_table')
-        self.assertEqual(full_table_name, 'some_schema.some_table')
+        full_table_name = Redshift.combine_schema_and_table_name(
+            "some_schema", "some_table"
+        )
+        self.assertEqual(full_table_name, "some_schema.some_table")
 
     def test_data_type(self):
 
         # Test bool
         self.rs.DO_PARSE_BOOLS = True
-        self.assertEqual(self.rs.data_type(1, ''), 'bool')
-        self.assertEqual(self.rs.data_type(True, ''), 'bool')
+        self.assertEqual(self.rs.data_type(1, ""), "bool")
+        self.assertEqual(self.rs.data_type(True, ""), "bool")
         self.rs.DO_PARSE_BOOLS = False
-        self.assertEqual(self.rs.data_type(1, ''), 'int')
-        self.assertEqual(self.rs.data_type(True, ''), 'varchar')
+        self.assertEqual(self.rs.data_type(1, ""), "int")
+        self.assertEqual(self.rs.data_type(True, ""), "varchar")
         # Test smallint
         # Currently smallints are coded as ints
-        self.assertEqual(self.rs.data_type(2, ''), 'int')
+        self.assertEqual(self.rs.data_type(2, ""), "int")
         # Test int
-        self.assertEqual(self.rs.data_type(32769, ''), 'int')
+        self.assertEqual(self.rs.data_type(32769, ""), "int")
         # Test bigint
-        self.assertEqual(self.rs.data_type(2147483648, ''), 'bigint')
+        self.assertEqual(self.rs.data_type(2147483648, ""), "bigint")
         # Test varchar that looks like an int
-        self.assertEqual(self.rs.data_type('00001', ''), 'varchar')
+        self.assertEqual(self.rs.data_type("00001", ""), "varchar")
         # Test a float as a float
-        self.assertEqual(self.rs.data_type(5.001, ''), 'float')
+        self.assertEqual(self.rs.data_type(5.001, ""), "float")
         # Test varchar
-        self.assertEqual(self.rs.data_type('word', ''), 'varchar')
+        self.assertEqual(self.rs.data_type("word", ""), "varchar")
         # Test int with underscore
-        self.assertEqual(self.rs.data_type('1_2', ''), 'varchar')
+        self.assertEqual(self.rs.data_type("1_2", ""), "varchar")
         # Test int with leading zero
-        self.assertEqual(self.rs.data_type('01', ''), 'varchar')
+        self.assertEqual(self.rs.data_type("01", ""), "varchar")
 
     def test_generate_data_types(self):
 
         # Test correct header labels
-        self.assertEqual(self.mapping['headers'], ['ID', 'Name'])
+        self.assertEqual(self.mapping["headers"], ["ID", "Name"])
         # Test correct data types
-        self.assertEqual(self.mapping['type_list'], ['int', 'varchar'])
+        self.assertEqual(self.mapping["type_list"], ["int", "varchar"])
 
         self.assertEqual(
-            self.mapping2['type_list'],
-            ['varchar', 'varchar', 'float', 'varchar', 'float', 'bool', 'varchar'])
+            self.mapping2["type_list"],
+            ["varchar", "varchar", "float", "varchar", "float", "bool", "varchar"],
+        )
 
         self.assertEqual(
-            self.mapping3['type_list'],
-            ['varchar', 'varchar', 'float', 'varchar', 'float', 'int', 'varchar'])
+            self.mapping3["type_list"],
+            ["varchar", "varchar", "float", "varchar", "float", "int", "varchar"],
+        )
         # Test correct lengths
-        self.assertEqual(self.mapping['longest'], [1, 5])
+        self.assertEqual(self.mapping["longest"], [1, 5])
 
     def test_vc_padding(self):
 
         # Test padding calculated correctly
-        self.assertEqual(self.rs.vc_padding(self.mapping, .2), [1, 6])
+        self.assertEqual(self.rs.vc_padding(self.mapping, 0.2), [1, 6])
 
     def test_vc_max(self):
 
         # Test max sets it to the max
-        self.assertEqual(self.rs.vc_max(self.mapping, ['Name']), [1, 65535])
+        self.assertEqual(self.rs.vc_max(self.mapping, ["Name"]), [1, 65535])
 
         # Test raises when can't find column
         # To Do
 
     def test_vc_validate(self):
 
         # Test that a column with a width of 0 is set to 1
-        self.mapping['longest'][0] = 0
+        self.mapping["longest"][0] = 0
         self.mapping = self.rs.vc_validate(self.mapping)
         self.assertEqual(self.mapping, [1, 5])
 
     def test_create_sql(self):
 
         # Test the the statement is expected
-        sql = self.rs.create_sql('tmc.test', self.mapping, distkey='ID')
-        exp_sql = "create table tmc.test (\n  id int,\n  name varchar(5)) \ndistkey(ID) ;"
+        sql = self.rs.create_sql("tmc.test", self.mapping, distkey="ID")
+        exp_sql = (
+            "create table tmc.test (\n  id int,\n  name varchar(5)) \ndistkey(ID) ;"
+        )
         self.assertEqual(sql, exp_sql)
 
     def test_compound_sortkey(self):
         # check single sortkey formatting
-        sql = self.rs.create_sql('tmc.test', self.mapping, sortkey='ID')
-        exp_sql = "create table tmc.test (\n  id int,\n  name varchar(5)) \nsortkey(ID);"
+        sql = self.rs.create_sql("tmc.test", self.mapping, sortkey="ID")
+        exp_sql = (
+            "create table tmc.test (\n  id int,\n  name varchar(5)) \nsortkey(ID);"
+        )
         self.assertEqual(sql, exp_sql)
 
         # check compound sortkey formatting
-        sql = self.rs.create_sql('tmc.test', self.mapping, sortkey=['ID1', 'ID2'])
+        sql = self.rs.create_sql("tmc.test", self.mapping, sortkey=["ID1", "ID2"])
         exp_sql = "create table tmc.test (\n  id int,\n  name varchar(5))"
         exp_sql += " \ncompound sortkey(ID1, ID2);"
         self.assertEqual(sql, exp_sql)
 
     def test_column_validate(self):
 
-        bad_cols = ['a', 'a', '', 'SELECT', 'asdfjkasjdfklasjdfklajskdfljaskldfjaklsdfjlaksdfjklasj'
-                    'dfklasjdkfljaskldfljkasjdkfasjlkdfjklasdfjklakjsfasjkdfljaslkdfjklasdfjklasjkl'
-                    'dfakljsdfjalsdkfjklasjdfklasjdfklasdkljf']
+        bad_cols = [
+            "a",
+            "a",
+            "",
+            "SELECT",
+            "asdfjkasjdfklasjdfklajskdfljaskldfjaklsdfjlaksdfjklasj"
+            "dfklasjdkfljaskldfljkasjdkfasjlkdfjklasdfjklakjsfasjkdfljaslkdfjklasdfjklasjkl"
+            "dfakljsdfjalsdkfjklasjdfklasjdfklasdkljf",
+        ]
         fixed_cols = [
-            'a', 'a_1', 'col_2', 'col_3', 'asdfjkasjdfklasjdfklajskdfljaskldfjaklsdfjlaks'
-            'dfjklasjdfklasjdkfljaskldfljkasjdkfasjlkdfjklasdfjklakjsfasjkdfljaslkdfjkl']
+            "a",
+            "a_1",
+            "col_2",
+            "col_3",
+            "asdfjkasjdfklasjdfklajskdfljaskldfjaklsdfjlaks"
+            "dfjklasjdfklasjdkfljaskldfljkasjdkfasjlkdfjklasdfjklakjsfasjkdfljaslkdfjkl",
+        ]
         self.assertEqual(self.rs.column_name_validate(bad_cols), fixed_cols)
 
     def test_create_statement(self):
 
         # Assert that copy statement is expected
-        sql = self.rs.create_statement(self.tbl, 'tmc.test', distkey='ID')
+        sql = self.rs.create_statement(self.tbl, "tmc.test", distkey="ID")
         exp_sql = """create table tmc.test (\n  "id" int,\n  "name" varchar(5)) \ndistkey(ID) ;"""  # noqa: E501
         self.assertEqual(sql, exp_sql)
 
         # Assert that an error is raised by an empty table
-        empty_table = Table([['Col_1', 'Col_2']])
-        self.assertRaises(ValueError, self.rs.create_statement, empty_table, 'tmc.test')
+        empty_table = Table([["Col_1", "Col_2"]])
+        self.assertRaises(ValueError, self.rs.create_statement, empty_table, "tmc.test")
 
     def test_get_creds_kwargs(self):
 
         # Test passing kwargs
-        creds = self.rs.get_creds('kwarg_key', 'kwarg_secret_key')
+        creds = self.rs.get_creds("kwarg_key", "kwarg_secret_key")
         expected = """credentials 'aws_access_key_id=kwarg_key;aws_secret_access_key=kwarg_secret_key'\n"""  # noqa: E501
         self.assertEqual(creds, expected)
 
         # Test grabbing from environmental variables
-        prior_aws_access_key_id = os.environ.get('AWS_ACCESS_KEY_ID', '')
-        prior_aws_secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY', '')
-        os.environ['AWS_ACCESS_KEY_ID'] = 'env_key'
-        os.environ['AWS_SECRET_ACCESS_KEY'] = 'env_secret_key'
+        prior_aws_access_key_id = os.environ.get("AWS_ACCESS_KEY_ID", "")
+        prior_aws_secret_access_key = os.environ.get("AWS_SECRET_ACCESS_KEY", "")
+        os.environ["AWS_ACCESS_KEY_ID"] = "env_key"
+        os.environ["AWS_SECRET_ACCESS_KEY"] = "env_secret_key"
         creds = self.rs.get_creds(None, None)
         expected = """credentials 'aws_access_key_id=env_key;aws_secret_access_key=env_secret_key'\n"""  # noqa: E501
         self.assertEqual(creds, expected)
 
         # Reset env vars
-        os.environ['AWS_ACCESS_KEY_ID'] = prior_aws_access_key_id
-        os.environ['AWS_SECRET_ACCESS_KEY'] = prior_aws_secret_access_key
+        os.environ["AWS_ACCESS_KEY_ID"] = prior_aws_access_key_id
+        os.environ["AWS_SECRET_ACCESS_KEY"] = prior_aws_secret_access_key
 
     def scrub_copy_tokens(self, s):
 
-        s = re.sub('=.+;', '=*HIDDEN*;', s)
-        s = re.sub('aws_secret_access_key=.+\'',
-                   'aws_secret_access_key=*HIDDEN*\'', s)
+        s = re.sub("=.+;", "=*HIDDEN*;", s)
+        s = re.sub("aws_secret_access_key=.+'", "aws_secret_access_key=*HIDDEN*'", s)
         return s
 
     def test_copy_statement_default(self):
 
-        sql = self.rs.copy_statement('test_schema.test', 'buck', 'file.csv',
-                                     aws_access_key_id='abc123',
-                                     aws_secret_access_key='abc123',
-                                     bucket_region='us-east-2')
+        sql = self.rs.copy_statement(
+            "test_schema.test",
+            "buck",
+            "file.csv",
+            aws_access_key_id="abc123",
+            aws_secret_access_key="abc123",
+            bucket_region="us-east-2",
+        )
 
         # Scrub the keys
-        sql = re.sub(r'id=.+;', '*id=HIDDEN*;', re.sub(r"key=.+'", "key=*HIDDEN*'", sql))
-
-        expected_options = ['ignoreheader 1', 'acceptanydate',
-                            "dateformat 'auto'", "timeformat 'auto'", "csv delimiter ','",
-                            "copy test_schema.test \nfrom 's3://buck/file.csv'",
-                            "'aws_access_key_*id=HIDDEN*;aws_secret_access_key=*HIDDEN*'",
-                            "region 'us-east-2'", 'emptyasnull', 'blanksasnull',
-                            'acceptinvchars']
+        sql = re.sub(
+            r"id=.+;", "*id=HIDDEN*;", re.sub(r"key=.+'", "key=*HIDDEN*'", sql)
+        )
+
+        expected_options = [
+            "ignoreheader 1",
+            "acceptanydate",
+            "dateformat 'auto'",
+            "timeformat 'auto'",
+            "csv delimiter ','",
+            "copy test_schema.test \nfrom 's3://buck/file.csv'",
+            "'aws_access_key_*id=HIDDEN*;aws_secret_access_key=*HIDDEN*'",
+            "region 'us-east-2'",
+            "emptyasnull",
+            "blanksasnull",
+            "acceptinvchars",
+        ]
 
         # Check that all of the expected options are there:
         [self.assertNotEqual(sql.find(o), -1, o) for o in expected_options]
 
     def test_copy_statement_statupdate(self):
 
         sql = self.rs.copy_statement(
-            'test_schema.test', 'buck', 'file.csv',
-            aws_access_key_id='abc123', aws_secret_access_key='abc123', statupdate=True)
+            "test_schema.test",
+            "buck",
+            "file.csv",
+            aws_access_key_id="abc123",
+            aws_secret_access_key="abc123",
+            statupdate=True,
+        )
 
         # Scrub the keys
-        sql = re.sub(r'id=.+;', '*id=HIDDEN*;', re.sub(r"key=.+'", "key=*HIDDEN*'", sql))
-
-        expected_options = ["statupdate on", 'ignoreheader 1', 'acceptanydate',
-                            "dateformat 'auto'", "timeformat 'auto'", "csv delimiter ','",
-                            "copy test_schema.test \nfrom 's3://buck/file.csv'",
-                            "'aws_access_key_*id=HIDDEN*;aws_secret_access_key=*HIDDEN*'",
-                            'emptyasnull', 'blanksasnull', 'acceptinvchars']
+        sql = re.sub(
+            r"id=.+;", "*id=HIDDEN*;", re.sub(r"key=.+'", "key=*HIDDEN*'", sql)
+        )
+
+        expected_options = [
+            "statupdate on",
+            "ignoreheader 1",
+            "acceptanydate",
+            "dateformat 'auto'",
+            "timeformat 'auto'",
+            "csv delimiter ','",
+            "copy test_schema.test \nfrom 's3://buck/file.csv'",
+            "'aws_access_key_*id=HIDDEN*;aws_secret_access_key=*HIDDEN*'",
+            "emptyasnull",
+            "blanksasnull",
+            "acceptinvchars",
+        ]
 
         # Check that all of the expected options are there:
         [self.assertNotEqual(sql.find(o), -1) for o in expected_options]
 
         sql2 = self.rs.copy_statement(
-            'test_schema.test', 'buck', 'file.csv',
-            aws_access_key_id='abc123', aws_secret_access_key='abc123', statupdate=False)
+            "test_schema.test",
+            "buck",
+            "file.csv",
+            aws_access_key_id="abc123",
+            aws_secret_access_key="abc123",
+            statupdate=False,
+        )
 
         # Scrub the keys
-        sql2 = re.sub(r'id=.+;', '*id=HIDDEN*;', re.sub(r"key=.+'", "key=*HIDDEN*'", sql2))
-
-        expected_options = ["statupdate off", 'ignoreheader 1', 'acceptanydate',
-                            "dateformat 'auto'", "timeformat 'auto'", "csv delimiter ','",
-                            "copy test_schema.test \nfrom 's3://buck/file.csv'",
-                            "'aws_access_key_*id=HIDDEN*;aws_secret_access_key=*HIDDEN*'",
-                            'emptyasnull', 'blanksasnull', 'acceptinvchars']
+        sql2 = re.sub(
+            r"id=.+;", "*id=HIDDEN*;", re.sub(r"key=.+'", "key=*HIDDEN*'", sql2)
+        )
+
+        expected_options = [
+            "statupdate off",
+            "ignoreheader 1",
+            "acceptanydate",
+            "dateformat 'auto'",
+            "timeformat 'auto'",
+            "csv delimiter ','",
+            "copy test_schema.test \nfrom 's3://buck/file.csv'",
+            "'aws_access_key_*id=HIDDEN*;aws_secret_access_key=*HIDDEN*'",
+            "emptyasnull",
+            "blanksasnull",
+            "acceptinvchars",
+        ]
 
         # Check that all of the expected options are there:
         [self.assertNotEqual(sql2.find(o), -1) for o in expected_options]
 
     def test_copy_statement_compupdate(self):
 
         sql = self.rs.copy_statement(
-            'test_schema.test', 'buck', 'file.csv',
-            aws_access_key_id='abc123', aws_secret_access_key='abc123', compupdate=True)
+            "test_schema.test",
+            "buck",
+            "file.csv",
+            aws_access_key_id="abc123",
+            aws_secret_access_key="abc123",
+            compupdate=True,
+        )
 
         # Scrub the keys
-        sql = re.sub(r'id=.+;', '*id=HIDDEN*;', re.sub(r"key=.+'", "key=*HIDDEN*'", sql))
-
-        expected_options = ["compupdate on", 'ignoreheader 1', 'acceptanydate',
-                            "dateformat 'auto'", "timeformat 'auto'", "csv delimiter ','",
-                            "copy test_schema.test \nfrom 's3://buck/file.csv'",
-                            "'aws_access_key_*id=HIDDEN*;aws_secret_access_key=*HIDDEN*'",
-                            'emptyasnull', 'blanksasnull', 'acceptinvchars']
+        sql = re.sub(
+            r"id=.+;", "*id=HIDDEN*;", re.sub(r"key=.+'", "key=*HIDDEN*'", sql)
+        )
+
+        expected_options = [
+            "compupdate on",
+            "ignoreheader 1",
+            "acceptanydate",
+            "dateformat 'auto'",
+            "timeformat 'auto'",
+            "csv delimiter ','",
+            "copy test_schema.test \nfrom 's3://buck/file.csv'",
+            "'aws_access_key_*id=HIDDEN*;aws_secret_access_key=*HIDDEN*'",
+            "emptyasnull",
+            "blanksasnull",
+            "acceptinvchars",
+        ]
 
         # Check that all of the expected options are there:
         [self.assertNotEqual(sql.find(o), -1) for o in expected_options]
 
         sql2 = self.rs.copy_statement(
-            'test_schema.test', 'buck', 'file.csv',
-            aws_access_key_id='abc123', aws_secret_access_key='abc123', compupdate=False)
+            "test_schema.test",
+            "buck",
+            "file.csv",
+            aws_access_key_id="abc123",
+            aws_secret_access_key="abc123",
+            compupdate=False,
+        )
 
         # Scrub the keys
-        sql2 = re.sub(r'id=.+;', '*id=HIDDEN*;', re.sub(r"key=.+'", "key=*HIDDEN*'", sql2))
-
-        expected_options = ["compupdate off", 'ignoreheader 1', 'acceptanydate',
-                            "dateformat 'auto'", "timeformat 'auto'", "csv delimiter ','",
-                            "copy test_schema.test \nfrom 's3://buck/file.csv'",
-                            "'aws_access_key_*id=HIDDEN*;aws_secret_access_key=*HIDDEN*'",
-                            'emptyasnull', 'blanksasnull', 'acceptinvchars']
+        sql2 = re.sub(
+            r"id=.+;", "*id=HIDDEN*;", re.sub(r"key=.+'", "key=*HIDDEN*'", sql2)
+        )
+
+        expected_options = [
+            "compupdate off",
+            "ignoreheader 1",
+            "acceptanydate",
+            "dateformat 'auto'",
+            "timeformat 'auto'",
+            "csv delimiter ','",
+            "copy test_schema.test \nfrom 's3://buck/file.csv'",
+            "'aws_access_key_*id=HIDDEN*;aws_secret_access_key=*HIDDEN*'",
+            "emptyasnull",
+            "blanksasnull",
+            "acceptinvchars",
+        ]
 
         # Check that all of the expected options are there:
         [self.assertNotEqual(sql2.find(o), -1) for o in expected_options]
 
     def test_copy_statement_columns(self):
 
-        cols = ['a', 'b', 'c']
+        cols = ["a", "b", "c"]
 
         sql = self.rs.copy_statement(
-            'test_schema.test', 'buck', 'file.csv',
-            aws_access_key_id='abc123', aws_secret_access_key='abc123', specifycols=cols)
+            "test_schema.test",
+            "buck",
+            "file.csv",
+            aws_access_key_id="abc123",
+            aws_secret_access_key="abc123",
+            specifycols=cols,
+        )
 
         # Scrub the keys
-        sql = re.sub(r'id=.+;', '*id=HIDDEN*;', re.sub(r"key=.+'", "key=*HIDDEN*'", sql))
-
-        expected_options = ['ignoreheader 1', 'acceptanydate',
-                            "dateformat 'auto'", "timeformat 'auto'", "csv delimiter ','",
-                            "copy test_schema.test(a, b, c) \nfrom 's3://buck/file.csv'",
-                            "'aws_access_key_*id=HIDDEN*;aws_secret_access_key=*HIDDEN*'",
-                            'emptyasnull', 'blanksasnull', 'acceptinvchars']
+        sql = re.sub(
+            r"id=.+;", "*id=HIDDEN*;", re.sub(r"key=.+'", "key=*HIDDEN*'", sql)
+        )
+
+        expected_options = [
+            "ignoreheader 1",
+            "acceptanydate",
+            "dateformat 'auto'",
+            "timeformat 'auto'",
+            "csv delimiter ','",
+            "copy test_schema.test(a, b, c) \nfrom 's3://buck/file.csv'",
+            "'aws_access_key_*id=HIDDEN*;aws_secret_access_key=*HIDDEN*'",
+            "emptyasnull",
+            "blanksasnull",
+            "acceptinvchars",
+        ]
 
         # Check that all of the expected options are there:
         [self.assertNotEqual(sql.find(o), -1) for o in expected_options]
 
+
 # These tests interact directly with the Redshift database
 
 
-@unittest.skipIf(not os.environ.get('LIVE_TEST'), 'Skipping because not running live test')
+@unittest.skipIf(
+    not os.environ.get("LIVE_TEST"), "Skipping because not running live test"
+)
 class TestRedshiftDB(unittest.TestCase):
-
     def setUp(self):
 
         self.temp_schema = TEMP_SCHEMA
 
         self.rs = Redshift()
 
-        self.tbl = Table([['ID', 'Name'],
-                          [1, 'Jim'],
-                          [2, 'John'],
-                          [3, 'Sarah']])
+        self.tbl = Table([["ID", "Name"], [1, "Jim"], [2, "John"], [3, "Sarah"]])
 
         # Create a schema, create a table, create a view
         setup_sql = f"""
                     drop schema if exists {self.temp_schema} cascade;
                     create schema {self.temp_schema};
                     """
 
@@ -328,16 +432,16 @@
 
         self.rs.query(setup_sql)
 
         self.rs.query(other_sql)
 
         self.s3 = S3()
 
-        self.temp_s3_bucket = os.environ['S3_TEMP_BUCKET']
-        self.temp_s3_prefix = 'test/'
+        self.temp_s3_bucket = os.environ["S3_TEMP_BUCKET"]
+        self.temp_s3_prefix = "test/"
 
     def tearDown(self):
 
         # Drop the view, the table and the schema
         teardown_sql = f"""
                        drop schema if exists {self.temp_schema} cascade;
                        """
@@ -346,54 +450,56 @@
         # Remove all test objects from S3
         for key in self.s3.list_keys(self.temp_s3_bucket, self.temp_s3_prefix):
             self.s3.remove_file(self.temp_s3_bucket, key)
 
     def test_query(self):
 
         # Check that query sending back expected result
-        r = self.rs.query('select 1')
-        self.assertEqual(r[0]['?column?'], 1)
+        r = self.rs.query("select 1")
+        self.assertEqual(r[0]["?column?"], 1)
 
     def test_query_with_parameters(self):
         table_name = f"{self.temp_schema}.test"
-        self.tbl.to_redshift(table_name, if_exists='append')
+        self.tbl.to_redshift(table_name, if_exists="append")
 
         sql = f"select * from {table_name} where name = %s"
-        name = 'Sarah'
+        name = "Sarah"
         r = self.rs.query(sql, parameters=[name])
-        self.assertEqual(r[0]['name'], name)
+        self.assertEqual(r[0]["name"], name)
 
         sql = f"select * from {table_name} where name in (%s, %s)"
-        names = ['Sarah', 'John']
+        names = ["Sarah", "John"]
         r = self.rs.query(sql, parameters=names)
         self.assertEqual(r.num_rows, 2)
 
     def test_schema_exists(self):
         self.assertTrue(self.rs.schema_exists(self.temp_schema))
-        self.assertFalse(self.rs.schema_exists('nonsense'))
+        self.assertFalse(self.rs.schema_exists("nonsense"))
 
     def test_table_exists(self):
 
         # Check if table_exists finds a table that exists
-        self.assertTrue(self.rs.table_exists(f'{self.temp_schema}.test'))
+        self.assertTrue(self.rs.table_exists(f"{self.temp_schema}.test"))
 
         # Check if table_exists is case insensitive
-        self.assertTrue(self.rs.table_exists(f'{self.temp_schema.upper()}.TEST'))
+        self.assertTrue(self.rs.table_exists(f"{self.temp_schema.upper()}.TEST"))
 
         # Check if table_exists doesn't find a table that doesn't exists
-        self.assertFalse(self.rs.table_exists(f'{self.temp_schema}.test_fake'))
+        self.assertFalse(self.rs.table_exists(f"{self.temp_schema}.test_fake"))
 
         # Check if table_exists finds a table that exists
-        self.assertTrue(self.rs.table_exists(f'{self.temp_schema}.test_view'))
+        self.assertTrue(self.rs.table_exists(f"{self.temp_schema}.test_view"))
 
         # Check if table_exists doesn't find a view that doesn't exists
-        self.assertFalse(self.rs.table_exists(f'{self.temp_schema}.test_view_fake'))
+        self.assertFalse(self.rs.table_exists(f"{self.temp_schema}.test_view_fake"))
 
         # Check that the view kwarg works
-        self.assertFalse(self.rs.table_exists(f'{self.temp_schema}.test_view', view=False))
+        self.assertFalse(
+            self.rs.table_exists(f"{self.temp_schema}.test_view", view=False)
+        )
 
     def test_temp_s3_create(self):
 
         key = self.rs.temp_s3_copy(self.tbl)
 
         # Test that you can get the object
         self.s3.get_file(self.temp_s3_bucket, key)
@@ -405,320 +511,449 @@
 
         # To Do
         pass
 
     def test_copy(self):
 
         # Copy a table
-        self.rs.copy(self.tbl, f'{self.temp_schema}.test_copy', if_exists='drop')
+        self.rs.copy(self.tbl, f"{self.temp_schema}.test_copy", if_exists="drop")
 
         # Test that file exists
-        r = self.rs.query(f"select * from {self.temp_schema}.test_copy where name='Jim'")
-        self.assertEqual(r[0]['id'], 1)
+        r = self.rs.query(
+            f"select * from {self.temp_schema}.test_copy where name='Jim'"
+        )
+        self.assertEqual(r[0]["id"], 1)
 
         # Copy to the same table, to verify that the "truncate" flag works.
-        self.rs.copy(self.tbl, f'{self.temp_schema}.test_copy', if_exists='truncate')
+        self.rs.copy(self.tbl, f"{self.temp_schema}.test_copy", if_exists="truncate")
         rows = self.rs.query(f"select count(*) from {self.temp_schema}.test_copy")
-        self.assertEqual(rows[0]['count'], 3)
+        self.assertEqual(rows[0]["count"], 3)
 
         # Copy to the same table, to verify that the "drop" flag works.
-        self.rs.copy(self.tbl, f'{self.temp_schema}.test_copy', if_exists='drop')
+        self.rs.copy(self.tbl, f"{self.temp_schema}.test_copy", if_exists="drop")
 
         # Verify that a warning message prints when a DIST/SORT key is omitted
         with LogCapture() as lc:
             self.rs.copy(
-                self.tbl, f'{self.temp_schema}.test_copy', if_exists='drop', sortkey='Name')
-            desired_log = [log for log in lc.records if "optimize your queries" in log.msg][0]
+                self.tbl,
+                f"{self.temp_schema}.test_copy",
+                if_exists="drop",
+                sortkey="Name",
+            )
+            desired_log = [
+                log for log in lc.records if "optimize your queries" in log.msg
+            ][0]
             self.assertTrue("DIST" in desired_log.msg)
             self.assertFalse("SORT" in desired_log.msg)
 
     def test_upsert(self):
 
         # Create a target table when no target table exists
-        self.rs.upsert(self.tbl, f'{self.temp_schema}.test_copy', 'ID')
+        self.rs.upsert(self.tbl, f"{self.temp_schema}.test_copy", "ID")
 
         # Run upsert
-        upsert_tbl = Table([['id', 'name'], [1, 'Jane'], [5, 'Bob']])
-        self.rs.upsert(upsert_tbl, f'{self.temp_schema}.test_copy', 'ID')
+        upsert_tbl = Table([["id", "name"], [1, "Jane"], [5, "Bob"]])
+        self.rs.upsert(upsert_tbl, f"{self.temp_schema}.test_copy", "ID")
 
         # Make sure that it is the expected table
-        expected_tbl = Table([['id', 'name'], [1, 'Jane'], [2, 'John'], [3, 'Sarah'], [5, 'Bob']])
-        updated_tbl = self.rs.query(f'select * from {self.temp_schema}.test_copy order by id;')
+        expected_tbl = Table(
+            [["id", "name"], [1, "Jane"], [2, "John"], [3, "Sarah"], [5, "Bob"]]
+        )
+        updated_tbl = self.rs.query(
+            f"select * from {self.temp_schema}.test_copy order by id;"
+        )
         assert_matching_tables(expected_tbl, updated_tbl)
 
         # Try to run it with a bad primary key
         self.rs.query(f"INSERT INTO {self.temp_schema}.test_copy VALUES (1, 'Jim')")
         self.assertRaises(
-            ValueError, self.rs.upsert, upsert_tbl, f'{self.temp_schema}.test_copy', 'ID')
+            ValueError,
+            self.rs.upsert,
+            upsert_tbl,
+            f"{self.temp_schema}.test_copy",
+            "ID",
+        )
 
         # Now try and upsert using two primary keys
-        upsert_tbl = Table([['id', 'name'], [1, 'Jane']])
-        self.rs.upsert(upsert_tbl, f'{self.temp_schema}.test_copy', ['id', 'name'])
+        upsert_tbl = Table([["id", "name"], [1, "Jane"]])
+        self.rs.upsert(upsert_tbl, f"{self.temp_schema}.test_copy", ["id", "name"])
 
         # Make sure our table looks like we expect
-        expected_tbl = Table([['id', 'name'],
-                              [2, 'John'], [3, 'Sarah'], [5, 'Bob'], [1, 'Jim'], [1, 'Jane']])
-        updated_tbl = self.rs.query(f'select * from {self.temp_schema}.test_copy order by id;')
+        expected_tbl = Table(
+            [
+                ["id", "name"],
+                [2, "John"],
+                [3, "Sarah"],
+                [5, "Bob"],
+                [1, "Jim"],
+                [1, "Jane"],
+            ]
+        )
+        updated_tbl = self.rs.query(
+            f"select * from {self.temp_schema}.test_copy order by id;"
+        )
         assert_matching_tables(expected_tbl, updated_tbl)
 
         # Try to run it with a bad primary key
         self.rs.query(f"INSERT INTO {self.temp_schema}.test_copy VALUES (1, 'Jim')")
-        self.assertRaises(ValueError, self.rs.upsert, upsert_tbl, f'{self.temp_schema}.test_copy',
-                          ['ID', 'name'])
+        self.assertRaises(
+            ValueError,
+            self.rs.upsert,
+            upsert_tbl,
+            f"{self.temp_schema}.test_copy",
+            ["ID", "name"],
+        )
 
-        self.rs.query(f'truncate table {self.temp_schema}.test_copy')
+        self.rs.query(f"truncate table {self.temp_schema}.test_copy")
 
         # Run upsert with nonmatching datatypes
-        upsert_tbl = Table([['id', 'name'], [3, 600],
-                            [6, 9999]])
-        self.rs.upsert(upsert_tbl, f'{self.temp_schema}.test_copy', 'ID')
+        upsert_tbl = Table([["id", "name"], [3, 600], [6, 9999]])
+        self.rs.upsert(upsert_tbl, f"{self.temp_schema}.test_copy", "ID")
 
         # Make sure our table looks like we expect
-        expected_tbl = Table([['id', 'name'],
-                              [3, '600'],
-                              [6, '9999']])
-        updated_tbl = self.rs.query(f'select * from {self.temp_schema}.test_copy order by id;')
+        expected_tbl = Table([["id", "name"], [3, "600"], [6, "9999"]])
+        updated_tbl = self.rs.query(
+            f"select * from {self.temp_schema}.test_copy order by id;"
+        )
         assert_matching_tables(expected_tbl, updated_tbl)
 
         # Run upsert requiring column resize
-        upsert_tbl = Table([['id', 'name'], [7, 'this name is very long']])
-        self.rs.upsert(upsert_tbl, f'{self.temp_schema}.test_copy', 'ID')
+        upsert_tbl = Table([["id", "name"], [7, "this name is very long"]])
+        self.rs.upsert(upsert_tbl, f"{self.temp_schema}.test_copy", "ID")
 
         # Make sure our table looks like we expect
-        expected_tbl = Table([['id', 'name'],
-                              [3, '600'],
-                              [6, '9999'],
-                              [7, 'this name is very long']])
-        updated_tbl = self.rs.query(f'select * from {self.temp_schema}.test_copy order by id;')
+        expected_tbl = Table(
+            [["id", "name"], [3, "600"], [6, "9999"], [7, "this name is very long"]]
+        )
+        updated_tbl = self.rs.query(
+            f"select * from {self.temp_schema}.test_copy order by id;"
+        )
         assert_matching_tables(expected_tbl, updated_tbl)
 
     def test_unload(self):
 
         # Copy a table to Redshift
-        self.rs.copy(self.tbl, f'{self.temp_schema}.test_copy', if_exists='drop')
+        self.rs.copy(self.tbl, f"{self.temp_schema}.test_copy", if_exists="drop")
 
         # Unload a table to S3
-        self.rs.unload(f'select * from {self.temp_schema}.test_copy',
-                       self.temp_s3_bucket,
-                       'unload_test')
+        self.rs.unload(
+            f"select * from {self.temp_schema}.test_copy",
+            self.temp_s3_bucket,
+            "unload_test",
+        )
 
         # Check that files are there
-        self.assertTrue(self.s3.key_exists(self.temp_s3_bucket, 'unload_test'))
+        self.assertTrue(self.s3.key_exists(self.temp_s3_bucket, "unload_test"))
 
     def test_to_from_redshift(self):
 
         # Test the parsons table methods
-        table_name = f'{self.temp_schema}.test_copy'
-        self.tbl.to_redshift(table_name, if_exists='drop')
+        table_name = f"{self.temp_schema}.test_copy"
+        self.tbl.to_redshift(table_name, if_exists="drop")
         sql = f"SELECT * FROM {table_name} ORDER BY id"
         result_tbl = Table.from_redshift(sql)
         # Don't bother checking columns names, since those were tweaked en route to Redshift.
         assert_matching_tables(self.tbl, result_tbl, ignore_headers=True)
 
     def test_generate_manifest(self):
 
         # Add some tables to buckets
-        self.tbl.to_s3_csv(self.temp_s3_bucket, f'{self.temp_s3_prefix}test_file_01.csv')
-        self.tbl.to_s3_csv(self.temp_s3_bucket, f'{self.temp_s3_prefix}test_file_02.csv')
-        self.tbl.to_s3_csv(self.temp_s3_bucket, f'{self.temp_s3_prefix}test_file_03.csv')
-        self.tbl.to_s3_csv(self.temp_s3_bucket, f'{self.temp_s3_prefix}dont_include.csv')
+        self.tbl.to_s3_csv(
+            self.temp_s3_bucket, f"{self.temp_s3_prefix}test_file_01.csv"
+        )
+        self.tbl.to_s3_csv(
+            self.temp_s3_bucket, f"{self.temp_s3_prefix}test_file_02.csv"
+        )
+        self.tbl.to_s3_csv(
+            self.temp_s3_bucket, f"{self.temp_s3_prefix}test_file_03.csv"
+        )
+        self.tbl.to_s3_csv(
+            self.temp_s3_bucket, f"{self.temp_s3_prefix}dont_include.csv"
+        )
 
         # Copy in a table to generate the headers and table
-        self.rs.copy(self.tbl, f'{self.temp_schema}.test_copy', if_exists='drop')
+        self.rs.copy(self.tbl, f"{self.temp_schema}.test_copy", if_exists="drop")
 
         # Generate the manifest
-        manifest_key = f'{self.temp_s3_prefix}test_manifest.json'
-        manifest = self.rs.generate_manifest(self.temp_s3_bucket,
-                                             prefix=f'{self.temp_s3_prefix}test_file',
-                                             manifest_bucket=self.temp_s3_bucket,
-                                             manifest_key=manifest_key)
+        manifest_key = f"{self.temp_s3_prefix}test_manifest.json"
+        manifest = self.rs.generate_manifest(
+            self.temp_s3_bucket,
+            prefix=f"{self.temp_s3_prefix}test_file",
+            manifest_bucket=self.temp_s3_bucket,
+            manifest_key=manifest_key,
+        )
 
         # Validate path formatted correctly
-        valid_url = f's3://{self.temp_s3_bucket}/{self.temp_s3_prefix}test_file_01.csv'
-        self.assertEqual(manifest['entries'][0]['url'], valid_url)
+        valid_url = f"s3://{self.temp_s3_bucket}/{self.temp_s3_prefix}test_file_01.csv"
+        self.assertEqual(manifest["entries"][0]["url"], valid_url)
 
         # Validate that there are three files
-        self.assertEqual(len(manifest['entries']), 3)
+        self.assertEqual(len(manifest["entries"]), 3)
 
         # Validate that manifest saved to bucket
-        keys = self.s3.list_keys(self.temp_s3_bucket, prefix=f'{self.temp_s3_prefix}test_manifest')
+        keys = self.s3.list_keys(
+            self.temp_s3_bucket, prefix=f"{self.temp_s3_prefix}test_manifest"
+        )
         self.assertTrue(manifest_key in keys)
 
     def test_move_table(self):
 
         # Run the method and check that new table created
-        self.rs.move_table(f'{self.temp_schema}.test', f'{self.temp_schema}.test2')
-        self.assertTrue(self.rs.table_exists(f'{self.temp_schema}.test2'))
+        self.rs.move_table(f"{self.temp_schema}.test", f"{self.temp_schema}.test2")
+        self.assertTrue(self.rs.table_exists(f"{self.temp_schema}.test2"))
 
         # Run the method again, but drop original
         self.rs.move_table(
-            f'{self.temp_schema}.test2', f'{self.temp_schema}.test3', drop_source_table=True)
-        self.assertFalse(self.rs.table_exists(f'{self.temp_schema}.test2'))
+            f"{self.temp_schema}.test2",
+            f"{self.temp_schema}.test3",
+            drop_source_table=True,
+        )
+        self.assertFalse(self.rs.table_exists(f"{self.temp_schema}.test2"))
 
     def test_get_tables(self):
 
         tbls_list = self.rs.get_tables(schema=self.temp_schema)
-        exp = ['schemaname', 'tablename', 'tableowner', 'tablespace', 'hasindexes',
-               'hasrules', 'hastriggers']
+        exp = [
+            "schemaname",
+            "tablename",
+            "tableowner",
+            "tablespace",
+            "hasindexes",
+            "hasrules",
+            "hastriggers",
+        ]
 
         self.assertTrue(validate_list(exp, tbls_list))
 
     def test_get_table_stats(self):
 
         tbls_list = self.rs.get_table_stats(schema=self.temp_schema)
 
         exp = [
-            'database', 'schema', 'table_id', 'table', 'encoded', 'diststyle', 'sortkey1',
-            'max_varchar', 'sortkey1_enc', 'sortkey_num', 'size', 'pct_used', 'empty',
-            'unsorted', 'stats_off', 'tbl_rows', 'skew_sortkey1', 'skew_rows',
-            'estimated_visible_rows', 'risk_event', 'vacuum_sort_benefit']
+            "database",
+            "schema",
+            "table_id",
+            "table",
+            "encoded",
+            "diststyle",
+            "sortkey1",
+            "max_varchar",
+            "sortkey1_enc",
+            "sortkey_num",
+            "size",
+            "pct_used",
+            "empty",
+            "unsorted",
+            "stats_off",
+            "tbl_rows",
+            "skew_sortkey1",
+            "skew_rows",
+            "estimated_visible_rows",
+            "risk_event",
+            "vacuum_sort_benefit",
+        ]
 
         # Having some issues testing that the filter is working correctly, as it
         # takes a little bit of time for a table to show in this table and is beating
         # the test suite. I feel confident that it works though.
 
         self.assertTrue(validate_list(exp, tbls_list))
 
     def test_get_views(self):
         # Assert that get_views returns filtered views
 
         # Assert that it works with schema filter
         views = self.rs.get_views(schema=self.temp_schema)
-        expected_row = (self.temp_schema,
-                        'test_view',
-                        f'SELECT test.id, test.name FROM {self.temp_schema}.test;')
+        expected_row = (
+            self.temp_schema,
+            "test_view",
+            f"SELECT test.id, test.name FROM {self.temp_schema}.test;",
+        )
         self.assertEqual(views.data[0], expected_row)
 
     def test_get_queries(self):
 
         # Validate that columns match expected columns
         queries_list = self.rs.get_queries()
-        exp = ['user', 'pid', 'xid', 'query', 'service_class', 'slot', 'start', 'state',
-               'queue_sec', 'exec_sec', 'cpu_sec', 'read_mb', 'spill_mb', 'return_rows',
-               'nl_rows', 'sql', 'alert']
+        exp = [
+            "user",
+            "pid",
+            "xid",
+            "query",
+            "service_class",
+            "slot",
+            "start",
+            "state",
+            "queue_sec",
+            "exec_sec",
+            "cpu_sec",
+            "read_mb",
+            "spill_mb",
+            "return_rows",
+            "nl_rows",
+            "sql",
+            "alert",
+        ]
         self.assertTrue(validate_list(exp, queries_list))
 
     def test_get_row_count(self):
-        table_name = f'{self.temp_schema}.test_row_count'
-        self.rs.copy(self.tbl, table_name, if_exists='drop')
+        table_name = f"{self.temp_schema}.test_row_count"
+        self.rs.copy(self.tbl, table_name, if_exists="drop")
         count = self.rs.get_row_count(table_name)
         self.assertEqual(count, 3)
 
     def test_rename_table(self):
 
-        self.rs.rename_table(self.temp_schema + '.test', 'test2')
+        self.rs.rename_table(self.temp_schema + ".test", "test2")
 
         # Test that renamed table exists
-        self.assertTrue(self.rs.table_exists(self.temp_schema + '.test2'))
+        self.assertTrue(self.rs.table_exists(self.temp_schema + ".test2"))
 
         # Test that old table name does not exist
-        self.assertFalse(self.rs.table_exists(self.temp_schema + '.test'))
+        self.assertFalse(self.rs.table_exists(self.temp_schema + ".test"))
 
     def test_union_tables(self):
 
         # Copy in two tables
-        self.rs.copy(self.tbl, f'{self.temp_schema}.union_base1', if_exists='drop')
-        self.rs.copy(self.tbl, f'{self.temp_schema}.union_base2', if_exists='drop')
+        self.rs.copy(self.tbl, f"{self.temp_schema}.union_base1", if_exists="drop")
+        self.rs.copy(self.tbl, f"{self.temp_schema}.union_base2", if_exists="drop")
 
         # Union all the two tables and check row count
-        self.rs.union_tables(f'{self.temp_schema}.union_all',
-                             [f'{self.temp_schema}.union_base1', f'{self.temp_schema}.union_base2'])
-        self.assertEqual(self.rs.query(f"select * from {self.temp_schema}.union_all").num_rows, 6)
+        self.rs.union_tables(
+            f"{self.temp_schema}.union_all",
+            [f"{self.temp_schema}.union_base1", f"{self.temp_schema}.union_base2"],
+        )
+        self.assertEqual(
+            self.rs.query(f"select * from {self.temp_schema}.union_all").num_rows, 6
+        )
 
         # Union the two tables and check row count
-        self.rs.union_tables(f'{self.temp_schema}.union_test',
-                             [f'{self.temp_schema}.union_base1', f'{self.temp_schema}.union_base2'],
-                             union_all=False)
-        self.assertEqual(self.rs.query(f"select * from {self.temp_schema}.union_test").num_rows, 3)
+        self.rs.union_tables(
+            f"{self.temp_schema}.union_test",
+            [f"{self.temp_schema}.union_base1", f"{self.temp_schema}.union_base2"],
+            union_all=False,
+        )
+        self.assertEqual(
+            self.rs.query(f"select * from {self.temp_schema}.union_test").num_rows, 3
+        )
 
     def test_populate_table_from_query(self):
         # Populate the source table
-        source_table = f'{self.temp_schema}.test_source'
-        self.rs.copy(self.tbl, source_table, if_exists='drop')
+        source_table = f"{self.temp_schema}.test_source"
+        self.rs.copy(self.tbl, source_table, if_exists="drop")
 
         query = f"SELECT * FROM {source_table}"
 
         # Populate the table
-        dest_table = f'{self.temp_schema}.test_dest'
+        dest_table = f"{self.temp_schema}.test_dest"
         self.rs.populate_table_from_query(query, dest_table)
 
         # Verify
         rows = self.rs.query(f"select count(*) from {dest_table}")
-        self.assertEqual(rows[0]['count'], 3)
+        self.assertEqual(rows[0]["count"], 3)
 
         # Try with if_exists='truncate'
-        self.rs.populate_table_from_query(query, dest_table, if_exists='truncate')
+        self.rs.populate_table_from_query(query, dest_table, if_exists="truncate")
         rows = self.rs.query(f"select count(*) from {dest_table}")
-        self.assertEqual(rows[0]['count'], 3)
+        self.assertEqual(rows[0]["count"], 3)
 
         # Try with if_exists='drop', and a distkey
-        self.rs.populate_table_from_query(query, dest_table, if_exists='drop', distkey='id')
+        self.rs.populate_table_from_query(
+            query, dest_table, if_exists="drop", distkey="id"
+        )
         rows = self.rs.query(f"select count(*) from {dest_table}")
-        self.assertEqual(rows[0]['count'], 3)
+        self.assertEqual(rows[0]["count"], 3)
 
         # Try with if_exists='fail'
         self.assertRaises(
-            ValueError, self.rs.populate_table_from_query, query, dest_table, if_exists='fail')
+            ValueError,
+            self.rs.populate_table_from_query,
+            query,
+            dest_table,
+            if_exists="fail",
+        )
 
     def test_duplicate_table(self):
         # Populate the source table
-        source_table = f'{self.temp_schema}.test_source'
-        self.rs.copy(self.tbl, source_table, if_exists='drop')
+        source_table = f"{self.temp_schema}.test_source"
+        self.rs.copy(self.tbl, source_table, if_exists="drop")
 
         # Duplicate the table
-        dest_table = f'{self.temp_schema}.test_dest'
+        dest_table = f"{self.temp_schema}.test_dest"
         self.rs.duplicate_table(source_table, dest_table)
 
         # Verify
         rows = self.rs.query(f"select count(*) from {dest_table}")
-        self.assertEqual(rows[0]['count'], 3)
+        self.assertEqual(rows[0]["count"], 3)
 
         # Try with if_exists='truncate'
-        self.rs.duplicate_table(source_table, dest_table, if_exists='truncate')
+        self.rs.duplicate_table(source_table, dest_table, if_exists="truncate")
         rows = self.rs.query(f"select count(*) from {dest_table}")
-        self.assertEqual(rows[0]['count'], 3)
+        self.assertEqual(rows[0]["count"], 3)
 
         # Try with if_exists='drop'
-        self.rs.duplicate_table(source_table, dest_table, if_exists='drop')
+        self.rs.duplicate_table(source_table, dest_table, if_exists="drop")
         rows = self.rs.query(f"select count(*) from {dest_table}")
-        self.assertEqual(rows[0]['count'], 3)
+        self.assertEqual(rows[0]["count"], 3)
 
         # Try with if_exists='append'
-        self.rs.duplicate_table(source_table, dest_table, if_exists='append')
+        self.rs.duplicate_table(source_table, dest_table, if_exists="append")
         rows = self.rs.query(f"select count(*) from {dest_table}")
-        self.assertEqual(rows[0]['count'], 6)
+        self.assertEqual(rows[0]["count"], 6)
 
         # Try with if_exists='fail'
-        self.assertRaises(ValueError, self.rs.duplicate_table, source_table,
-                          dest_table, if_exists='fail')
+        self.assertRaises(
+            ValueError,
+            self.rs.duplicate_table,
+            source_table,
+            dest_table,
+            if_exists="fail",
+        )
 
         # Try with invalid if_exists arg
-        self.assertRaises(ValueError, self.rs.duplicate_table, source_table,
-                          dest_table, if_exists='nonsense')
+        self.assertRaises(
+            ValueError,
+            self.rs.duplicate_table,
+            source_table,
+            dest_table,
+            if_exists="nonsense",
+        )
 
     def test_get_max_value(self):
 
-        date_tbl = Table([['id', 'date_modified'], [1, '2020-01-01'], [2, '1900-01-01']])
-        self.rs.copy(date_tbl, f'{self.temp_schema}.test_date')
+        date_tbl = Table(
+            [["id", "date_modified"], [1, "2020-01-01"], [2, "1900-01-01"]]
+        )
+        self.rs.copy(date_tbl, f"{self.temp_schema}.test_date")
 
         # Test return string
-        self.assertEqual(self.rs.get_max_value(f'{self.temp_schema}.test_date', 'date_modified'),
-                         '2020-01-01')
+        self.assertEqual(
+            self.rs.get_max_value(f"{self.temp_schema}.test_date", "date_modified"),
+            "2020-01-01",
+        )
 
     def test_get_columns(self):
-        cols = self.rs.get_columns(self.temp_schema, 'test')
+        cols = self.rs.get_columns(self.temp_schema, "test")
 
         # id int,name varchar(5)
         expected_cols = {
-            'id':   {
-                'data_type': 'int', 'max_length': None,
-                'max_precision': 32, 'max_scale': 0, 'is_nullable': True},
-            'name': {
-                'data_type': 'character varying', 'max_length': 5,
-                'max_precision': None, 'max_scale': None, 'is_nullable': True},
+            "id": {
+                "data_type": "int",
+                "max_length": None,
+                "max_precision": 32,
+                "max_scale": 0,
+                "is_nullable": True,
+            },
+            "name": {
+                "data_type": "character varying",
+                "max_length": 5,
+                "max_precision": None,
+                "max_scale": None,
+                "is_nullable": True,
+            },
         }
 
         self.assertEqual(cols, expected_cols)
 
     def test_get_object_type(self):
         # Test a table
         expected_type_table = "table"
@@ -748,105 +983,114 @@
 
         self.assertTrue(self.rs.is_table("pg_catalog.pg_class"))
 
         self.assertFalse(self.rs.is_table("pg_catalog.pg_views"))
 
     def test_get_table_definition(self):
         expected_table_def = (
-            '--DROP TABLE pg_catalog.pg_amop;'
-            '\nCREATE TABLE IF NOT EXISTS pg_catalog.pg_amop'
-            '\n('
-            '\n\tamopclaid OID NOT NULL  ENCODE RAW'
-            '\n\t,amopsubtype OID NOT NULL  ENCODE RAW'
-            '\n\t,amopstrategy SMALLINT NOT NULL  ENCODE RAW'
-            '\n\t,amopreqcheck BOOLEAN NOT NULL  ENCODE RAW'
-            '\n\t,amopopr OID NOT NULL  ENCODE RAW'
-            '\n)\nDISTSTYLE EVEN\n;')
+            "--DROP TABLE pg_catalog.pg_amop;"
+            "\nCREATE TABLE IF NOT EXISTS pg_catalog.pg_amop"
+            "\n("
+            "\n\tamopclaid OID NOT NULL  ENCODE RAW"
+            "\n\t,amopsubtype OID NOT NULL  ENCODE RAW"
+            "\n\t,amopstrategy SMALLINT NOT NULL  ENCODE RAW"
+            "\n\t,amopreqcheck BOOLEAN NOT NULL  ENCODE RAW"
+            "\n\t,amopopr OID NOT NULL  ENCODE RAW"
+            "\n)\nDISTSTYLE EVEN\n;"
+        )
         actual_table_def = self.rs.get_table_definition("pg_catalog.pg_amop")
 
         self.assertEqual(expected_table_def, actual_table_def)
 
     def test_get_table_definitions(self):
-        expected_table_defs = [{
-            'tablename': 'pg_catalog.pg_amop',
-            'ddl': '--DROP TABLE pg_catalog.pg_amop;'
-                   '\nCREATE TABLE IF NOT EXISTS pg_catalog.pg_amop'
-                   '\n('
-                   '\n\tamopclaid OID NOT NULL  ENCODE RAW'
-                   '\n\t,amopsubtype OID NOT NULL  ENCODE RAW'
-                   '\n\t,amopstrategy SMALLINT NOT NULL  ENCODE RAW'
-                   '\n\t,amopreqcheck BOOLEAN NOT NULL  ENCODE RAW'
-                   '\n\t,amopopr OID NOT NULL  ENCODE RAW'
-                   '\n)\nDISTSTYLE EVEN\n;'}, {
-            'tablename': 'pg_catalog.pg_amproc',
-            'ddl': '--DROP TABLE pg_catalog.pg_amproc;'
-                   '\nCREATE TABLE IF NOT EXISTS pg_catalog.pg_amproc'
-                   '\n('
-                   '\n\tamopclaid OID NOT NULL  ENCODE RAW'
-                   '\n\t,amprocsubtype OID NOT NULL  ENCODE RAW'
-                   '\n\t,amprocnum SMALLINT NOT NULL  ENCODE RAW'
-                   '\n\t,amproc REGPROC NOT NULL  ENCODE RAW'
-                   '\n)'
-                   '\nDISTSTYLE EVEN\n;'}]
+        expected_table_defs = [
+            {
+                "tablename": "pg_catalog.pg_amop",
+                "ddl": "--DROP TABLE pg_catalog.pg_amop;"
+                "\nCREATE TABLE IF NOT EXISTS pg_catalog.pg_amop"
+                "\n("
+                "\n\tamopclaid OID NOT NULL  ENCODE RAW"
+                "\n\t,amopsubtype OID NOT NULL  ENCODE RAW"
+                "\n\t,amopstrategy SMALLINT NOT NULL  ENCODE RAW"
+                "\n\t,amopreqcheck BOOLEAN NOT NULL  ENCODE RAW"
+                "\n\t,amopopr OID NOT NULL  ENCODE RAW"
+                "\n)\nDISTSTYLE EVEN\n;",
+            },
+            {
+                "tablename": "pg_catalog.pg_amproc",
+                "ddl": "--DROP TABLE pg_catalog.pg_amproc;"
+                "\nCREATE TABLE IF NOT EXISTS pg_catalog.pg_amproc"
+                "\n("
+                "\n\tamopclaid OID NOT NULL  ENCODE RAW"
+                "\n\t,amprocsubtype OID NOT NULL  ENCODE RAW"
+                "\n\t,amprocnum SMALLINT NOT NULL  ENCODE RAW"
+                "\n\t,amproc REGPROC NOT NULL  ENCODE RAW"
+                "\n)"
+                "\nDISTSTYLE EVEN\n;",
+            },
+        ]
         actual_table_defs = self.rs.get_table_definitions(table="pg_am%p%")
 
         self.assertEqual(expected_table_defs, actual_table_defs)
 
     def test_get_view_definition(self):
         expected_view_def = (
-            '--DROP VIEW pg_catalog.pg_views;'
-            '\nCREATE OR REPLACE VIEW pg_catalog.pg_views AS'
-            '\n SELECT n.nspname AS schemaname'
-            ', c.relname AS viewname'
-            ', pg_get_userbyid(c.relowner) AS viewowner'
-            ', pg_get_viewdef(c.oid) AS definition'
-            '\n   FROM pg_class c'
-            '\n   LEFT JOIN pg_namespace n ON n.oid = c.relnamespace'
-            '\n  WHERE c.relkind = \'v\'::"char";')
+            "--DROP VIEW pg_catalog.pg_views;"
+            "\nCREATE OR REPLACE VIEW pg_catalog.pg_views AS"
+            "\n SELECT n.nspname AS schemaname"
+            ", c.relname AS viewname"
+            ", pg_get_userbyid(c.relowner) AS viewowner"
+            ", pg_get_viewdef(c.oid) AS definition"
+            "\n   FROM pg_class c"
+            "\n   LEFT JOIN pg_namespace n ON n.oid = c.relnamespace"
+            "\n  WHERE c.relkind = 'v'::\"char\";"
+        )
         actual_view_def = self.rs.get_view_definition("pg_catalog.pg_views")
 
         self.assertEqual(expected_view_def, actual_view_def)
 
     def test_get_view_definitions(self):
-        expected_view_defs = [{
-            'viewname': 'pg_catalog.pg_class_info',
-            'ddl': "--DROP VIEW pg_catalog.pg_class_info;"
-            "\nCREATE OR REPLACE VIEW pg_catalog.pg_class_info AS"
-            "\n SELECT pgc.oid AS reloid, pgc.relname, pgc.relnamespace, "
-            "pgc.reltype, pgc.relowner, pgc.relam, pgc.relfilenode, "
-            "pgc.reltablespace, pgc.relpages, pgc.reltuples, "
-            "pgc.reltoastrelid, pgc.reltoastidxid, pgc.relhasindex, "
-            "pgc.relisshared, pgc.relkind, pgc.relnatts, pgc.relexternid, "
-            "pgc.relisreplicated, pgc.relispinned, pgc.reldiststyle, "
-            "pgc.relprojbaseid, pgc.relchecks, pgc.reltriggers, pgc.relukeys, "
-            "pgc.relfkeys, pgc.relrefs, pgc.relhasoids, pgc.relhaspkey, "
-            "pgc.relhasrules, pgc.relhassubclass, pgc.relacl, "
-            "pgce0.value::smallint AS releffectivediststyle, "
-            "date_add('microsecond'::text, pgce1.value::bigint, "
-            "'2000-01-01 00:00:00'::timestamp without time zone) AS "
-            "relcreationtime"
-            "\n   FROM pg_class pgc"
-            "\n   LEFT JOIN pg_class_extended pgce0 "
-            "ON pgc.oid = pgce0.reloid AND pgce0.colnum = 0"
-            "\n   LEFT JOIN pg_class_extended pgce1 "
-            "ON pgc.oid = pgce1.reloid AND pgce1.colnum = 1;"}]
+        expected_view_defs = [
+            {
+                "viewname": "pg_catalog.pg_class_info",
+                "ddl": "--DROP VIEW pg_catalog.pg_class_info;"
+                "\nCREATE OR REPLACE VIEW pg_catalog.pg_class_info AS"
+                "\n SELECT pgc.oid AS reloid, pgc.relname, pgc.relnamespace, "
+                "pgc.reltype, pgc.relowner, pgc.relam, pgc.relfilenode, "
+                "pgc.reltablespace, pgc.relpages, pgc.reltuples, "
+                "pgc.reltoastrelid, pgc.reltoastidxid, pgc.relhasindex, "
+                "pgc.relisshared, pgc.relkind, pgc.relnatts, pgc.relexternid, "
+                "pgc.relisreplicated, pgc.relispinned, pgc.reldiststyle, "
+                "pgc.relprojbaseid, pgc.relchecks, pgc.reltriggers, pgc.relukeys, "
+                "pgc.relfkeys, pgc.relrefs, pgc.relhasoids, pgc.relhaspkey, "
+                "pgc.relhasrules, pgc.relhassubclass, pgc.relacl, "
+                "pgce0.value::smallint AS releffectivediststyle, "
+                "date_add('microsecond'::text, pgce1.value::bigint, "
+                "'2000-01-01 00:00:00'::timestamp without time zone) AS "
+                "relcreationtime"
+                "\n   FROM pg_class pgc"
+                "\n   LEFT JOIN pg_class_extended pgce0 "
+                "ON pgc.oid = pgce0.reloid AND pgce0.colnum = 0"
+                "\n   LEFT JOIN pg_class_extended pgce1 "
+                "ON pgc.oid = pgce1.reloid AND pgce1.colnum = 1;",
+            }
+        ]
         actual_view_def = self.rs.get_view_definitions(view="pg_c%")
 
         self.assertEqual(expected_view_defs, actual_view_def)
 
     def test_alter_varchar_column_widths(self):
 
-        append_tbl = Table([['ID', 'Name'],
-                            [4, 'Jim'],
-                            [5, 'John'],
-                            [6, 'Joanna']])
+        append_tbl = Table([["ID", "Name"], [4, "Jim"], [5, "John"], [6, "Joanna"]])
 
         # You can't alter column types if the table has a dependent view
         self.rs.query(f"DROP VIEW {self.temp_schema}.test_view")
 
         # Base table 'Name' column has a width of 5. This should expand it to 6.
-        self.rs.alter_varchar_column_widths(append_tbl, f'{self.temp_schema}.test')
-        self.assertEqual(self.rs.get_columns(self.temp_schema, 'test')['name']['max_length'], 6)
+        self.rs.alter_varchar_column_widths(append_tbl, f"{self.temp_schema}.test")
+        self.assertEqual(
+            self.rs.get_columns(self.temp_schema, "test")["name"]["max_length"], 6
+        )
 
 
 if __name__ == "__main__":
     unittest.main()
```

### Comparing `parsons-1.0.0/test/test_s3.py` & `parsons-1.1.0/test/test_s3.py`

 * *Files 10% similar despite different names*

```diff
@@ -7,47 +7,48 @@
 import time
 from test.utils import assert_matching_tables
 
 # Requires a s3 credentials stored in aws config or env variable
 # to run properly.
 
 
-@unittest.skipIf(not os.environ.get('LIVE_TEST'), 'Skipping because not running live test')
+@unittest.skipIf(
+    not os.environ.get("LIVE_TEST"), "Skipping because not running live test"
+)
 class TestS3(unittest.TestCase):
-
     def setUp(self):
 
         self.s3 = S3()
 
         self.s3.aws.session.get_credentials()
 
         # Create a new bucket
-        self.test_bucket = os.environ['S3_TEMP_BUCKET']
+        self.test_bucket = os.environ["S3_TEMP_BUCKET"]
         # Trying miss random errors on not finding buckets
         self.s3.create_bucket(self.test_bucket)
 
-        self.test_key = 'test.csv'
-        self.tbl = Table([{'first': 'Bob', 'last': 'Smith'}])
+        self.test_key = "test.csv"
+        self.tbl = Table([{"first": "Bob", "last": "Smith"}])
         csv_path = self.tbl.to_csv()
 
-        self.test_key_2 = 'test2.csv'
-        self.tbl_2 = Table([{'first': 'Jack', 'last': 'Smith'}])
+        self.test_key_2 = "test2.csv"
+        self.tbl_2 = Table([{"first": "Jack", "last": "Smith"}])
         csv_path_2 = self.tbl_2.to_csv()
 
         # Sometimes it runs into issues putting the file
         retry = 1
 
         while retry <= 5:
             try:
                 # Put a test file in the bucket
                 self.s3.put_file(self.test_bucket, self.test_key, csv_path)
                 self.s3.put_file(self.test_bucket, self.test_key_2, csv_path_2)
                 break
             except Exception:
-                print('Retrying putting file in bucket...')
+                print("Retrying putting file in bucket...")
                 retry += 1
 
     def tearDown(self):
         for k in self.s3.list_keys(self.test_bucket):
             self.s3.remove_file(self.test_bucket, k)
 
     def test_list_buckets(self):
@@ -59,61 +60,61 @@
 
     def test_bucket_exists(self):
 
         # Test that a bucket that exists returns True
         self.assertTrue(self.s3.bucket_exists(self.test_bucket))
 
         # Test that a bucket that doesn't exist returns False
-        self.assertFalse(self.s3.bucket_exists('idontexist_bucket'))
+        self.assertFalse(self.s3.bucket_exists("idontexist_bucket"))
 
     def test_list_keys(self):
 
         # Put a file in the bucket
         csv_path = self.tbl.to_csv()
-        key = 'test/test.csv'
+        key = "test/test.csv"
         self.s3.put_file(self.test_bucket, key, csv_path)
 
         # Test that basic bucket list works
-        keys = self.s3.list_keys(self.test_bucket, prefix='test/test')
+        keys = self.s3.list_keys(self.test_bucket, prefix="test/test")
         self.assertTrue(key in keys)
 
         # Test that prefix filter works -- when there
-        keys = self.s3.list_keys(self.test_bucket, prefix='test')
+        keys = self.s3.list_keys(self.test_bucket, prefix="test")
         self.assertTrue(key in keys)
 
         # Test that prefix filter works -- when not there
-        keys = self.s3.list_keys(self.test_bucket, prefix='nope')
+        keys = self.s3.list_keys(self.test_bucket, prefix="nope")
         self.assertFalse(key in keys)
 
     def test_key_exists(self):
 
         csv_path = self.tbl.to_csv()
-        key = 'test/test.csv'
+        key = "test/test.csv"
         self.s3.put_file(self.test_bucket, key, csv_path)
 
         # Test that returns True if key exists
         self.assertTrue(self.s3.key_exists(self.test_bucket, key))
 
         # Test that returns True if key does not exist
-        self.assertFalse(self.s3.key_exists(self.test_bucket, 'akey'))
+        self.assertFalse(self.s3.key_exists(self.test_bucket, "akey"))
 
     def test_list_keys_suffix(self):
 
         # Put a file in the bucket
         csv_path = self.tbl.to_csv()
-        key_1 = 'test/test.csv'
-        key_2 = 'test/test.gz'
+        key_1 = "test/test.csv"
+        key_2 = "test/test.gz"
         self.s3.put_file(self.test_bucket, key_1, csv_path)
         self.s3.put_file(self.test_bucket, key_2, csv_path)
 
-        keys = self.s3.list_keys(self.test_bucket, suffix='csv')
+        keys = self.s3.list_keys(self.test_bucket, suffix="csv")
         self.assertTrue(key_1 in keys)
         self.assertFalse(key_2 in keys)
 
-        keys = self.s3.list_keys(self.test_bucket, suffix='gz')
+        keys = self.s3.list_keys(self.test_bucket, suffix="gz")
         self.assertFalse(key_1 in keys)
         self.assertTrue(key_2 in keys)
 
     def test_list_keys_date_modified(self):
 
         # Set current utc timestamp with timezone
         current_utc = datetime.utcnow().astimezone(pytz.utc)
@@ -163,13 +164,22 @@
         result_tbl = Table.from_csv(path)
         assert_matching_tables(self.tbl, result_tbl)
         # Test that original still exists in original bucket
         self.assertTrue(self.s3.key_exists(self.test_bucket, self.test_key))
 
         # Transfer and delete original
         self.s3.transfer_bucket(
-            self.test_bucket, self.test_key_2, destination_bucket,
-            None, None, None, None, None, False, True)
+            self.test_bucket,
+            self.test_key_2,
+            destination_bucket,
+            None,
+            None,
+            None,
+            None,
+            None,
+            False,
+            True,
+        )
         path_2 = self.s3.get_file(destination_bucket, self.test_key_2)
         result_tbl_2 = Table.from_csv(path_2)
         assert_matching_tables(self.tbl_2, result_tbl_2)
         self.assertFalse(self.s3.key_exists(self.test_bucket, self.test_key_2))
```

### Comparing `parsons-1.0.0/test/test_sendmail.py` & `parsons-1.1.0/test/test_sendmail.py`

 * *Files 3% similar despite different names*

```diff
@@ -11,20 +11,22 @@
 
 
 @pytest.fixture(scope="function")
 def dummy_sendmail():
     """Have to create a dummy class that inherits from SendMail and defines a couple
     of methods in order to test out the methods that aren't abstract.
     """
+
     class DummySendMail(SendMail):
         def __init__(self):
             pass
 
         def _send_message(self, message):
             pass
+
     return DummySendMail()
 
 
 class TestSendMailCreateMessageSimple:
     def test_creates_mimetext_message(self, dummy_sendmail):
         message = dummy_sendmail._create_message_simple("from", "to", "subject", "text")
         assert isinstance(message, MIMEText)
@@ -35,77 +37,95 @@
         assert message.get("to") == "to"
         assert message.get("subject") == "subject"
         assert message.get_payload() == "text"
 
 
 class TestSendMailCreateMessageHtml:
     def test_creates_multipart_message(self, dummy_sendmail):
-        message = dummy_sendmail._create_message_html("from", "to", "subject", "text", "html")
+        message = dummy_sendmail._create_message_html(
+            "from", "to", "subject", "text", "html"
+        )
         assert isinstance(message, MIMEMultipart)
 
     def test_sets_to_from_subject(self, dummy_sendmail):
-        message = dummy_sendmail._create_message_html("from", "to", "subject", "text", "html")
+        message = dummy_sendmail._create_message_html(
+            "from", "to", "subject", "text", "html"
+        )
         assert message.get("from") == "from"
         assert message.get("to") == "to"
         assert message.get("subject") == "subject"
 
     def test_works_if_no_message_text(self, dummy_sendmail):
-        message = dummy_sendmail._create_message_html("from", "to", "subject", None, "html")
+        message = dummy_sendmail._create_message_html(
+            "from", "to", "subject", None, "html"
+        )
         assert len(message.get_payload()) == 1
         assert message.get_payload()[0].get_payload() == "html"
         assert message.get_payload()[0].get_content_type() == "text/html"
 
     def test_works_with_text_and_html(self, dummy_sendmail):
-        message = dummy_sendmail._create_message_html("from", "to", "subject", "text", "html")
+        message = dummy_sendmail._create_message_html(
+            "from", "to", "subject", "text", "html"
+        )
         assert len(message.get_payload()) == 2
         assert message.get_payload()[0].get_payload() == "text"
         assert message.get_payload()[0].get_content_type() == "text/plain"
         assert message.get_payload()[1].get_payload() == "html"
         assert message.get_payload()[1].get_content_type() == "text/html"
 
 
 class TestSendMailCreateMessageAttachments:
     def test_creates_multipart_message(self, dummy_sendmail):
-        message = dummy_sendmail._create_message_attachments("from", "to", "subject", "text", [])
+        message = dummy_sendmail._create_message_attachments(
+            "from", "to", "subject", "text", []
+        )
         assert isinstance(message, MIMEMultipart)
 
     def test_can_handle_html(self, dummy_sendmail):
-        message = dummy_sendmail._create_message_attachments("from", "to", "subject", "text", [],
-                                                             message_html="html")
+        message = dummy_sendmail._create_message_attachments(
+            "from", "to", "subject", "text", [], message_html="html"
+        )
         assert len(message.get_payload()) == 2
         assert message.get_payload()[0].get_payload() == "text"
         assert message.get_payload()[0].get_content_type() == "text/plain"
         assert message.get_payload()[1].get_payload() == "html"
         assert message.get_payload()[1].get_content_type() == "text/html"
 
     @pytest.mark.parametrize(
         "filename,expected_type",
         [
             ("image.png", MIMEImage),
             ("application.exe", MIMEApplication),
             ("text.txt", MIMEText),
             ("audio.mp3", MIMEAudio),
-            ("video.mp4", MIMEBase)  # This will fail if the method is updated to parse video
-        ]
+            (
+                "video.mp4",
+                MIMEBase,
+            ),  # This will fail if the method is updated to parse video
+        ],
     )
-    def test_properly_detects_file_types(self, tmp_path, dummy_sendmail, filename, expected_type):
+    def test_properly_detects_file_types(
+        self, tmp_path, dummy_sendmail, filename, expected_type
+    ):
         filename = tmp_path / filename
         filename.write_bytes(b"Parsons")
-        message = dummy_sendmail._create_message_attachments("from", "to", "subject", "text",
-                                                             [filename])
+        message = dummy_sendmail._create_message_attachments(
+            "from", "to", "subject", "text", [filename]
+        )
         assert len(message.get_payload()) == 2  # text body plus attachment
         assert isinstance(message.get_payload()[1], expected_type)
 
     @pytest.mark.parametrize("buffer", [io.StringIO, io.BytesIO])
     def test_works_with_buffers(self, dummy_sendmail, buffer):
         value = "Parsons"
         if buffer is io.BytesIO:
             value = b"Parsons"
-        message = dummy_sendmail._create_message_attachments("from", "to", "subject", "text",
-                                                             [buffer(value)])
+        message = dummy_sendmail._create_message_attachments(
+            "from", "to", "subject", "text", [buffer(value)]
+        )
         assert len(message.get_payload()) == 2  # text body plus attachment
         assert isinstance(message.get_payload()[1], MIMEApplication)
 
 
 class TestSendMailValidateEmailString:
     @pytest.mark.parametrize("bad_email", ["a", "a@", "a+b", "@b.com"])
     def test_errors_with_invalid_emails(self, dummy_sendmail, bad_email):
@@ -114,66 +134,59 @@
 
     @pytest.mark.parametrize("good_email", ["a@b", "a+b@c", "a@d.com", "a@b.org"])
     def test_passes_valid_emails(self, dummy_sendmail, good_email):
         dummy_sendmail._validate_email_string(good_email)
 
 
 class TestSendMailSendEmail:
-
     @pytest.fixture(scope="function")
     def patched_sendmail(self):
         class PatchedSendMail(SendMail):
             def __init__(self):
                 pass
 
             def _send_message(self, message):
                 self.message = message  # Stores message for post-call introspection
+
         return PatchedSendMail()
 
     def test_errors_when_send_message_not_implemented(self):
         with pytest.raises(
-                TypeError,
-                match="Can't instantiate abstract class SendMail"
+            TypeError, match="Can't instantiate abstract class SendMail"
         ):
             SendMail().send_email("from@from.com", "to@to.com", "subject", "text")
 
     def test_can_handle_lists_of_emails(self, patched_sendmail):
         patched_sendmail.send_email(
-            "from",
-            ["to1@to1.com", "to2@to2.com"],
-            "subject", "text"
+            "from", ["to1@to1.com", "to2@to2.com"], "subject", "text"
         )
         assert patched_sendmail.message.get("to") == "to1@to1.com, to2@to2.com"
 
     def test_errors_if_an_email_in_a_list_doesnt_validate(self, patched_sendmail):
         with pytest.raises(ValueError, match="Invalid email address"):
             patched_sendmail.send_email(
-                "from",
-                ["to1@to1.com", "invalid", "to2@to2.com"],
-                "subject", "text"
+                "from", ["to1@to1.com", "invalid", "to2@to2.com"], "subject", "text"
             )
 
     def test_errors_if_no_to_email_is_specified(self, patched_sendmail):
         with pytest.raises(EmptyListError, match="Must contain at least 1 email"):
-            patched_sendmail.send_email(
-                "from",
-                [],
-                "subject", "text"
-            )
+            patched_sendmail.send_email("from", [], "subject", "text")
 
     def test_appropriately_dispatches_html_email(self, patched_sendmail):
         patched_sendmail.send_email(
-            "from", "to@to.com",
-            "subject", "text", message_html="html"
+            "from", "to@to.com", "subject", "text", message_html="html"
         )
         assert len(patched_sendmail.message.get_payload()) == 2
-        assert patched_sendmail.message.get_payload()[1].get_content_type() == "text/html"
+        assert (
+            patched_sendmail.message.get_payload()[1].get_content_type() == "text/html"
+        )
 
-    def test_appropriately_handles_filename_specified_as_string(self, tmp_path, patched_sendmail):
+    def test_appropriately_handles_filename_specified_as_string(
+        self, tmp_path, patched_sendmail
+    ):
         filename = tmp_path / "test.txt"
         filename.write_bytes(b"Parsons")
         patched_sendmail.send_email(
-            "from", "to@to.com",
-            "subject", "text", files=str(filename)
+            "from", "to@to.com", "subject", "text", files=str(filename)
         )
         assert len(patched_sendmail.message.get_payload()) == 2
         assert isinstance(patched_sendmail.message.get_payload()[1], MIMEText)
```

### Comparing `parsons-1.0.0/test/test_sftp.py` & `parsons-1.1.0/test/test_sftp.py`

 * *Files 8% similar despite different names*

```diff
@@ -3,27 +3,38 @@
 import paramiko
 from contextlib import contextmanager
 from copy import deepcopy
 from unittest.mock import MagicMock, patch, call
 from parsons import Table, SFTP
 from parsons.utilities import files as file_util
 from test.utils import mark_live_test, assert_matching_tables
-from test.fixtures import simple_table, simple_csv_path, simple_compressed_csv_path  # noqa; F401
+from test.fixtures import (  # noqa: F401
+    simple_table,
+    simple_csv_path,
+    simple_compressed_csv_path,
+)
 
 #
 # Fixtures and constants
 #
 
 REMOTE_DIR, CSV, COMPRESSED_CSV, EMPTY, SUBDIR_A, SUBDIR_B, CSV_A, CSV_B = [
-    'parsons_test', 'test.csv', 'test.csv.gz', 'empty', 'subdir_a', 'subdir_b', 'test_a.csv',
-    'test_b.csv'
+    "parsons_test",
+    "test.csv",
+    "test.csv.gz",
+    "empty",
+    "subdir_a",
+    "subdir_b",
+    "test_a.csv",
+    "test_b.csv",
 ]
 
 CSV_PATH, COMPRESSED_CSV_PATH, EMPTY_PATH, SUBDIR_A_PATH, SUBDIR_B_PATH = [
-    f"{REMOTE_DIR}/{content}" for content in (CSV, COMPRESSED_CSV, EMPTY, SUBDIR_A, SUBDIR_B)
+    f"{REMOTE_DIR}/{content}"
+    for content in (CSV, COMPRESSED_CSV, EMPTY, SUBDIR_A, SUBDIR_B)
 ]
 
 CSV_A_PATH, CSV_B_PATH = [
     f"{d}/{content}" for d, content in ((SUBDIR_A_PATH, CSV_B), (SUBDIR_B_PATH, CSV_A))
 ]
 
 FILE_PATHS = [CSV_PATH, COMPRESSED_CSV_PATH, CSV_A_PATH, CSV_B_PATH]
@@ -32,49 +43,51 @@
 
 def sup(sftp, simple_csv_path, simple_compressed_csv_path):  # noqa: F811
     # The setup function creates remote directories and files needed for live tests
     for remote_dir in DIR_PATHS:
         sftp.make_directory(remote_dir)
 
     for remote_file in FILE_PATHS:
-        fixture = simple_compressed_csv_path if 'gz' in remote_file else simple_csv_path
+        fixture = simple_compressed_csv_path if "gz" in remote_file else simple_csv_path
         sftp.put_file(fixture, remote_file)
 
 
 def cleanup(sftp):
     for f in FILE_PATHS:
         sftp.remove_file(f)
     for remote_dir in reversed(DIR_PATHS):
         sftp.remove_directory(remote_dir)
 
 
 def generate_live_sftp_connection():
-    host = os.environ['SFTP_HOST']
-    username = os.environ['SFTP_USERNAME']
-    password = os.environ['SFTP_PASSWORD']
+    host = os.environ["SFTP_HOST"]
+    username = os.environ["SFTP_USERNAME"]
+    password = os.environ["SFTP_PASSWORD"]
     return SFTP(host, username, password)
 
 
 @pytest.fixture
 def live_sftp(simple_csv_path, simple_compressed_csv_path, simple_table):  # noqa: F811
     sftp = generate_live_sftp_connection()
     sup(sftp, simple_csv_path, simple_compressed_csv_path)
     yield sftp
     cleanup(sftp)
 
 
 # This second live_sftp fixture is used for test_get_files so that files are never downloaded and
 # mocks can be inspected.
 @pytest.fixture
-def live_sftp_with_mocked_get(simple_csv_path, simple_compressed_csv_path):  # noqa: F811
+def live_sftp_with_mocked_get(
+    simple_csv_path, simple_compressed_csv_path  # noqa: F811
+):
     SFTP_with_mocked_get = deepcopy(SFTP)
 
     # The names of temp files are long arbitrary strings. This makes them predictable.
     def rv(magic_mock):
-        return ['foo', 'bar', 'baz'][magic_mock.call_count]
+        return ["foo", "bar", "baz"][magic_mock.call_count]
 
     get = MagicMock()
     create_temp_file_for_path = MagicMock()
     create_temp_file_for_path.return_value = rv(create_temp_file_for_path)
 
     # The following two methods are identical to their cognates in the SFTP class, but they
     # substitue a mock for `conn.get` and `files.create_temp_file_for_path`, respectively.
@@ -111,43 +124,44 @@
     sftp = generate_live_sftp_connection()
     sup(sftp, simple_csv_path, simple_compressed_csv_path)
 
     yield sftp, get
 
     cleanup(sftp)
 
+
 #
 # Tests
 #
 
 
 def test_credential_validation():
     with pytest.raises(ValueError):
         SFTP(host=None, username=None, password=None)
 
     with pytest.raises(ValueError):
-        SFTP(host=None, username='sam', password='abc123')
+        SFTP(host=None, username="sam", password="abc123")
 
 
 @mark_live_test
 def test_list_non_existent_directory(live_sftp):
     with pytest.raises(FileNotFoundError):
-        live_sftp.list_directory('abc123')
+        live_sftp.list_directory("abc123")
 
 
 @mark_live_test
 def test_list_directory_with_files(live_sftp):
     result = sorted(live_sftp.list_directory(REMOTE_DIR))
     assert result == [EMPTY, SUBDIR_A, SUBDIR_B, CSV, COMPRESSED_CSV]
 
 
 @mark_live_test
 def test_get_non_existent_file(live_sftp):
     with pytest.raises(FileNotFoundError):
-        live_sftp.get_file('abc123')
+        live_sftp.get_file("abc123")
 
 
 # Helper function
 def assert_file_matches_table(local_path, table):
     downloaded_tbl = Table.from_csv(local_path)
     assert_matching_tables(table, downloaded_tbl)
 
@@ -169,30 +183,33 @@
 @mark_live_test
 def test_get_temp_file(live_sftp, simple_table):  # noqa F811
     local_path = live_sftp.get_file(CSV_PATH)
     assert_file_matches_table(local_path, simple_table)
 
 
 @mark_live_test
-@pytest.mark.parametrize('compression', [None, 'gzip'])
+@pytest.mark.parametrize("compression", [None, "gzip"])
 def test_table_to_sftp_csv(live_sftp, simple_table, compression):  # noqa F811
-    host = os.environ['SFTP_HOST']
-    username = os.environ['SFTP_USERNAME']
-    password = os.environ['SFTP_PASSWORD']
-    remote_path = f'{REMOTE_DIR}/test_to_sftp.csv'
-    if compression == 'gzip':
-        remote_path += '.gz'
-    simple_table.to_sftp_csv(remote_path, host, username, password, compression=compression)
+    host = os.environ["SFTP_HOST"]
+    username = os.environ["SFTP_USERNAME"]
+    password = os.environ["SFTP_PASSWORD"]
+    remote_path = f"{REMOTE_DIR}/test_to_sftp.csv"
+    if compression == "gzip":
+        remote_path += ".gz"
+    simple_table.to_sftp_csv(
+        remote_path, host, username, password, compression=compression
+    )
 
     local_path = live_sftp.get_file(remote_path)
     assert_file_matches_table(local_path, simple_table)
 
     # Cleanup
     live_sftp.remove_file(remote_path)
 
+
 #
 # Helper Functions
 #
 
 
 def assert_results_match_expected(expected, results):
     assert len(results) == len(expected)
@@ -212,107 +229,117 @@
 def test_list_files(live_sftp):
     result = sorted(live_sftp.list_files(REMOTE_DIR))
     assert result == [CSV_PATH, COMPRESSED_CSV_PATH]
 
 
 @mark_live_test
 def test_list_files_with_pattern(live_sftp):
-    result = live_sftp.list_files(REMOTE_DIR, pattern='gz')
+    result = live_sftp.list_files(REMOTE_DIR, pattern="gz")
     assert result == [COMPRESSED_CSV_PATH]
 
 
 @mark_live_test
 def test_list_subdirectories(live_sftp):
     result = sorted(live_sftp.list_subdirectories(REMOTE_DIR))
     assert result == [EMPTY_PATH, SUBDIR_A_PATH, SUBDIR_B_PATH]
 
 
 @mark_live_test
 def test_list_subdirectories_with_pattern(live_sftp):
-    result = sorted(live_sftp.list_subdirectories(REMOTE_DIR, pattern='sub'))
+    result = sorted(live_sftp.list_subdirectories(REMOTE_DIR, pattern="sub"))
     assert result == [SUBDIR_A_PATH, SUBDIR_B_PATH]
 
 
-local_paths = ['foo', 'bar']
+local_paths = ["foo", "bar"]
 
 # The following are values for the arguments to pass to `get_files` and `walk_tree` as well as the
 # strings expected to be found in the returned results.
 args_and_expected = {
-    'get_files': [
-        ({'remote': REMOTE_DIR}, [CSV_PATH, COMPRESSED_CSV_PATH]),
-        ({'remote': [SUBDIR_A_PATH, SUBDIR_B_PATH]}, [CSV_B_PATH, CSV_A_PATH]),
-        ({'remote': SUBDIR_B_PATH, 'files_to_download': [CSV_B_PATH]}, [CSV_A_PATH, CSV_B_PATH]),
-        ({'remote': [SUBDIR_A_PATH, SUBDIR_B_PATH], 'pattern': 'a'}, [CSV_B_PATH])
+    "get_files": [
+        ({"remote": REMOTE_DIR}, [CSV_PATH, COMPRESSED_CSV_PATH]),
+        ({"remote": [SUBDIR_A_PATH, SUBDIR_B_PATH]}, [CSV_B_PATH, CSV_A_PATH]),
+        (
+            {"remote": SUBDIR_B_PATH, "files_to_download": [CSV_B_PATH]},
+            [CSV_A_PATH, CSV_B_PATH],
+        ),
+        ({"remote": [SUBDIR_A_PATH, SUBDIR_B_PATH], "pattern": "a"}, [CSV_B_PATH]),
     ],
-    'walk_tree': [
+    "walk_tree": [
         (
             [REMOTE_DIR],
-            {'download': False, 'dir_pattern': SUBDIR_A},
-            [[SUBDIR_A], [COMPRESSED_CSV, CSV, CSV_B]]
+            {"download": False, "dir_pattern": SUBDIR_A},
+            [[SUBDIR_A], [COMPRESSED_CSV, CSV, CSV_B]],
         ),
         (
             [REMOTE_DIR],
-            {'download': False, 'file_pattern': CSV_B},
-            [[SUBDIR_A, SUBDIR_B, EMPTY], [CSV_B]]
+            {"download": False, "file_pattern": CSV_B},
+            [[SUBDIR_A, SUBDIR_B, EMPTY], [CSV_B]],
         ),
         (
             [REMOTE_DIR],
-            {'download': False, 'dir_pattern': SUBDIR_A, 'file_pattern': CSV_B},
-            [[SUBDIR_A], [CSV_B]]
+            {"download": False, "dir_pattern": SUBDIR_A, "file_pattern": CSV_B},
+            [[SUBDIR_A], [CSV_B]],
         ),
         (
             [REMOTE_DIR],
-            {'download': False, 'max_depth': 1},
-            [[EMPTY, SUBDIR_A, SUBDIR_B], [CSV, COMPRESSED_CSV]]
-        )
-    ]
+            {"download": False, "max_depth": 1},
+            [[EMPTY, SUBDIR_A, SUBDIR_B], [CSV, COMPRESSED_CSV]],
+        ),
+    ],
 }
 
 
 @mark_live_test
-def test_get_files_calls_get_to_write_to_provided_local_paths(live_sftp_with_mocked_get):
+def test_get_files_calls_get_to_write_to_provided_local_paths(
+    live_sftp_with_mocked_get,
+):
     live_sftp, get = live_sftp_with_mocked_get
-    results = live_sftp.get_files(remote=[SUBDIR_A_PATH, SUBDIR_B_PATH], local_paths=local_paths)
+    results = live_sftp.get_files(
+        remote=[SUBDIR_A_PATH, SUBDIR_B_PATH], local_paths=local_paths
+    )
     assert get.call_count == 2
     calls = [call(CSV_A_PATH, local_paths[0]), call(CSV_B_PATH, local_paths[1])]
     assert_has_calls(get, calls)
     assert_results_match_expected(local_paths, results)
 
 
 @mark_live_test
-@pytest.mark.parametrize('kwargs,expected', args_and_expected['get_files'])
-def test_get_files_calls_get_to_write_temp_files(kwargs, expected, live_sftp_with_mocked_get):
+@pytest.mark.parametrize("kwargs,expected", args_and_expected["get_files"])
+def test_get_files_calls_get_to_write_temp_files(
+    kwargs, expected, live_sftp_with_mocked_get
+):
     live_sftp, get = live_sftp_with_mocked_get
     live_sftp.get_files(**kwargs)
     assert get.call_count == len(expected)
     calls = [call(e, local_paths[i]) for i, e in enumerate(expected)]
     assert_has_calls(get, calls)
 
 
 @mark_live_test
 def test_get_files_raises_error_when_no_file_source_is_provided(live_sftp):
     with pytest.raises(ValueError):
         live_sftp.get_files()
 
 
 @mark_live_test
-@patch('parsons.sftp.SFTP.get_file')
+@patch("parsons.sftp.SFTP.get_file")
 def test_get_files_with_files_paths_mismatch(get_file, live_sftp):
     live_sftp.get_files(files_to_download=[CSV_A_PATH], local_paths=local_paths)
-    assert get_file.call_args[1]['local_path'] is None
+    assert get_file.call_args[1]["local_path"] is None
 
 
 @mark_live_test
-@pytest.mark.parametrize('args,kwargs,expected', args_and_expected['walk_tree'])
+@pytest.mark.parametrize("args,kwargs,expected", args_and_expected["walk_tree"])
 def test_walk_tree(args, kwargs, expected, live_sftp_with_mocked_get):
     live_sftp, get = live_sftp_with_mocked_get
     results = live_sftp.walk_tree(*args, **kwargs)
     # `results` will be a list of first dirs then files, as will `expected`
     for res, expect in zip(results, expected):
         assert_results_match_expected(expect, res)
 
+
 # Stuff that is tested by the live_sftp fixture, so no need to test explicitly:
 # test_make_directory
 # test_put_file
 # test_put_file_compressed
 # def test_remove_file
 # def test_remove_directory
```

### Comparing `parsons-1.0.0/test/test_sftp_ssh.py` & `parsons-1.1.0/test/test_sftp_ssh.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,32 +1,37 @@
 import pytest
 import os
 from parsons import Table, SFTP
 from parsons.utilities import files
 from test.utils import mark_live_test, assert_matching_tables
-from test.fixtures import simple_table, simple_csv_path, simple_compressed_csv_path  # noqa: F401
+from test.fixtures import (  # noqa: F401
+    simple_table,
+    simple_csv_path,
+    simple_compressed_csv_path,
+)
+
 
 #
 # Fixtures and constants
 #
 
-REMOTE_DIR = 'parsons-test'
-REMOTE_CSV = 'test.csv'
-REMOTE_CSV_PATH = f'{REMOTE_DIR}/{REMOTE_CSV}'
-REMOTE_COMPRESSED_CSV = 'test.csv.gz'
-REMOTE_COMPRESSED_CSV_PATH = f'{REMOTE_DIR}/{REMOTE_COMPRESSED_CSV}'
+REMOTE_DIR = "parsons-test"
+REMOTE_CSV = "test.csv"
+REMOTE_CSV_PATH = f"{REMOTE_DIR}/{REMOTE_CSV}"
+REMOTE_COMPRESSED_CSV = "test.csv.gz"
+REMOTE_COMPRESSED_CSV_PATH = f"{REMOTE_DIR}/{REMOTE_COMPRESSED_CSV}"
 
 
 @pytest.fixture
 def live_sftp(simple_table, simple_csv_path, simple_compressed_csv_path):  # noqa: F811
     # Generate a live SFTP connection based on these env vars
-    host = os.environ['SFTP_HOST']
-    username = os.environ['SFTP_USERNAME']
+    host = os.environ["SFTP_HOST"]
+    username = os.environ["SFTP_USERNAME"]
     password = None
-    rsa_private_key_file = os.environ['SFTP_RSA_PRIVATE_KEY_FILE']
+    rsa_private_key_file = os.environ["SFTP_RSA_PRIVATE_KEY_FILE"]
 
     sftp = SFTP(host, username, password, rsa_private_key_file=rsa_private_key_file)
 
     # Add a test directory and test files
 
     sftp.make_directory(REMOTE_DIR)
     sftp.put_file(simple_csv_path, REMOTE_CSV_PATH)
@@ -35,53 +40,62 @@
     yield sftp
 
     # Cleanup after test
     sftp.remove_file(REMOTE_CSV_PATH)
     sftp.remove_file(REMOTE_COMPRESSED_CSV_PATH)
     sftp.remove_directory(REMOTE_DIR)
 
+
 #
 # Tests
 #
 
 
 def test_credential_validation():
     with pytest.raises(ValueError):
         SFTP(host=None, username=None, password=None, rsa_private_key_file=None)
 
     with pytest.raises(ValueError):
         SFTP(
-            host=None, username='sam', password='abc123', rsa_private_key_file='/path/to/key/file')
+            host=None,
+            username="sam",
+            password="abc123",
+            rsa_private_key_file="/path/to/key/file",
+        )
 
     with pytest.raises(ValueError):
         SFTP(
-            host='host', username=None, password='abc123', rsa_private_key_file='/path/to/key/file')
+            host="host",
+            username=None,
+            password="abc123",
+            rsa_private_key_file="/path/to/key/file",
+        )
 
     with pytest.raises(ValueError):
-        SFTP(host='host', username='sam', password=None, rsa_private_key_file=None)
+        SFTP(host="host", username="sam", password=None, rsa_private_key_file=None)
 
 
 @mark_live_test
 def test_list_non_existent_directory(live_sftp):
-    file_list = live_sftp.list_directory('abc123')
+    file_list = live_sftp.list_directory("abc123")
     assert len(file_list) == 0
 
 
 @mark_live_test
 def test_list_directory_with_files(live_sftp):
     file_list = live_sftp.list_directory(REMOTE_DIR)
     assert len(file_list) == 2
     assert REMOTE_COMPRESSED_CSV in file_list
     assert REMOTE_CSV in file_list
 
 
 @mark_live_test
 def test_get_non_existent_file(live_sftp):
     with pytest.raises(FileNotFoundError):
-        live_sftp.get_file('abc123')
+        live_sftp.get_file("abc123")
 
 
 # Helper function
 def assert_file_matches_table(local_path, table):
     downloaded_tbl = Table.from_csv(local_path)
     assert_matching_tables(table, downloaded_tbl)
 
@@ -96,52 +110,62 @@
 @mark_live_test
 def test_get_temp_file(live_sftp, simple_table):  # noqa: F811
     local_path = live_sftp.get_file(REMOTE_CSV_PATH)
     assert_file_matches_table(local_path, simple_table)
 
 
 @mark_live_test
-@pytest.mark.parametrize('compression', [None, 'gzip'])
+@pytest.mark.parametrize("compression", [None, "gzip"])
 def test_table_to_sftp_csv(live_sftp, simple_table, compression):  # noqa: F811
-    host = os.environ['SFTP_HOST']
-    username = os.environ['SFTP_USERNAME']
-    password = os.environ['SFTP_PASSWORD']
-    rsa_private_key_file = os.environ['SFTP_RSA_PRIVATE_KEY_FILE']
-
-    remote_path = f'{REMOTE_DIR}/test_to_sftp.csv'
-    if compression == 'gzip':
-        remote_path += '.gz'
+    host = os.environ["SFTP_HOST"]
+    username = os.environ["SFTP_USERNAME"]
+    password = os.environ["SFTP_PASSWORD"]
+    rsa_private_key_file = os.environ["SFTP_RSA_PRIVATE_KEY_FILE"]
+
+    remote_path = f"{REMOTE_DIR}/test_to_sftp.csv"
+    if compression == "gzip":
+        remote_path += ".gz"
 
     simple_table.to_sftp_csv(
-        remote_path, host, username, password,
+        remote_path,
+        host,
+        username,
+        password,
         rsa_private_key_file=rsa_private_key_file,
-        compression=compression)
+        compression=compression,
+    )
 
     local_path = live_sftp.get_file(remote_path)
     assert_file_matches_table(local_path, simple_table)
 
     # Cleanup
     live_sftp.remove_file(remote_path)
 
 
 @mark_live_test
-@pytest.mark.parametrize('compression', [None, 'gzip'])
-def test_table_to_sftp_csv_no_password(live_sftp, simple_table, compression):  # noqa: F811
-    host = os.environ.get('SFTP_HOST')
-    username = os.environ.get('SFTP_USERNAME')
-    rsa_private_key_file = os.environ.get('SFTP_RSA_PRIVATE_KEY_FILE')
-
-    remote_path = f'{REMOTE_DIR}/test_to_sftp.csv'
-    if compression == 'gzip':
-        remote_path += '.gz'
+@pytest.mark.parametrize("compression", [None, "gzip"])
+def test_table_to_sftp_csv_no_password(
+    live_sftp, simple_table, compression  # noqa: F811
+):
+    host = os.environ.get("SFTP_HOST")
+    username = os.environ.get("SFTP_USERNAME")
+    rsa_private_key_file = os.environ.get("SFTP_RSA_PRIVATE_KEY_FILE")
+
+    remote_path = f"{REMOTE_DIR}/test_to_sftp.csv"
+    if compression == "gzip":
+        remote_path += ".gz"
 
     simple_table.to_sftp_csv(
-        remote_path, host, username, None,
+        remote_path,
+        host,
+        username,
+        None,
         rsa_private_key_file=rsa_private_key_file,
-        compression=compression)
+        compression=compression,
+    )
 
     local_path = live_sftp.get_file(remote_path)
     assert_file_matches_table(local_path, simple_table)
 
     # Cleanup
     live_sftp.remove_file(remote_path)
```

### Comparing `parsons-1.0.0/test/test_shopify.py` & `parsons-1.1.0/test/test_shopify.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,123 +1,168 @@
 from parsons import Table, Shopify
 from test.utils import assert_matching_tables
 import requests_mock
 import unittest
 
-SUBDOMAIN = 'myorg'
-PASSWORD = 'abc123'
-API_KEY = 'abc123'
-API_VERSION = '2020-10'
+SUBDOMAIN = "myorg"
+PASSWORD = "abc123"
+API_KEY = "abc123"
+API_VERSION = "2020-10"
 
 
 class TestShopify(unittest.TestCase):
-    mock_count_all = {
-        'count': 2
-    }
-    mock_count_date = mock_count_since = {
-        'count': 1
-    }
-    mock_graphql = {
-        'data': {
-            'orders': {
-                'edges': [{
-                    'node': {
-                        'id': 1
-                    }
-                }]
-            }
-        }
-    }
+    mock_count_all = {"count": 2}
+    mock_count_date = mock_count_since = {"count": 1}
+    mock_graphql = {"data": {"orders": {"edges": [{"node": {"id": 1}}]}}}
     mock_orders_all = {
-        'orders': [{
-            'created_at': '2020-10-19T12:00:00-04:00',
-            'financial_status': 'paid',
-            'id': 1
-        }, {
-            'created_at': '2020-10-20T12:00:00-04:00',
-            'financial_status': 'refunded',
-            'id': 2
-        }]
+        "orders": [
+            {
+                "created_at": "2020-10-19T12:00:00-04:00",
+                "financial_status": "paid",
+                "id": 1,
+            },
+            {
+                "created_at": "2020-10-20T12:00:00-04:00",
+                "financial_status": "refunded",
+                "id": 2,
+            },
+        ]
     }
     mock_orders_completed = {
-        'orders': [{
-            'created_at': '2020-10-19T12:00:00-04:00',
-            'financial_status': 'paid',
-            'id': 1
-        }]
+        "orders": [
+            {
+                "created_at": "2020-10-19T12:00:00-04:00",
+                "financial_status": "paid",
+                "id": 1,
+            }
+        ]
     }
     mock_orders_date = mock_orders_since = {
-        'orders': [{
-            'created_at': '2020-10-20T12:00:00-04:00',
-            'financial_status': 'refunded',
-            'id': 2
-        }]
+        "orders": [
+            {
+                "created_at": "2020-10-20T12:00:00-04:00",
+                "financial_status": "refunded",
+                "id": 2,
+            }
+        ]
     }
-    mock_result_all = Table([('created_at', 'financial_status', 'id'),
-                             ('2020-10-19T12:00:00-04:00', 'paid', 1), ('2020-10-20T12:00:00-04:00',
-                                                                        'refunded', 2)])
-    mock_result_completed = Table([('created_at', 'financial_status', 'id'),
-                                   ('2020-10-19T12:00:00-04:00', 'paid', 1)])
-    mock_result_date = mock_result_since = Table([('created_at', 'financial_status', 'id'),
-                                                  ('2020-10-20T12:00:00-04:00', 'refunded', 2)])
+    mock_result_all = Table(
+        [
+            ("created_at", "financial_status", "id"),
+            ("2020-10-19T12:00:00-04:00", "paid", 1),
+            ("2020-10-20T12:00:00-04:00", "refunded", 2),
+        ]
+    )
+    mock_result_completed = Table(
+        [
+            ("created_at", "financial_status", "id"),
+            ("2020-10-19T12:00:00-04:00", "paid", 1),
+        ]
+    )
+    mock_result_date = mock_result_since = Table(
+        [
+            ("created_at", "financial_status", "id"),
+            ("2020-10-20T12:00:00-04:00", "refunded", 2),
+        ]
+    )
 
     def setUp(self):
         self.shopify = Shopify(SUBDOMAIN, PASSWORD, API_KEY, API_VERSION)
 
     @requests_mock.Mocker()
     def test_get_count(self, m):
-        m.get(self.shopify.get_query_url(None, None, "orders", True), json=self.mock_count_all)
-        m.get(self.shopify.get_query_url('2020-10-20', None, "orders", True),
-              json=self.mock_count_date)
-        m.get(self.shopify.get_query_url(None, 2, "orders", True), json=self.mock_count_since)
+        m.get(
+            self.shopify.get_query_url(None, None, "orders", True),
+            json=self.mock_count_all,
+        )
+        m.get(
+            self.shopify.get_query_url("2020-10-20", None, "orders", True),
+            json=self.mock_count_date,
+        )
+        m.get(
+            self.shopify.get_query_url(None, 2, "orders", True),
+            json=self.mock_count_since,
+        )
         self.assertEqual(self.shopify.get_count(None, None, "orders"), 2)
-        self.assertEqual(self.shopify.get_count('2020-10-20', None, "orders"), 1)
+        self.assertEqual(self.shopify.get_count("2020-10-20", None, "orders"), 1)
         self.assertEqual(self.shopify.get_count(None, 2, "orders"), 1)
 
     @requests_mock.Mocker()
     def test_get_orders(self, m):
-        m.get(self.shopify.get_query_url(None, None, 'orders', False), json=self.mock_orders_all)
-        m.get(self.shopify.get_query_url('2020-10-20', None, 'orders', False),
-              json=self.mock_orders_date)
-        m.get(self.shopify.get_query_url(None, 2, 'orders', False), json=self.mock_orders_since)
-        m.get(self.shopify.get_query_url(None, None, 'orders', False) + '&financial_status=paid',
-              json=self.mock_orders_completed)
-        assert_matching_tables(self.shopify.get_orders(None, None, False), self.mock_result_all)
-        assert_matching_tables(self.shopify.get_orders('2020-10-20', None, False),
-                               self.mock_result_date)
-        assert_matching_tables(self.shopify.get_orders(None, 2, False), self.mock_result_since)
-        assert_matching_tables(self.shopify.get_orders(None, None, True),
-                               self.mock_result_completed)
+        m.get(
+            self.shopify.get_query_url(None, None, "orders", False),
+            json=self.mock_orders_all,
+        )
+        m.get(
+            self.shopify.get_query_url("2020-10-20", None, "orders", False),
+            json=self.mock_orders_date,
+        )
+        m.get(
+            self.shopify.get_query_url(None, 2, "orders", False),
+            json=self.mock_orders_since,
+        )
+        m.get(
+            self.shopify.get_query_url(None, None, "orders", False)
+            + "&financial_status=paid",
+            json=self.mock_orders_completed,
+        )
+        assert_matching_tables(
+            self.shopify.get_orders(None, None, False), self.mock_result_all
+        )
+        assert_matching_tables(
+            self.shopify.get_orders("2020-10-20", None, False), self.mock_result_date
+        )
+        assert_matching_tables(
+            self.shopify.get_orders(None, 2, False), self.mock_result_since
+        )
+        assert_matching_tables(
+            self.shopify.get_orders(None, None, True), self.mock_result_completed
+        )
 
     @requests_mock.Mocker()
     def test_get_query_url(self, m):
-        self.assertEqual(self.shopify.get_query_url(None, None, "orders", True),
-                         f'https://{SUBDOMAIN}.myshopify.com/admin/api/{API_VERSION}/orders/' +
-                         'count.json?limit=250&status=any')
-        self.assertEqual(self.shopify.get_query_url('2020-10-20', None, "orders", True),
-                         f'https://{SUBDOMAIN}.myshopify.com/admin/api/{API_VERSION}/orders/' +
-                         'count.json?limit=250&status=any&created_at_min=2020-10-20T00:00:00&' +
-                         'created_at_max=2020-10-21T00:00:00')
-        self.assertEqual(self.shopify.get_query_url(None, 2, "orders", True),
-                         f'https://{SUBDOMAIN}.myshopify.com/admin/api/{API_VERSION}/orders/' +
-                         'count.json?limit=250&status=any&since_id=2')
-        self.assertEqual(self.shopify.get_query_url(None, None, "orders", False),
-                         f'https://{SUBDOMAIN}.myshopify.com/admin/api/{API_VERSION}/orders.json?' +
-                         'limit=250&status=any')
+        self.assertEqual(
+            self.shopify.get_query_url(None, None, "orders", True),
+            f"https://{SUBDOMAIN}.myshopify.com/admin/api/{API_VERSION}/orders/"
+            + "count.json?limit=250&status=any",
+        )
+        self.assertEqual(
+            self.shopify.get_query_url("2020-10-20", None, "orders", True),
+            f"https://{SUBDOMAIN}.myshopify.com/admin/api/{API_VERSION}/orders/"
+            + "count.json?limit=250&status=any&created_at_min=2020-10-20T00:00:00&"
+            + "created_at_max=2020-10-21T00:00:00",
+        )
+        self.assertEqual(
+            self.shopify.get_query_url(None, 2, "orders", True),
+            f"https://{SUBDOMAIN}.myshopify.com/admin/api/{API_VERSION}/orders/"
+            + "count.json?limit=250&status=any&since_id=2",
+        )
+        self.assertEqual(
+            self.shopify.get_query_url(None, None, "orders", False),
+            f"https://{SUBDOMAIN}.myshopify.com/admin/api/{API_VERSION}/orders.json?"
+            + "limit=250&status=any",
+        )
 
     @requests_mock.Mocker()
     def test_graphql(self, m):
         m.post(
-            'https://{0}.myshopify.com/admin/api/{1}/graphql.json'.format(SUBDOMAIN, API_VERSION),
-            json=self.mock_graphql
+            "https://{0}.myshopify.com/admin/api/{1}/graphql.json".format(
+                SUBDOMAIN, API_VERSION
+            ),
+            json=self.mock_graphql,
         )
-        self.assertEqual(self.shopify.graphql("""
+        self.assertEqual(
+            self.shopify.graphql(
+                """
             {{
                 orders(query: "financial_status:=paid", first: 100) {{
                     edges {{
                         node {{
                             id
                         }}
                     }}
                 }}
             }}
-        """), self.mock_graphql['data'])
+        """
+            ),
+            self.mock_graphql["data"],
+        )
```

### Comparing `parsons-1.0.0/test/test_smtp.py` & `parsons-1.1.0/test/test_smtp.py`

 * *Files 21% similar despite different names*

```diff
@@ -2,103 +2,142 @@
 import base64
 import io
 import re
 import unittest
 
 
 class FakeConnection(object):
-
     def __init__(self, result_obj):
         self.result_obj = result_obj
 
     def sendmail(self, sender, to, message_body):
         self.result_obj.result = (sender, to, message_body)
-        if 'willfail@example.com' in to:
+        if "willfail@example.com" in to:
             return {"willfail@example.com": (550, "User unknown")}
 
     def quit(self):
         self.result_obj.quit_ran = True
 
 
 class TestSMTP(unittest.TestCase):
-
     def setUp(self):
-        self.smtp = SMTP('fake.example.com', username='fake', password='fake')
+        self.smtp = SMTP("fake.example.com", username="fake", password="fake")
         self.smtp.conn = FakeConnection(self)
         self.result = None
         self.quit_ran = False
 
     def test_send_message_simple(self):
-        self.smtp.send_email('foo@example.com', 'recipient1@example.com',
-                             'Simple subject', 'Fake body')
-        self.assertEqual(self.result[0], 'foo@example.com')
-        self.assertEqual(self.result[1], ['recipient1@example.com'])
+        self.smtp.send_email(
+            "foo@example.com", "recipient1@example.com", "Simple subject", "Fake body"
+        )
+        self.assertEqual(self.result[0], "foo@example.com")
+        self.assertEqual(self.result[1], ["recipient1@example.com"])
         self.assertTrue(
             self.result[2].endswith(
-                '\nto: recipient1@example.com\nfrom: foo@example.com'
-                '\nsubject: Simple subject\n\nFake body'
-            ))
+                "\nto: recipient1@example.com\nfrom: foo@example.com"
+                "\nsubject: Simple subject\n\nFake body"
+            )
+        )
         self.assertTrue(self.quit_ran)
 
     def test_send_message_html(self):
-        self.smtp.send_email('foohtml@example.com', 'recipienthtml@example.com',
-                             'Simple subject', 'Fake body', '<p>Really Fake html</p>')
-        self.assertEqual(self.result[0], 'foohtml@example.com')
-        self.assertEqual(self.result[1], ['recipienthtml@example.com'])
-        self.assertRegex(self.result[2], r'<p>Really Fake html</p>\n--=======')
-        self.assertRegex(self.result[2], r'\nFake body\n--======')
-        self.assertRegex(self.result[2], r'ubject: Simple subject\n')
+        self.smtp.send_email(
+            "foohtml@example.com",
+            "recipienthtml@example.com",
+            "Simple subject",
+            "Fake body",
+            "<p>Really Fake html</p>",
+        )
+        self.assertEqual(self.result[0], "foohtml@example.com")
+        self.assertEqual(self.result[1], ["recipienthtml@example.com"])
+        self.assertRegex(self.result[2], r"<p>Really Fake html</p>\n--=======")
+        self.assertRegex(self.result[2], r"\nFake body\n--======")
+        self.assertRegex(self.result[2], r"ubject: Simple subject\n")
         self.assertTrue(self.quit_ran)
 
     def test_send_message_manualclose(self):
-        smtp = SMTP('fake.example.com', username='fake', password='fake',
-                    close_manually=True)
+        smtp = SMTP(
+            "fake.example.com", username="fake", password="fake", close_manually=True
+        )
         smtp.conn = FakeConnection(self)
-        smtp.send_email('foo@example.com', 'recipient1@example.com', 'Simple subject', 'Fake body')
+        smtp.send_email(
+            "foo@example.com", "recipient1@example.com", "Simple subject", "Fake body"
+        )
         self.assertFalse(self.quit_ran)
 
     def test_send_message_files(self):
-        named_file_content = 'x,y,z\n1,2,3\r\n3,4,5\r\n'
-        unnamed_file_content = 'foo,bar\n1,2\r\n3,4\r\n'
+        named_file_content = "x,y,z\n1,2,3\r\n3,4,5\r\n"
+        unnamed_file_content = "foo,bar\n1,2\r\n3,4\r\n"
         bytes_file_content = bytes(
-            [71, 73, 70, 56, 57, 97, 1, 0, 1, 0, 0, 255,
-             0, 44, 0, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 59])
+            [
+                71,
+                73,
+                70,
+                56,
+                57,
+                97,
+                1,
+                0,
+                1,
+                0,
+                0,
+                255,
+                0,
+                44,
+                0,
+                0,
+                0,
+                0,
+                1,
+                0,
+                1,
+                0,
+                0,
+                2,
+                0,
+                59,
+            ]
+        )
         named_file = io.StringIO(named_file_content)
-        named_file.name = 'xyz.csv'
+        named_file.name = "xyz.csv"
 
         bytes_file = io.BytesIO(bytes_file_content)
-        bytes_file.name = 'xyz.gif'
+        bytes_file.name = "xyz.gif"
 
         self.smtp.send_email(
-            'foofiles@example.com', 'recipientfiles@example.com', 'Simple subject', 'Fake body',
-            files=[io.StringIO(unnamed_file_content), named_file, bytes_file]
+            "foofiles@example.com",
+            "recipientfiles@example.com",
+            "Simple subject",
+            "Fake body",
+            files=[io.StringIO(unnamed_file_content), named_file, bytes_file],
         )
-        self.assertEqual(self.result[0], 'foofiles@example.com')
-        self.assertEqual(self.result[1], ['recipientfiles@example.com'])
-        self.assertRegex(self.result[2], r'\nFake body\n--======')
+        self.assertEqual(self.result[0], "foofiles@example.com")
+        self.assertEqual(self.result[1], ["recipientfiles@example.com"])
+        self.assertRegex(self.result[2], r"\nFake body\n--======")
         found = re.findall(r'filename="file"\n\n([\w=/]+)\n\n--===', self.result[2])
-        self.assertEqual(base64.b64decode(found[0]).decode(),
-                         unnamed_file_content)
+        self.assertEqual(base64.b64decode(found[0]).decode(), unnamed_file_content)
         found_named = re.findall(
             r'Content-Type: text/csv; charset="utf-8"\nMIME-Version: 1.0'
-            r'\nContent-Transfer-Encoding: base64\nContent-Disposition: '
+            r"\nContent-Transfer-Encoding: base64\nContent-Disposition: "
             r'attachment; filename="xyz.csv"\n\n([\w=/]+)\n\n--======',
-            self.result[2])
-        self.assertEqual(base64.b64decode(found_named[0]).decode(),
-                         named_file_content)
+            self.result[2],
+        )
+        self.assertEqual(base64.b64decode(found_named[0]).decode(), named_file_content)
 
         found_gif = re.findall(
-            r'Content-Type: image/gif\nMIME-Version: 1.0'
-            r'\nContent-Transfer-Encoding: base64\nContent-ID: <xyz.gif>'
+            r"Content-Type: image/gif\nMIME-Version: 1.0"
+            r"\nContent-Transfer-Encoding: base64\nContent-ID: <xyz.gif>"
             r'\nContent-Disposition: attachment; filename="xyz.gif"\n\n([\w=/]+)\n\n--==',
-            self.result[2])
-        self.assertEqual(base64.b64decode(found_gif[0]),
-                         bytes_file_content)
+            self.result[2],
+        )
+        self.assertEqual(base64.b64decode(found_gif[0]), bytes_file_content)
         self.assertTrue(self.quit_ran)
 
     def test_send_message_partial_fail(self):
         simple_msg = self.smtp._create_message_simple(
-            'foo@example.com',
-            'recipient1@example.com, willfail@example.com',
-            'Simple subject', 'Fake body')
+            "foo@example.com",
+            "recipient1@example.com, willfail@example.com",
+            "Simple subject",
+            "Fake body",
+        )
         send_result = self.smtp._send_message(simple_msg)
-        self.assertEqual(send_result,  {"willfail@example.com": (550, "User unknown")})
+        self.assertEqual(send_result, {"willfail@example.com": (550, "User unknown")})
```

### Comparing `parsons-1.0.0/test/test_targetsmart/test_targetsmart_api.py` & `parsons-1.1.0/test/test_targetsmart/test_targetsmart_api.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,152 +1,220 @@
 import unittest
 import requests_mock
 from parsons import TargetSmartAPI, Table
 from test.utils import validate_list
 from test.responses.ts_responses import (
-    address_response, district_point, district_expected, district_zip, zip_expected,
-    phone_response, phone_expected, radius_response)
-
-output_list = [{
-    'vb.tsmart_zip': '60625',
-    'vb.vf_g2014': 'Y',
-    'vb.vf_g2016': 'Y',
-    'vb.tsmart_middle_name': 'H',
-    'ts.tsmart_midterm_general_turnout_score': '85.5',
-    'vb.tsmart_name_suffix': '',
-    'vb.voterbase_gender': 'Male',
-    'vb.tsmart_city': 'CHICAGO',
-    'vb.tsmart_full_address': '908 N MAIN AVE APT 2',
-    'vb.voterbase_phone': '5125705356',
-    'vb.tsmart_partisan_score': '99.6',
-    'vb.tsmart_last_name': 'BLANKS',
-    'vb.voterbase_id': 'IL-12568670',
-    'vb.tsmart_first_name': 'BILLY',
-    'vb.voterid': 'Q8W8R82Z',
-    'vb.voterbase_age': '37',
-    'vb.tsmart_state': 'IL',
-    'vb.voterbase_registration_status': 'Registered'
-}]
+    address_response,
+    district_point,
+    district_expected,
+    district_zip,
+    zip_expected,
+    phone_response,
+    phone_expected,
+    radius_response,
+)
+
+output_list = [
+    {
+        "vb.tsmart_zip": "60625",
+        "vb.vf_g2014": "Y",
+        "vb.vf_g2016": "Y",
+        "vb.tsmart_middle_name": "H",
+        "ts.tsmart_midterm_general_turnout_score": "85.5",
+        "vb.tsmart_name_suffix": "",
+        "vb.voterbase_gender": "Male",
+        "vb.tsmart_city": "CHICAGO",
+        "vb.tsmart_full_address": "908 N MAIN AVE APT 2",
+        "vb.voterbase_phone": "5125705356",
+        "vb.tsmart_partisan_score": "99.6",
+        "vb.tsmart_last_name": "BLANKS",
+        "vb.voterbase_id": "IL-12568670",
+        "vb.tsmart_first_name": "BILLY",
+        "vb.voterid": "Q8W8R82Z",
+        "vb.voterbase_age": "37",
+        "vb.tsmart_state": "IL",
+        "vb.voterbase_registration_status": "Registered",
+    }
+]
 
 
 class TestTargetSmartAPI(unittest.TestCase):
-
     def setUp(self):
 
-        self.ts = TargetSmartAPI(api_key='FAKEKEY')
+        self.ts = TargetSmartAPI(api_key="FAKEKEY")
 
     def tearDown(self):
 
         pass
 
     @requests_mock.Mocker()
     def test_data_enhance(self, m):
 
-        json = {'input':
-                {'search_id': 'IL-12568670',
-                 'search_id_type': 'voterbase'},
-                'error': None,
-                'output': output_list,
-                'output_size': 1,
-                'match_found': True,
-                'gateway_id': 'b8c86f27-fb32-11e8-9cc1-45bc340a4d22',
-                'function_id': 'b8c98093-fb32-11e8-8b25-e99c70f6fe74'}
-
-        expected = ['vb.tsmart_zip', 'vb.vf_g2014', 'vb.vf_g2016', 'vb.tsmart_middle_name',
-                    'ts.tsmart_midterm_general_turnout_score', 'vb.tsmart_name_suffix',
-                    'vb.voterbase_gender', 'vb.tsmart_city', 'vb.tsmart_full_address',
-                    'vb.voterbase_phone', 'vb.tsmart_partisan_score', 'vb.tsmart_last_name',
-                    'vb.voterbase_id', 'vb.tsmart_first_name', 'vb.voterid', 'vb.voterbase_age',
-                    'vb.tsmart_state', 'vb.voterbase_registration_status']
+        json = {
+            "input": {"search_id": "IL-12568670", "search_id_type": "voterbase"},
+            "error": None,
+            "output": output_list,
+            "output_size": 1,
+            "match_found": True,
+            "gateway_id": "b8c86f27-fb32-11e8-9cc1-45bc340a4d22",
+            "function_id": "b8c98093-fb32-11e8-8b25-e99c70f6fe74",
+        }
+
+        expected = [
+            "vb.tsmart_zip",
+            "vb.vf_g2014",
+            "vb.vf_g2016",
+            "vb.tsmart_middle_name",
+            "ts.tsmart_midterm_general_turnout_score",
+            "vb.tsmart_name_suffix",
+            "vb.voterbase_gender",
+            "vb.tsmart_city",
+            "vb.tsmart_full_address",
+            "vb.voterbase_phone",
+            "vb.tsmart_partisan_score",
+            "vb.tsmart_last_name",
+            "vb.voterbase_id",
+            "vb.tsmart_first_name",
+            "vb.voterid",
+            "vb.voterbase_age",
+            "vb.tsmart_state",
+            "vb.voterbase_registration_status",
+        ]
 
-        m.get(self.ts.connection.uri + 'person/data-enhance', json=json)
+        m.get(self.ts.connection.uri + "person/data-enhance", json=json)
 
         # Assert response is expected structure
-        self.assertTrue(validate_list(expected, self.ts.data_enhance('IL-12568678')))
+        self.assertTrue(validate_list(expected, self.ts.data_enhance("IL-12568678")))
 
         # Assert exception on missing state
         with self.assertRaises(Exception):
-            self.ts.data_enhance('vb0001', search_id_type='votebuilder')
+            self.ts.data_enhance("vb0001", search_id_type="votebuilder")
 
         # Assert exception on missing state
         with self.assertRaises(Exception):
-            self.ts.data_enhance('vb0001', search_id_type='smartvan')
+            self.ts.data_enhance("vb0001", search_id_type="smartvan")
 
         # Assert exception on missing state
         with self.assertRaises(Exception):
-            self.ts.data_enhance('vb0001', search_id_type='voter')
+            self.ts.data_enhance("vb0001", search_id_type="voter")
 
         # Assert works with state provided
-        for i in ['votebuilder', 'voter', 'smartvan']:
-            self.assertTrue(validate_list(expected, self.ts.data_enhance(
-                'IL-12568678',
-                search_id_type=i,
-                state='IL')))
+        for i in ["votebuilder", "voter", "smartvan"]:
+            self.assertTrue(
+                validate_list(
+                    expected,
+                    self.ts.data_enhance("IL-12568678", search_id_type=i, state="IL"),
+                )
+            )
 
     @requests_mock.Mocker()
     def test_radius_search(self, m):
 
-        m.get(self.ts.connection.uri + 'person/radius-search', json=radius_response)
+        m.get(self.ts.connection.uri + "person/radius-search", json=radius_response)
 
-        expected = ['similarity_score', 'distance_km', 'distance_meters', 'distance_miles',
-                    'distance_feet', 'proximity_score', 'composite_score', 'uniqueness_score',
-                    'confidence_indicator', 'ts.tsmart_midterm_general_turnout_score',
-                    'vb.tsmart_city', 'vb.tsmart_first_name', 'vb.tsmart_full_address',
-                    'vb.tsmart_last_name', 'vb.tsmart_middle_name', 'vb.tsmart_name_suffix',
-                    'vb.tsmart_partisan_score', 'vb.tsmart_precinct_id', 'vb.tsmart_precinct_name',
-                    'vb.tsmart_state', 'vb.tsmart_zip', 'vb.tsmart_zip4',
-                    'vb.vf_earliest_registration_date', 'vb.vf_g2014', 'vb.vf_g2016',
-                    'vb.vf_precinct_id', 'vb.vf_precinct_name', 'vb.vf_reg_cass_address_full',
-                    'vb.vf_reg_cass_city', 'vb.vf_reg_cass_state', 'vb.vf_reg_cass_zip',
-                    'vb.vf_reg_cass_zip4', 'vb.vf_registration_date', 'vb.voterbase_age',
-                    'vb.voterbase_gender', 'vb.voterbase_id', 'vb.voterbase_phone',
-                    'vb.voterbase_registration_status', 'vb.voterid']
+        expected = [
+            "similarity_score",
+            "distance_km",
+            "distance_meters",
+            "distance_miles",
+            "distance_feet",
+            "proximity_score",
+            "composite_score",
+            "uniqueness_score",
+            "confidence_indicator",
+            "ts.tsmart_midterm_general_turnout_score",
+            "vb.tsmart_city",
+            "vb.tsmart_first_name",
+            "vb.tsmart_full_address",
+            "vb.tsmart_last_name",
+            "vb.tsmart_middle_name",
+            "vb.tsmart_name_suffix",
+            "vb.tsmart_partisan_score",
+            "vb.tsmart_precinct_id",
+            "vb.tsmart_precinct_name",
+            "vb.tsmart_state",
+            "vb.tsmart_zip",
+            "vb.tsmart_zip4",
+            "vb.vf_earliest_registration_date",
+            "vb.vf_g2014",
+            "vb.vf_g2016",
+            "vb.vf_precinct_id",
+            "vb.vf_precinct_name",
+            "vb.vf_reg_cass_address_full",
+            "vb.vf_reg_cass_city",
+            "vb.vf_reg_cass_state",
+            "vb.vf_reg_cass_zip",
+            "vb.vf_reg_cass_zip4",
+            "vb.vf_registration_date",
+            "vb.voterbase_age",
+            "vb.voterbase_gender",
+            "vb.voterbase_id",
+            "vb.voterbase_phone",
+            "vb.voterbase_registration_status",
+            "vb.voterid",
+        ]
 
         # Assert response is expected structure
         def rad_search():
-            return self.ts.radius_search('BILLY', 'Burchard', radius_size=100,
-                                         address='908 N Washtenaw, Chicago, IL')
+            return self.ts.radius_search(
+                "BILLY",
+                "Burchard",
+                radius_size=100,
+                address="908 N Washtenaw, Chicago, IL",
+            )
 
         self.assertTrue(validate_list(expected, rad_search()))
 
     def test_district_args(self):
 
-        self.assertRaises(ValueError, self.ts.district, search_type='address')
-        self.assertRaises(ValueError, self.ts.district, search_type='zip', zip4=9)
-        self.assertRaises(ValueError, self.ts.district, search_type='zip', zip5=0)
-        self.assertRaises(ValueError, self.ts.district, search_type='point')
-        self.assertRaises(ValueError, self.ts.district, search_type='zip')
+        self.assertRaises(ValueError, self.ts.district, search_type="address")
+        self.assertRaises(ValueError, self.ts.district, search_type="zip", zip4=9)
+        self.assertRaises(ValueError, self.ts.district, search_type="zip", zip5=0)
+        self.assertRaises(ValueError, self.ts.district, search_type="point")
+        self.assertRaises(ValueError, self.ts.district, search_type="zip")
 
     @requests_mock.Mocker()
     def test_district_point(self, m):
 
         # Test Points
-        m.get(self.ts.connection.uri + 'service/district', json=district_point)
-        self.assertTrue(validate_list(district_expected,
-                                      self.ts.district(search_type='point',
-                                                       latitude='41.898369',
-                                                       longitude='-87.694382')))
+        m.get(self.ts.connection.uri + "service/district", json=district_point)
+        self.assertTrue(
+            validate_list(
+                district_expected,
+                self.ts.district(
+                    search_type="point", latitude="41.898369", longitude="-87.694382"
+                ),
+            )
+        )
 
     @requests_mock.Mocker()
     def test_district_zip(self, m):
         # Test Zips
-        m.get(self.ts.connection.uri + 'service/district', json=district_zip)
-        self.assertTrue(validate_list(zip_expected,
-                                      self.ts.district(search_type='zip',
-                                                       zip5='60622',
-                                                       zip4='7194')))
+        m.get(self.ts.connection.uri + "service/district", json=district_zip)
+        self.assertTrue(
+            validate_list(
+                zip_expected,
+                self.ts.district(search_type="zip", zip5="60622", zip4="7194"),
+            )
+        )
 
     @requests_mock.Mocker()
     def test_district_address(self, m):
         # Test Address
-        m.get(self.ts.connection.uri + 'service/district', json=address_response)
-        self.assertTrue(validate_list(district_expected,
-                                      self.ts.district(search_type='address',
-                                                       address='908 N Main St, Chicago, IL 60611')))
+        m.get(self.ts.connection.uri + "service/district", json=address_response)
+        self.assertTrue(
+            validate_list(
+                district_expected,
+                self.ts.district(
+                    search_type="address", address="908 N Main St, Chicago, IL 60611"
+                ),
+            )
+        )
 
     @requests_mock.Mocker()
     def test_phone(self, m):
 
         # Test phone
-        m.get(self.ts.connection.uri + 'person/phone-search', json=phone_response)
-        self.assertTrue(validate_list(phone_expected,
-                                      self.ts.phone(Table([{'phone': 4435705355}]))))
+        m.get(self.ts.connection.uri + "person/phone-search", json=phone_response)
+        self.assertTrue(
+            validate_list(phone_expected, self.ts.phone(Table([{"phone": 4435705355}])))
+        )
```

### Comparing `parsons-1.0.0/test/test_targetsmart/test_targetsmart_automation.py` & `parsons-1.1.0/test/test_targetsmart/test_targetsmart_automation.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,74 +1,79 @@
 from parsons import TargetSmartAutomation, SFTP
 import unittest
 from test.utils import mark_live_test
 import os
 
 
 class TestTargetSmartAutomation(unittest.TestCase):
-
     def setUp(self):
 
         self.ts = TargetSmartAutomation()
-        self.job_name = 'a-test-job'
-        self.sftp = SFTP(self.ts.sftp_host,
-                         os.environ['TS_SFTP_USERNAME'],
-                         os.environ['TS_SFTP_PASSWORD'],
-                         self.ts.sftp_port)
-        self.test_xml = 'test/test_targetsmart/job_config.xml'
+        self.job_name = "a-test-job"
+        self.sftp = SFTP(
+            self.ts.sftp_host,
+            os.environ["TS_SFTP_USERNAME"],
+            os.environ["TS_SFTP_PASSWORD"],
+            self.ts.sftp_port,
+        )
+        self.test_xml = "test/test_targetsmart/job_config.xml"
 
     def tearDown(self):
 
         # Clean up the files were put on the SFTP
         self.ts.remove_files(self.job_name)
 
     @mark_live_test
     def test_create_job_xml(self):
 
         # Assert that job xml creates the file correctly
-        job_xml = self.ts.create_job_xml('job_type',
-                                         'match_job',
-                                         ['test@gmail.com', 'test2@gmail.com'])
-        with open(self.test_xml, 'r') as xml:
+        job_xml = self.ts.create_job_xml(
+            "job_type", "match_job", ["test@gmail.com", "test2@gmail.com"]
+        )
+        with open(self.test_xml, "r") as xml:
             test_xml = xml.read()
-        with open(job_xml, 'r') as xml:
+        with open(job_xml, "r") as xml:
             real_xml = xml.read()
         self.assertEqual(test_xml, real_xml)
 
     @mark_live_test
     def test_config_status(self):
 
         # Find good configuration
-        self.sftp.put_file(self.test_xml, f'{self.ts.sftp_dir}/{self.job_name}.job.xml.good')
+        self.sftp.put_file(
+            self.test_xml, f"{self.ts.sftp_dir}/{self.job_name}.job.xml.good"
+        )
         self.assertTrue(self.ts.config_status(self.job_name))
         self.ts.remove_files(self.job_name)
 
         # Find bad configuration
-        self.sftp.put_file(self.test_xml, f'{self.ts.sftp_dir}/{self.job_name}.job.xml.bad')
+        self.sftp.put_file(
+            self.test_xml, f"{self.ts.sftp_dir}/{self.job_name}.job.xml.bad"
+        )
         self.assertRaises(ValueError, self.ts.config_status, self.job_name)
 
     @mark_live_test
     def test_match_status(self):
 
         # Find good configuration
-        good_match = 'test/test_targetsmart/match_good.xml'
-        self.sftp.put_file(good_match, f'{self.ts.sftp_dir}/{self.job_name}.finish.xml')
+        good_match = "test/test_targetsmart/match_good.xml"
+        self.sftp.put_file(good_match, f"{self.ts.sftp_dir}/{self.job_name}.finish.xml")
         self.assertTrue(self.ts.match_status(self.job_name))
         self.ts.remove_files(self.job_name)
 
         # Find bad configuration
-        bad_match = 'test/test_targetsmart/match_bad.xml'
-        self.sftp.put_file(bad_match, f'{self.ts.sftp_dir}/{self.job_name}.finish.xml')
+        bad_match = "test/test_targetsmart/match_bad.xml"
+        self.sftp.put_file(bad_match, f"{self.ts.sftp_dir}/{self.job_name}.finish.xml")
         self.assertRaises(ValueError, self.ts.match_status, self.job_name)
 
     @mark_live_test
     def test_remove_files(self):
 
         # Add a file
-        self.sftp.put_file(self.test_xml, f'{self.ts.sftp_dir}/{self.job_name}.txt')
+        self.sftp.put_file(self.test_xml, f"{self.ts.sftp_dir}/{self.job_name}.txt")
 
         # Remove files
         self.ts.remove_files(self.job_name)
 
         # Check that file is not there
-        dir_list = self.sftp.list_directory(f'{self.ts.sftp_dir}/')
-        self.assertNotIn(f'{self.job_name}.txt', dir_list)
+        dir_list = self.sftp.list_directory(f"{self.ts.sftp_dir}/")
+        self.assertNotIn(f"{self.job_name}.txt", dir_list)
```

### Comparing `parsons-1.0.0/test/test_targetsmart/test_targetsmart_smartmatch.py` & `parsons-1.1.0/test/test_targetsmart/test_targetsmart_smartmatch.py`

 * *Files identical despite different names*

### Comparing `parsons-1.0.0/test/test_utilities.py` & `parsons-1.1.0/test/test_utilities.py`

 * *Files 9% similar despite different names*

```diff
@@ -11,27 +11,31 @@
 from parsons.utilities import json_format
 from parsons.utilities import sql_helpers
 from test.conftest import xfail_value_error
 
 
 @pytest.mark.parametrize(
     ["date", "exp_ts"],
-    [pytest.param("2018-12-13", 1544659200),
-     pytest.param("2018-12-13T00:00:00-08:00", 1544688000),
-     pytest.param("", None),
-     pytest.param("2018-12-13 PST", None, marks=[xfail_value_error]),
-     ])
+    [
+        pytest.param("2018-12-13", 1544659200),
+        pytest.param("2018-12-13T00:00:00-08:00", 1544688000),
+        pytest.param("", None),
+        pytest.param("2018-12-13 PST", None, marks=[xfail_value_error]),
+    ],
+)
 def test_date_to_timestamp(date, exp_ts):
     assert date_to_timestamp(date) == exp_ts
 
 
 def test_parse_date():
     # Test parsing an ISO8601 string
-    expected = datetime.datetime(year=2020, month=1, day=1, tzinfo=datetime.timezone.utc)
-    parsed = parse_date('2020-01-01T00:00:00.000 UTC')
+    expected = datetime.datetime(
+        year=2020, month=1, day=1, tzinfo=datetime.timezone.utc
+    )
+    parsed = parse_date("2020-01-01T00:00:00.000 UTC")
     assert parsed == expected, parsed
 
     # Test parsing a unix timestamp
     parsed = parse_date(1577836800)
     assert parsed == expected, parsed
 
     # Test "parsing" a datetime object
@@ -41,96 +45,96 @@
 
 #
 # File utility tests (pytest-style)
 #
 
 
 def test_create_temp_file_for_path():
-    temp_path = files.create_temp_file_for_path('some/file.gz')
-    assert temp_path[-3:] == '.gz'
+    temp_path = files.create_temp_file_for_path("some/file.gz")
+    assert temp_path[-3:] == ".gz"
 
 
 def test_create_temp_directory():
     temp_directory = files.create_temp_directory()
-    test_file1 = f'{temp_directory}/test.txt'
-    test_file2 = f'{temp_directory}/test2.txt'
-    with open(test_file1, 'w') as fh1, open(test_file2, 'w') as fh2:
-        fh1.write('TEST')
-        fh2.write('TEST')
+    test_file1 = f"{temp_directory}/test.txt"
+    test_file2 = f"{temp_directory}/test2.txt"
+    with open(test_file1, "w") as fh1, open(test_file2, "w") as fh2:
+        fh1.write("TEST")
+        fh2.write("TEST")
 
     assert files.has_data(test_file1)
     assert files.has_data(test_file2)
 
     files.cleanup_temp_directory(temp_directory)
 
     # Verify the temp file no longer exists
     with pytest.raises(FileNotFoundError):
-        open(test_file1, 'r')
+        open(test_file1, "r")
 
 
 def test_close_temp_file():
     temp = files.create_temp_file()
     files.close_temp_file(temp)
 
     # Verify the temp file no longer exists
     with pytest.raises(FileNotFoundError):
-        open(temp, 'r')
+        open(temp, "r")
 
 
 def test_is_gzip_path():
-    assert files.is_gzip_path('some/file.gz')
-    assert not files.is_gzip_path('some/file')
-    assert not files.is_gzip_path('some/file.csv')
+    assert files.is_gzip_path("some/file.gz")
+    assert not files.is_gzip_path("some/file")
+    assert not files.is_gzip_path("some/file.csv")
 
 
 def test_suffix_for_compression_type():
-    assert files.suffix_for_compression_type(None) == ''
-    assert files.suffix_for_compression_type('') == ''
-    assert files.suffix_for_compression_type('gzip') == '.gz'
+    assert files.suffix_for_compression_type(None) == ""
+    assert files.suffix_for_compression_type("") == ""
+    assert files.suffix_for_compression_type("gzip") == ".gz"
 
 
 def test_compression_type_for_path():
-    assert files.compression_type_for_path('some/file') is None
-    assert files.compression_type_for_path('some/file.csv') is None
-    assert files.compression_type_for_path('some/file.csv.gz') == 'gzip'
+    assert files.compression_type_for_path("some/file") is None
+    assert files.compression_type_for_path("some/file.csv") is None
+    assert files.compression_type_for_path("some/file.csv.gz") == "gzip"
 
 
 def test_empty_file():
 
     # Create fake files.
-    os.mkdir('tmp')
-    with open('tmp/empty.csv', 'w+') as _:
+    os.mkdir("tmp")
+    with open("tmp/empty.csv", "w+") as _:
         pass
 
-    Table([['1'], ['a']]).to_csv('tmp/full.csv')
+    Table([["1"], ["a"]]).to_csv("tmp/full.csv")
 
-    assert not files.has_data('tmp/empty.csv')
-    assert files.has_data('tmp/full.csv')
+    assert not files.has_data("tmp/empty.csv")
+    assert files.has_data("tmp/full.csv")
 
     # Remove fake files and dir
-    shutil.rmtree('tmp')
+    shutil.rmtree("tmp")
 
 
 def test_json_format():
-    assert json_format.arg_format('my_arg') == 'myArg'
+    assert json_format.arg_format("my_arg") == "myArg"
 
 
 def test_remove_empty_keys():
 
     # Assert key removed when None
-    test_dict = {'a': None, 'b': 2}
-    assert json_format.remove_empty_keys(test_dict) == {'b': 2}
+    test_dict = {"a": None, "b": 2}
+    assert json_format.remove_empty_keys(test_dict) == {"b": 2}
 
     # Assert key not removed when None
-    test_dict = {'a': 1, 'b': 2}
-    assert json_format.remove_empty_keys(test_dict) == {'a': 1, 'b': 2}
+    test_dict = {"a": 1, "b": 2}
+    assert json_format.remove_empty_keys(test_dict) == {"a": 1, "b": 2}
 
     # Assert that a nested empty string is removed
-    test_dict = {'a': '', 'b': 2}
-    assert json_format.remove_empty_keys(test_dict) == {'b': 2}
+    test_dict = {"a": "", "b": 2}
+    assert json_format.remove_empty_keys(test_dict) == {"b": 2}
 
 
 def test_redact_credentials():
 
     # Test with quotes, escape characters, and line breaks
     test_str = """COPY schema.tablename
     FROM 's3://bucket/path/to/file.csv'
@@ -143,29 +147,28 @@
     CREDENTIALS REDACTED
     MANIFEST"""
 
     assert sql_helpers.redact_credentials(test_str) == test_result
 
 
 class TestCheckEnv(unittest.TestCase):
-
     def test_environment_field(self):
         """Test check field"""
-        result = check_env.check('PARAM', 'param')
-        self.assertEqual(result, 'param')
+        result = check_env.check("PARAM", "param")
+        self.assertEqual(result, "param")
 
-    @mock.patch.dict(os.environ, {'PARAM': 'env_param'})
+    @mock.patch.dict(os.environ, {"PARAM": "env_param"})
     def test_environment_env(self):
         """Test check env"""
-        result = check_env.check('PARAM', None)
-        self.assertEqual(result, 'env_param')
+        result = check_env.check("PARAM", None)
+        self.assertEqual(result, "env_param")
 
-    @mock.patch.dict(os.environ, {'PARAM': 'env_param'})
+    @mock.patch.dict(os.environ, {"PARAM": "env_param"})
     def test_environment_field_env(self):
         """Test check field with env and field"""
-        result = check_env.check('PARAM', 'param')
-        self.assertEqual(result, 'param')
+        result = check_env.check("PARAM", "param")
+        self.assertEqual(result, "param")
 
     def test_envrionment_error(self):
         """Test check env raises error"""
         with self.assertRaises(KeyError) as _:
-            check_env.check('PARAM', None)
+            check_env.check("PARAM", None)
```

### Comparing `parsons-1.0.0/test/test_van/test_bulkimport.py` & `parsons-1.1.0/test/test_van/test_bulkimport.py`

 * *Files 16% similar despite different names*

```diff
@@ -2,201 +2,239 @@
 import os
 import requests_mock
 import unittest.mock as mock
 from parsons import VAN, Table
 from test.utils import assert_matching_tables
 from parsons.utilities import cloud_storage
 
-os.environ['VAN_API_KEY'] = 'SOME_KEY'
+os.environ["VAN_API_KEY"] = "SOME_KEY"
 
 
 class TestBulkImport(unittest.TestCase):
-
     def setUp(self):
 
-        self.van = VAN(os.environ['VAN_API_KEY'],
-                       db="MyVoters", raise_for_status=False)
+        self.van = VAN(os.environ["VAN_API_KEY"], db="MyVoters", raise_for_status=False)
 
     @requests_mock.Mocker()
     def test_get_bulk_import_resources(self, m):
 
-        json = ['Contacts', 'Contributions',
-                'ActivistCodes', 'ContactsActivistCodes']
+        json = ["Contacts", "Contributions", "ActivistCodes", "ContactsActivistCodes"]
 
-        m.get(self.van.connection.uri + 'bulkImportJobs/resources', json=json)
+        m.get(self.van.connection.uri + "bulkImportJobs/resources", json=json)
 
         self.assertEqual(self.van.get_bulk_import_resources(), json)
 
     @requests_mock.Mocker()
     def test_get_bulk_import_job(self, m):
 
-        m.get(self.van.connection.uri +
-              'bulkImportJobs/53407', json=bulk_import_job)
+        m.get(self.van.connection.uri + "bulkImportJobs/53407", json=bulk_import_job)
 
         self.assertEqual(self.van.get_bulk_import_job(53407), bulk_import_job)
 
     @requests_mock.Mocker()
     def test_get_bulk_import_job_results(self, m):
 
-        results_tbl = Table([['BulkUploadDataID', 'ULFileID', 'PrimaryKey',
-                              'PrimaryKeyType', 'MailingAddress_3581'],
-                             ['1', '1983', '101596008', 'VanID', 'Processed']])
-
-        bulk_import_job = {'id': 92,
-                           'status': 'Completed',
-                           'resourceType': 'Contacts',
-                           'webhookUrl': None,
-                           'resultFileSizeLimitKb': 5000,
-                           'errors': [],
-                           'resultFiles': [{
-                               'url': Table.to_csv(results_tbl),
-                               'dateExpired': '2020-09-04T22:07:04.0770295-04:00'
-                           }]
-                           }
+        results_tbl = Table(
+            [
+                [
+                    "BulkUploadDataID",
+                    "ULFileID",
+                    "PrimaryKey",
+                    "PrimaryKeyType",
+                    "MailingAddress_3581",
+                ],
+                ["1", "1983", "101596008", "VanID", "Processed"],
+            ]
+        )
+
+        bulk_import_job = {
+            "id": 92,
+            "status": "Completed",
+            "resourceType": "Contacts",
+            "webhookUrl": None,
+            "resultFileSizeLimitKb": 5000,
+            "errors": [],
+            "resultFiles": [
+                {
+                    "url": Table.to_csv(results_tbl),
+                    "dateExpired": "2020-09-04T22:07:04.0770295-04:00",
+                }
+            ],
+        }
 
-        m.get(self.van.connection.uri +
-              'bulkImportJobs/53407', json=bulk_import_job)
-        assert_matching_tables(
-            self.van.get_bulk_import_job_results(53407), results_tbl)
+        m.get(self.van.connection.uri + "bulkImportJobs/53407", json=bulk_import_job)
+        assert_matching_tables(self.van.get_bulk_import_job_results(53407), results_tbl)
 
     @requests_mock.Mocker()
     def test_get_bulk_import_mapping_types(self, m):
 
-        m.get(self.van.connection.uri +
-              'bulkImportMappingTypes', json=mapping_type)
+        m.get(self.van.connection.uri + "bulkImportMappingTypes", json=mapping_type)
 
         assert_matching_tables(
-            self.van.get_bulk_import_mapping_types(), Table(mapping_type))
+            self.van.get_bulk_import_mapping_types(), Table(mapping_type)
+        )
 
     @requests_mock.Mocker()
     def test_get_bulk_import_mapping_type(self, m):
 
-        m.get(self.van.connection.uri +
-              'bulkImportMappingTypes/ActivistCode', json=mapping_type)
-
-        self.assertEqual(self.van.get_bulk_import_mapping_type(
-            'ActivistCode'), mapping_type)
+        m.get(
+            self.van.connection.uri + "bulkImportMappingTypes/ActivistCode",
+            json=mapping_type,
+        )
+
+        self.assertEqual(
+            self.van.get_bulk_import_mapping_type("ActivistCode"), mapping_type
+        )
 
     @requests_mock.Mocker()
     def get_bulk_import_mapping_type_fields(self, m):
 
-        json = [{'name': 'Unsubscribed', 'id': '0', 'parents': None},
-                {'name': 'Not Subscribed', 'id': '1', 'parents': None},
-                {'name': 'Subscribed', 'id': '2', 'parents': None}]
-        m.get(self.van.connection.uri +
-              'bulkImportMappingTypes/Email/EmailSubscriptionStatusId/values')
+        json = [
+            {"name": "Unsubscribed", "id": "0", "parents": None},
+            {"name": "Not Subscribed", "id": "1", "parents": None},
+            {"name": "Subscribed", "id": "2", "parents": None},
+        ]
+        m.get(
+            self.van.connection.uri
+            + "bulkImportMappingTypes/Email/EmailSubscriptionStatusId/values"
+        )
 
         r = self.van.get_bulk_import_mapping_type_fields(
-            'Email', 'EmailSubscriptionStatusId')
+            "Email", "EmailSubscriptionStatusId"
+        )
         self.assertEqual(json, r)
 
     @requests_mock.Mocker()
     def test_post_bulk_import(self, m):
 
         # Mock Cloud Storage
         cloud_storage.post_file = mock.MagicMock()
-        cloud_storage.post_file.return_value = 'https://s3.com/my_file.zip'
+        cloud_storage.post_file.return_value = "https://s3.com/my_file.zip"
 
-        tbl = Table([['Vanid', 'ActivistCodeID'], [1234, 345345]])
+        tbl = Table([["Vanid", "ActivistCodeID"], [1234, 345345]])
 
-        m.post(self.van.connection.uri +
-               'bulkImportJobs', json={'jobId': 54679})
+        m.post(self.van.connection.uri + "bulkImportJobs", json={"jobId": 54679})
 
-        r = self.van.post_bulk_import(tbl,
-                                      'S3',
-                                      'ContactsActivistCodes',
-                                      [{"name": "ActivistCode"}],
-                                      'Activist Code Upload',
-                                      bucket='my-bucket')
+        r = self.van.post_bulk_import(
+            tbl,
+            "S3",
+            "ContactsActivistCodes",
+            [{"name": "ActivistCode"}],
+            "Activist Code Upload",
+            bucket="my-bucket",
+        )
 
         self.assertEqual(r, 54679)
 
     @requests_mock.Mocker()
     def test_bulk_apply_activist_codes(self, m):
 
         # Mock Cloud Storage
         cloud_storage.post_file = mock.MagicMock()
-        cloud_storage.post_file.return_value = 'https://s3.com/my_file.zip'
+        cloud_storage.post_file.return_value = "https://s3.com/my_file.zip"
 
-        tbl = Table([['Vanid', 'ActivistCodeID'], [1234, 345345]])
+        tbl = Table([["Vanid", "ActivistCodeID"], [1234, 345345]])
 
-        m.post(self.van.connection.uri +
-               'bulkImportJobs', json={'jobId': 54679})
+        m.post(self.van.connection.uri + "bulkImportJobs", json={"jobId": 54679})
 
         job_id = self.van.bulk_apply_activist_codes(
-            tbl, url_type="S3", bucket='my-bucket')
+            tbl, url_type="S3", bucket="my-bucket"
+        )
 
         self.assertEqual(job_id, 54679)
 
     @requests_mock.Mocker()
-    def test_bulk_upsert_contacts(self, m):
+    def test_bulk_apply_suppressions(self, m):
 
         # Mock Cloud Storage
         cloud_storage.post_file = mock.MagicMock()
-        cloud_storage.post_file.return_value = 'https://s3.com/my_file.zip'
+        cloud_storage.post_file.return_value = "https://s3.com/my_file.zip"
 
-        tbl = Table([['Vanid', 'email'], [1234, 'me@me.com']])
+        tbl = Table([["Vanid", "suppressionid"], [1234, 18]])
 
-        m.post(self.van.connection.uri +
-               'bulkImportJobs', json={'jobId': 54679})
+        m.post(self.van.connection.uri + "bulkImportJobs", json={"jobId": 54679})
 
-        job_id = self.van.bulk_upsert_contacts(
-            tbl, url_type="S3", bucket='my-bucket')
+        job_id = self.van.bulk_apply_suppressions(
+            tbl, url_type="S3", bucket="my-bucket"
+        )
 
         self.assertEqual(job_id, 54679)
 
+    @requests_mock.Mocker()
+    def test_bulk_upsert_contacts(self, m):
 
-mapping_type = {'name': 'ActivistCode',
-                'displayName': 'Apply Activist Code',
-                'allowMultipleMode': 'Multiple',
-                'resourceTypes': ['Contacts', 'ContactsActivistCodes'],
-                'fields': [{
-                    'name': 'ActivistCodeID',
-                    'description': 'Activist Code ID',
-                    'hasPredefinedValues': True,
-                    'isRequired': True,
-                    'canBeMappedToColumn': True,
-                    'canBeMappedByName': True,
-                    'parents': None},
-                    {'name': 'CanvassedBy',
-                     'description': 'Recruited By, Must be a Valid User ID',
-                     'hasPredefinedValues': False,
-                     'isRequired': False,
-                     'canBeMappedToColumn': True,
-                     'canBeMappedByName': True,
-                     'parents': None},
-                    {'name': 'DateCanvassed',
-                     'description': 'Contacted When',
-                     'hasPredefinedValues': False,
-                     'isRequired': False,
-                     'canBeMappedToColumn': True,
-                     'canBeMappedByName': True,
-                     'parents': [{
-                         'parentFieldName': 'CanvassedBy',
-                         'limitedToParentValues': None
-                     }]
-                     }, {
-                    'name': 'ContactTypeID',
-                    'description': 'Contacted How',
-                    'hasPredefinedValues': True,
-                    'isRequired': False,
-                    'canBeMappedToColumn': True,
-                    'canBeMappedByName': True,
-                    'parents': [{
-                        'parentFieldName': 'CanvassedBy',
-                        'limitedToParentValues': None
-                    }]
-                }]
+        # Mock Cloud Storage
+        cloud_storage.post_file = mock.MagicMock()
+        cloud_storage.post_file.return_value = "https://s3.com/my_file.zip"
+
+        tbl = Table([["Vanid", "email"], [1234, "me@me.com"]])
+
+        m.post(self.van.connection.uri + "bulkImportJobs", json={"jobId": 54679})
+
+        job_id = self.van.bulk_upsert_contacts(tbl, url_type="S3", bucket="my-bucket")
+
+        self.assertEqual(job_id, 54679)
 
-                }
 
-bulk_import_job = {'id': 92,
-                   'status': 'Completed',
-                   'resourceType': 'Contacts',
-                   'webhookUrl': None,
-                   'resultFileSizeLimitKb': 5000,
-                   'errors': [],
-                   'resultFiles': [{
-                       'url': 'https://ngpvan.com/bulk-import-jobs/f023.csv',
-                       'dateExpired': '2020-09-04T22:07:04.0770295-04:00'
-                   }]
-                   }
+mapping_type = {
+    "name": "ActivistCode",
+    "displayName": "Apply Activist Code",
+    "allowMultipleMode": "Multiple",
+    "resourceTypes": ["Contacts", "ContactsActivistCodes"],
+    "fields": [
+        {
+            "name": "ActivistCodeID",
+            "description": "Activist Code ID",
+            "hasPredefinedValues": True,
+            "isRequired": True,
+            "canBeMappedToColumn": True,
+            "canBeMappedByName": True,
+            "parents": None,
+        },
+        {
+            "name": "CanvassedBy",
+            "description": "Recruited By, Must be a Valid User ID",
+            "hasPredefinedValues": False,
+            "isRequired": False,
+            "canBeMappedToColumn": True,
+            "canBeMappedByName": True,
+            "parents": None,
+        },
+        {
+            "name": "DateCanvassed",
+            "description": "Contacted When",
+            "hasPredefinedValues": False,
+            "isRequired": False,
+            "canBeMappedToColumn": True,
+            "canBeMappedByName": True,
+            "parents": [
+                {"parentFieldName": "CanvassedBy", "limitedToParentValues": None}
+            ],
+        },
+        {
+            "name": "ContactTypeID",
+            "description": "Contacted How",
+            "hasPredefinedValues": True,
+            "isRequired": False,
+            "canBeMappedToColumn": True,
+            "canBeMappedByName": True,
+            "parents": [
+                {"parentFieldName": "CanvassedBy", "limitedToParentValues": None}
+            ],
+        },
+    ],
+}
+
+bulk_import_job = {
+    "id": 92,
+    "status": "Completed",
+    "resourceType": "Contacts",
+    "webhookUrl": None,
+    "resultFileSizeLimitKb": 5000,
+    "errors": [],
+    "resultFiles": [
+        {
+            "url": "https://ngpvan.com/bulk-import-jobs/f023.csv",
+            "dateExpired": "2020-09-04T22:07:04.0770295-04:00",
+        }
+    ],
+}
```

### Comparing `parsons-1.0.0/test/test_van/test_changed_entities.py` & `parsons-1.1.0/test/test_van/test_changed_entities.py`

 * *Files 20% similar despite different names*

```diff
@@ -3,82 +3,96 @@
 import requests_mock
 import unittest.mock as mock
 from parsons import VAN, Table
 from test.utils import assert_matching_tables
 
 
 class TestNGPVAN(unittest.TestCase):
-
     def setUp(self):
 
-        self.van = VAN(os.environ['VAN_API_KEY'], db="MyVoters", raise_for_status=False)
+        self.van = VAN(os.environ["VAN_API_KEY"], db="MyVoters", raise_for_status=False)
 
     @requests_mock.Mocker()
     def test_get_changed_entity_resources(self, m):
 
-        json = ['ActivistCodes', 'ContactHistory', 'Contacts', 'ContactsActivistCodes']
-        m.get(self.van.connection.uri + 'changedEntityExportJobs/resources', json=json)
+        json = ["ActivistCodes", "ContactHistory", "Contacts", "ContactsActivistCodes"]
+        m.get(self.van.connection.uri + "changedEntityExportJobs/resources", json=json)
         self.assertEqual(json, self.van.get_changed_entity_resources())
 
     @requests_mock.Mocker()
     def test_get_changed_entity_resource_fields(self, m):
 
-        json = [{
-            'fieldName': 'ActivistCodeID',
-            'fieldType': 'N',
-            'maxTextboxCharacters': None,
-            'isCoreField': True,
-            'availableValues': None
-        }, {
-            'fieldName': 'ActivistCodeType',
-            'fieldType': 'T',
-            'maxTextboxCharacters': 20,
-            'isCoreField': True,
-            'availableValues': None
-        }, {
-            'fieldName': 'Campaign',
-            'fieldType': 'T',
-            'maxTextboxCharacters': 150,
-            'isCoreField': True,
-            'availableValues': None
-        }]
-
-        m.get(self.van.connection.uri + 'changedEntityExportJobs/fields/ActivistCodes', json=json)
+        json = [
+            {
+                "fieldName": "ActivistCodeID",
+                "fieldType": "N",
+                "maxTextboxCharacters": None,
+                "isCoreField": True,
+                "availableValues": None,
+            },
+            {
+                "fieldName": "ActivistCodeType",
+                "fieldType": "T",
+                "maxTextboxCharacters": 20,
+                "isCoreField": True,
+                "availableValues": None,
+            },
+            {
+                "fieldName": "Campaign",
+                "fieldType": "T",
+                "maxTextboxCharacters": 150,
+                "isCoreField": True,
+                "availableValues": None,
+            },
+        ]
+
+        m.get(
+            self.van.connection.uri + "changedEntityExportJobs/fields/ActivistCodes",
+            json=json,
+        )
         assert_matching_tables(
-            Table(json), self.van.get_changed_entity_resource_fields('ActivistCodes'))
+            Table(json), self.van.get_changed_entity_resource_fields("ActivistCodes")
+        )
 
     @requests_mock.Mocker()
     def test_get_changed_entities(self, m):
 
-        json = {"dateChangedFrom": "2021-10-10T00:00:00-04:00",
-                "dateChangedTo": "2021-10-11T00:00:00-04:00",
-                "files": [],
-                "message": "Created export job",
-                "code": None,
-                "exportedRecordCount": 0,
-                "exportJobId": 2170181229,
-                "jobStatus": "Pending"}
+        json = {
+            "dateChangedFrom": "2021-10-10T00:00:00-04:00",
+            "dateChangedTo": "2021-10-11T00:00:00-04:00",
+            "files": [],
+            "message": "Created export job",
+            "code": None,
+            "exportedRecordCount": 0,
+            "exportJobId": 2170181229,
+            "jobStatus": "Pending",
+        }
 
         json2 = {
-                    "dateChangedFrom": "2021-10-10T00:00:00-04:00",
-                    "dateChangedTo": "2021-10-11T00:00:00-04:00",
-                    "files": [
-                        {"downloadUrl": "https://box.com/file.csv",
-                         "dateExpired": "2021-11-03T15:27:01.8687339-04:00"}
-                    ],
-                    "message": "Finished processing export job",
-                    "code": None,
-                    "exportedRecordCount": 6110,
-                    "exportJobId": 2170181229,
-                    "jobStatus": "Complete"}
-
-        tbl = Table([{'a': 1, 'b': 2}])
-
-        m.post(self.van.connection.uri + 'changedEntityExportJobs', json=json)
-        m.get(self.van.connection.uri + 'changedEntityExportJobs/2170181229', json=json2)
+            "dateChangedFrom": "2021-10-10T00:00:00-04:00",
+            "dateChangedTo": "2021-10-11T00:00:00-04:00",
+            "files": [
+                {
+                    "downloadUrl": "https://box.com/file.csv",
+                    "dateExpired": "2021-11-03T15:27:01.8687339-04:00",
+                }
+            ],
+            "message": "Finished processing export job",
+            "code": None,
+            "exportedRecordCount": 6110,
+            "exportJobId": 2170181229,
+            "jobStatus": "Complete",
+        }
+
+        tbl = Table([{"a": 1, "b": 2}])
+
+        m.post(self.van.connection.uri + "changedEntityExportJobs", json=json)
+        m.get(
+            self.van.connection.uri + "changedEntityExportJobs/2170181229", json=json2
+        )
 
         Table.from_csv = mock.MagicMock()
         Table.from_csv.return_value = tbl
 
-        out_tbl = self.van.get_changed_entities('ContactHistory', '2021-10-10')
+        out_tbl = self.van.get_changed_entities("ContactHistory", "2021-10-10")
 
         assert_matching_tables(out_tbl, tbl)
```

### Comparing `parsons-1.0.0/test/test_van/test_codes.py` & `parsons-1.1.0/test/test_van/test_codes.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,104 +1,115 @@
 import unittest
 import os
 import requests_mock
 from parsons import VAN
 from test.utils import assert_matching_tables
 from requests.exceptions import HTTPError
 
-os.environ['VAN_API_KEY'] = 'SOME_KEY'
+os.environ["VAN_API_KEY"] = "SOME_KEY"
 
 
 class TestCodes(unittest.TestCase):
-
     def setUp(self):
 
-        self.van = VAN(os.environ['VAN_API_KEY'], db="MyVoters")
+        self.van = VAN(os.environ["VAN_API_KEY"], db="MyVoters")
 
     def tearDown(self):
 
         pass
 
     @requests_mock.Mocker()
     def test_get_codes(self, m):
 
-        json = {'items': [{'codeId': 1004916,
-                           'parentCodeId': None,
-                           'name': 'Data Entry',
-                           'description': 'for test.',
-                           'codePath': 'Data Entry',
-                           'createdByName': '',
-                           'dateCreated': '2018-07-13T15:16:00Z',
-                           'supportedEntities': None,
-                           'codeType': 'Tag',
-                           'campaign': None,
-                           'contactType': None}],
-                'nextPageLink': None, 'count': 8}
+        json = {
+            "items": [
+                {
+                    "codeId": 1004916,
+                    "parentCodeId": None,
+                    "name": "Data Entry",
+                    "description": "for test.",
+                    "codePath": "Data Entry",
+                    "createdByName": "",
+                    "dateCreated": "2018-07-13T15:16:00Z",
+                    "supportedEntities": None,
+                    "codeType": "Tag",
+                    "campaign": None,
+                    "contactType": None,
+                }
+            ],
+            "nextPageLink": None,
+            "count": 8,
+        }
 
-        m.get(self.van.connection.uri + 'codes', json=json)
-        assert_matching_tables(json['items'], self.van.get_codes())
+        m.get(self.van.connection.uri + "codes", json=json)
+        assert_matching_tables(json["items"], self.van.get_codes())
 
     @requests_mock.Mocker()
     def test_get_code(self, m):
 
-        json = {'codeId': 1004916,
-                'parentCodeId': None,
-                'name': 'Data Entry',
-                'description': 'for test.',
-                'codePath': 'Data Entry',
-                'createdByName': '',
-                'dateCreated': '2018-07-13T15:16:00Z',
-                'supportedEntities': None,
-                'codeType': 'Tag',
-                'campaign': None,
-                'contactType': None}
+        json = {
+            "codeId": 1004916,
+            "parentCodeId": None,
+            "name": "Data Entry",
+            "description": "for test.",
+            "codePath": "Data Entry",
+            "createdByName": "",
+            "dateCreated": "2018-07-13T15:16:00Z",
+            "supportedEntities": None,
+            "codeType": "Tag",
+            "campaign": None,
+            "contactType": None,
+        }
 
-        m.get(self.van.connection.uri + 'codes/1004916', json=json)
+        m.get(self.van.connection.uri + "codes/1004916", json=json)
         self.assertEqual(json, self.van.get_code(1004916))
 
     @requests_mock.Mocker()
     def test_get_code_types(self, m):
 
-        json = ['Tag', 'SourceCode']
-        m.get(self.van.connection.uri + 'codeTypes', json=json)
+        json = ["Tag", "SourceCode"]
+        m.get(self.van.connection.uri + "codeTypes", json=json)
         self.assertEqual(json, self.van.get_code_types())
 
     @requests_mock.Mocker()
     def test_create_code(self, m):
 
-        m.post(self.van.connection.uri + 'codes', json=1004960, status_code=201)
+        m.post(self.van.connection.uri + "codes", json=1004960, status_code=201)
 
         # Test that it doesn't throw and error
-        r = self.van.create_code('Test Code', supported_entities=[{'name': 'Events',
-                                                                   'is_searchable': True,
-                                                                   'is_applicable': True}])
+        r = self.van.create_code(
+            "Test Code",
+            supported_entities=[
+                {"name": "Events", "is_searchable": True, "is_applicable": True}
+            ],
+        )
 
         self.assertEqual(r, 1004960)
 
     @requests_mock.Mocker()
     def test_update_code(self, m):
 
         # Test a good input
-        m.put(self.van.connection.uri + 'codes/1004960', status_code=204)
-        self.van.update_code(1004960, name='Test')
+        m.put(self.van.connection.uri + "codes/1004960", status_code=204)
+        self.van.update_code(1004960, name="Test")
 
         # Test a bad input
-        m.put(self.van.connection.uri + 'codes/100496Q', status_code=404)
-        self.assertRaises(HTTPError, self.van.update_code, '100496Q')
+        m.put(self.van.connection.uri + "codes/100496Q", status_code=404)
+        self.assertRaises(HTTPError, self.van.update_code, "100496Q")
 
     @requests_mock.Mocker()
     def test_delete_code(self, m):
 
         # Test a good input
-        m.delete(self.van.connection.uri + 'codes/1004960', status_code=204)
+        m.delete(self.van.connection.uri + "codes/1004960", status_code=204)
         self.van.delete_code(1004960)
 
         # Test a bad input
-        m.delete(self.van.connection.uri + 'codes/100496Q', status_code=404)
-        self.assertRaises(HTTPError, self.van.delete_code, '100496Q')
+        m.delete(self.van.connection.uri + "codes/100496Q", status_code=404)
+        self.assertRaises(HTTPError, self.van.delete_code, "100496Q")
 
     @requests_mock.Mocker()
     def test_get_code_supported_entities(self, m):
 
-        json = ['Contacts', 'Events', 'Locations']
-        m.get(self.van.connection.uri + 'codes/supportedEntities', json=json)
+        json = ["Contacts", "Events", "Locations"]
+        m.get(self.van.connection.uri + "codes/supportedEntities", json=json)
         self.assertEqual(json, self.van.get_code_supported_entities())
```

### Comparing `parsons-1.0.0/test/test_van/test_custom_fields.py` & `parsons-1.1.0/test/test_van/test_custom_fields.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,57 +1,70 @@
 import unittest
 import os
 import requests_mock
 from parsons import VAN
 from test.utils import assert_matching_tables
 
 
-custom_field = [{
-    "customFieldId": 157,
-    "customFieldParentId": None,
-    "customFieldName": "Education level",
-    "customFieldGroupId": 52,
-    "customFieldGroupName": "Education",
-    "customFieldGroupType": "Contacts",
-    "customFieldTypeId": "S",
-    "isEditable": True,
-    "isExportable": False,
-    "maxTextboxCharacters": None,
-    "availableValues": [
-      {"id": 1, "name": "High School diploma", "parentValueId": None},
-      {"id": 2, "name": "College degree", "parentValueId": None},
-      {"id": 3, "name": "Postgraduate degree", "parentValueId": None},
-      {"id": 4, "name": "Doctorate", "parentValueId": None}]}]
+custom_field = [
+    {
+        "customFieldId": 157,
+        "customFieldParentId": None,
+        "customFieldName": "Education level",
+        "customFieldGroupId": 52,
+        "customFieldGroupName": "Education",
+        "customFieldGroupType": "Contacts",
+        "customFieldTypeId": "S",
+        "isEditable": True,
+        "isExportable": False,
+        "maxTextboxCharacters": None,
+        "availableValues": [
+            {"id": 1, "name": "High School diploma", "parentValueId": None},
+            {"id": 2, "name": "College degree", "parentValueId": None},
+            {"id": 3, "name": "Postgraduate degree", "parentValueId": None},
+            {"id": 4, "name": "Doctorate", "parentValueId": None},
+        ],
+    }
+]
 
 custom_field_values = [
-  {'customFieldId': 157, 'id': 1, 'name': 'High School diploma', 'parentValueId': None},
-  {'customFieldId': 157, 'id': 2, 'name': 'College degree', 'parentValueId': None},
-  {'customFieldId': 157, 'id': 3, 'name': 'Postgraduate degree', 'parentValueId': None},
-  {'customFieldId': 157, 'id': 4, 'name': 'Doctorate', 'parentValueId': None}
-  ]
+    {
+        "customFieldId": 157,
+        "id": 1,
+        "name": "High School diploma",
+        "parentValueId": None,
+    },
+    {"customFieldId": 157, "id": 2, "name": "College degree", "parentValueId": None},
+    {
+        "customFieldId": 157,
+        "id": 3,
+        "name": "Postgraduate degree",
+        "parentValueId": None,
+    },
+    {"customFieldId": 157, "id": 4, "name": "Doctorate", "parentValueId": None},
+]
 
-os.environ['VAN_API_KEY'] = 'SOME_KEY'
+os.environ["VAN_API_KEY"] = "SOME_KEY"
 
 
 class TestCustomFields(unittest.TestCase):
-
     def setUp(self):
 
-        self.van = VAN(os.environ['VAN_API_KEY'], db="MyVoters")
+        self.van = VAN(os.environ["VAN_API_KEY"], db="MyVoters")
 
     @requests_mock.Mocker()
     def test_get_custom_fields(self, m):
 
-        m.get(self.van.connection.uri + 'customFields', json=custom_field)
+        m.get(self.van.connection.uri + "customFields", json=custom_field)
         assert_matching_tables(custom_field, self.van.get_custom_fields())
 
     @requests_mock.Mocker()
     def test_get_custom_field_values(self, m):
 
-        m.get(self.van.connection.uri + 'customFields', json=custom_field)
+        m.get(self.van.connection.uri + "customFields", json=custom_field)
         assert_matching_tables(custom_field_values, self.van.get_custom_fields_values())
 
     @requests_mock.Mocker()
     def test_get_custom_field(self, m):
 
-        m.get(self.van.connection.uri + 'customFields/157', json=custom_field)
+        m.get(self.van.connection.uri + "customFields/157", json=custom_field)
         assert_matching_tables(custom_field, self.van.get_custom_field(157))
```

### Comparing `parsons-1.0.0/test/test_van/test_locations.py` & `parsons-1.1.0/test/test_van/test_locations.py`

 * *Files 26% similar despite different names*

```diff
@@ -2,82 +2,110 @@
 import os
 import requests_mock
 from parsons import VAN
 from test.utils import validate_list
 from requests.exceptions import HTTPError
 
 
-os.environ['VAN_API_KEY'] = 'SOME_KEY'
+os.environ["VAN_API_KEY"] = "SOME_KEY"
 
 
-location_json = {'locationId': 34,
-                 'name': 'Chicagowide',
-                 'displayName': 'Chicagowide, Chicago, IL ',
-                 'address': {'addressId': None,
-                             'addressLine1': None,
-                             'addressLine2': None,
-                             'addressLine3': None,
-                             'city': 'Chicago',
-                             'stateOrProvince': 'IL',
-                             'zipOrPostalCode': None,
-                             'geoLocation': None,
-                             'countryCode': 'US',
-                             'preview': 'Chicago, IL ',
-                             'type': None,
-                             'isPreferred': None,
-                             'streetAddress': None,
-                             'displayMode': 'Standardized'},
-                 'id': 34,
-                 'notes': None,
-                 'codes': None}
-
-expected_loc = ['locationId', 'name', 'displayName', 'id', 'notes', 'codes', 'addressId',
-                'addressLine1', 'addressLine2', 'addressLine3', 'city', 'countryCode',
-                'displayMode', 'isPreferred', 'preview', 'stateOrProvince', 'streetAddress',
-                'type', 'zipOrPostalCode']
+location_json = {
+    "locationId": 34,
+    "name": "Chicagowide",
+    "displayName": "Chicagowide, Chicago, IL ",
+    "address": {
+        "addressId": None,
+        "addressLine1": None,
+        "addressLine2": None,
+        "addressLine3": None,
+        "city": "Chicago",
+        "stateOrProvince": "IL",
+        "zipOrPostalCode": None,
+        "geoLocation": None,
+        "countryCode": "US",
+        "preview": "Chicago, IL ",
+        "type": None,
+        "isPreferred": None,
+        "streetAddress": None,
+        "displayMode": "Standardized",
+    },
+    "id": 34,
+    "notes": None,
+    "codes": None,
+}
+
+expected_loc = [
+    "locationId",
+    "name",
+    "displayName",
+    "id",
+    "notes",
+    "codes",
+    "addressId",
+    "addressLine1",
+    "addressLine2",
+    "addressLine3",
+    "city",
+    "countryCode",
+    "displayMode",
+    "isPreferred",
+    "preview",
+    "stateOrProvince",
+    "streetAddress",
+    "type",
+    "zipOrPostalCode",
+]
 
 
 class TestLocations(unittest.TestCase):
-
     def setUp(self):
 
-        self.van = VAN(os.environ['VAN_API_KEY'], db="EveryAction", raise_for_status=False)
+        self.van = VAN(
+            os.environ["VAN_API_KEY"], db="EveryAction", raise_for_status=False
+        )
 
     def tearDown(self):
 
         pass
 
     @requests_mock.Mocker()
     def test_get_locations(self, m):
 
-        json = {'items': [location_json], 'nextPageLink': None, 'count': 1}
-        m.get(self.van.connection.uri + 'locations', json=json)
+        json = {"items": [location_json], "nextPageLink": None, "count": 1}
+        m.get(self.van.connection.uri + "locations", json=json)
 
         self.assertTrue(validate_list(expected_loc, self.van.get_locations()))
 
     @requests_mock.Mocker()
     def test_get_location(self, m):
 
         # Valid location id
-        m.get(self.van.connection.uri + 'locations/34', json=location_json)
+        m.get(self.van.connection.uri + "locations/34", json=location_json)
         self.assertEqual(location_json, self.van.get_location(34))
 
     @requests_mock.Mocker()
     def test_delete_location(self, m):
 
         # Test good location delete
-        m.delete(self.van.connection.uri + 'locations/1', status_code=200)
+        m.delete(self.van.connection.uri + "locations/1", status_code=200)
         self.van.delete_location(1)
 
         # Test invalid location delete
-        m.delete(self.van.connection.uri + 'locations/2', status_code=404)
+        m.delete(self.van.connection.uri + "locations/2", status_code=404)
         self.assertRaises(HTTPError, self.van.delete_location, 2)
 
     @requests_mock.Mocker()
     def test_create_location(self, m):
 
         loc_id = 32
 
-        m.post(self.van.connection.uri + 'locations/findOrCreate', json=loc_id, status_code=204)
-
-        self.assertEqual(self.van.create_location(
-            name='Chicagowide', city='Chicago', state='IL'), loc_id)
+        m.post(
+            self.van.connection.uri + "locations/findOrCreate",
+            json=loc_id,
+            status_code=204,
+        )
+
+        self.assertEqual(
+            self.van.create_location(name="Chicagowide", city="Chicago", state="IL"),
+            loc_id,
+        )
```

### Comparing `parsons-1.0.0/test/test_van/test_ngpvan.py` & `parsons-1.1.0/test/test_van/test_ngpvan.py`

 * *Files 16% similar despite different names*

```diff
@@ -3,169 +3,193 @@
 import requests_mock
 from parsons import VAN, Table
 from test.utils import validate_list, assert_matching_tables
 from requests.exceptions import HTTPError
 
 
 class TestNGPVAN(unittest.TestCase):
-
     def setUp(self):
 
-        self.van = VAN(os.environ['VAN_API_KEY'], db="MyVoters", raise_for_status=False)
+        self.van = VAN(os.environ["VAN_API_KEY"], db="MyVoters", raise_for_status=False)
 
     def tearDown(self):
 
         pass
 
     @requests_mock.Mocker()
     def test_get_canvass_responses_contact_types(self, m):
 
-        json = {"name": "Auto Dial",
-                "contactTypeId": 19,
-                "channelTypeName": "Phone"}
+        json = {"name": "Auto Dial", "contactTypeId": 19, "channelTypeName": "Phone"}
 
-        m.get(self.van.connection.uri + 'canvassResponses/contactTypes', json=json)
+        m.get(self.van.connection.uri + "canvassResponses/contactTypes", json=json)
 
-        assert_matching_tables(Table(json), self.van.get_canvass_responses_contact_types())
+        assert_matching_tables(
+            Table(json), self.van.get_canvass_responses_contact_types()
+        )
 
     @requests_mock.Mocker()
     def test_get_canvass_responses_input_types(self, m):
 
         json = {"inputTypeId": 11, "name": "API"}
-        m.get(self.van.connection.uri + 'canvassResponses/inputTypes', json=json)
-        assert_matching_tables(Table(json), self.van.get_canvass_responses_input_types())
+        m.get(self.van.connection.uri + "canvassResponses/inputTypes", json=json)
+        assert_matching_tables(
+            Table(json), self.van.get_canvass_responses_input_types()
+        )
 
     @requests_mock.Mocker()
     def test_get_canvass_responses_result_codes(self, m):
 
         json = {
             "shortName": "BZ",
             "resultCodeId": 18,
             "name": "Busy",
-            "mediumName": "Busy"
+            "mediumName": "Busy",
         }
 
-        m.get(self.van.connection.uri + 'canvassResponses/resultCodes', json=json)
-        assert_matching_tables(Table(json), self.van.get_canvass_responses_result_codes())
+        m.get(self.van.connection.uri + "canvassResponses/resultCodes", json=json)
+        assert_matching_tables(
+            Table(json), self.van.get_canvass_responses_result_codes()
+        )
 
     @requests_mock.Mocker()
     def test_get_survey_questions(self, m):
 
-        json = {u'count': 67, u'items': [{
-            "status": "Active",
-            "responses": [
-                {"shortName": "1",
-                 "surveyResponseId": 1288926,
-                 "name": "1-Strong Walz",
-                         "mediumName": "1"},
-                {"shortName": "2",
-                 "surveyResponseId": 1288928,
-                 "name": "2-Lean Walz",
-                         "mediumName": "2"}],
-            "scriptQuestion": "Who do you support for Governor?",
-            "name": "MN Governor Gen",
+        json = {
+            "count": 67,
+            "items": [
+                {
+                    "status": "Active",
+                    "responses": [
+                        {
+                            "shortName": "1",
+                            "surveyResponseId": 1288926,
+                            "name": "1-Strong Walz",
+                            "mediumName": "1",
+                        },
+                        {
+                            "shortName": "2",
+                            "surveyResponseId": 1288928,
+                            "name": "2-Lean Walz",
+                            "mediumName": "2",
+                        },
+                    ],
+                    "scriptQuestion": "Who do you support for Governor?",
+                    "name": "MN Governor Gen",
                     "surveyQuestionId": 311838,
                     "mediumName": "MNGovG",
                     "shortName": "MGG",
                     "type": "Candidate",
-                    "cycle": 2018
-        }],
-            u'nextPageLink': None}
-
-        m.get(self.van.connection.uri + 'surveyQuestions', json=json)
-
-        expected = ['status', 'responses', 'scriptQuestion', 'name',
-                    'surveyQuestionId', 'mediumName', 'shortName',
-                    'type', 'cycle']
+                    "cycle": 2018,
+                }
+            ],
+            "nextPageLink": None,
+        }
+
+        m.get(self.van.connection.uri + "surveyQuestions", json=json)
+
+        expected = [
+            "status",
+            "responses",
+            "scriptQuestion",
+            "name",
+            "surveyQuestionId",
+            "mediumName",
+            "shortName",
+            "type",
+            "cycle",
+        ]
 
         self.assertTrue(validate_list(expected, self.van.get_survey_questions()))
 
     @requests_mock.Mocker()
     def test_get_supporter_groups(self, m):
 
-        json = {"items": [
-            {
-                "id": 12,
-                "name": "tmc",
-                "description": "A fun group."
-            },
-            {
-                "id": 13,
-                "name": "tmc",
-                "description": "A fun group."
-            },
-        ],
+        json = {
+            "items": [
+                {"id": 12, "name": "tmc", "description": "A fun group."},
+                {"id": 13, "name": "tmc", "description": "A fun group."},
+            ],
             "nextPageLink": None,
-            "count": 3
+            "count": 3,
         }
 
-        m.get(self.van.connection.uri + 'supporterGroups', json=json)
+        m.get(self.van.connection.uri + "supporterGroups", json=json)
 
-        ['id', 'name', 'description']
+        ["id", "name", "description"]
 
         self.van.get_supporter_groups()
 
     @requests_mock.Mocker()
     def test_get_supporter_group(self, m):
 
         json = {"id": 12, "name": "tmc", "description": "A fun group."}
-        m.get(self.van.connection.uri + 'supporterGroups/12', json=json)
+        m.get(self.van.connection.uri + "supporterGroups/12", json=json)
 
         # Test that columns are expected columns
         self.assertEqual(self.van.get_supporter_group(12), json)
 
     @requests_mock.Mocker()
     def test_delete_supporter_group(self, m):
 
         # Test good input
         good_supporter_group_id = 5
-        good_ep = f'supporterGroups/{good_supporter_group_id}'
+        good_ep = f"supporterGroups/{good_supporter_group_id}"
         m.delete(self.van.connection.uri + good_ep, status_code=204)
         self.van.delete_supporter_group(good_supporter_group_id)
 
         # Test bad input raises
         bad_supporter_group_id = 999
         # bad_vanid = 99999
-        bad_ep = f'supporterGroups/{bad_supporter_group_id}'
+        bad_ep = f"supporterGroups/{bad_supporter_group_id}"
         m.delete(self.van.connection.uri + bad_ep, status_code=404)
-        self.assertRaises(HTTPError, self.van.delete_supporter_group, bad_supporter_group_id)
+        self.assertRaises(
+            HTTPError, self.van.delete_supporter_group, bad_supporter_group_id
+        )
 
     @requests_mock.Mocker()
     def test_add_person_supporter_group(self, m):
 
         # Test good input
         good_supporter_group_id = 5
         good_vanid = 12345
-        good_uri = f'supporterGroups/{good_vanid}/people/{good_supporter_group_id}'
+        good_uri = f"supporterGroups/{good_vanid}/people/{good_supporter_group_id}"
         m.put(self.van.connection.uri + good_uri, status_code=204)
         self.van.add_person_supporter_group(good_vanid, good_supporter_group_id)
 
         # Test bad input
         bad_supporter_group_id = 999
         bad_vanid = 99999
-        bad_uri = f'supporterGroups/{bad_vanid}/people/{bad_supporter_group_id}'
+        bad_uri = f"supporterGroups/{bad_vanid}/people/{bad_supporter_group_id}"
         m.put(self.van.connection.uri + bad_uri, status_code=404)
         self.assertRaises(
-            HTTPError, self.van.add_person_supporter_group, bad_vanid, bad_supporter_group_id)
+            HTTPError,
+            self.van.add_person_supporter_group,
+            bad_vanid,
+            bad_supporter_group_id,
+        )
 
     @requests_mock.Mocker()
     def test_delete_person_supporter_group(self, m):
 
         # Test good input
         good_supporter_group_id = 5
         good_vanid = 12345
-        good_ep = f'supporterGroups/{good_vanid}/people/{good_supporter_group_id}'
+        good_ep = f"supporterGroups/{good_vanid}/people/{good_supporter_group_id}"
         m.delete(self.van.connection.uri + good_ep, status_code=204)
         self.van.delete_person_supporter_group(good_vanid, good_supporter_group_id)
 
         # Test bad input raises
         bad_supporter_group_id = 999
         bad_vanid = 99999
-        bad_ep = f'supporterGroups/{bad_vanid}/people/{bad_supporter_group_id}'
+        bad_ep = f"supporterGroups/{bad_vanid}/people/{bad_supporter_group_id}"
         m.delete(self.van.connection.uri + bad_ep, status_code=404)
         self.assertRaises(
-            HTTPError, self.van.delete_person_supporter_group, bad_vanid, bad_supporter_group_id)
+            HTTPError,
+            self.van.delete_person_supporter_group,
+            bad_vanid,
+            bad_supporter_group_id,
+        )
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
 
     unittest.main()
```

### Comparing `parsons-1.0.0/test/test_van/test_people.py` & `parsons-1.1.0/test/test_van/test_people.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,45 +1,55 @@
 import unittest
 import os
 import requests_mock
 from parsons import VAN
 from requests.exceptions import HTTPError
-from test.test_van.responses_people import find_people_response, get_person_response,\
-    merge_contacts_response
+from test.test_van.responses_people import (
+    find_people_response,
+    get_person_response,
+    merge_contacts_response,
+)
 
-os.environ['VAN_API_KEY'] = 'SOME_KEY'
+os.environ["VAN_API_KEY"] = "SOME_KEY"
 
 
 class TestNGPVAN(unittest.TestCase):
-
     def setUp(self):
 
-        self.van = VAN(os.environ['VAN_API_KEY'], db="MyVoters", raise_for_status=False)
+        self.van = VAN(os.environ["VAN_API_KEY"], db="MyVoters", raise_for_status=False)
 
     @requests_mock.Mocker()
     def test_find_person(self, m):
 
-        m.post(self.van.connection.uri + 'people/find', json=find_people_response, status_code=200)
-
-        person = self.van.find_person(first_name='Bob', last_name='Smith', phone=4142020792)
+        m.post(
+            self.van.connection.uri + "people/find",
+            json=find_people_response,
+            status_code=200,
+        )
+
+        person = self.van.find_person(
+            first_name="Bob", last_name="Smith", phone=4142020792
+        )
 
         self.assertEqual(person, find_people_response)
 
     @requests_mock.Mocker()
     def test_find_person_json(self, m):
 
         json = {
             "firstName": "Bob",
             "lastName": "Smith",
-            "phones": [{
-                "phoneNumber": 4142020792
-            }]
+            "phones": [{"phoneNumber": 4142020792}],
         }
 
-        m.post(self.van.connection.uri + 'people/find', json=find_people_response, status_code=200)
+        m.post(
+            self.van.connection.uri + "people/find",
+            json=find_people_response,
+            status_code=200,
+        )
 
         person = self.van.find_person_json(match_json=json)
 
         self.assertEqual(person, find_people_response)
 
     def test_upsert_person(self):
 
@@ -61,118 +71,180 @@
 
         # Already tested as part of upsert and find person methods
         pass
 
     def test_valid_search(self):
 
         # Fails with FN / LN Only
-        self.assertRaises(ValueError, self.van._valid_search, 'Barack',
-                          'Obama', None, None, None, None, None)
+        self.assertRaises(
+            ValueError,
+            self.van._valid_search,
+            "Barack",
+            "Obama",
+            None,
+            None,
+            None,
+            None,
+            None,
+        )
 
         # Fails with only Zip
-        self.assertRaises(ValueError, self.van._valid_search, 'Barack',
-                          'Obama', None, None, None, None, 60622)
+        self.assertRaises(
+            ValueError,
+            self.van._valid_search,
+            "Barack",
+            "Obama",
+            None,
+            None,
+            None,
+            None,
+            60622,
+        )
 
         # Fails with no street number
-        self.assertRaises(ValueError, self.van._valid_search, 'Barack',
-                          'Obama', None, None, None, 'Pennsylvania Ave', None)
+        self.assertRaises(
+            ValueError,
+            self.van._valid_search,
+            "Barack",
+            "Obama",
+            None,
+            None,
+            None,
+            "Pennsylvania Ave",
+            None,
+        )
 
         # Successful with FN/LN/Email
-        self.van._valid_search('Barack', 'Obama', 'barack@email.com', None, None, None,
-                               None)
+        self.van._valid_search(
+            "Barack", "Obama", "barack@email.com", None, None, None, None
+        )
 
         # Successful with FN/LN/DOB/ZIP
-        self.van._valid_search('Barack', 'Obama', 'barack@email.com', None, '2000-01-01',
-                               None, 20009)
+        self.van._valid_search(
+            "Barack", "Obama", "barack@email.com", None, "2000-01-01", None, 20009
+        )
 
         # Successful with FN/LN/Phone
-        self.van._valid_search('Barack', 'Obama', None, 2024291000, None, None,
-                               None)
+        self.van._valid_search("Barack", "Obama", None, 2024291000, None, None, None)
 
     @requests_mock.Mocker()
     def test_get_person(self, m):
 
         json = get_person_response
 
         # Test works with external ID
-        m.get(self.van.connection.uri + 'people/DWID:15406767', json=json)
-        person = self.van.get_person('15406767', id_type='DWID')
+        m.get(self.van.connection.uri + "people/DWID:15406767", json=json)
+        person = self.van.get_person("15406767", id_type="DWID")
         self.assertEqual(get_person_response, person)
 
         # Test works with vanid
-        m.get(self.van.connection.uri + 'people/19722445', json=json)
-        person = self.van.get_person('19722445')
+        m.get(self.van.connection.uri + "people/19722445", json=json)
+        person = self.van.get_person("19722445")
         self.assertEqual(get_person_response, person)
 
     @requests_mock.Mocker()
     def test_apply_canvass_result(self, m):
 
         # Test a valid attempt
-        m.post(self.van.connection.uri + 'people/2335282/canvassResponses', status_code=204)
+        m.post(
+            self.van.connection.uri + "people/2335282/canvassResponses", status_code=204
+        )
         self.van.apply_canvass_result(2335282, 18)
 
         # Test a bad result code
-        json = {'errors':
-                [{'code': 'INVALID_PARAMETER',
-                    'text': "'resultCodeId' must be a valid result code in the current context.",
-                    'properties': ['resultCodeId']}
-                 ]}
-        m.post(self.van.connection.uri + 'people/2335282/canvassResponses',
-               json=json, status_code=400)
+        json = {
+            "errors": [
+                {
+                    "code": "INVALID_PARAMETER",
+                    "text": "'resultCodeId' must be a valid result code in the current context.",
+                    "properties": ["resultCodeId"],
+                }
+            ]
+        }
+        m.post(
+            self.van.connection.uri + "people/2335282/canvassResponses",
+            json=json,
+            status_code=400,
+        )
         self.assertRaises(HTTPError, self.van.apply_canvass_result, 2335282, 0)
 
         # Test a bad vanid
-        json = {'errors':
-                [{'code': 'INTERNAL_SERVER_ERROR',
-                  'text': 'An unknown error occurred',
-                  'referenceCode': '88A111-E2FF8'}
-                 ]}
-        m.post(self.van.connection.uri + 'people/0/canvassResponses', json=json, status_code=400)
+        json = {
+            "errors": [
+                {
+                    "code": "INTERNAL_SERVER_ERROR",
+                    "text": "An unknown error occurred",
+                    "referenceCode": "88A111-E2FF8",
+                }
+            ]
+        }
+        m.post(
+            self.van.connection.uri + "people/0/canvassResponses",
+            json=json,
+            status_code=400,
+        )
         self.assertRaises(HTTPError, self.van.apply_canvass_result, 0, 18)
 
         # Test a good dwid
-        m.post(self.van.connection.uri + 'people/DWID:2335282/canvassResponses', status_code=204)
-        self.van.apply_canvass_result(2335282, 18, id_type='DWID')
+        m.post(
+            self.van.connection.uri + "people/DWID:2335282/canvassResponses",
+            status_code=204,
+        )
+        self.van.apply_canvass_result(2335282, 18, id_type="DWID")
 
         # test canvassing via phone or sms without providing phone number
-        self.assertRaises(Exception, self.van.apply_canvass_result, 2335282, 18, contact_type_id=37)
+        self.assertRaises(
+            Exception, self.van.apply_canvass_result, 2335282, 18, contact_type_id=37
+        )
 
         # test canvassing via phone or sms with providing phone number
-        m.post(self.van.connection.uri + 'people/2335282/canvassResponses', status_code=204)
-        self.van.apply_canvass_result(2335282, 18, contact_type_id=37, phone='(516)-555-2342')
+        m.post(
+            self.van.connection.uri + "people/2335282/canvassResponses", status_code=204
+        )
+        self.van.apply_canvass_result(
+            2335282, 18, contact_type_id=37, phone="(516)-555-2342"
+        )
 
     @requests_mock.Mocker()
     def test_apply_survey_question(self, m):
 
         # Test valid survey question
-        m.post(self.van.connection.uri + 'people/2335282/canvassResponses', status_code=204)
+        m.post(
+            self.van.connection.uri + "people/2335282/canvassResponses", status_code=204
+        )
         self.van.apply_survey_response(2335282, 351006, 1443891)
 
         # Test bad survey response id
         # json = {
         #     'errors': [{
         #         'code': 'INVALID_PARAMETER',
         #         'text': ("'surveyResponseId' must be a valid Response to the given "
         #                  "Survey Question."),
         #         'properties': ['responses[0].surveyResponseId']
         #     }]
         # }
-        m.post(self.van.connection.uri + 'people/2335282/canvassResponses', status_code=400)
-        self.assertRaises(HTTPError, self.van.apply_survey_response, 2335282, 0, 1443891)
+        m.post(
+            self.van.connection.uri + "people/2335282/canvassResponses", status_code=400
+        )
+        self.assertRaises(
+            HTTPError, self.van.apply_survey_response, 2335282, 0, 1443891
+        )
 
         # Test bad survey question id
         # json = {
         #     'errors': [{
         #         'code': 'INVALID_PARAMETER',
         #         'text': ("'surveyQuestionId' must be a valid Survey Question that is "
         #                 "available in the current context."),
         #         'properties': ['responses[0].surveyQuestionId']
         #     }]
         # }
-        m.post(self.van.connection.uri + 'people/2335282/canvassResponses', status_code=400)
+        m.post(
+            self.van.connection.uri + "people/2335282/canvassResponses", status_code=400
+        )
         self.assertRaises(HTTPError, self.van.apply_survey_response, 2335282, 351006, 0)
 
     def test_toggle_volunteer_action(self):
 
         pass
 
     def test_apply_response(self):
@@ -184,26 +256,40 @@
 
         relationship_id = 12
         bad_vanid_1 = 99999
         good_vanid_1 = 12345
         vanid_2 = 54321
 
         # Bad request
-        m.post(self.van.connection.uri + "people/{}/relationships".format(bad_vanid_1),
-               status_code=404)
+        m.post(
+            self.van.connection.uri + "people/{}/relationships".format(bad_vanid_1),
+            status_code=404,
+        )
 
         # Good request
-        m.post(self.van.connection.uri + "people/{}/relationships".format(good_vanid_1),
-               status_code=204)
+        m.post(
+            self.van.connection.uri + "people/{}/relationships".format(good_vanid_1),
+            status_code=204,
+        )
 
         # Test bad input
         self.assertRaises(
-            HTTPError, self.van.create_relationship, bad_vanid_1, vanid_2, relationship_id)
+            HTTPError,
+            self.van.create_relationship,
+            bad_vanid_1,
+            vanid_2,
+            relationship_id,
+        )
         self.assertRaises(
-            HTTPError, self.van.create_relationship, bad_vanid_1, vanid_2, relationship_id)
+            HTTPError,
+            self.van.create_relationship,
+            bad_vanid_1,
+            vanid_2,
+            relationship_id,
+        )
 
         self.van.create_relationship(good_vanid_1, vanid_2, relationship_id)
 
     @requests_mock.Mocker()
     def test_apply_person_code(self, m):
 
         vanid = 999
@@ -218,13 +304,16 @@
         self.assertRaises(HTTPError, self.van.apply_person_code, vanid, code_id)
 
     @requests_mock.Mocker()
     def test_merge_contacts(self, m):
 
         source_vanid = 12345
 
-        m.put(self.van.connection.uri + f'people/{source_vanid}/mergeInto',
-              json=merge_contacts_response, status_code=200)
+        m.put(
+            self.van.connection.uri + f"people/{source_vanid}/mergeInto",
+            json=merge_contacts_response,
+            status_code=200,
+        )
 
         person = self.van.merge_contacts(source_vanid=source_vanid, primary_vanid=56789)
 
         self.assertEqual(person, merge_contacts_response)
```

### Comparing `parsons-1.0.0/test/test_van/test_printed_lists.py` & `parsons-1.1.0/test/test_van/test_printed_lists.py`

 * *Files 16% similar despite different names*

```diff
@@ -2,29 +2,28 @@
 import os
 import requests_mock
 from parsons import VAN
 from test.test_van.responses_printed_lists import list_json, single_list_json
 
 
 class TestSavedLists(unittest.TestCase):
-
     def setUp(self):
 
-        self.van = VAN(os.environ['VAN_API_KEY'], db="MyVoters", raise_for_status=False)
+        self.van = VAN(os.environ["VAN_API_KEY"], db="MyVoters", raise_for_status=False)
 
     @requests_mock.Mocker()
     def test_get_printed_lists(self, m):
 
-        m.get(self.van.connection.uri + 'printedLists', json=list_json)
+        m.get(self.van.connection.uri + "printedLists", json=list_json)
 
-        result = self.van.get_printed_lists(folder_name='Covington Canvass Turfs')
+        result = self.van.get_printed_lists(folder_name="Covington Canvass Turfs")
 
         self.assertEqual(result.num_rows, 14)
 
     @requests_mock.Mocker()
     def test_get_printed_list(self, m):
 
-        m.get(self.van.connection.uri + 'printedLists/43-0000', json=single_list_json)
+        m.get(self.van.connection.uri + "printedLists/43-0000", json=single_list_json)
 
-        result = self.van.get_printed_list(printed_list_number='43-0000')
+        result = self.van.get_printed_list(printed_list_number="43-0000")
 
-        self.assertEqual(result['number'], '43-0000')
+        self.assertEqual(result["number"], "43-0000")
```

### Comparing `parsons-1.0.0/test/test_van/test_saved_lists.py` & `parsons-1.1.0/test/test_van/test_saved_lists.py`

 * *Files 7% similar despite different names*

```diff
@@ -4,158 +4,170 @@
 import unittest.mock as mock
 from parsons import VAN, Table
 from test.utils import validate_list
 from parsons.utilities import cloud_storage
 
 
 class TestSavedLists(unittest.TestCase):
-
     def setUp(self):
 
-        self.van = VAN(os.environ['VAN_API_KEY'], db="MyVoters", raise_for_status=False)
+        self.van = VAN(os.environ["VAN_API_KEY"], db="MyVoters", raise_for_status=False)
 
     def tearDown(self):
 
         pass
 
     @requests_mock.Mocker()
     def test_get_saved_lists(self, m):
 
-        json = {'count': 1, 'items': [
-            {"savedListId": 517612,
-             "listCount": 974656,
-             "name": "LikelyParents(16andunder)_DWID_S... - MN",
-             "doorCount": 520709,
-             "description": "null"
-             }
-        ], 'nextPageLink': None}
+        json = {
+            "count": 1,
+            "items": [
+                {
+                    "savedListId": 517612,
+                    "listCount": 974656,
+                    "name": "LikelyParents(16andunder)_DWID_S... - MN",
+                    "doorCount": 520709,
+                    "description": "null",
+                }
+            ],
+            "nextPageLink": None,
+        }
 
-        m.get(self.van.connection.uri + 'savedLists', json=json)
+        m.get(self.van.connection.uri + "savedLists", json=json)
 
-        expected = ['savedListId', 'listCount', 'name', 'doorCount', 'description']
+        expected = ["savedListId", "listCount", "name", "doorCount", "description"]
 
         self.assertTrue(validate_list(expected, self.van.get_saved_lists()))
 
     @requests_mock.Mocker()
     def test_get_saved_list(self, m):
 
         saved_list_id = 517612
 
-        json = {"savedListId": 517612,
-                "listCount": 974656,
-                "name": "LikelyParents(16andunder)_DWID_S... - MN",
-                "doorCount": 520709,
-                "description": "null"
-                }
+        json = {
+            "savedListId": 517612,
+            "listCount": 974656,
+            "name": "LikelyParents(16andunder)_DWID_S... - MN",
+            "doorCount": 520709,
+            "description": "null",
+        }
 
-        m.get(self.van.connection.uri + f'savedLists/{saved_list_id}', json=json)
+        m.get(self.van.connection.uri + f"savedLists/{saved_list_id}", json=json)
 
         # expected = ['savedListId', 'listCount', 'name', 'doorCount', 'description']
 
         self.assertEqual(self.van.get_saved_list(saved_list_id), json)
 
     def test_upload_saved_list(self):
 
         cloud_storage.post_file = mock.MagicMock()
-        cloud_storage.post_file.return_value = 'https://box.com/my_file.zip'
+        cloud_storage.post_file.return_value = "https://box.com/my_file.zip"
 
         self.van.connection._soap_client = mock.MagicMock()
         self.van.get_folders = mock.MagicMock()
-        self.van.get_folders.return_value = [{'folderId': 1}]
+        self.van.get_folders.return_value = [{"folderId": 1}]
 
-        tbl = Table([['VANID'], ['1'], ['2'], ['3']])
+        tbl = Table([["VANID"], ["1"], ["2"], ["3"]])
         self.van.upload_saved_list(
-            tbl, 'GOTV List', 1, replace=True, url_type='S3', bucket='tmc-scratch')
+            tbl, "GOTV List", 1, replace=True, url_type="S3", bucket="tmc-scratch"
+        )
         assert self.van.connection._soap_client.service.CreateAndStoreSavedList.called
 
         @requests_mock.Mocker()
         def test_upload_saved_list_rest(self):
 
             cloud_storage.post_file = mock.MagicMock()
-            cloud_storage.post_file.return_value = 'https://box.com/my_file.zip'
+            cloud_storage.post_file.return_value = "https://box.com/my_file.zip"
             self.van.get_folders = mock.MagicMock()
-            self.van.get_folders.return_value = [{'folderId': 1}]
+            self.van.get_folders.return_value = [{"folderId": 1}]
 
-            tbl = Table([['VANID'], ['1'], ['2'], ['3']])
+            tbl = Table([["VANID"], ["1"], ["2"], ["3"]])
             response = self.van.upload_saved_list_rest(
-                tbl=tbl, url_type="S3",
-                folder_id=1, list_name="GOTV List", description="parsons test list",
+                tbl=tbl,
+                url_type="S3",
+                folder_id=1,
+                list_name="GOTV List",
+                description="parsons test list",
                 callback_url="https://webhook.site/69ab58c3-a3a7-4ed8-828c-1ea850cb4160",
-                columns=["VANID"], id_column="VANID",
+                columns=["VANID"],
+                id_column="VANID",
                 bucket="tmc-scratch",
-                overwrite=517612
-                )
+                overwrite=517612,
+            )
             self.assertIn("jobId", response)
 
     @requests_mock.Mocker()
     def test_get_folders(self, m):
 
-        json = {u'count': 2,
-                u'items': [
-                    {
-                        u'folderId': 5046,
-                        u'name': u'#2018_MN_active_universe'
-                    },
-                    {u'folderId': 2168,
-                     u'name': u'API Generated Lists'
-                     }
-                ], u'nextPageLink': None}
+        json = {
+            "count": 2,
+            "items": [
+                {"folderId": 5046, "name": "#2018_MN_active_universe"},
+                {"folderId": 2168, "name": "API Generated Lists"},
+            ],
+            "nextPageLink": None,
+        }
 
-        m.get(self.van.connection.uri + 'folders', json=json)
+        m.get(self.van.connection.uri + "folders", json=json)
 
-        expected = ['folderId', 'name']
+        expected = ["folderId", "name"]
 
         self.assertTrue(validate_list(expected, self.van.get_folders()))
 
     @requests_mock.Mocker()
     def test_get_folder(self, m):
 
         folder_id = 5046
 
         json = {"folderId": 5046, "name": "#2018_MN_active_universe"}
 
-        m.get(self.van.connection.uri + f'folders/{folder_id}', json=json)
+        m.get(self.van.connection.uri + f"folders/{folder_id}", json=json)
 
         self.assertEqual(json, self.van.get_folder(folder_id))
 
     @requests_mock.Mocker()
     def test_export_job_types(self, m):
 
-        json = {u'count': 1, u'items':
-                [{u'exportJobTypeId': 4, u'name': u'SavedListExport'}],
-                u'nextPageLink': None}
+        json = {
+            "count": 1,
+            "items": [{"exportJobTypeId": 4, "name": "SavedListExport"}],
+            "nextPageLink": None,
+        }
 
-        m.get(self.van.connection.uri + 'exportJobTypes', json=json)
+        m.get(self.van.connection.uri + "exportJobTypes", json=json)
 
-        expected = ['exportJobTypeId', 'name']
+        expected = ["exportJobTypeId", "name"]
 
         self.assertTrue(validate_list(expected, self.van.get_export_job_types()))
 
     @requests_mock.Mocker()
     def test_export_job_create(self, m):
 
         saved_list_id = 517612
 
-        json = {"status": "Completed",
-                "errorCode": "null",
-                "exportJobGuid": "bf4d1297-1c77-3fb2-03bd-f0acda122d37",
-                "activistCodes": "null",
-                "canvassFileRequestId": 448,
-                "dateExpired": "2018-09-08T16:04:00Z",
-                "surveyQuestions": "null",
-                "webhookUrl": "https://www.nothing.com/",
-                "downloadUrl": "https://ngpvan.blob.core.windows.net/canvass-files-savedlistexport/bf4d1297-1c77-3fb2-03bd-f0acda122d37_2018-09-08T13:03:27.7191831-04:00.csv",  # noqa: E501
-                "savedListId": 517612,
-                "districtFields": "null",
-                "canvassFileRequestGuid": "bf4d1297-1c77-3fb2-03bd-f0acda122d37",
-                "customFields": "null",
-                "type": 4,
-                "exportJobId": 448}
+        json = {
+            "status": "Completed",
+            "errorCode": "null",
+            "exportJobGuid": "bf4d1297-1c77-3fb2-03bd-f0acda122d37",
+            "activistCodes": "null",
+            "canvassFileRequestId": 448,
+            "dateExpired": "2018-09-08T16:04:00Z",
+            "surveyQuestions": "null",
+            "webhookUrl": "https://www.nothing.com/",
+            "downloadUrl": "https://ngpvan.blob.core.windows.net/canvass-files-savedlistexport/bf4d1297-1c77-3fb2-03bd-f0acda122d37_2018-09-08T13:03:27.7191831-04:00.csv",  # noqa: E501
+            "savedListId": 517612,
+            "districtFields": "null",
+            "canvassFileRequestGuid": "bf4d1297-1c77-3fb2-03bd-f0acda122d37",
+            "customFields": "null",
+            "type": 4,
+            "exportJobId": 448,
+        }
 
-        m.post(self.van.connection.uri + 'exportJobs', json=json, status_code=201)
+        m.post(self.van.connection.uri + "exportJobs", json=json, status_code=201)
 
         # expected = [
         #     'status',
         #     'errorCode',
         #     'exportJobGuid',
         #     'activistCodes',
         #     'canvassFileRequestId',
@@ -173,29 +185,31 @@
         self.assertEqual(json, self.van.export_job_create(saved_list_id))
 
     @requests_mock.Mocker()
     def test_get_export_job(self, m):
 
         export_job_id = 448
 
-        json = {"status": "Completed",
-                "errorCode": "null",
-                "exportJobGuid": "bf4d1297-1c77-3fb2-03bd-f0acda122d37",
-                "activistCodes": "null",
-                "canvassFileRequestId": 448,
-                "dateExpired": "2018-09-08T16:04:00Z",
-                "surveyQuestions": "null",
-                "webhookUrl": "https://www.nothing.com/",
-                "downloadUrl": "https://ngpvan.blob.core.windows.net/canvass-files-savedlistexport/bf4d1297-1c77-3fb2-03bd-f0acda122d37_2018-09-08T13:03:27.7191831-04:00.csv",  # noqa: E501
-                "savedListId": 517612,
-                "districtFields": "null",
-                "canvassFileRequestGuid": "bf4d1297-1c77-3fb2-03bd-f0acda122d37",
-                "customFields": "null",
-                "type": 4,
-                "exportJobId": 448}
+        json = {
+            "status": "Completed",
+            "errorCode": "null",
+            "exportJobGuid": "bf4d1297-1c77-3fb2-03bd-f0acda122d37",
+            "activistCodes": "null",
+            "canvassFileRequestId": 448,
+            "dateExpired": "2018-09-08T16:04:00Z",
+            "surveyQuestions": "null",
+            "webhookUrl": "https://www.nothing.com/",
+            "downloadUrl": "https://ngpvan.blob.core.windows.net/canvass-files-savedlistexport/bf4d1297-1c77-3fb2-03bd-f0acda122d37_2018-09-08T13:03:27.7191831-04:00.csv",  # noqa: E501
+            "savedListId": 517612,
+            "districtFields": "null",
+            "canvassFileRequestGuid": "bf4d1297-1c77-3fb2-03bd-f0acda122d37",
+            "customFields": "null",
+            "type": 4,
+            "exportJobId": 448,
+        }
 
         # expected = [
         #     'status',
         #     'errorCode',
         #     'exportJobGuid',
         #     'activistCodes',
         #     'canvassFileRequestId',
@@ -206,10 +220,10 @@
         #     'savedListId',
         #     'districtFields',
         #     'canvassFileRequestGuid',
         #     'customFields',
         #     'type',
         #     'exportJobId']
 
-        m.get(self.van.connection.uri + f'exportJobs/{export_job_id}', json=json)
+        m.get(self.van.connection.uri + f"exportJobs/{export_job_id}", json=json)
 
         self.assertEqual(json, self.van.get_export_job(export_job_id))
```

### Comparing `parsons-1.0.0/test/test_zoom.py` & `parsons-1.1.0/test/test_zoom.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,391 +1,434 @@
 import unittest
-import requests_mock
 from test.utils import assert_matching_tables
 
+import requests_mock
 from parsons import Table, Zoom
 
-API_KEY = 'fake_api_key'
-API_SECRET = 'fake_api_secret'
-ZOOM_URI = 'https://api.zoom.us/v2/'
+API_KEY = "fake_api_key"
+API_SECRET = "fake_api_secret"
+ZOOM_URI = "https://api.zoom.us/v2/"
 
 
 class TestZoom(unittest.TestCase):
-
     def setUp(self):
         self.zoom = Zoom(API_KEY, API_SECRET)
 
     @requests_mock.Mocker()
     def test_get_users(self, m):
         user_json = {
-            'page_count': 1,
-            'page_number': 1,
-            'page_size': 30,
-            'total_records': 1,
-            'users': [{
-                'id': 'C5A2nRWwTMm_hXyJb1JXMh',
-                'first_name': 'Bob',
-                'last_name': 'McBob',
-                'email': 'bob@bob.com',
-                'type': 2,
-                'pmi': 8374523641,
-                'timezone': 'America/New_York',
-                'verified': 1,
-                'dept': '',
-                'created_at': '2017-10-06T15:22:34Z',
-                'last_login_time': '2020-05-06T16:50:45Z',
-                'last_client_version': '',
-                'language': '',
-                'phone_number': '',
-                'status': 'active'}]}
-
-        tbl = Table([{'id': 'C5A2nRWwTMm_hXyJb1JXMh',
-                      'first_name': 'Bob',
-                      'last_name': 'McBob',
-                      'email': 'bob@bob.com',
-                      'type': 2,
-                      'pmi': 8374523641,
-                      'timezone': 'America/New_York',
-                      'verified': 1,
-                      'dept': '',
-                      'created_at': '2017-10-06T15:22:34Z',
-                      'last_login_time': '2020-05-06T16:50:45Z',
-                      'last_client_version': '',
-                      'language': '',
-                      'phone_number': '',
-                      'status': 'active'}])
+            "page_count": 1,
+            "page_number": 1,
+            "page_size": 30,
+            "total_records": 1,
+            "users": [
+                {
+                    "id": "C5A2nRWwTMm_hXyJb1JXMh",
+                    "first_name": "Bob",
+                    "last_name": "McBob",
+                    "email": "bob@bob.com",
+                    "type": 2,
+                    "pmi": 8374523641,
+                    "timezone": "America/New_York",
+                    "verified": 1,
+                    "dept": "",
+                    "created_at": "2017-10-06T15:22:34Z",
+                    "last_login_time": "2020-05-06T16:50:45Z",
+                    "last_client_version": "",
+                    "language": "",
+                    "phone_number": "",
+                    "status": "active",
+                }
+            ],
+        }
+
+        tbl = Table(
+            [
+                {
+                    "id": "C5A2nRWwTMm_hXyJb1JXMh",
+                    "first_name": "Bob",
+                    "last_name": "McBob",
+                    "email": "bob@bob.com",
+                    "type": 2,
+                    "pmi": 8374523641,
+                    "timezone": "America/New_York",
+                    "verified": 1,
+                    "dept": "",
+                    "created_at": "2017-10-06T15:22:34Z",
+                    "last_login_time": "2020-05-06T16:50:45Z",
+                    "last_client_version": "",
+                    "language": "",
+                    "phone_number": "",
+                    "status": "active",
+                }
+            ]
+        )
 
-        m.get(ZOOM_URI + 'users', json=user_json)
+        m.get(ZOOM_URI + "users", json=user_json)
         assert_matching_tables(self.zoom.get_users(), tbl)
 
     @requests_mock.Mocker()
     def test_get_meeting_participants(self, m):
         participants = {
-            'page_count': 1,
-            'page_size': 30,
-            'total_records': 4,
-            'next_page_token': '',
-            'participants': [{
-                'id': '',
-                'user_id': '16778240',
-                'name': 'Barack Obama',
-                'user_email': '',
-                'join_time': '2020-04-24T21:00:26Z',
-                'leave_time': '2020-04-24T22:24:38Z',
-                'duration': 5052,
-                'attentiveness_score': ''
-            }, {
-                'id': '',
-                'user_id': '16779264',
-                'name': '',
-                'user_email': '',
-                'join_time': '2020-04-24T21:00:45Z',
-                'leave_time': '2020-04-24T22:24:38Z',
-                'duration': 5033,
-                'attentiveness_score': ''
-            }]}
-
-        tbl = Table([{
-            'id': '',
-            'user_id': '16778240',
-            'name': 'Barack Obama',
-            'user_email': '',
-            'join_time': '2020-04-24T21:00:26Z',
-            'leave_time': '2020-04-24T22:24:38Z',
-            'duration': 5052,
-            'attentiveness_score': ''
-        }, {
-            'id': '',
-            'user_id': '16779264',
-            'name': '',
-            'user_email': '',
-            'join_time': '2020-04-24T21:00:45Z',
-            'leave_time': '2020-04-24T22:24:38Z',
-            'duration': 5033,
-            'attentiveness_score': ''}])
+            "page_count": 1,
+            "page_size": 30,
+            "total_records": 4,
+            "next_page_token": "",
+            "participants": [
+                {
+                    "id": "",
+                    "user_id": "16778240",
+                    "name": "Barack Obama",
+                    "user_email": "",
+                    "join_time": "2020-04-24T21:00:26Z",
+                    "leave_time": "2020-04-24T22:24:38Z",
+                    "duration": 5052,
+                    "attentiveness_score": "",
+                },
+                {
+                    "id": "",
+                    "user_id": "16779264",
+                    "name": "",
+                    "user_email": "",
+                    "join_time": "2020-04-24T21:00:45Z",
+                    "leave_time": "2020-04-24T22:24:38Z",
+                    "duration": 5033,
+                    "attentiveness_score": "",
+                },
+            ],
+        }
+
+        tbl = Table(
+            [
+                {
+                    "id": "",
+                    "user_id": "16778240",
+                    "name": "Barack Obama",
+                    "user_email": "",
+                    "join_time": "2020-04-24T21:00:26Z",
+                    "leave_time": "2020-04-24T22:24:38Z",
+                    "duration": 5052,
+                    "attentiveness_score": "",
+                },
+                {
+                    "id": "",
+                    "user_id": "16779264",
+                    "name": "",
+                    "user_email": "",
+                    "join_time": "2020-04-24T21:00:45Z",
+                    "leave_time": "2020-04-24T22:24:38Z",
+                    "duration": 5033,
+                    "attentiveness_score": "",
+                },
+            ]
+        )
 
-        m.get(ZOOM_URI + 'report/meetings/123/participants', json=participants)
+        m.get(ZOOM_URI + "report/meetings/123/participants", json=participants)
         assert_matching_tables(self.zoom.get_past_meeting_participants(123), tbl)
 
     @requests_mock.Mocker()
     def test_get_meeting_registrants(self, m):
         registrants = {
-            'page_count': 1,
-            'page_size': 30,
-            'total_records': 4,
-            'next_page_token': '',
-            'registrants': [{
-                'id': '',
-                'user_id': '16778240',
-                'name': 'Barack Obama',
-                'user_email': '',
-                'purchasing_time_frame': 'Within a month',
-                'role_in_purchase_process': 'Not involved'
-            }, {
-                'id': '',
-                'user_id': '16779264',
-                'name': '',
-                'user_email': '',
-                'purchasing_time_frame': 'Within a month',
-                'role_in_purchase_process': 'Not involved'
-            }]}
-
-        tbl = Table([
-            {
-                'id': '',
-                'user_id': '16778240',
-                'name': 'Barack Obama',
-                'user_email': '',
-                'purchasing_time_frame': 'Within a month',
-                'role_in_purchase_process': 'Not involved'
-            },
-            {
-                'id': '',
-                'user_id': '16779264',
-                'name': '',
-                'user_email': '',
-                'purchasing_time_frame': 'Within a month',
-                'role_in_purchase_process': 'Not involved'
-            }
-        ])
+            "page_count": 1,
+            "page_size": 30,
+            "total_records": 4,
+            "next_page_token": "",
+            "registrants": [
+                {
+                    "id": "",
+                    "user_id": "16778240",
+                    "name": "Barack Obama",
+                    "user_email": "",
+                    "purchasing_time_frame": "Within a month",
+                    "role_in_purchase_process": "Not involved",
+                },
+                {
+                    "id": "",
+                    "user_id": "16779264",
+                    "name": "",
+                    "user_email": "",
+                    "purchasing_time_frame": "Within a month",
+                    "role_in_purchase_process": "Not involved",
+                },
+            ],
+        }
+
+        tbl = Table(
+            [
+                {
+                    "id": "",
+                    "user_id": "16778240",
+                    "name": "Barack Obama",
+                    "user_email": "",
+                    "purchasing_time_frame": "Within a month",
+                    "role_in_purchase_process": "Not involved",
+                },
+                {
+                    "id": "",
+                    "user_id": "16779264",
+                    "name": "",
+                    "user_email": "",
+                    "purchasing_time_frame": "Within a month",
+                    "role_in_purchase_process": "Not involved",
+                },
+            ]
+        )
 
-        m.get(ZOOM_URI + 'meetings/123/registrants', json=registrants)
+        m.get(ZOOM_URI + "meetings/123/registrants", json=registrants)
         assert_matching_tables(self.zoom.get_meeting_registrants(123), tbl)
 
     @requests_mock.Mocker()
     def test_get_user_webinars(self, m):
         webinars = {
-            'page_count': 1,
-            'page_size': 30,
-            'total_records': 4,
-            'next_page_token': '',
-            'webinars': [{
-                "uuid": "dsghfkhaewfds",
-                "id": '',
-                "host_id": "24654130000000",
-                "topic": "My Webinar",
-                "agenda": "Learn more about Zoom APIs",
-                "type": "5",
-                "duration": "60",
-                "start_time": "2019-09-24T22:00:00Z",
-                "timezone": "America/Los_Angeles",
-                "created_at": "2019-08-30T22:00:00Z",
-                "join_url": "https://zoom.us/0001000/awesomewebinar"
-            }, {
-                "uuid": "dhf8as7dhf",
-                "id": '',
-                "host_id": "24654130000345",
-                "topic": "My Webinar",
-                "agenda": "Learn more about Zoom APIs",
-                "type": "5",
-                "duration": "60",
-                "start_time": "2019-09-24T22:00:00Z",
-                "timezone": "America/Los_Angeles",
-                "created_at": "2019-08-30T22:00:00Z",
-                "join_url": "https://zoom.us/0001000/awesomewebinar"
-            }]}
-
-        tbl = Table([
-            {
-                "uuid": "dsghfkhaewfds",
-                "id": '',
-                "host_id": "24654130000000",
-                "topic": "My Webinar",
-                "agenda": "Learn more about Zoom APIs",
-                "type": "5",
-                "duration": "60",
-                "start_time": "2019-09-24T22:00:00Z",
-                "timezone": "America/Los_Angeles",
-                "created_at": "2019-08-30T22:00:00Z",
-                "join_url": "https://zoom.us/0001000/awesomewebinar"
-            },
-            {
-                "uuid": "dhf8as7dhf",
-                "id": '',
-                "host_id": "24654130000345",
-                "topic": "My Webinar",
-                "agenda": "Learn more about Zoom APIs",
-                "type": "5",
-                "duration": "60",
-                "start_time": "2019-09-24T22:00:00Z",
-                "timezone": "America/Los_Angeles",
-                "created_at": "2019-08-30T22:00:00Z",
-                "join_url": "https://zoom.us/0001000/awesomewebinar"
-            }
-        ])
+            "page_count": 1,
+            "page_size": 30,
+            "total_records": 4,
+            "next_page_token": "",
+            "webinars": [
+                {
+                    "uuid": "dsghfkhaewfds",
+                    "id": "",
+                    "host_id": "24654130000000",
+                    "topic": "My Webinar",
+                    "agenda": "Learn more about Zoom APIs",
+                    "type": "5",
+                    "duration": "60",
+                    "start_time": "2019-09-24T22:00:00Z",
+                    "timezone": "America/Los_Angeles",
+                    "created_at": "2019-08-30T22:00:00Z",
+                    "join_url": "https://zoom.us/0001000/awesomewebinar",
+                },
+                {
+                    "uuid": "dhf8as7dhf",
+                    "id": "",
+                    "host_id": "24654130000345",
+                    "topic": "My Webinar",
+                    "agenda": "Learn more about Zoom APIs",
+                    "type": "5",
+                    "duration": "60",
+                    "start_time": "2019-09-24T22:00:00Z",
+                    "timezone": "America/Los_Angeles",
+                    "created_at": "2019-08-30T22:00:00Z",
+                    "join_url": "https://zoom.us/0001000/awesomewebinar",
+                },
+            ],
+        }
+
+        tbl = Table(
+            [
+                {
+                    "uuid": "dsghfkhaewfds",
+                    "id": "",
+                    "host_id": "24654130000000",
+                    "topic": "My Webinar",
+                    "agenda": "Learn more about Zoom APIs",
+                    "type": "5",
+                    "duration": "60",
+                    "start_time": "2019-09-24T22:00:00Z",
+                    "timezone": "America/Los_Angeles",
+                    "created_at": "2019-08-30T22:00:00Z",
+                    "join_url": "https://zoom.us/0001000/awesomewebinar",
+                },
+                {
+                    "uuid": "dhf8as7dhf",
+                    "id": "",
+                    "host_id": "24654130000345",
+                    "topic": "My Webinar",
+                    "agenda": "Learn more about Zoom APIs",
+                    "type": "5",
+                    "duration": "60",
+                    "start_time": "2019-09-24T22:00:00Z",
+                    "timezone": "America/Los_Angeles",
+                    "created_at": "2019-08-30T22:00:00Z",
+                    "join_url": "https://zoom.us/0001000/awesomewebinar",
+                },
+            ]
+        )
 
-        m.get(ZOOM_URI + 'users/123/webinars', json=webinars)
+        m.get(ZOOM_URI + "users/123/webinars", json=webinars)
         assert_matching_tables(self.zoom.get_user_webinars(123), tbl)
 
     @requests_mock.Mocker()
     def test_get_past_webinar_participants(self, m):
         participants = {
-            'page_count': 1,
-            'page_size': 30,
-            'total_records': 4,
-            'next_page_token': '',
-            'participants': [{
-                "id": "",
-                "user_id": "sdfjkldsf87987",
-                "name": "Barack",
-                "user_email": "riya@sdfjkldsf87987.fdjfhdf",
-                "join_time": "2019-02-01T12:34:12.660Z",
-                "leave_time": "2019-03-01T12:34:12.660Z",
-                "duration": "20"
-            }, {
-                "id": "",
-                "user_id": "sdfjkldsfdfgdfg",
-                "name": "Joe",
-                "user_email": "riya@sdfjkldsf87987.fdjfhdf",
-                "join_time": "2019-02-01T12:34:12.660Z",
-                "leave_time": "2019-03-01T12:34:12.660Z",
-                "duration": "20"
-            }]}
-
-        tbl = Table([
-            {
-                "id": "",
-                "user_id": "sdfjkldsf87987",
-                "name": "Barack",
-                "user_email": "riya@sdfjkldsf87987.fdjfhdf",
-                "join_time": "2019-02-01T12:34:12.660Z",
-                "leave_time": "2019-03-01T12:34:12.660Z",
-                "duration": "20"
-            },
-            {
-                "id": "",
-                "user_id": "sdfjkldsfdfgdfg",
-                "name": "Joe",
-                "user_email": "riya@sdfjkldsf87987.fdjfhdf",
-                "join_time": "2019-02-01T12:34:12.660Z",
-                "leave_time": "2019-03-01T12:34:12.660Z",
-                "duration": "20"
-            }
-        ])
+            "page_count": 1,
+            "page_size": 30,
+            "total_records": 4,
+            "next_page_token": "",
+            "participants": [
+                {
+                    "id": "",
+                    "user_id": "sdfjkldsf87987",
+                    "name": "Barack",
+                    "user_email": "riya@sdfjkldsf87987.fdjfhdf",
+                    "join_time": "2019-02-01T12:34:12.660Z",
+                    "leave_time": "2019-03-01T12:34:12.660Z",
+                    "duration": "20",
+                },
+                {
+                    "id": "",
+                    "user_id": "sdfjkldsfdfgdfg",
+                    "name": "Joe",
+                    "user_email": "riya@sdfjkldsf87987.fdjfhdf",
+                    "join_time": "2019-02-01T12:34:12.660Z",
+                    "leave_time": "2019-03-01T12:34:12.660Z",
+                    "duration": "20",
+                },
+            ],
+        }
+
+        tbl = Table(
+            [
+                {
+                    "id": "",
+                    "user_id": "sdfjkldsf87987",
+                    "name": "Barack",
+                    "user_email": "riya@sdfjkldsf87987.fdjfhdf",
+                    "join_time": "2019-02-01T12:34:12.660Z",
+                    "leave_time": "2019-03-01T12:34:12.660Z",
+                    "duration": "20",
+                },
+                {
+                    "id": "",
+                    "user_id": "sdfjkldsfdfgdfg",
+                    "name": "Joe",
+                    "user_email": "riya@sdfjkldsf87987.fdjfhdf",
+                    "join_time": "2019-02-01T12:34:12.660Z",
+                    "leave_time": "2019-03-01T12:34:12.660Z",
+                    "duration": "20",
+                },
+            ]
+        )
 
-        m.get(ZOOM_URI + 'report/webinars/123/participants', json=participants)
+        m.get(ZOOM_URI + "report/webinars/123/participants", json=participants)
         assert_matching_tables(self.zoom.get_past_webinar_participants(123), tbl)
 
     @requests_mock.Mocker()
     def test_get_webinar_registrants(self, m):
         registrants = {
-            'page_count': 1,
-            'page_size': 30,
-            'total_records': 4,
-            'next_page_token': '',
-            'registrants': [{
-                "id": "",
-                "email": "barack@obama.com",
-                "first_name": "Barack",
-                "last_name": "Obama",
-                "address": "dsfhkdjsfh st",
-                "city": "jackson heights",
-                "country": "US",
-                "zip": "11371",
-                "state": "NY",
-                "phone": "00000000",
-                "industry": "Food",
-                "org": "Cooking Org",
-                "job_title": "Chef",
-                "purchasing_time_frame": "1-3 months",
-                "role_in_purchase_process": "Influencer",
-                "no_of_employees": "10",
-                "comments": "Looking forward to the Webinar",
-                "custom_questions": [
-                    {
-                      "title": "What do you hope to learn from this Webinar?",
-                      "value": "Look forward to learning how you come up with recipes and services"
-                    }
-                ],
-                "status": "approved",
-                "create_time": "2019-02-26T23:01:16.899Z",
-                "join_url": "https://zoom.us/webinar/mywebinariscool"
-            }, {
-                "id": "",
-                "email": "joe@biden.com",
-                "first_name": "Joe",
-                "last_name": "Biden",
-                "address": "dsfhkdjsfh st",
-                "city": "jackson heights",
-                "country": "US",
-                "zip": "11371",
-                "state": "NY",
-                "phone": "00000000",
-                "industry": "Food",
-                "org": "Cooking Org",
-                "job_title": "Chef",
-                "purchasing_time_frame": "1-3 months",
-                "role_in_purchase_process": "Influencer",
-                "no_of_employees": "10",
-                "comments": "Looking forward to the Webinar",
-                "custom_questions": [
-                    {
-                      "title": "What do you hope to learn from this Webinar?",
-                      "value": "Look forward to learning how you come up with recipes and services"
-                    }
-                ],
-                "status": "approved",
-                "create_time": "2019-02-26T23:01:16.899Z",
-                "join_url": "https://zoom.us/webinar/mywebinariscool"
-            }]}
-
-        tbl = Table([
-            {
-                "id": "",
-                "email": "barack@obama.com",
-                "first_name": "Barack",
-                "last_name": "Obama",
-                "address": "dsfhkdjsfh st",
-                "city": "jackson heights",
-                "country": "US",
-                "zip": "11371",
-                "state": "NY",
-                "phone": "00000000",
-                "industry": "Food",
-                "org": "Cooking Org",
-                "job_title": "Chef",
-                "purchasing_time_frame": "1-3 months",
-                "role_in_purchase_process": "Influencer",
-                "no_of_employees": "10",
-                "comments": "Looking forward to the Webinar",
-                "custom_questions": [
-                    {
-                      "title": "What do you hope to learn from this Webinar?",
-                      "value": "Look forward to learning how you come up with recipes and services"
-                    }
-                ],
-                "status": "approved",
-                "create_time": "2019-02-26T23:01:16.899Z",
-                "join_url": "https://zoom.us/webinar/mywebinariscool"
-            },
-            {
-                "id": "",
-                "email": "joe@biden.com",
-                "first_name": "Joe",
-                "last_name": "Biden",
-                "address": "dsfhkdjsfh st",
-                "city": "jackson heights",
-                "country": "US",
-                "zip": "11371",
-                "state": "NY",
-                "phone": "00000000",
-                "industry": "Food",
-                "org": "Cooking Org",
-                "job_title": "Chef",
-                "purchasing_time_frame": "1-3 months",
-                "role_in_purchase_process": "Influencer",
-                "no_of_employees": "10",
-                "comments": "Looking forward to the Webinar",
-                "custom_questions": [
-                    {
-                      "title": "What do you hope to learn from this Webinar?",
-                      "value": "Look forward to learning how you come up with recipes and services"
-                    }
-                ],
-                "status": "approved",
-                "create_time": "2019-02-26T23:01:16.899Z",
-                "join_url": "https://zoom.us/webinar/mywebinariscool"
-            }
-        ])
+            "page_count": 1,
+            "page_size": 30,
+            "total_records": 4,
+            "next_page_token": "",
+            "registrants": [
+                {
+                    "id": "",
+                    "email": "barack@obama.com",
+                    "first_name": "Barack",
+                    "last_name": "Obama",
+                    "address": "dsfhkdjsfh st",
+                    "city": "jackson heights",
+                    "country": "US",
+                    "zip": "11371",
+                    "state": "NY",
+                    "phone": "00000000",
+                    "industry": "Food",
+                    "org": "Cooking Org",
+                    "job_title": "Chef",
+                    "purchasing_time_frame": "1-3 months",
+                    "role_in_purchase_process": "Influencer",
+                    "no_of_employees": "10",
+                    "comments": "Looking forward to the Webinar",
+                    "custom_questions": [
+                        {
+                            "title": "What do you hope to learn from this Webinar?",
+                            "value": "Look forward to learning how you come up with recipes and services",  # noqa: E501
+                        }
+                    ],
+                    "status": "approved",
+                    "create_time": "2019-02-26T23:01:16.899Z",
+                    "join_url": "https://zoom.us/webinar/mywebinariscool",
+                },
+                {
+                    "id": "",
+                    "email": "joe@biden.com",
+                    "first_name": "Joe",
+                    "last_name": "Biden",
+                    "address": "dsfhkdjsfh st",
+                    "city": "jackson heights",
+                    "country": "US",
+                    "zip": "11371",
+                    "state": "NY",
+                    "phone": "00000000",
+                    "industry": "Food",
+                    "org": "Cooking Org",
+                    "job_title": "Chef",
+                    "purchasing_time_frame": "1-3 months",
+                    "role_in_purchase_process": "Influencer",
+                    "no_of_employees": "10",
+                    "comments": "Looking forward to the Webinar",
+                    "custom_questions": [
+                        {
+                            "title": "What do you hope to learn from this Webinar?",
+                            "value": "Look forward to learning how you come up with recipes and services",  # noqa: E501
+                        }
+                    ],
+                    "status": "approved",
+                    "create_time": "2019-02-26T23:01:16.899Z",
+                    "join_url": "https://zoom.us/webinar/mywebinariscool",
+                },
+            ],
+        }
+
+        tbl = Table(
+            [
+                {
+                    "id": "",
+                    "email": "barack@obama.com",
+                    "first_name": "Barack",
+                    "last_name": "Obama",
+                    "address": "dsfhkdjsfh st",
+                    "city": "jackson heights",
+                    "country": "US",
+                    "zip": "11371",
+                    "state": "NY",
+                    "phone": "00000000",
+                    "industry": "Food",
+                    "org": "Cooking Org",
+                    "job_title": "Chef",
+                    "purchasing_time_frame": "1-3 months",
+                    "role_in_purchase_process": "Influencer",
+                    "no_of_employees": "10",
+                    "comments": "Looking forward to the Webinar",
+                    "custom_questions": [
+                        {
+                            "title": "What do you hope to learn from this Webinar?",
+                            "value": "Look forward to learning how you come up with recipes and services",  # noqa: E501
+                        }
+                    ],
+                    "status": "approved",
+                    "create_time": "2019-02-26T23:01:16.899Z",
+                    "join_url": "https://zoom.us/webinar/mywebinariscool",
+                },
+                {
+                    "id": "",
+                    "email": "joe@biden.com",
+                    "first_name": "Joe",
+                    "last_name": "Biden",
+                    "address": "dsfhkdjsfh st",
+                    "city": "jackson heights",
+                    "country": "US",
+                    "zip": "11371",
+                    "state": "NY",
+                    "phone": "00000000",
+                    "industry": "Food",
+                    "org": "Cooking Org",
+                    "job_title": "Chef",
+                    "purchasing_time_frame": "1-3 months",
+                    "role_in_purchase_process": "Influencer",
+                    "no_of_employees": "10",
+                    "comments": "Looking forward to the Webinar",
+                    "custom_questions": [
+                        {
+                            "title": "What do you hope to learn from this Webinar?",
+                            "value": "Look forward to learning how you come up with recipes and services",  # noqa: E501
+                        }
+                    ],
+                    "status": "approved",
+                    "create_time": "2019-02-26T23:01:16.899Z",
+                    "join_url": "https://zoom.us/webinar/mywebinariscool",
+                },
+            ]
+        )
 
-        m.get(ZOOM_URI + 'webinars/123/registrants', json=registrants)
+        m.get(ZOOM_URI + "webinars/123/registrants", json=registrants)
         assert_matching_tables(self.zoom.get_webinar_registrants(123), tbl)
```

### Comparing `parsons-1.0.0/test/utils.py` & `parsons-1.1.0/test/utils.py`

 * *Files 8% similar despite different names*

```diff
@@ -9,22 +9,23 @@
 
 @mark_live_test
 def test_something():
     service = SomeService()
     ...
 """
 mark_live_test = pytest.mark.skipif(
-    not os.environ.get('LIVE_TEST'), reason='Skipping because not running live test')
+    not os.environ.get("LIVE_TEST"), reason="Skipping because not running live test"
+)
 
 
 # Tests whether a table has the expected structure
 def validate_list(expected_keys, table):
 
     if set(expected_keys) != set(table.columns):
-        raise KeyError('Not all expected keys found.')
+        raise KeyError("Not all expected keys found.")
 
     return True
 
 
 def assert_matching_tables(table1, table2, ignore_headers=False):
     if ignore_headers:
         data1 = table1.data
```

