# Comparing `tmp/ms1searchpy-2.3.7-py3-none-any.whl.zip` & `tmp/ms1searchpy-2.4.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,16 +1,17 @@
-Zip file size: 34930 bytes, number of entries: 14
--rw-r--r--  2.0 unx        0 b- defN 21-Dec-13 13:18 ms1searchpy/__init__.py
--rw-r--r--  2.0 unx     2885 b- defN 22-Apr-06 14:56 ms1searchpy/combine.py
--rw-r--r--  2.0 unx    15507 b- defN 22-Apr-06 14:56 ms1searchpy/directms1quant.py
--rw-r--r--  2.0 unx    73591 b- defN 22-Apr-07 14:24 ms1searchpy/main.py
--rw-r--r--  2.0 unx     7413 b- defN 22-Apr-06 14:56 ms1searchpy/ms1todiffacto.py
--rw-r--r--  2.0 unx     3616 b- defN 22-Apr-06 15:26 ms1searchpy/search.py
--rw-r--r--  2.0 unx    14223 b- defN 22-Apr-06 14:56 ms1searchpy/utils.py
--rw-r--r--  2.0 unx    12868 b- defN 22-Jan-28 11:00 ms1searchpy/utils_figures.py
--rwxr-xr-x  2.0 unx      551 b- defN 22-Apr-08 10:53 ms1searchpy-2.3.7.dist-info/LICENSE
--rw-r--r--  2.0 unx     6263 b- defN 22-Apr-08 10:53 ms1searchpy-2.3.7.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 22-Apr-08 10:53 ms1searchpy-2.3.7.dist-info/WHEEL
--rw-r--r--  2.0 unx      229 b- defN 22-Apr-08 10:53 ms1searchpy-2.3.7.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       12 b- defN 22-Apr-08 10:53 ms1searchpy-2.3.7.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     1152 b- defN 22-Apr-08 10:53 ms1searchpy-2.3.7.dist-info/RECORD
-14 files, 138402 bytes uncompressed, 33018 bytes compressed:  76.1%
+Zip file size: 36962 bytes, number of entries: 15
+-rw-r--r--  2.0 unx        0 b- defN 23-Mar-16 13:01 ms1searchpy/__init__.py
+-rw-r--r--  2.0 unx     4185 b- defN 23-Mar-16 13:01 ms1searchpy/combine.py
+-rw-r--r--  2.0 unx     2746 b- defN 23-Mar-16 13:01 ms1searchpy/combine_proteins.py
+-rw-r--r--  2.0 unx    16453 b- defN 23-Mar-16 13:01 ms1searchpy/directms1quant.py
+-rw-r--r--  2.0 unx    74207 b- defN 23-Jun-15 09:03 ms1searchpy/main.py
+-rw-r--r--  2.0 unx     7413 b- defN 23-Mar-16 13:01 ms1searchpy/ms1todiffacto.py
+-rw-r--r--  2.0 unx     4528 b- defN 23-Jun-15 09:03 ms1searchpy/search.py
+-rw-r--r--  2.0 unx    14882 b- defN 23-Jun-15 09:03 ms1searchpy/utils.py
+-rw-r--r--  2.0 unx    12868 b- defN 23-Mar-16 13:01 ms1searchpy/utils_figures.py
+-rwxr-xr-x  2.0 unx      551 b- defN 23-Jun-15 09:04 ms1searchpy-2.4.1.dist-info/LICENSE
+-rw-r--r--  2.0 unx     6264 b- defN 23-Jun-15 09:04 ms1searchpy-2.4.1.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jun-15 09:04 ms1searchpy-2.4.1.dist-info/WHEEL
+-rw-r--r--  2.0 unx      283 b- defN 23-Jun-15 09:04 ms1searchpy-2.4.1.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       12 b- defN 23-Jun-15 09:04 ms1searchpy-2.4.1.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     1240 b- defN 23-Jun-15 09:04 ms1searchpy-2.4.1.dist-info/RECORD
+15 files, 145724 bytes uncompressed, 34912 bytes compressed:  76.0%
```

## zipnote {}

```diff
@@ -1,13 +1,16 @@
 Filename: ms1searchpy/__init__.py
 Comment: 
 
 Filename: ms1searchpy/combine.py
 Comment: 
 
+Filename: ms1searchpy/combine_proteins.py
+Comment: 
+
 Filename: ms1searchpy/directms1quant.py
 Comment: 
 
 Filename: ms1searchpy/main.py
 Comment: 
 
 Filename: ms1searchpy/ms1todiffacto.py
@@ -18,26 +21,26 @@
 
 Filename: ms1searchpy/utils.py
 Comment: 
 
 Filename: ms1searchpy/utils_figures.py
 Comment: 
 
-Filename: ms1searchpy-2.3.7.dist-info/LICENSE
+Filename: ms1searchpy-2.4.1.dist-info/LICENSE
 Comment: 
 
-Filename: ms1searchpy-2.3.7.dist-info/METADATA
+Filename: ms1searchpy-2.4.1.dist-info/METADATA
 Comment: 
 
-Filename: ms1searchpy-2.3.7.dist-info/WHEEL
+Filename: ms1searchpy-2.4.1.dist-info/WHEEL
 Comment: 
 
-Filename: ms1searchpy-2.3.7.dist-info/entry_points.txt
+Filename: ms1searchpy-2.4.1.dist-info/entry_points.txt
 Comment: 
 
-Filename: ms1searchpy-2.3.7.dist-info/top_level.txt
+Filename: ms1searchpy-2.4.1.dist-info/top_level.txt
 Comment: 
 
-Filename: ms1searchpy-2.3.7.dist-info/RECORD
+Filename: ms1searchpy-2.4.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## ms1searchpy/combine.py

```diff
@@ -1,8 +1,8 @@
-from .main import final_iteration
+from .main import final_iteration, filter_results
 import pandas as pd
 from collections import defaultdict
 import argparse
 import logging
 
 logger = logging.getLogger(__name__)
 
@@ -20,36 +20,61 @@
 
     parser.add_argument('file', nargs='+', help='input tsv PFMs_ML files')
     parser.add_argument('-out', help='prefix output file names', default='combined')
     parser.add_argument('-prots_full', help='path to any of *_proteins_full.tsv file. By default this file will be searched in the folder with PFMs_ML files', default='')
     parser.add_argument('-fdr', help='protein fdr filter in %%', default=1.0, type=float)
     parser.add_argument('-prefix', help='decoy prefix', default='DECOY_')
     parser.add_argument('-nproc', help='number of processes', default=1, type=int)
+    parser.add_argument('-pp', help='protein priority table for keeping protein groups when merge results by scoring', default='')
     args = vars(parser.parse_args())
     logging.basicConfig(format='%(levelname)9s: %(asctime)s %(message)s',
             datefmt='[%H:%M:%S]', level=logging.INFO)
 
+
+    d_tmp = dict()
+
     df1 = None
     for idx, filen in enumerate(args['file']):
-        df3 = pd.read_csv(filen, sep='\t')
+        print('Reading file %s' % (filen, ))
+        df3 = pd.read_csv(filen, sep='\t', usecols=['ids', 'qpreds', 'preds', 'decoy', 'seqs', 'proteins', 'peptide', 'iorig'])
         df3['ids'] = df3['ids'].apply(lambda x: '%d:%s' % (idx, str(x)))
+        df3['fidx'] = idx
+
+        df3 = df3[df3['qpreds'] <= 10]
+
+
+        qval_ok = 0
+        for qval_cur in range(10):
+            if qval_cur != 10:
+                df1ut = df3[df3['qpreds'] == qval_cur]
+                decoy_ratio = df1ut['decoy'].sum() / len(df1ut)
+                d_tmp[(idx, qval_cur)] = decoy_ratio
+                # print(filen, qval_cur, decoy_ratio)
+
         if df1 is None:
             df1 = df3
             if args['prots_full']:
                 df2 = pd.read_csv(args['prots_full'], sep='\t')
             else:
                 try:
                     df2 = pd.read_csv(filen.replace('_PFMs_ML.tsv', '_proteins_full.tsv'), sep='\t')
                 except:
                     logging.critical('Proteins_full file is missing!')
                     break
 
         else:
             df1 = pd.concat([df1, df3], ignore_index=True)
 
+    d_tmp = [z[0] for z in sorted(d_tmp.items(), key=lambda x: x[1])]
+    qdict = {}
+    for idx, val in enumerate(d_tmp):
+        qdict[val] = int(idx / len(args['file']))
+    df1['qpreds'] = df1.apply(lambda x: qdict[(x['fidx'], x['qpreds'])], axis=1)
+
+
     pept_prot = defaultdict(set)
     for seq, prots in df1[['seqs', 'proteins']].values:
         for dbname in prots.split(';'):
             pept_prot[seq].add(dbname)
 
     protsN = dict()
     for dbname, theorpept in df2[['dbname', 'theoretical peptides']].values:
@@ -66,16 +91,29 @@
 
     resdict['qpreds'] = df1['qpreds'].values
     resdict['preds'] = df1['preds'].values
     resdict['seqs'] = df1['peptide'].values
     resdict['ids'] = df1['ids'].values
     resdict['iorig'] = df1['iorig'].values
 
+    # mass_diff = resdict['qpreds']
+    # rt_diff = resdict['qpreds']
+
+    base_out_name = args['out']
+
+    e_ind = resdict['qpreds'] <= 9
+    resdict = filter_results(resdict, e_ind)
+
     mass_diff = resdict['qpreds']
     rt_diff = resdict['qpreds']
 
-    base_out_name = args['out']
-    final_iteration(resdict, mass_diff, rt_diff, pept_prot, protsN, base_out_name, prefix, isdecoy, isdecoy_key, escore, fdr, args['nproc'])
+    if args['pp']:
+        df4 = pd.read_table(args['pp'])
+        prots_spc_basic2 = df4.set_index('dbname')['score'].to_dict()
+    else:
+        prots_spc_basic2 = False
+    
+    final_iteration(resdict, mass_diff, rt_diff, pept_prot, protsN, base_out_name, prefix, isdecoy, isdecoy_key, escore, fdr, args['nproc'], prots_spc_basic2=prots_spc_basic2)
 
 
 if __name__ == '__main__':
     run()
```

## ms1searchpy/directms1quant.py

```diff
@@ -1,13 +1,14 @@
 from __future__ import division
 import argparse
 import pandas as pd
 import numpy as np
 from scipy.stats import binom, ttest_ind
 import logging
+from pyteomics import fasta
 
 def calc_sf_all(v, n, p):
     sf_values = -np.log10(binom.sf(v-1, n, p))
     sf_values[v <= 1] = 0
     sf_values[np.isinf(sf_values)] = 20
     sf_values[n == 0] = 0
     return sf_values
@@ -31,14 +32,15 @@
     parser.add_argument('-fold_change', help='FC threshold standard deviations', default=3.0, type=float)
     parser.add_argument('-fold_change_abs', help='Use absolute log2 scale FC threshold instead of standard deviations', action='store_true')
     parser.add_argument('-qval', help='qvalue threshold', default=0.05, type=float)
     parser.add_argument('-intensity_norm', help='Intensity normalization: 0-none, 1-median', default=1, type=int)
     parser.add_argument('-all_proteins', help='use all proteins instead of FDR controlled', action='store_true')
     parser.add_argument('-all_pfms', help='use all PFMs instead of ML controlled', action='store_true')
     parser.add_argument('-output_peptides', help='Add output table with peptides', action='store_true')
+    parser.add_argument('-d', '-db', help='path to uniprot fasta file for gene annotation')
     args = vars(parser.parse_args())
     logging.basicConfig(format='%(levelname)9s: %(asctime)s %(message)s',
             datefmt='[%H:%M:%S]', level=logging.INFO)
     logger = logging.getLogger(__name__)
 
 
     replace_label = '_proteins_full.tsv'
@@ -290,14 +292,15 @@
 
     v_arr = np.array(list(prots_up.get(k, 0) for k in names_arr))
     n_arr = np.array(list(protsN.get(k, 0) for k in names_arr))
 
     all_pvals = calc_sf_all(v_arr, n_arr, p_up)
 
     total_set = set()
+    total_set_genes = set()
 
     FC_up_dict_basic = df_final.groupby('proteins')['FC'].median().to_dict()
     FC_up_dict_raw_basic = df_final.groupby('proteins')['FC_raw'].median().to_dict()
 
     df_final = df_final[df_final['up']>0]
 
     df_final['bestmissing'] = df_final.groupby('proteins')['nummissing'].transform('min')
@@ -343,18 +346,47 @@
 
     df_out.to_csv(path_or_buf=args['out']+'_quant_full.tsv', sep='\t', index=False)
 
     df_out_f = df_out[(df_out['BH_pass']) & (df_out['FC_pass'])]
 
     df_out_f.to_csv(path_or_buf=args['out']+'.tsv', sep='\t', index=False)
 
-    total_set.update([z.split('|')[1] for z in set(df_out_f['dbname'])])
+    genes_map = {}
+    if args['d']:
+        for prot, protseq in fasta.read(args['d']):
+            try:
+                prot_name = prot.split('|')[1]
+            except:
+                prot_name = prot
+            try:
+                gene_name = prot.split('GN=')[1].split(' ')[0]
+            except:
+                gene_name = prot
+            genes_map[prot_name] = gene_name
+
+
+    for z in set(df_out_f['dbname']):
+        try:
+            prot_name = z.split('|')[1]
+        except:
+            prot_name = z
+
+        gene_name = genes_map.get(prot_name, prot_name)
+
+        total_set.add(prot_name)
+        total_set_genes.add(gene_name)
 
     logger.info('Total number of significantly changed proteins: %d', len(total_set))
+    logger.info('Total number of significantly changed genes: %d', len(total_set_genes))
 
     f1 = open(args['out'] + '_proteins_for_stringdb.txt', 'w')
     for z in total_set:
         f1.write(z + '\n')
     f1.close()
 
+    f1 = open(args['out'] + '_genes_for_stringdb.txt', 'w')
+    for z in total_set_genes:
+        f1.write(z + '\n')
+    f1.close()
+
 if __name__ == '__main__':
     run()
```

## ms1searchpy/main.py

```diff
@@ -1,15 +1,15 @@
 import os
 import numpy as np
 from scipy.stats import scoreatpercentile
 from scipy.optimize import curve_fit
 from scipy import exp
 import operator
 from copy import copy, deepcopy
-from collections import defaultdict, deque, Counter
+from collections import defaultdict
 from pyteomics import parser, mass, auxiliary as aux, achrom
 try:
     from pyteomics import cmass
 except ImportError:
     cmass = mass
 import subprocess
 import tempfile
@@ -35,16 +35,14 @@
 
 import logging
 import numpy
 import pandas
 from sklearn import metrics
 import csv
 import ast
-import itertools as it
-import re
 
 logger = logging.getLogger(__name__)
 
 def worker_RT(qin, qout, shift, step, RC=False, elude_path=False, ns=False, nr=False, win_sys=False):
     pepdict = dict()
     if elude_path:
         outtrain_name = os.path.join(tempfile.gettempdir(), os.urandom(24).hex())
@@ -220,16 +218,18 @@
                     prots_spc_basic[k] = all_pvals[idx]
 
                 for k, v in prots_spc_basic.items():
                     if k not in prots_spc_final:
                         prots_spc_final[k] = v
 
                 break
-
-            prot_fdr = aux.fdr(prots_spc_final.items(), is_decoy=isdecoy)
+            try:
+                prot_fdr = aux.fdr(prots_spc_final.items(), is_decoy=isdecoy)
+            except:
+                prot_fdr = 100.0
             if prot_fdr >= 12.5 * fdr:
 
                 v_arr = np.array([prots_spc[k] for k in names_arr])
                 all_pvals = utils.calc_sf_all(v_arr, n_arr, p)
                 for idx, k in enumerate(names_arr):
                     prots_spc_basic[k] = all_pvals[idx]
 
@@ -387,143 +387,144 @@
     return mass_shift, mass_sigma, pcov[0][0]
 
 def process_file(args):
     utils.seen_target.clear()
     utils.seen_decoy.clear()
     args = utils.prepare_decoy_db(args)
     for filename in args['files']:
+
+        # Temporary for pyteomics <= Version 4.5.5 bug
+        from pyteomics import mass
+        if 'H-' in mass.std_aa_mass:
+            del mass.std_aa_mass['H-']
+        if '-OH' in mass.std_aa_mass:
+            del mass.std_aa_mass['-OH']
+
         try:
             args['file'] = filename
             process_peptides(deepcopy(args))
         except Exception as e:
             logger.error(e)
             logger.error('Search is failed for file: %s', filename)
     return 1
 
 
-def peptide_processor(peptide, **kwargs):
-    seqm = peptide
-    results = []
-    m = cmass.fast_mass(seqm, aa_mass=kwargs['aa_mass']) + kwargs['aa_mass'].get('Nterm', 0) + kwargs['aa_mass'].get('Cterm', 0)
-    acc_l = kwargs['acc_l']
-    acc_r = kwargs['acc_r']
-    dm_l = acc_l * m / 1.0e6
-    if acc_r == acc_l:
-        dm_r = dm_l
-    else:
-        dm_r = acc_r * m / 1.0e6
-    start = nmasses.searchsorted(m - dm_l)
-    end = nmasses.searchsorted(m + dm_r)
-    for i in range(start, end):
-        massdiff = (m - nmasses[i]) / m * 1e6
-        mods = 0
-        results.append((seqm, massdiff, mods, i))
-    return results
-
 
 def prepare_peptide_processor(fname, args):
     global nmasses
     global rts
     global charges
     global ids
     global Is
+    global Isums
     global Scans
     global Isotopes
     global mzraw
     global avraw
     global imraw
 
     min_ch = args['cmin']
     max_ch = args['cmax']
 
     min_isotopes = args['i']
     min_scans = args['sc']
 
     logger.info('Reading file %s', fname)
 
-    df_features = utils.iterate_spectra(fname, min_ch, max_ch, min_isotopes, min_scans)
+    df_features = utils.iterate_spectra(fname, min_ch, max_ch, min_isotopes, min_scans, args['nproc'], args['check_unique'])
 
     # Sort by neutral mass
     df_features = df_features.sort_values(by='massCalib')
 
     nmasses = df_features['massCalib'].values
     rts = df_features['rtApex'].values
     charges = df_features['charge'].values
     ids = df_features['id'].values
     Is = df_features['intensityApex'].values
+    if 'intensitySum' in df_features.columns:
+        Isums = df_features['intensitySum'].values
+    else:
+        Isums = df_features['intensityApex'].values
+        logger.info('intensitySum column is missing in peptide features. Using intensityApex instead')
+
     Scans = df_features['nScans'].values
     Isotopes = df_features['nIsotopes'].values
     mzraw = df_features['mz'].values
     avraw = np.zeros(len(df_features))
     if len(set(df_features['FAIMS'])) > 1:
         imraw = df_features['FAIMS'].values
     else:
         imraw = df_features['ion_mobility'].values
 
     logger.info('Number of peptide isotopic clusters passed filters: %d', len(nmasses))
 
-    fmods = args['fmods']
-    aa_mass = deepcopy(mass.std_aa_mass)
-    if fmods:
-        for mod in fmods.split(','):
-            m, aa = mod.split('@')
-            if aa == '[':
-                aa_mass['Nterm'] = float(m)
-            elif aa == ']':
-                aa_mass['Cterm'] = float(m)
-            else:
-                aa_mass[aa] += float(m)
+    aa_mass, aa_to_psi = utils.get_aa_mass_with_fixed_mods(args['fmods'], args['fmods_legend'])
 
     acc_l = args['ptol']
     acc_r = args['ptol']
 
-    return {'aa_mass': aa_mass, 'acc_l': acc_l, 'acc_r': acc_r, 'args': args}, df_features
+    return {'aa_mass': aa_mass, 'aa_to_psi': aa_to_psi, 'acc_l': acc_l, 'acc_r': acc_r, 'args': args}, df_features
+
+def get_resdict(it, **kwargs):
 
+    resdict = {
+        'seqs': [],
+        'md': [],
+        'mods': [],
+        'iorig': [],
+    }
+
+    for seqm in it:
+        results = []
+        m = cmass.fast_mass(seqm, aa_mass=kwargs['aa_mass']) + kwargs['aa_mass'].get('Nterm', 0) + kwargs['aa_mass'].get('Cterm', 0)
+        acc_l = kwargs['acc_l']
+        acc_r = kwargs['acc_r']
+        dm_l = acc_l * m / 1.0e6
+        if acc_r == acc_l:
+            dm_r = dm_l
+        else:
+            dm_r = acc_r * m / 1.0e6
+        start = nmasses.searchsorted(m - dm_l)
+        end = nmasses.searchsorted(m + dm_r, side='right')
+        for i in range(start, end):
+            massdiff = (m - nmasses[i]) / m * 1e6
+            mods = 0
+
+            resdict['seqs'].append(seqm)
+            resdict['md'].append(massdiff)
+            resdict['mods'].append(mods)
+            resdict['iorig'].append(i)
+
+
+    for k in list(resdict.keys()):
+        resdict[k] = np.array(resdict[k])
 
-def peptide_processor_iter_isoforms(peptide, **kwargs):
-    out = []
-    out.append(peptide_processor(peptide, **kwargs))
-    return out
-
-def get_results(ms1results):
-    resdict = dict()
-    labels = [
-        'seqs',
-        'md',
-        'mods',
-        'iorig',
-        # 'rt',
-        # 'ids',
-        # 'Is',
-        # 'Scans',
-        # 'Isotopes',
-        # 'mzraw',
-        # 'av',
-        # 'ch',
-        # 'im',
-    ]
-    for label, val in zip(labels, zip(*ms1results)):
-        resdict[label] = np.array(val)
     return resdict
 
+
+
 def filter_results(resultdict, idx):
     tmp = dict()
     for label in resultdict:
         tmp[label] = resultdict[label][idx]
     return tmp
 
 def process_peptides(args):
+
+    logger.info('Starting search...')
+
     fname_orig = args['file']
     if fname_orig.lower().endswith('mzml'):
         fname = os.path.splitext(fname_orig)[0] + '.features.tsv'
     else:
         fname = fname_orig
 
     fdr = args['fdr'] / 100
     min_isotopes_calibration = args['ci']
+    min_scans_calibration = args['csc']
     try:
         outpath = args['outpath']
     except:
         outpath = False
 
 
     if outpath:
@@ -562,46 +563,47 @@
 
     if calib_path and args['ts']:
         args['ts'] = 0
         logger.info('Two-stage RT prediction does not work with list of MS/MS identified peptides...')
 
     args['enzyme'] = utils.get_enzyme(args['e'])
 
-    ms1results = []
-    peps = utils.peptide_gen(args)
+
+    prefix = args['prefix']
+    protsN, pept_prot, ml_correction = utils.get_prot_pept_map(args)
+    # peps = pept_prot.keys()
+
     kwargs, df_features = prepare_peptide_processor(fname_orig, args)
-    func = peptide_processor_iter_isoforms
     logger.info('Running the search ...')
-    for y in utils.multimap(1, func, peps, **kwargs):
-        for result in y:
-            if len(result):
-                ms1results.extend(result)
 
-    prefix = args['prefix']
-    protsN, pept_prot = utils.get_prot_pept_map(args)
-    # protsN, pept_prot, protsNc = utils.get_prot_pept_map(args)
+    resdict = get_resdict(pept_prot, **kwargs)
 
-    resdict = get_results(ms1results)
-    del ms1results
+    aa_to_psi = kwargs['aa_to_psi']
 
-    resdict['mc'] = np.array([parser.num_sites(z, args['enzyme']) for z in resdict['seqs']])
+    if args['mc'] > 0:
+        resdict['mc'] = np.array([parser.num_sites(z, args['enzyme']) for z in resdict['seqs']])
 
     isdecoy = lambda x: x[0].startswith(prefix)
     isdecoy_key = lambda x: x.startswith(prefix)
     escore = lambda x: -x[1]
 
 
     e_ind = np.array([Isotopes[iorig] for iorig in resdict['iorig']]) >= min_isotopes_calibration
-    # e_ind = resdict['Isotopes'] >= min_isotopes_calibration
-    # e_ind = resdict['Isotopes'] >= 1
     resdict2 = filter_results(resdict, e_ind)
 
-    e_ind = resdict2['mc'] == 0
+    e_ind = np.array([Scans[iorig] for iorig in resdict2['iorig']]) >= min_scans_calibration
+    resdict2 = filter_results(resdict2, e_ind)
+
+    e_ind = resdict2['mods'] == 0
     resdict2 = filter_results(resdict2, e_ind)
 
+    if args['mc'] > 0:
+        e_ind = resdict2['mc'] == 0
+        resdict2 = filter_results(resdict2, e_ind)
+
     p1 = set(resdict2['seqs'])
 
     if len(p1):
         prots_spc2 = defaultdict(set)
         for pep, proteins in pept_prot.items():
             if pep in p1:
                 for protein in proteins:
@@ -642,67 +644,70 @@
                                     full_output=True)
 
         identified_proteins = 0
 
         for x in filtered_prots:
             identified_proteins += 1
         logger.info('Stage 0 search: identified proteins = %d', identified_proteins)
+        if identified_proteins <= 25:
+            logger.info('Low number of identified proteins, using first 25 top scored proteins for calibration...')
+            filtered_prots = sorted(prots_spc.items(), key=lambda x: -x[1])[:25]
 
         logger.info('Running mass recalibration...')
 
         df1 = pd.DataFrame()
         df1['mass diff'] = resdict['md']
-        df1['mc'] = resdict['mc']
+        df1['mc'] = (resdict['mc'] if args['mc'] > 0 else 0)
         df1['iorig'] = resdict['iorig']
         df1['seqs'] = resdict['seqs']
+
+        df1['mods'] = resdict['mods']
+        
         # df1['orig_md'] = true_md
 
 
         true_seqs = set()
         true_prots = set(x[0] for x in filtered_prots)
         for pep, proteins in pept_prot.items():
             if any(protein in true_prots for protein in proteins):
                 true_seqs.add(pep)
 
-
         df1['top_peps'] = (df1['mc'] == 0) & (df1['seqs'].apply(lambda x: x in true_seqs))
 
-        df1['mz'] = df1['iorig'].apply(lambda x: mzraw[x])
-        df1['nIsotopes'] = df1['iorig'].apply(lambda x: Isotopes[x])
-        df1['RT'] = df1['iorig'].apply(lambda x: rts[x])
-        df1['Intensity'] = df1['iorig'].apply(lambda x: Is[x])
-
         mass_calib_arg = args['mcalib']
 
         assert mass_calib_arg in [0, 1, 2]
 
         if mass_calib_arg:
+            df1['RT'] = rts[df1['iorig'].values]#df1['iorig'].apply(lambda x: rts[x])
+
             if mass_calib_arg == 2:
-                df1['im'] = df1['iorig'].apply(lambda x: imraw[x])
+                df1['im'] = imraw[df1['iorig'].values]#df1['iorig'].apply(lambda x: imraw[x])
             elif mass_calib_arg == 1:
                 df1['im'] = 0
 
-            im_set = set(df1['im'])
-            if len(im_set) <= 10:
+            im_set = set(df1['im'].unique())
+            if len(im_set) <= 5:
                 df1['im_qcut'] = df1['im']
                 for im_value in im_set:
                     idx1 = df1['im'] == im_value
-                    df1.loc[idx1, 'qpreds'] = str(im_value) + pd.qcut(df1.loc[idx1, 'RT'], 10, labels=range(10)).astype(str)
+                    df1.loc[idx1, 'qpreds'] = str(im_value) + pd.qcut(df1.loc[idx1, 'RT'], 5, labels=range(5)).astype(str)
             else:
-                df1['im_qcut'] = pd.qcut(df1['im'], 10, labels=range(10)).astype(str)
-                for im_value in set(df1['im_qcut']):
+                df1['im_qcut'] = pd.qcut(df1['im'], 5, labels=range(5)).astype(str)
+                im_set = set(df1['im_qcut'].unique())
+                for im_value in set(df1['im_qcut'].unique()):
                     idx1 = df1['im_qcut'] == im_value
-                    df1.loc[idx1, 'qpreds'] = str(im_value) + pd.qcut(df1.loc[idx1, 'RT'], 10, labels=range(10)).astype(str)
+                    df1.loc[idx1, 'qpreds'] = str(im_value) + pd.qcut(df1.loc[idx1, 'RT'], 5, labels=range(5)).astype(str)
 
             # df1['qpreds'] = pd.qcut(df1['RT'], 10, labels=range(10))#.astype(int)
 
             cor_dict = df1[df1['top_peps']].groupby('qpreds')['mass diff'].median().to_dict()
 
-            rt_q_list = list(range(10))
-            for im_value in set(df1['im_qcut']):
+            rt_q_list = list(range(5))
+            for im_value in im_set:
                 for rt_q in rt_q_list:
                     lbl_cur = str(im_value) + str(rt_q)
                     if lbl_cur not in cor_dict:
 
                         best_diff = 1e6
                         best_val = 0
                         for rt_q2 in rt_q_list:
@@ -718,33 +723,34 @@
 
             df1['mass diff q median'] = df1['qpreds'].apply(lambda x: cor_dict[x])
             df1['mass diff corrected'] = df1['mass diff'] - df1['mass diff q median']
 
         else:
             df1['qpreds'] = 0
             df1['mass diff q median'] = 0
-            df1['mass diff corrected'] = df1['mass diff'] - df1['mass diff q median']
+            df1['mass diff corrected'] = df1['mass diff']
 
 
 
 
         mass_left = args['ptol']
         mass_right = args['ptol']
 
         try:
             mass_shift_cor, mass_sigma_cor, covvalue_cor = calibrate_mass(0.001, mass_left, mass_right, df1[df1['top_peps']]['mass diff corrected'])
         except:
             mass_shift_cor, mass_sigma_cor, covvalue_cor = calibrate_mass(0.01, mass_left, mass_right, df1[df1['top_peps']]['mass diff corrected'])
 
-        try:
-            mass_shift, mass_sigma, covvalue = calibrate_mass(0.001, mass_left, mass_right, df1[df1['top_peps']]['mass diff'])
-        except:
-            mass_shift, mass_sigma, covvalue = calibrate_mass(0.01, mass_left, mass_right, df1[df1['top_peps']]['mass diff'])
-
         if mass_calib_arg:
+
+            try:
+                mass_shift, mass_sigma, covvalue = calibrate_mass(0.001, mass_left, mass_right, df1[df1['top_peps']]['mass diff'])
+            except:
+                mass_shift, mass_sigma, covvalue = calibrate_mass(0.01, mass_left, mass_right, df1[df1['top_peps']]['mass diff'])
+
             logger.info('Uncalibrated mass shift: %.3f ppm', mass_shift)
             logger.info('Uncalibrated mass sigma: %.3f ppm', mass_sigma)
 
         logger.info('Estimated mass shift: %.3f ppm', mass_shift_cor)
         logger.info('Estimated mass sigma: %.3f ppm', mass_sigma_cor)
 
         out_log.write('Estimated mass shift: %s ppm\n' % (mass_shift_cor, ))
@@ -756,20 +762,30 @@
         mass_sigma = mass_sigma_cor
 
         e_all = abs(resdict['md'] - mass_shift) / (mass_sigma)
         r = 3.0
         e_ind = e_all <= r
         resdict = filter_results(resdict, e_ind)
 
+
         e_ind = np.array([Isotopes[iorig] for iorig in resdict['iorig']]) >= min_isotopes_calibration
         resdict2 = filter_results(resdict, e_ind)
+        
 
-        e_ind = resdict2['mc'] == 0
+        e_ind = np.array([Scans[iorig] for iorig in resdict2['iorig']]) >= min_scans_calibration
         resdict2 = filter_results(resdict2, e_ind)
 
+        e_ind = resdict2['mods'] == 0
+        resdict2 = filter_results(resdict2, e_ind)
+
+
+        if args['mc'] > 0:
+            e_ind = resdict2['mc'] == 0
+            resdict2 = filter_results(resdict2, e_ind)
+
         p1 = set(resdict2['seqs'])
 
         prots_spc2 = defaultdict(set)
         for pep, proteins in pept_prot.items():
             if pep in p1:
                 for protein in proteins:
                     prots_spc2[protein].add(pep)
@@ -809,26 +825,33 @@
                                     full_output=True)
 
         identified_proteins = 0
 
         for x in filtered_prots:
             identified_proteins += 1
         logger.info('Stage 1 search: identified proteins = %d', identified_proteins)
+        if identified_proteins <= 25:
+            logger.info('Low number of identified proteins, using first 25 top scored proteins for calibration...')
+            filtered_prots = sorted(prots_spc.items(), key=lambda x: -x[1])[:25]
 
 
 
         logger.info('Running RT prediction...')
 
 
         e_ind = np.array([Isotopes[iorig] for iorig in resdict['iorig']]) >= 1
         resdict2 = filter_results(resdict, e_ind)
 
-        e_ind = resdict2['mc'] == 0
+        e_ind = resdict2['mods'] == 0
         resdict2 = filter_results(resdict2, e_ind)
 
+        if args['mc'] > 0:
+            e_ind = resdict2['mc'] == 0
+            resdict2 = filter_results(resdict2, e_ind)
+
 
         true_seqs = []
         true_rt = []
         true_isotopes = []
         true_prots = set(x[0] for x in filtered_prots)#[:5])
         for pep, proteins in pept_prot.items():
             if any(protein in true_prots for protein in proteins):
@@ -892,34 +915,36 @@
             nr = true_rt
 
             ns2 = true_seqs2
             nr2 = true_rt2
 
             outtrain.write('seq,modifications,tr\n')
             for seq, RT in zip(ns2, nr2):
-                mods_tmp = '|'.join([str(idx+1)+'|Carbamidomethyl' for idx, aa in enumerate(seq) if aa == 'C'])
+                # mods_tmp = '|'.join([str(idx+1)+'|Carbamidomethyl' for idx, aa in enumerate(seq) if aa == 'C'])
+                mods_tmp = utils.mods_for_deepLC(seq, aa_to_psi)
                 outtrain.write(seq + ',' + str(mods_tmp) + ',' + str(RT) + '\n')
             outtrain.close()
 
+            train_RT = []
+            train_seq = []
             outcalib.write('seq,modifications,tr\n')
             for seq, RT in zip(ns, nr):
-                mods_tmp = '|'.join([str(idx+1)+'|Carbamidomethyl' for idx, aa in enumerate(seq) if aa == 'C'])
+                # mods_tmp = '|'.join([str(idx+1)+'|Carbamidomethyl' for idx, aa in enumerate(seq) if aa == 'C'])
+                mods_tmp = utils.mods_for_deepLC(seq, aa_to_psi)
                 outcalib.write(seq + ',' + str(mods_tmp) + ',' + str(RT) + '\n')
+                train_seq.append(seq)
+                train_RT.append(float(RT))
             outcalib.close()
 
             subprocess.call([deeplc_path, '--file_pred', outcalib_name, '--file_cal', outtrain_name, '--file_pred_out', outres_name] + deeplc_extra_args, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
 
             pepdict = dict()
-            train_RT = []
-            train_seq = []
-            for x in open(outres_name).readlines()[1:]:
-                _, seq, _, RTexp, RT = x.strip().split(',')
+            for seq, x in zip(train_seq, open(outres_name).readlines()[1:]):
+                _, RT = x.strip().split(',')
                 pepdict[seq] = float(RT)
-                train_seq.append(seq)
-                train_RT.append(float(RTexp))
 
 
             train_RT = np.array(train_RT)
             RT_pred = np.array([pepdict[s] for s in train_seq])
 
             rt_diff_tmp = RT_pred - train_RT
             RT_left = -min(rt_diff_tmp)
@@ -1018,37 +1043,49 @@
 
         if deeplc_path:
 
 
 
             outtrain_name = os.path.join(tempfile.gettempdir(), os.urandom(24).hex())
             outtrain = open(outtrain_name, 'w')
+
+            outcal_name = os.path.join(tempfile.gettempdir(), os.urandom(24).hex())
+            outcal = open(outcal_name, 'w')
+
             outres_name = os.path.join(tempfile.gettempdir(), os.urandom(24).hex())
 
             ll = len(ns)
             ns = ns[:ll]
             nr = nr[:ll]
 
+            outcal.write('seq,modifications,tr\n')
+            for seq, RT in zip(ns[:int(ll/2)], nr[:int(ll/2)]):
+                # mods_tmp = '|'.join([str(idx+1)+'|Carbamidomethyl' for idx, aa in enumerate(seq) if aa == 'C'])
+                mods_tmp = utils.mods_for_deepLC(seq, aa_to_psi)
+                outcal.write(seq + ',' + str(mods_tmp) + ',' + str(RT) + '\n')
+            outcal.close()
+
+            train_RT = []
+            train_seq = []
             outtrain.write('seq,modifications,tr\n')
-            for seq, RT in zip(ns, nr):
-                mods_tmp = '|'.join([str(idx+1)+'|Carbamidomethyl' for idx, aa in enumerate(seq) if aa == 'C'])
+            for seq, RT in zip(ns[int(ll/2):], nr[int(ll/2):]):
+                # mods_tmp = '|'.join([str(idx+1)+'|Carbamidomethyl' for idx, aa in enumerate(seq) if aa == 'C'])
+                mods_tmp = utils.mods_for_deepLC(seq, aa_to_psi)
                 outtrain.write(seq + ',' + str(mods_tmp) + ',' + str(RT) + '\n')
+                train_seq.append(seq)
+                train_RT.append(float(RT))
             outtrain.close()
 
             # [:int(len(ns)/2)]
 
-            subprocess.call([deeplc_path, '--file_pred', outtrain_name, '--file_cal', outtrain_name, '--file_pred_out', outres_name] + deeplc_extra_args, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
+            subprocess.call([deeplc_path, '--file_pred', outtrain_name, '--file_cal', outcal_name, '--file_pred_out', outres_name] + deeplc_extra_args, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
             pepdict = dict()
-            train_RT = []
-            train_seq = []
-            for x in open(outres_name).readlines()[1:]:
-                _, seq, _, RTexp, RT = x.strip().split(',')
+            for seq, x in zip(train_seq, open(outres_name).readlines()[1:]):
+                _, RT = x.strip().split(',')
                 pepdict[seq] = float(RT)
-                train_seq.append(seq)
-                train_RT.append(float(RTexp))
 
 
             train_RT = np.array(train_RT)
             RT_pred = np.array([pepdict[s] for s in train_seq])
 
             rt_diff_tmp = RT_pred - train_RT
             RT_left = -min(rt_diff_tmp)
@@ -1139,32 +1176,42 @@
 
         outtrain_name = os.path.join(tempfile.gettempdir(), os.urandom(24).hex())
         outtrain = open(outtrain_name, 'w')
         outres_name = os.path.join(tempfile.gettempdir(), os.urandom(24).hex())
 
         outtrain.write('seq,modifications,tr\n')
         for seq, RT in zip(ns, nr):
-            mods_tmp = '|'.join([str(idx+1)+'|Carbamidomethyl' for idx, aa in enumerate(seq) if aa == 'C'])
+            # mods_tmp = '|'.join([str(idx+1)+'|Carbamidomethyl' for idx, aa in enumerate(seq) if aa == 'C'])
+            mods_tmp = utils.mods_for_deepLC(seq, aa_to_psi)
             outtrain.write(seq + ',' + str(mods_tmp) + ',' + str(RT) + '\n')
         outtrain.close()
 
+        if args['save_calib']:
+            with open(base_out_name + '_calib.tsv', 'w') as output:
+                output.write('peptide\tRT exp\n')
+                for seq, RT in zip(ns, nr):
+                    output.write('%s\t%s\n' % (seq, str(RT)))
+
+
 
         outtest_name = os.path.join(tempfile.gettempdir(), os.urandom(24).hex())
         outtest = open(outtest_name, 'w')
 
-
+        test_seqs = []
         outtest.write('seq,modifications\n')
         for seq in p1:
-            mods_tmp = '|'.join([str(idx+1)+'|Carbamidomethyl' for idx, aa in enumerate(seq) if aa == 'C'])
+            # mods_tmp = '|'.join([str(idx+1)+'|Carbamidomethyl' for idx, aa in enumerate(seq) if aa == 'C'])
+            mods_tmp = utils.mods_for_deepLC(seq, aa_to_psi)
             outtest.write(seq + ',' + str(mods_tmp) + '\n')
+            test_seqs.append(seq)
         outtest.close()
 
         subprocess.call([deeplc_path, '--file_pred', outtest_name, '--file_cal', outtrain_name, '--file_pred_out', outres_name] + deeplc_extra_args, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
-        for x in open(outres_name).readlines()[1:]:
-            _, seq, _, RT = x.strip().split(',')
+        for seq, x in zip(test_seqs, open(outres_name).readlines()[1:]):
+            _, RT = x.strip().split(',')
             pepdict[seq] = float(RT)
 
     else:
 
         if n == 1 or os.name == 'nt':
             qin = list(p1)
             qout = []
@@ -1189,54 +1236,56 @@
                 for item in iter(qout.get, None):
                     for k, v in item.items():
                         pepdict[k] = v
 
             for p in procs:
                 p.join()
 
-
     rt_pred = np.array([pepdict[s] for s in resdict['seqs']])
-    rt_diff = np.array([rts[iorig] for iorig in resdict['iorig']]) - rt_pred
+    # rt_diff = np.array([rts[iorig] for iorig in resdict['iorig']]) - rt_pred
+    rt_diff = np.array([rts[iorig] for iorig in resdict['iorig']]) - rt_pred - XRT_shift
     # rt_diff = resdict['rt'] - rt_pred
     e_all = (rt_diff) ** 2 / (RT_sigma ** 2)
     r = 9.0
     e_ind = e_all <= r
     resdict = filter_results(resdict, e_ind)
     rt_diff = rt_diff[e_ind]
     rt_pred = rt_pred[e_ind]
 
 
+
     with open(base_out_name + '_protsN.tsv', 'w') as output:
         output.write('dbname\ttheor peptides\n')
         for k, v in protsN.items():
             output.write('\t'.join((k, str(v))) + '\n')
 
     with open(base_out_name + '_PFMs.tsv', 'w') as output:
-        output.write('sequence\tmass diff\tRT diff\tpeak_id\tIntensity\tnScans\tnIsotopes\tproteins\tm/z\tRT\taveragineCorr\tcharge\tion_mobility\n')
+        output.write('sequence\tmass diff\tRT diff\tpeak_id\tIntensity\tIntensitySum\tnScans\tnIsotopes\tproteins\tm/z\tRT\taveragineCorr\tcharge\tion_mobility\n')
         # for seq, md, rtd, peak_id, I, nScans, nIsotopes, mzr, rtr, av, ch, im in zip(resdict['seqs'], resdict['md'], rt_diff, resdict['ids'], resdict['Is'], resdict['Scans'], resdict['Isotopes'], resdict['mzraw'], resdict['rt'], resdict['av'], resdict['ch'], resdict['im']):
         for seq, md, rtd, iorig in zip(resdict['seqs'], resdict['md'], rt_diff, resdict['iorig']):
             peak_id = ids[iorig]
             I = Is[iorig]
+            Isum = Isums[iorig]
             nScans = Scans[iorig]
             nIsotopes = Isotopes[iorig]
             mzr = mzraw[iorig]
             rtr = rts[iorig]
             av = avraw[iorig]
             ch = charges[iorig]
             im = imraw[iorig]
-            output.write('\t'.join((seq, str(md), str(rtd), str(peak_id), str(I), str(nScans), str(nIsotopes), ';'.join(pept_prot[seq]), str(mzr), str(rtr), str(av), str(ch), str(im))) + '\n')
+            output.write('\t'.join((seq, str(md), str(rtd), str(peak_id), str(I), str(Isum), str(nScans), str(nIsotopes), ';'.join(pept_prot[seq]), str(mzr), str(rtr), str(av), str(ch), str(im))) + '\n')
 
     # e_ind = resdict['mc'] == 0
     # resdict = filter_results(resdict, e_ind)
     # rt_diff = rt_diff[e_ind]
     # rt_pred = rt_pred[e_ind]
 
     mass_diff = (resdict['md'] - mass_shift) / (mass_sigma)
 
-    rt_diff = (np.array([rts[iorig] for iorig in resdict['iorig']]) - rt_pred) / RT_sigma
+    rt_diff = (np.array([rts[iorig] for iorig in resdict['iorig']]) - rt_pred - XRT_shift) / RT_sigma
     # rt_diff = (resdict['rt'] - rt_pred) / RT_sigma
 
     prefix = 'DECOY_'
     isdecoy = lambda x: x[0].startswith(prefix)
     isdecoy_key = lambda x: x.startswith(prefix)
     escore = lambda x: -x[1]
 
@@ -1281,55 +1330,71 @@
         banned_features = {
             'iorig',
             'ids',
             'seqs',
             'decoy',
             'preds',
             'av',
+            'Is',
             # 'Scans',
             'proteins',
             'peptide',
             'md',
             'qpreds',
             'decoy2',
+            'top_25_targets',
+            'G',
         }
 
         for feature in feature_columns:
             if feature in banned_features:
                 columns_to_remove.append(feature)
         feature_columns = feature_columns.drop(columns_to_remove)
         return feature_columns
 
     def objective_pfms(df, hyperparameters, iteration, threshold=0):
         """Objective function for grid and random search. Returns
         the cross validation score from a set of hyperparameters."""
 
         all_res = []
 
-        groups = df['peptide']
-        ix = df.index.values
-        unique = np.unique(groups)
-        np.random.RandomState(SEED).shuffle(unique)
-        for split in np.array_split(unique, 3):
-            mask = groups.isin(split)
-            train, test = ix[~mask], ix[mask]
-            train_df = df.iloc[train]
-            test_df = df.iloc[test]
+
+        # groups = df['peptide']
+        # ix = df.index.values
+        # unique = np.unique(groups)
+        # np.random.RandomState(SEED).shuffle(unique)
+        # for split in np.array_split(unique, 3):
+        #     mask = groups.isin(split)
+        #     train, test = ix[~mask], ix[mask]
+        #     train_df = df.iloc[train]
+        #     test_df = df.iloc[test]
+
+        for group_val in range(3):
+            
+            mask = df['G'] == group_val
+            test_df = df[mask]
+            test_ids = set(test_df['ids'])
+    #         train_df = df[~mask]
+    #         train_df = train_df[train_df['ids'].apply(lambda x: x not in test_ids)]
+            train_df = df[(~mask) & (df['ids'].apply(lambda x: x not in test_ids))]
+
+
 
             feature_columns = get_features_pfms(df)
             ### 1
             # model = get_cat_model_pfms(df[~df['decoy2']], hyperparameters, feature_columns, train_df[~train_df['decoy2']], test_df[~test_df['decoy2']])
             # all_iters.append(model.best_iteration)
             model = get_cat_model_final_pfms(train_df[~train_df['decoy2']], hyperparameters, feature_columns)
             # model = get_cat_model_final_pfms(train_df, hyperparameters, feature_columns)
 
             df.loc[mask, 'preds'] = model.predict(get_X_array(df.loc[mask, :], feature_columns))
 
-            train_df = df.iloc[train]
-            test_df = df.iloc[test]
+            # train_df = df.iloc[train]
+            # test_df = df.iloc[test]
+            test_df = df[mask]
 
             ### 1
             fpr, tpr, thresholds = metrics.roc_curve(get_Y_array_pfms(test_df[~test_df['decoy2']]), test_df[~test_df['decoy2']]['preds'])
             # fpr, tpr, thresholds = metrics.roc_curve(get_Y_array_pfms(test_df), test_df['preds'])
             shr_v = metrics.auc(fpr, tpr)
             # shr_v = len(aux.filter(test_df, fdr=0.25, key='preds', is_decoy='decoy'))
 
@@ -1346,14 +1411,16 @@
 
     def random_search_pfms(df, param_grid, out_file, max_evals):
         """Random search for hyperparameter optimization.
         Writes result of search to csv file every search iteration."""
 
         threshold = 0
 
+        
+
         # Dataframe for results
         results = pd.DataFrame(columns = ['sharpe', 'params', 'iteration', 'all_res'],
                                     index = list(range(max_evals)))
         for i in range(max_evals):
 
             # Choose random hyperparameters
             random_params = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}
@@ -1396,23 +1463,23 @@
         model = lgb.train(hyperparameters, dtrain, num_boost_round=100)
         return model
 
     df1 = pd.DataFrame()
     for k in resdict.keys():
         df1[k] = resdict[k]
 
-    df1['ids'] = df1['iorig'].apply(lambda x: ids[x])
-    df1['Is'] = df1['iorig'].apply(lambda x: Is[x])
-    df1['Scans'] = df1['iorig'].apply(lambda x: Scans[x])
-    df1['Isotopes'] = df1['iorig'].apply(lambda x: Isotopes[x])
-    df1['mzraw'] = df1['iorig'].apply(lambda x: mzraw[x])
-    df1['rt'] = df1['iorig'].apply(lambda x: rts[x])
-    df1['av'] = df1['iorig'].apply(lambda x: avraw[x])
-    df1['ch'] = df1['iorig'].apply(lambda x: charges[x])
-    df1['im'] = df1['iorig'].apply(lambda x: imraw[x])
+    df1['ids'] = ids[df1['iorig'].values]#df1['iorig'].apply(lambda x: ids[x])
+    df1['Is'] = Is[df1['iorig'].values]#df1['iorig'].apply(lambda x: Is[x])
+    df1['Scans'] = Scans[df1['iorig'].values]#df1['iorig'].apply(lambda x: Scans[x])
+    df1['Isotopes'] = Isotopes[df1['iorig'].values]#df1['iorig'].apply(lambda x: Isotopes[x])
+    df1['mzraw'] = mzraw[df1['iorig'].values]#df1['iorig'].apply(lambda x: mzraw[x])
+    df1['rt'] = rts[df1['iorig'].values]#df1['iorig'].apply(lambda x: rts[x])
+    df1['av'] = avraw[df1['iorig'].values]#df1['iorig'].apply(lambda x: avraw[x])
+    df1['ch'] = charges[df1['iorig'].values]#df1['iorig'].apply(lambda x: charges[x])
+    df1['im'] = imraw[df1['iorig'].values]#df1['iorig'].apply(lambda x: imraw[x])
 
     df1['mass_diff'] = mass_diff
     df1['rt_diff'] = rt_diff
     df1['decoy'] = df1['seqs'].apply(lambda x: all(z.startswith(prefix) for z in pept_prot[x]))
 
     df1['peptide'] = df1['seqs']
     mass_dict = {}
@@ -1438,14 +1505,19 @@
     df1['c_DP'] = df1['peptide'].apply(lambda x: x.count('DP'))
     df1['c_KP'] = df1['peptide'].apply(lambda x: x.count('KP'))
     df1['c_RP'] = df1['peptide'].apply(lambda x: x.count('RP'))
 
     df1['rt_diff_abs'] = df1['rt_diff'].abs()
     df1['rt_diff_abs_pdiff'] = df1['rt_diff_abs'] - df1.groupby('ids')['rt_diff_abs'].transform('median')
     df1['rt_diff_abs_pnorm'] = df1['rt_diff_abs'] / (df1.groupby('ids')['rt_diff_abs'].transform('sum') + 1e-2)
+
+    df1['mass_diff_abs'] = df1['mass_diff'].abs()
+    df1['mass_diff_abs_pdiff'] = df1['mass_diff_abs'] - df1.groupby('ids')['mass_diff_abs'].transform('median')
+    df1['mass_diff_abs_pnorm'] = df1['mass_diff_abs'] / (df1.groupby('ids')['mass_diff_abs'].transform('sum') + 1e-2)
+
     df1['id_count'] = df1.groupby('ids')['mass_diff'].transform('count')
     # df1['seq_count'] = df1.groupby('peptide')['mass_diff'].transform('count')
     # df1['charge_count'] = df1.groupby('peptide')['ch'].transform('nunique')
     # df1['im_count'] = df1.groupby('peptide')['im'].transform('nunique')
 
     # df1['id_count_mass'] = df1.groupby('ids')['mass'].transform('nunique')
 
@@ -1455,119 +1527,14 @@
     #         'mz_diff_ppm_2',
     #         'I-0-1',
     #         'I-0-2',
     #     ]:
     #         tmp_dict = df_features.set_index('id').to_dict()[k]
     #         df1[k] = df1['ids'].apply(lambda x: tmp_dict[x])
 
-    # import pickle
-    # df11 = df1.copy()
-    # model2 = pickle.load(open('/home/mark/model2.pickle', 'rb'))
-    # feature_columns2 = pickle.load(open('/home/mark/model2_feature_columns.pickle', 'rb'))
-    # df11['RT'] = df11['rt']
-    # df11['charge'] = df11['ch']
-    # df11['ch'] = df11['charge_theor']
-    # df11['length'] = df11['plen']
-    # df11['m/z'] = df11['mzraw']
-    # df11['RT diff'] = df11['rt_diff']
-    # df11['mass diff'] = df11['mass_diff']
-
-    # from pyteomics import fasta
-    # def custom_cleave(sequence, rule, missed_cleavages=0, min_length=None):
-    #     peptides = []
-    #     ml = missed_cleavages+2
-    #     trange = list(range(ml))
-    #     cleavage_sites = deque([0], maxlen=ml)
-    #     cl = 1
-    #     for i in it.chain([x.end() for x in re.finditer(rule, sequence)],
-    #                 [None]):
-    #         cleavage_sites.append(i)
-    #         if cl < ml:
-    #             cl += 1
-    #         for j in trange[:cl-1]:
-    #             seq = sequence[cleavage_sites[j]:cleavage_sites[-1]]
-    #             if seq:
-    #                 if min_length is None or len(seq) >= min_length:
-    #                     peptides.append((seq, cleavage_sites[j]))
-    #     return peptides
-
-    # best_protein_tmp_dict = {}
-
-    # all_aa_set = set()
-    # prot_pep_cleave_map = defaultdict(dict)
-    # for p in fasta.read(args['d']):
-    #     dbname = p[0].split(' ')[0]
-    #     all_aa_set.update(p[1])
-    #     startposition = 0
-    #     peps_info = custom_cleave(p[1], parser.expasy_rules['trypsin'], 2)
-    #     prot_len = len(p[1])
-    #     for pep, pos in peps_info:
-            
-    #         pep_len = len(pep)
-            
-    #         prev_aa = p[1][max(0, pos-3):pos]
-    #         prev_aa = '-'*(3-len(prev_aa)) + prev_aa
-            
-    #         next_aa = p[1][pos+pep_len:min(pos+pep_len+3, prot_len-1)]
-    #         next_aa = next_aa + '-'*(3-len(next_aa))
-            
-    #         prot_pep_cleave_map[dbname][pep] = [prev_aa, next_aa]
-
-    #         best_protein_tmp_dict[pep] = dbname
-
-    # dbname_default = dbname
-            
-    # aa_map = {'-': 0}
-    # for idx, aa in enumerate(all_aa_set):
-    #     aa_map[aa] = idx+1
-        
-    # for k1 in list(prot_pep_cleave_map.keys()):
-    #     for k2 in list(prot_pep_cleave_map[k1].keys()):
-    #         val = prot_pep_cleave_map[k1][k2]
-    #         prot_pep_cleave_map[k1][k2] = [[aa_map[aa] for aa in val[0]], [aa_map[aa] for aa in val[1]]]
-
-    # df11['bestprotein'] = df11['peptide'].apply(lambda x: best_protein_tmp_dict.get(x, dbname_default))
-
-
-    # df11['prev1'] = df11[['peptide', 'bestprotein']].apply(lambda x: prot_pep_cleave_map[x[1]].get(x[0], [[0, 0, 0], [0, 0, 0]])[0][-1], axis=1)
-    # df11['prev2'] = df11[['peptide', 'bestprotein']].apply(lambda x: prot_pep_cleave_map[x[1]].get(x[0], [[0, 0, 0], [0, 0, 0]])[0][-2], axis=1)
-    # df11['prev3'] = df11[['peptide', 'bestprotein']].apply(lambda x: prot_pep_cleave_map[x[1]].get(x[0], [[0, 0, 0], [0, 0, 0]])[0][-3], axis=1)
-    # df11['next1'] = df11[['peptide', 'bestprotein']].apply(lambda x: prot_pep_cleave_map[x[1]].get(x[0], [[0, 0, 0], [0, 0, 0]])[1][0], axis=1)
-    # df11['next2'] = df11[['peptide', 'bestprotein']].apply(lambda x: prot_pep_cleave_map[x[1]].get(x[0], [[0, 0, 0], [0, 0, 0]])[1][1], axis=1)
-    # df11['next3'] = df11[['peptide', 'bestprotein']].apply(lambda x: prot_pep_cleave_map[x[1]].get(x[0], [[0, 0, 0], [0, 0, 0]])[1][2], axis=1)
-    
-
-    # for cc in ['m/z', 'length', 'RT', 'mass diff', 'RT diff']:
-    #     df11['%s_rank' % (cc, )] = df11.groupby('bestprotein')[cc].rank(pct=True)
-
-    # df11['IntensityNorm_predicted'] = model2.predict(get_X_array(df11, feature_columns2))
-
-    # df11['IntensityNorm'] = df11['Is'].rank(pct=True)
-    # df11['IntensityNorm_predicted'] = df11['IntensityNorm_predicted'].rank(pct=True)
-
-    # # df11['IntensityNorm'] = df11.groupby('bestprotein')['Is'].rank(pct=True)
-    # # df11['IntensityNorm_predicted'] = df11.groupby('bestprotein')['IntensityNorm_predicted'].rank(pct=True)
-
-    # df1['IntensityNorm_predicted'] = df11['IntensityNorm_predicted']
-    # df1['IntensityNorm'] = df11['IntensityNorm']
-    # df1['Idiff'] = df1['IntensityNorm'] - df1['IntensityNorm_predicted']
-
-    # df1['prev1'] = df11['prev1']
-    # df1['prev2'] = df11['prev2']
-    # df1['prev3'] = df11['prev3']
-    # df1['next1'] = df11['next1']
-    # df1['next2'] = df11['next2']
-    # df1['next3'] = df11['next3']
-
-
-
-
-
-
-
 
     p1 = set(resdict['seqs'])
 
     prots_spc2 = defaultdict(set)
     for pep, proteins in pept_prot.items():
         if pep in p1:
             for protein in proteins:
@@ -1592,58 +1559,88 @@
     for idx, k in enumerate(names_arr):
         prots_spc[k] = all_pvals[idx]
 
     target_prots_25_fdr = set([x[0] for x in aux.filter(prots_spc.items(), fdr=0.25, key=escore, is_decoy=isdecoy, remove_decoy=False, formula=1, full_output=True, correction=0)])
     df1['proteins'] = df1['seqs'].apply(lambda x: ';'.join(pept_prot[x]))
     df1['decoy2'] = df1['decoy']
     df1['decoy'] = df1['proteins'].apply(lambda x: all(z not in target_prots_25_fdr for z in x.split(';')))
+    df1['top_25_targets'] = df1['decoy']
 
-    if args['ml']:
+
+    if len(target_prots_25_fdr) <= 25:
+        logger.info('Low number of identified proteins, turning off LightGBM...')
+        filtered_prots = sorted(prots_spc.items(), key=lambda x: -x[1])[:25]
+        skip_ml = 1
+    else:
+        skip_ml = 0
+
+    if args['ml'] and not skip_ml:
 
         logger.info('Start Machine Learning on PFMs...')
 
         MAX_EVALS = 25
 
         out_file = os.path.join(tempfile.gettempdir(), os.urandom(24).hex())
         of_connection = open(out_file, 'w')
         writer = csv.writer(of_connection)
 
         # Write column names
         headers = ['auc', 'params', 'iteration', 'all_res']
         writer.writerow(headers)
         of_connection.close()
 
+
+
+        all_id_list = list(set(df1[df1['decoy']]['peptide']))
+        all_id_list = random.sample(all_id_list, len(all_id_list))
+        seq_gmap = {}
+        for idx, split in enumerate(np.array_split(all_id_list, 3)):
+            for id_ftr in split:
+                seq_gmap[id_ftr] = idx
+                
+        all_id_list = list(set(df1[~df1['decoy']]['peptide']))
+        all_id_list = random.sample(all_id_list, len(all_id_list))
+        for idx, split in enumerate(np.array_split(all_id_list, 3)):
+            for id_ftr in split:
+                seq_gmap[id_ftr] = idx
+
+
+
+        df1['G'] = df1['peptide'].apply(lambda x: seq_gmap[x])
+
+
+
         random_results = random_search_pfms(df1, param_grid, out_file, MAX_EVALS)
 
         random_results = pd.read_csv(out_file)
         random_results = random_results[random_results['auc'] != 'auc']
         random_results['params'] = random_results['params'].apply(lambda x: ast.literal_eval(x))
         convert_dict = {'auc': float,
                     }
         random_results = random_results.astype(convert_dict)
 
+
         bestparams = random_results.sort_values(by='auc',ascending=False)['params'].values[0]
+
         bestparams['num_threads'] = args['nproc']
 
-        groups = df1['peptide']
-        ix = df1.index.values
-        unique = np.unique(groups)
-        np.random.RandomState(SEED).shuffle(unique)
-        result = []
-        for split in np.array_split(unique, 3):
-            mask = groups.isin(split)
-            train, test = ix[~mask], ix[mask]
-            train_df = df1.iloc[train]
-            test_df = df1.iloc[test]
+
+
+        for group_val in range(3):
+            
+            mask = df1['G'] == group_val
+            test_df = df1[mask]
+            test_ids = set(test_df['ids'])
+            train_df = df1[(~mask) & (df1['ids'].apply(lambda x: x not in test_ids))]
+
 
             feature_columns = list(get_features_pfms(train_df))
-            ### 1
             model = get_cat_model_final_pfms(train_df[~train_df['decoy2']], bestparams, feature_columns)
 
-            df1.loc[test, 'preds'] = model.predict(get_X_array(test_df, feature_columns))
+            df1.loc[mask, 'preds'] = model.predict(get_X_array(test_df, feature_columns))
 
     else:
         df1['preds'] = np.power(df1['mass_diff'], 2) + np.power(df1['rt_diff'], 2)
 
     df1['qpreds'] = pd.qcut(df1['preds'], 50, labels=range(50))
 
     df1['decoy'] = df1['decoy2']
@@ -1652,43 +1649,79 @@
     df1u = df1.sort_values(by='preds')
     df1u = df1u.drop_duplicates(subset='seqs')
 
     qval_ok = 0
     for qval_cur in range(50):
         df1ut = df1u[df1u['qpreds'] == qval_cur]
         decoy_ratio = df1ut['decoy'].sum() / len(df1ut)
-        if decoy_ratio < 0.5:
+        print(decoy_ratio)
+        if decoy_ratio < ml_correction:
             qval_ok = qval_cur
         else:
             break
     logger.info('%d %% of PFMs were removed from protein scoring after Machine Learning', (100 - (qval_ok+1)*2))
 
+    # df1un = df1[df1['qpreds'] <= qval_ok].copy()
     df1un = df1u[df1u['qpreds'] <= qval_ok].copy()
+
+
+    # df1un['dec_cum'] = np.cumsum(df1un['decoy'])
+
+    # max_dec_number = df1un['dec_cum'].max()
+
+    # for idx, i in list(enumerate(np.arange(0.1, 1.1, 0.1)))[::-1]:
+    #     print(idx, i)
+        
+    #     max_p = df1un[df1un['dec_cum'] <= i * max_dec_number]['preds'].max()
+        
+    #     df1un.loc[df1un['preds'] <= max_p, 'qpreds'] = idx
+
+
+
+
     df1un['qpreds'] = pd.qcut(df1un['preds'], 10, labels=range(10))
 
     qdict = df1un.set_index('seqs').to_dict()['qpreds']
+    # qdict = df1un.set_index(['seqs', 'ids']).to_dict()['qpreds']
+
+
+    
+
+
+
 
     df1['qpreds'] = df1['seqs'].apply(lambda x: qdict.get(x, 11))
+    # df1['qpreds'] = df1.apply(lambda x: qdict.get((x['seqs'], x['ids']), 11), axis=1)
+
+
+
     # df1['qz'] = df1['seqs'].apply(lambda x: qval_dict.get(qdict.get(x, 11), 1.0))
 
     df1.to_csv(base_out_name + '_PFMs_ML.tsv', sep='\t', index=False)
 
+    df1 = df1[df1['qpreds'] <= 10]
+
+    resdict = {}
+    resdict['seqs'] = df1['seqs'].values
     resdict['qpreds'] = df1['qpreds'].values
     resdict['ids'] = df1['ids'].values
-    resdict['Is'] = df1['Is'].values
-    resdict['ch'] = df1['ch'].values
-    resdict['im'] = df1['im'].values
+    # resdict['Is'] = df1['Is'].values
+    # resdict['ch'] = df1['ch'].values
+    # resdict['im'] = df1['im'].values
     # residct['qz'] = df1['qz'].values
 
 
-    e_ind = resdict['qpreds'] <= 10
-    resdict = filter_results(resdict, e_ind)
+    # e_ind = resdict['qpreds'] <= 10
+    # resdict = filter_results(resdict, e_ind)
 
     mass_diff = resdict['qpreds']
-    rt_diff = resdict['qpreds']
+    rt_diff = []
+    if skip_ml:
+        # mass_diff = np.array([0 for _ in range(len(mass_diff))])
+        mass_diff = np.zeros(len(mass_diff))
 
     p1 = set(resdict['seqs'])
 
     prots_spc2 = defaultdict(set)
     for pep, proteins in pept_prot.items():
         if pep in p1:
             for protein in proteins:
@@ -1868,15 +1901,15 @@
                         if k not in prots_spc_final:
                             prots_spc_final[k] = v
 
                     break
 
                 try:
                     prot_fdr = aux.fdr(prots_spc_final.items(), is_decoy=isdecoy)
-                except ZeroDivisionError:
+                except:
                     prot_fdr = 100.0
                 if prot_fdr >= 12.5 * fdr:
 
                     v_arr = np.array([prots_spc[k] for k in names_arr])
                     all_pvals = utils.calc_sf_all(v_arr, n_arr, p)
                     for idx, k in enumerate(names_arr):
                         prots_spc_basic[k] = all_pvals[idx]
```

## ms1searchpy/search.py

```diff
@@ -1,10 +1,11 @@
 from . import main
 import argparse
 import logging
+import os
 
 def run():
     parser = argparse.ArgumentParser(
         description='Search proteins using LC-MS spectra',
         epilog='''
 
     Example usage
@@ -16,40 +17,53 @@
 
     parser.add_argument('files', help='input mzML or .tsv files with peptide features', nargs='+')
     parser.add_argument('-d', '-db', help='path to protein fasta file', required=True)
     parser.add_argument('-ptol', help='precursor mass tolerance in ppm', default=10.0, type=float)
     parser.add_argument('-fdr', help='protein fdr filter in %%', default=1.0, type=float)
     parser.add_argument('-i', help='minimum number of isotopes', default=2, type=int)
     parser.add_argument('-ci', help='minimum number of isotopes for mass and RT calibration', default=4, type=int)
+    parser.add_argument('-csc', help='minimum number of scans for mass and RT calibration', default=4, type=int)
     parser.add_argument('-ts', help='Two-stage RT training: 0 - turn off, 1 - turn one, 2 - turn on and use additive model in the first stage (Default)', default=2, type=int)
     parser.add_argument('-sc', help='minimum number of scans for peptide feature', default=2, type=int)
     parser.add_argument('-lmin', help='min length of peptides', default=7, type=int)
     parser.add_argument('-lmax', help='max length of peptides', default=30, type=int)
     parser.add_argument('-e', help='cleavage rule in quotes!. X!Tandem style for cleavage rules: "[RK]|{P}" for trypsin,\
      "[X]|[D]" for asp-n or "[RK]|{P},[K]|[X]" for mix of trypsin and lys-c', default='[RK]|{P}')
     parser.add_argument('-mc', help='number of missed cleavages', default=0, type=int)
     parser.add_argument('-cmin', help='min precursor charge', default=1, type=int)
     parser.add_argument('-cmax', help='max precursor charge', default=4, type=int)
-    parser.add_argument('-fmods', help='fixed modifications. in mass1@aminoacid1,mass2@aminoacid2 format', default='57.021464@C')
+    parser.add_argument('-fmods', help='fixed modifications. Use "[" and "]" for N-term and C-term amino acids. in psiname1@aminoacid1,psiname2@aminoacid2 format', default='Carbamidomethyl@C')
+    parser.add_argument('-fmods_legend', help='PSI Names for extra fixed modifications. Oxidation, Carbamidomethyl and TMT6plex are stored by default in source code. in psiname1@monomass1,psiname2@monomass2 format', default='')
     parser.add_argument('-ad', help='add decoy', default=0, type=int)
     parser.add_argument('-ml', help='use machine learning for PFMs', default=1, type=int)
     parser.add_argument('-prefix', help='decoy prefix', default='DECOY_')
-    parser.add_argument('-nproc',   help='number of processes', default=1, type=int)
+    parser.add_argument('-nproc',   help='number of processes', default=4, type=int)
+    parser.add_argument('-force_nproc', help='Force using multiprocessing for Windows', action='store_true')
     parser.add_argument('-elude', help='path to elude binary file. If empty, the built-in additive model will be used for RT prediction', default='')
     parser.add_argument('-deeplc', help='path to deeplc', default='')
     parser.add_argument('-deeplc_model_path', help='path to deeplc model or folder with deeplc models', default='')
     parser.add_argument('-deeplc_library', help='path to deeplc library', default='')
     parser.add_argument('-pl', help='path to list of peptides for RT calibration', default='')
-    parser.add_argument('-mcalib', help='mass calibration: 2 - group by ion mobility and RT, 1 - by RT, 0 - no calibration', default=2, type=int)
+    parser.add_argument('-mcalib', help='mass calibration: 2 - group by ion mobility and RT, 1 - by RT, 0 - no calibration', default=0, type=int)
     parser.add_argument('-debug', help='Produce debugging output', action='store_true')
+    parser.add_argument('-save_calib', help='Save RT calibration list', action='store_true')
+    parser.add_argument('-check_unique', help='Experimental. check_unique', default=1, type=int)
+
+    
     args = vars(parser.parse_args())
     logging.basicConfig(format='%(levelname)9s: %(asctime)s %(message)s',
             datefmt='[%H:%M:%S]', level=[logging.INFO, logging.DEBUG][args['debug']])
     logging.getLogger('matplotlib.font_manager').disabled = True
     logging.getLogger('matplotlib.category').disabled = True
     logging.getLogger('matplotlib').setLevel(logging.WARNING)
     logger = logging.getLogger(__name__)
+
+
+    if os.name == 'nt' and not args['force_nproc']:
+        logger.warning('Turning off multiprocessing for Windows system. Use -force_nproc option to turn it on')
+        args['nproc'] = 1
+
     logger.debug('Starting with args: %s', args)
     main.process_file(args)
 
 if __name__ == '__main__':
     run()
```

## ms1searchpy/utils.py

```diff
@@ -1,47 +1,110 @@
-from pyteomics import fasta, parser
+from pyteomics import fasta, parser, mass
 import os
 from scipy.stats import binom
 import numpy as np
 import pandas as pd
 import random
 import itertools
 from biosaur2 import main as bio_main
 import logging
+from copy import deepcopy
 
 logger = logging.getLogger(__name__)
 
+# Temporary for pyteomics <= Version 4.5.5 bug
+if 'H-' in mass.std_aa_mass:
+    del mass.std_aa_mass['H-']
+if '-OH' in mass.std_aa_mass:
+    del mass.std_aa_mass['-OH']
+
+mods_custom_dict = {
+    'Oxidation': 15.994915,
+    'Carbamidomethyl': 57.021464,
+    'TMT6plex': 229.162932,
+}
+
+
+def get_aa_mass_with_fixed_mods(fmods, fmods_legend):
+
+    if fmods_legend:
+        for mod in fmods_legend.split(','):
+            psiname, m = mod.split('@')
+            mods_custom_dict[psiname] = float(m)
+
+    aa_mass = deepcopy(mass.std_aa_mass)
+    aa_to_psi = dict()
+
+    mass_h2o = mass.calculate_mass('H2O')
+    for k in list(aa_mass.keys()):
+        aa_mass[k] = round(mass.calculate_mass(sequence=k) - mass_h2o, 7)
+
+    if fmods:
+        for mod in fmods.split(','):
+            psiname, aa = mod.split('@')
+            if psiname not in mods_custom_dict:
+                logger.error('PSI Name for modification %s is missing in the modification legend' % (psiname, ))
+                raise Exception('Exception: missing PSI Name for modification')
+            if aa == '[':
+                aa_mass['Nterm'] = float(mods_custom_dict[psiname])#float(m)
+                aa_to_psi['Nterm'] = psiname
+            elif aa == ']':
+                aa_mass['Cterm'] = float(mods_custom_dict[psiname])#float(m)
+                aa_to_psi['Cterm'] = psiname
+            else:
+                aa_mass[aa] += float(mods_custom_dict[psiname])#float(m)
+                aa_to_psi[aa] = psiname
+
+    logger.debug(aa_mass)
+
+    return aa_mass, aa_to_psi
+
+
+def mods_for_deepLC(seq, aa_to_psi):
+    if 'Nterm' in aa_to_psi:
+        mods_list = ['0|%s' % (aa_to_psi['Nterm'], ), ]
+    else:
+        mods_list = []
+    mods_list.extend([str(idx+1)+'|%s' % (aa_to_psi[aa]) for idx, aa in enumerate(seq) if aa in aa_to_psi])
+    if 'Cterm' in aa_to_psi:
+        mods_list.append(['-1|%s' % (aa_to_psi['Cterm'], ), ])
+    return '|'.join(mods_list)
 
 def recalc_spc(banned_dict, unstable_prots, prots_spc2):
     tmp = dict()
     for k in unstable_prots:
         tmp[k] = sum(banned_dict.get(l, 1) > 0 for l in prots_spc2[k])
     return tmp
 
-def iterate_spectra(fname, min_ch, max_ch, min_isotopes, min_scans):
+def iterate_spectra(fname, min_ch, max_ch, min_isotopes, min_scans, nproc, check_unique=True):
     if os.path.splitext(fname)[-1].lower() == '.mzml':
         args = {
             'file': fname,
             'mini': 1,
             'minmz': 350,
             'maxmz': 1500,
             'pasefmini': 100,
             'htol': 8,
             'itol': 8,
             'paseftol': 0.05,
             'nm': 0,
             'o': '',
             'hvf': 1.3,
             'minlh': 2,
+            'pasefminlh': 1,
+            'nprocs': nproc,
             'cmin': 1,
             'cmax': 6,
             'dia': False,
             'diahtol': 25,
             'diaminlh': 1,
             'mgf': '',
+            'tof': False,
+            'profile': False,
+            'write_hills': False,
             'debug': False  # actual debug value is set through logging, not here
         }
         bio_main.process_file(args)
         fname = os.path.splitext(fname)[0] + '.features.tsv'
 
     df_features = pd.read_csv(fname, sep='\t')
 
@@ -71,17 +134,18 @@
     #     df_features['mz_diff_ppm_2'] = -100
     #     df_features.loc[df_features['intensity_2'] > 0, 'mz_diff_ppm_2'] = df_features.loc[df_features['intensity_2'] > 0, :].apply(lambda x: 1e6 * (x['mz'] - (x['mz_std_2'] - 2 * 1.00335 / x['charge'])) / x['mz'], axis=1)
 
     #     df_features['I-0-1'] = df_features.apply(lambda x: x['intensityApex'] / x['intensity_1'], axis=1)
     #     df_features['I-0-2'] = -1
     #     df_features.loc[df_features['intensity_2'] > 0, 'I-0-2'] = df_features.loc[df_features['intensity_2'] > 0, :].apply(lambda x: x['intensityApex'] / x['intensity_2'], axis=1)
 
-    # Check unique ids
-    if len(df_features['id']) != len(set(df_features['id'])):
-        df_features['id'] = df_features.index + 1
+    if check_unique:
+        # Check unique ids
+        if len(df_features['id']) != len(set(df_features['id'])):
+            df_features['id'] = df_features.index + 1
 
     # Remove features with low number of isotopes
     df_features = df_features[df_features['nIsotopes'] >= min_isotopes]
 
     # Remove features with low number of Scans
     df_features = df_features[df_features['nScans'] >= min_scans]
 
@@ -265,37 +329,40 @@
     target_prot_count = 0
     decoy_prot_count = 0
     target_peps = set()
     decoy_peps = set()
 
     for desc, prot in prot_gen(args):
         dbinfo = desc.split(' ')[0]
-        if dbinfo.startswith(prefix):
-            decoy_prot_count += 1
-        else:
-            target_prot_count += 1
         for pep in prot_peptides(prot, enzyme, mc, minlen, maxlen, desc.startswith(prefix), dont_use_seen_peptides=True):
             pept_prot.setdefault(pep, []).append(dbinfo)
             protsN.setdefault(dbinfo, set()).add(pep)
     for k, v in protsN.items():
         if k.startswith(prefix):
+            decoy_prot_count += 1
             decoy_peps.update(v)
         else:
+            target_prot_count += 1
             target_peps.update(v)
 
         protsN[k] = len(v)
 
     logger.info('Database information:')
     logger.info('Target/Decoy proteins: %d/%d', target_prot_count, decoy_prot_count)
-    logger.info('Target/Decoy peptides: %d/%d', len(target_peps), len(decoy_peps))
+    target_peps_number = len(target_peps)
+    decoy_peps_number = len(decoy_peps)
+    intersection_number = len(target_peps.intersection(decoy_peps)) / (target_peps_number + decoy_peps_number)
+    logger.info('Target/Decoy peptides: %d/%d', target_peps_number, decoy_peps_number)
     logger.info('Target-Decoy peptide intersection: %.1f %%',
-        100 * len(target_peps.intersection(decoy_peps)) / (len(target_peps) + len(decoy_peps)))
+        100 * intersection_number)
+       
+    ml_correction = decoy_peps_number * (1 - intersection_number) / target_peps_number * 0.5
     del decoy_peps
     del target_peps
-    return protsN, pept_prot
+    return protsN, pept_prot, ml_correction
 
 
 def convert_tandem_cleave_rule_to_regexp(cleavage_rule):
 
     def get_sense(c_term_rule, n_term_rule):
         if '{' in c_term_rule:
             return 'N'
@@ -335,74 +402,21 @@
         else:
             if sense == 'C':
                 out_rules.append('([%s])' % (cut, ))
             else:
                 out_rules.append('(?=[%s])' % (cut, ))
     return '|'.join(out_rules)
 
-def multimap(n, func, it, **kw):
-    for s in it:
-        yield func(s, **kw)
 
 def keywithmaxval(d):
      """ a) create a list of the dict's keys and values;
          b) return the key with the max value"""
      v=list(d.values())
      k=list(d.keys())
      return k[v.index(max(v))]
 
 def calc_sf_all(v, n, p, prev_best_score=False):
     sf_values = -np.log10(binom.sf(v-1, n, p))
     sf_values[np.isnan(sf_values)] = 0
     sf_values[np.isinf(sf_values)] = (prev_best_score if prev_best_score is not False else max(sf_values[~np.isinf(sf_values)]) * 2)
     return sf_values
 
-
-# def multimap(n, func, it, **kw):
-#     if n == 0:
-#         try:
-#             n = cpu_count()
-#         except NotImplementedError:
-#             n = 1
-#     # if n == 1:
-#     #     for s in it:
-#     #         result = func(s, best_res, **kw)
-#     #         if result:
-#     #             for x in result:
-#     #                 peptide, m, snp_label, res = x
-
-#     #                 for score, spec_t, c, info in res:
-#     #                     if -score <= best_res.get(spec_t, 0):
-#     #                         best_res_raw[spec_t] = [peptide, m, snp_label, score, spec_t, c, info]
-#     #                         best_res[spec_t] = -score
-#     #     return best_res_raw, best_res
-
-#     else:
-
-#         qout = Queue()
-#         count = 0
-
-#         while True:
-#             qin = list(islice(it, 5000000))
-#             if not len(qin):
-#                 break
-# #           print 'Loaded 500000 items. Ending cycle.'
-#             procs = []
-#             for proc_num in range(n):
-#                 p = Process(target=worker, args=(qin, qout, proc_num, n, best_res, best_res_raw))
-#                 p.start()
-#                 procs.append(p)
-
-#             count = len(qin)
-
-#             for _ in range(n):
-#                 for item in iter(qout.get, None):
-#                     for k, v in item.items():
-#                         if -v[3] <= best_res.get(k, 0):
-#                             best_res_raw[k] = v
-#                             best_res[k] = -v[3]
-#                     # yield item
-
-#             for p in procs:
-#                 p.join()
-
-#         return best_res_raw, best_res
```

## Comparing `ms1searchpy-2.3.7.dist-info/LICENSE` & `ms1searchpy-2.4.1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `ms1searchpy-2.3.7.dist-info/METADATA` & `ms1searchpy-2.4.1.dist-info/METADATA`

 * *Files 3% similar despite different names*

```diff
@@ -1,12 +1,11 @@
 Metadata-Version: 2.1
 Name: ms1searchpy
-Version: 2.3.7
+Version: 2.4.1
 Summary: A proteomics search engine for LC-MS1 spectra.
-Home-page: UNKNOWN
 Author: Mark Ivanov
 Author-email: pyteomics@googlegroups.com
 License: License :: OSI Approved :: Apache Software License
 Platform: UNKNOWN
 Classifier: Intended Audience :: Science/Research
 Classifier: Programming Language :: Python :: 3
 Classifier: Topic :: Education
@@ -15,15 +14,15 @@
 Classifier: Topic :: Scientific/Engineering :: Physics
 Description-Content-Type: text/markdown
 License-File: LICENSE
 Requires-Dist: pyteomics (>=4.5.1)
 Requires-Dist: lxml
 Requires-Dist: scipy
 Requires-Dist: numpy
-Requires-Dist: sklearn
+Requires-Dist: scikit-learn
 Requires-Dist: lightgbm
 Requires-Dist: pandas
 Requires-Dist: biosaur2
 Requires-Dist: matplotlib
 
 # ms1searchpy - a DirectMS1 proteomics search engine for LC-MS1 spectra
 
@@ -49,15 +48,15 @@
 
 ## Installation
 
 Using pip:
 
     pip install ms1searchpy
 
-It is recommended to additionally install [DeepLC](https://github.com/compomics/DeepLC); you may also want to install
+It is recommended to additionally install [DeepLC](https://github.com/compomics/DeepLC) version 2.2.0+; you may also want to install
 [diffacto](https://github.com/statisticalbiotechnology/diffacto):
 
     pip install deeplc diffacto
 
 This should work on recent versions of Python (3.8-3.10).
 
 ## Usage tutorial: protein identification
```

