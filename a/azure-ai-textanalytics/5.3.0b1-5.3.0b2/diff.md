# Comparing `tmp/azure-ai-textanalytics-5.3.0b1.zip` & `tmp/azure-ai-textanalytics-5.3.0b2.zip`

## zipinfo {}

```diff
@@ -1,994 +1,252 @@
-Zip file size: 2327019 bytes, number of entries: 992
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/tests/
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure/
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure_ai_textanalytics.egg-info/
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/samples/
--rw-rw-r--  2.0 unx       38 b- defN 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/setup.cfg
--rw-rw-r--  2.0 unx    29671 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/CHANGELOG.md
--rw-rw-r--  2.0 unx     2668 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/setup.py
--rw-rw-r--  2.0 unx    43666 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/README.md
--rw-rw-r--  2.0 unx      202 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/MANIFEST.in
--rw-rw-r--  2.0 unx     1073 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/LICENSE
--rw-rw-r--  2.0 unx    74288 b- defN 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/PKG-INFO
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/tests/perfstress_tests/
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/tests/recordings/
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/tests/mock_test_responses/
--rw-rw-r--  2.0 unx    31836 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/test_recognize_pii_entities_async.py
--rw-rw-r--  2.0 unx     4018 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/test_json_pointer.py
--rw-rw-r--  2.0 unx    27829 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/test_detect_language.py
--rw-rw-r--  2.0 unx    28839 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/test_detect_language_async.py
--rw-rw-r--  2.0 unx    17287 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/test_custom_text.py
--rw-rw-r--  2.0 unx    30543 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/test_recognize_linked_entities_async.py
--rw-rw-r--  2.0 unx    18241 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/test_custom_text_async.py
--rw-rw-r--  2.0 unx    29193 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/test_recognize_entities.py
--rw-rw-r--  2.0 unx    40114 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/test_analyze_sentiment_async.py
--rw-rw-r--  2.0 unx    30867 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/test_recognize_pii_entities.py
--rw-rw-r--  2.0 unx    30181 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/test_recognize_entities_async.py
--rw-rw-r--  2.0 unx    27560 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/test_analyze_healthcare.py
--rw-rw-r--  2.0 unx     2527 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/test_auth_async.py
--rw-rw-r--  2.0 unx    98894 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/test_analyze.py
--rw-rw-r--  2.0 unx     1590 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/test_context_manager.py
--rw-rw-r--  2.0 unx    23368 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/test_extract_key_phrases.py
--rw-rw-r--  2.0 unx    24195 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/test_extract_key_phrases_async.py
--rw-rw-r--  2.0 unx      420 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/test_dict_mixin.py
--rw-rw-r--  2.0 unx    28783 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/test_analyze_healthcare_async.py
--rw-rw-r--  2.0 unx     2615 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/test_dynamic_classification_async.py
--rw-rw-r--  2.0 unx      803 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/conftest.py
--rw-rw-r--  2.0 unx     3616 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/test_encoding_async.py
--rw-rw-r--  2.0 unx    37605 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/test_analyze_sentiment.py
--rw-rw-r--  2.0 unx     1518 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/test_multiapi_async.py
--rw-rw-r--  2.0 unx    16513 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/test_repr.py
--rw-rw-r--  2.0 unx     3419 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/test_encoding.py
--rw-rw-r--  2.0 unx   106523 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/test_analyze_async.py
--rw-rw-r--  2.0 unx     4788 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/testcase.py
--rw-rw-r--  2.0 unx     2397 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/test_dynamic_classification.py
--rw-rw-r--  2.0 unx     1529 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/test_multiapi.py
--rw-rw-r--  2.0 unx     2265 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/test_context_manager_async.py
--rw-rw-r--  2.0 unx     2470 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/test_auth.py
--rw-rw-r--  2.0 unx    29518 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/test_recognize_linked_entities.py
--rw-rw-r--  2.0 unx     1570 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/perfstress_tests/perf_detect_language.py
--rw-rw-r--  2.0 unx        0 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/perfstress_tests/__init__.py
--rw-rw-r--  2.0 unx     1779 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_duplicate_ids_error.json
--rw-rw-r--  2.0 unx     1403 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_bad_credentials.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_no_single_input.json
--rw-rw-r--  2.0 unx     1505 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_string_index_type_not_fail_v3.json
--rw-rw-r--  2.0 unx     3449 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_output_same_order_as_input.json
--rw-rw-r--  2.0 unx     6199 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_all_successful_passing_dict.json
--rw-rw-r--  2.0 unx     2671 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_dynamic_classification.pyTestDynamicClassificationtest_dynamic_classification_single.json
--rw-rw-r--  2.0 unx     3950 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_show_stats_and_model_version.json
--rw-rw-r--  2.0 unx     7123 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_rotate_subscription_key.json
--rw-rw-r--  2.0 unx     6211 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_passing_only_string.json
--rw-rw-r--  2.0 unx    16452 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_relations.json
--rw-rw-r--  2.0 unx     4096 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_language_kwarg_spanish.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_missing_input_records_error.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_no_single_input.json
--rw-rw-r--  2.0 unx     2821 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_whole_batch_country_hint_and_obj_input.json
--rw-rw-r--  2.0 unx     3876 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_invalid_language_hint_method.json
--rw-rw-r--  2.0 unx   131687 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_batch_size_over_limit.json
--rw-rw-r--  2.0 unx     2015 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_document_attribute_error_no_result_attribute.json
--rw-rw-r--  2.0 unx     2440 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_whole_batch_dont_use_language_hint.json
--rw-rw-r--  2.0 unx     7883 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_client_passed_default_language_hint.json
--rw-rw-r--  2.0 unx     2367 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding.pyTestEncodingtest_emoji_family.json
--rw-rw-r--  2.0 unx     3226 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_all_successful_passing_text_document_input.json
--rw-rw-r--  2.0 unx     4136 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_all_successful_passing_text_document_input.json
--rw-rw-r--  2.0 unx     2651 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_per_item_dont_use_language_hint.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_not_passing_list_for_docs.json
--rw-rw-r--  2.0 unx     1925 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_document_attribute_error_nonexistent_attribute.json
--rw-rw-r--  2.0 unx     2727 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_whole_batch_dont_use_language_hint.json
--rw-rw-r--  2.0 unx     5629 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_multi_label_classify.json
--rw-rw-r--  2.0 unx     2306 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_disable_service_logs.json
--rw-rw-r--  2.0 unx     2655 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_whole_batch_language_hint_and_dict_per_item_hints.json
--rw-rw-r--  2.0 unx     3446 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_input_with_some_errors.json
--rw-rw-r--  2.0 unx     1403 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_empty_credential_class.json
--rw-rw-r--  2.0 unx     2463 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding.pyTestEncodingtest_emoji_family_with_skin_tone_modifier.json
--rw-rw-r--  2.0 unx     2982 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_entity_resolutions.json
--rw-rw-r--  2.0 unx     1961 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_document_attribute_error_no_result_attribute.json
--rw-rw-r--  2.0 unx     4157 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_user_agent.json
--rw-rw-r--  2.0 unx     2844 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_whole_batch_country_hint.json
--rw-rw-r--  2.0 unx     1422 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_empty_credential_class.json
--rw-rw-r--  2.0 unx     2167 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_explicit_set_string_index_type.json
--rw-rw-r--  2.0 unx     3016 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_out_of_order_ids.json
--rw-rw-r--  2.0 unx     6176 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_passing_dict_abstract_summary_action.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_mixing_inputs.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_no_single_input.json
--rw-rw-r--  2.0 unx    12948 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_custom_text_async.pyTestCustomTextAsynctest_recognize_custom_entities_continuation_token.json
--rw-rw-r--  2.0 unx    10054 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_document_errors.json
--rw-rw-r--  2.0 unx     1984 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_disable_service_logs_body_param.json
--rw-rw-r--  2.0 unx     4279 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_opinion_mining.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_recognize_pii_entities_v3.json
--rw-rw-r--  2.0 unx     3590 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_out_of_order_ids.json
--rw-rw-r--  2.0 unx     1819 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_duplicate_ids_error.json
--rw-rw-r--  2.0 unx     4212 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_whole_batch_language_hint_and_obj_input.json
--rw-rw-r--  2.0 unx     2619 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_whole_batch_language_hint_and_dict_per_item_hints.json
--rw-rw-r--  2.0 unx     1773 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_bad_model_version_error.json
--rw-rw-r--  2.0 unx     1909 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_explicit_set_string_index_type_body_param.json
--rw-rw-r--  2.0 unx     3661 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_passing_only_string.json
--rw-rw-r--  2.0 unx     6177 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_pass_cls.json
--rw-rw-r--  2.0 unx     1873 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_explicit_set_string_index_type_body_param.json
--rw-rw-r--  2.0 unx     2541 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_invalid_language_hint_method.json
--rw-rw-r--  2.0 unx     1937 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_pass_cls.json
--rw-rw-r--  2.0 unx     2141 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_detect_language_script.json
--rw-rw-r--  2.0 unx     1616 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_bad_request_on_empty_document.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_missing_input_records_error.json
--rw-rw-r--  2.0 unx     3933 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_all_successful_passing_dict.json
--rw-rw-r--  2.0 unx     7140 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_client_passed_default_language_hint.json
--rw-rw-r--  2.0 unx     2435 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_default_string_index_type_UnicodeCodePoint_body_param.json
--rw-rw-r--  2.0 unx     2556 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_whole_batch_language_hint.json
--rw-rw-r--  2.0 unx     4875 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_too_many_documents.json
--rw-rw-r--  2.0 unx     7961 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_rotate_subscription_key.json
--rw-rw-r--  2.0 unx     4158 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_input_with_some_errors.json
--rw-rw-r--  2.0 unx     4573 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_whole_batch_language_hint.json
--rw-rw-r--  2.0 unx     3467 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_whole_batch_language_hint.json
--rw-rw-r--  2.0 unx    70749 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_healthcare_fhir_bundle.json
--rw-rw-r--  2.0 unx     4047 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_input_with_some_errors.json
--rw-rw-r--  2.0 unx     4714 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_entity_action_resolutions.json
--rw-rw-r--  2.0 unx     1799 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_bad_model_version_error.json
--rw-rw-r--  2.0 unx     5028 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_all_successful_passing_text_document_input.json
--rw-rw-r--  2.0 unx    23534 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_custom_text.pyTestCustomTexttest_recognize_custom_entities_continuation_token.json
--rw-rw-r--  2.0 unx     2824 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_default_string_index_type_is_UnicodeCodePoint.json
--rw-rw-r--  2.0 unx     1978 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_disable_service_logs_body_param.json
--rw-rw-r--  2.0 unx     8336 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_autodetect_with_default.json
--rw-rw-r--  2.0 unx     2514 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_per_item_dont_use_language_hint.json
--rw-rw-r--  2.0 unx     1422 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_bad_credentials.json
--rw-rw-r--  2.0 unx     2250 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding.pyTestEncodingtest_diacritics_nfc.json
--rw-rw-r--  2.0 unx    99530 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_multiple_pages_of_results_returned_successfully.json
--rw-rw-r--  2.0 unx     2604 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_offset.json
--rw-rw-r--  2.0 unx     5558 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_custom_text_async.pyTestCustomTextAsynctest_multi_label_classify_cont_token.json
--rw-rw-r--  2.0 unx     4083 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_input_with_some_errors.json
--rw-rw-r--  2.0 unx     3459 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_user_agent.json
--rw-rw-r--  2.0 unx     2938 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_input_with_some_errors.json
--rw-rw-r--  2.0 unx    46398 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_analyze_continuation_token.json
--rw-rw-r--  2.0 unx     5610 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_too_many_documents.json
--rw-rw-r--  2.0 unx     1422 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_empty_credential_class.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_mixing_inputs.json
--rw-rw-r--  2.0 unx     2015 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_document_attribute_error_nonexistent_attribute.json
--rw-rw-r--  2.0 unx     1697 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_explicit_set_string_index_type.json
--rw-rw-r--  2.0 unx     2691 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_whole_batch_dont_use_language_hint.json
--rw-rw-r--  2.0 unx     2209 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_redacted_text.json
--rw-rw-r--  2.0 unx     2203 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_explicit_set_string_index_type.json
--rw-rw-r--  2.0 unx    18577 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_passing_only_string.json
--rw-rw-r--  2.0 unx     8229 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_client_passed_default_country_hint.json
--rw-rw-r--  2.0 unx     2505 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_invalid_language_hint_method.json
--rw-rw-r--  2.0 unx     4313 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_all_successful_passing_dict.json
--rw-rw-r--  2.0 unx     3201 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_whole_batch_language_hint_and_dict_per_item_hints.json
--rw-rw-r--  2.0 unx     2538 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_default_string_index_type_UnicodeCodePoint_body_param.json
--rw-rw-r--  2.0 unx   134787 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_batch_size_over_limit.json
--rw-rw-r--  2.0 unx     2518 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_user_agent.json
--rw-rw-r--  2.0 unx     4766 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_explicit_set_string_index_type.json
--rw-rw-r--  2.0 unx     2015 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_pass_cls.json
--rw-rw-r--  2.0 unx     3017 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_out_of_order_ids.json
--rw-rw-r--  2.0 unx     2808 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_all_successful_passing_dict.json
--rw-rw-r--  2.0 unx     7421 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_passing_dict_extract_summary_action.json
--rw-rw-r--  2.0 unx     6247 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_passing_only_string.json
--rw-rw-r--  2.0 unx     3897 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_all_successful_passing_dict.json
--rw-rw-r--  2.0 unx     2309 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_no_offset_v3_categorized_entities.json
--rw-rw-r--  2.0 unx     2770 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_user_agent.json
--rw-rw-r--  2.0 unx    30704 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_healthcare_assertion.json
--rw-rw-r--  2.0 unx    11536 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_out_of_order_ids_multiple_tasks.json
--rw-rw-r--  2.0 unx     2045 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_disable_service_logs_body_param.json
--rw-rw-r--  2.0 unx     5574 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_all_successful_passing_dict.json
--rw-rw-r--  2.0 unx     3211 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_out_of_order_ids.json
--rw-rw-r--  2.0 unx     2252 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_bad_credentials.json
--rw-rw-r--  2.0 unx     3876 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_invalid_language_hint_docs.json
--rw-rw-r--  2.0 unx     4817 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_passing_only_string.json
--rw-rw-r--  2.0 unx     8410 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_autodetect_with_default.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_passing_none_docs.json
--rw-rw-r--  2.0 unx   128515 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_batch_size_over_limit_error.json
--rw-rw-r--  2.0 unx     2784 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_whole_batch_country_hint_and_obj_input.json
--rw-rw-r--  2.0 unx     3414 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_output_same_order_as_input.json
--rw-rw-r--  2.0 unx     2575 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_default_string_index_type_UnicodeCodePoint_body_param.json
--rw-rw-r--  2.0 unx     7349 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_passing_dict_extract_summary_action.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_bad_document_input.json
--rw-rw-r--  2.0 unx     2565 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_disable_service_logs_body_param.json
--rw-rw-r--  2.0 unx     3691 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_bing_id.json
--rw-rw-r--  2.0 unx     1629 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_string_index_type_not_fail_v3.json
--rw-rw-r--  2.0 unx     2586 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_whole_batch_dont_use_language_hint.json
--rw-rw-r--  2.0 unx    11530 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_analyze_works_with_v3_1.json
--rw-rw-r--  2.0 unx     1961 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_default_string_index_type_UnicodeCodePoint_body_param.json
--rw-rw-r--  2.0 unx   131656 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_batch_size_over_limit.json
--rw-rw-r--  2.0 unx   110309 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_cancellation.json
--rw-rw-r--  2.0 unx    10053 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_rotate_subscription_key.json
--rw-rw-r--  2.0 unx     3691 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_offset.json
--rw-rw-r--  2.0 unx     2907 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_whole_batch_language_hint_and_obj_input.json
--rw-rw-r--  2.0 unx     6408 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_passing_only_string.json
--rw-rw-r--  2.0 unx     6011 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_show_stats_and_model_version.json
--rw-rw-r--  2.0 unx     1699 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_default_string_index_type_is_UnicodeCodePoint.json
--rw-rw-r--  2.0 unx     1458 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_bad_credentials.json
--rw-rw-r--  2.0 unx   727512 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_payload_too_large.json
--rw-rw-r--  2.0 unx     1785 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_duplicate_ids_error.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_no_single_input.json
--rw-rw-r--  2.0 unx     8604 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_input_with_all_errors.json
--rw-rw-r--  2.0 unx    18468 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_relations.json
--rw-rw-r--  2.0 unx     2325 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_default_string_index_type_is_UnicodeCodePoint.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_bad_document_input.json
--rw-rw-r--  2.0 unx     8792 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_custom_text.pyTestCustomTexttest_multi_label_classify_cont_token.json
--rw-rw-r--  2.0 unx     3169 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_bad_model_version_error_all_tasks.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_no_single_input.json
--rw-rw-r--  2.0 unx     3132 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_input_with_some_errors.json
--rw-rw-r--  2.0 unx     2924 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_whole_batch_language_hint_and_obj_input.json
--rw-rw-r--  2.0 unx     5779 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_all_successful_passing_text_document_input.json
--rw-rw-r--  2.0 unx     2743 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_output_same_order_as_input.json
--rw-rw-r--  2.0 unx    11499 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_document_errors.json
--rw-rw-r--  2.0 unx    13749 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_multiple_of_same_action_with_partial_results.json
--rw-rw-r--  2.0 unx   125476 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_batch_size_over_limit_error.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_no_single_input.json
--rw-rw-r--  2.0 unx     6549 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_rotate_subscription_key.json
--rw-rw-r--  2.0 unx     4155 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_whole_batch_language_hint_and_dict_input.json
--rw-rw-r--  2.0 unx     1727 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_duplicate_ids_error.json
--rw-rw-r--  2.0 unx    15437 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_passing_only_string_v3_1.json
--rw-rw-r--  2.0 unx     2778 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_output_same_order_as_input.json
--rw-rw-r--  2.0 unx     9319 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_client_passed_default_language_hint.json
--rw-rw-r--  2.0 unx     3948 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_invalid_language_hint_docs.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_no_single_input.json
--rw-rw-r--  2.0 unx     2007 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_document_attribute_error_no_result_attribute.json
--rw-rw-r--  2.0 unx     3201 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_user_agent.json
--rw-rw-r--  2.0 unx     3165 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_whole_batch_language_hint_and_dict_per_item_hints.json
--rw-rw-r--  2.0 unx     1808 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_disable_service_logs.json
--rw-rw-r--  2.0 unx     2011 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_document_warnings.json
--rw-rw-r--  2.0 unx    30776 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_healthcare_assertion.json
--rw-rw-r--  2.0 unx   125530 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_batch_size_over_limit_error.json
--rw-rw-r--  2.0 unx    60835 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_analyze_continuation_token.json
--rw-rw-r--  2.0 unx     5460 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_passing_only_string.json
--rw-rw-r--  2.0 unx    12946 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_recognize_custom_entities.json
--rw-rw-r--  2.0 unx     2008 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_invalid_country_hint_method.json
--rw-rw-r--  2.0 unx     8428 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_document_errors.json
--rw-rw-r--  2.0 unx     1663 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_default_string_index_type_is_UnicodeCodePoint.json
--rw-rw-r--  2.0 unx     2210 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_invalid_language_hint_docs.json
--rw-rw-r--  2.0 unx     3030 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_language_kwarg_english.json
--rw-rw-r--  2.0 unx    24193 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_opinion_mining_more_than_5_documents.json
--rw-rw-r--  2.0 unx     5893 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_custom_text_async.pyTestCustomTextAsynctest_single_label_classify_cont_token.json
--rw-rw-r--  2.0 unx     2560 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_opinion_mining_no_mined_opinions.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_bad_document_input.json
--rw-rw-r--  2.0 unx     2245 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_redacted_text.json
--rw-rw-r--  2.0 unx     2924 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_whole_batch_language_hint_and_obj_per_item_hints.json
--rw-rw-r--  2.0 unx     5579 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_output_same_order_as_input.json
--rw-rw-r--  2.0 unx     4191 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_whole_batch_language_hint_and_dict_input.json
--rw-rw-r--  2.0 unx     2246 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding_async.pyTestEncodingtest_emoji_with_skin_tone_modifier.json
--rw-rw-r--  2.0 unx     2427 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding_async.pyTestEncodingtest_emoji_family_with_skin_tone_modifier.json
--rw-rw-r--  2.0 unx     2015 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_document_attribute_error_no_result_attribute.json
--rw-rw-r--  2.0 unx     4186 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_output_same_order_as_input.json
--rw-rw-r--  2.0 unx 12018079 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_cancellation.json
--rw-rw-r--  2.0 unx     1702 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_default_string_index_type_is_UnicodeCodePoint.json
--rw-rw-r--  2.0 unx     4315 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_opinion_mining.json
--rw-rw-r--  2.0 unx     8661 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_document_errors.json
--rw-rw-r--  2.0 unx     1921 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_document_warnings.json
--rw-rw-r--  2.0 unx    55409 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_healthcare_action.json
--rw-rw-r--  2.0 unx     3197 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_per_item_dont_use_language_hint.json
--rw-rw-r--  2.0 unx     4349 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_all_successful_passing_text_document_input.json
--rw-rw-r--  2.0 unx     2020 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_disable_service_logs_body_param.json
--rw-rw-r--  2.0 unx   727548 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_payload_too_large.json
--rw-rw-r--  2.0 unx    19477 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_out_of_order_ids_multiple_tasks.json
--rw-rw-r--  2.0 unx     2636 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_input_with_all_errors.json
--rw-rw-r--  2.0 unx   125569 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_batch_size_over_limit_error.json
--rw-rw-r--  2.0 unx     2105 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_detect_language_script.json
--rw-rw-r--  2.0 unx     6872 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_rotate_subscription_key.json
--rw-rw-r--  2.0 unx     3570 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_disable_service_logs.json
--rw-rw-r--  2.0 unx     1700 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_explicit_set_string_index_type.json
--rw-rw-r--  2.0 unx     2203 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_too_many_documents.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_mixing_inputs.json
--rw-rw-r--  2.0 unx    11160 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_sentiment_analysis_task_with_opinion_mining.json
--rw-rw-r--  2.0 unx     2820 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_whole_batch_country_hint_and_obj_per_item_hints.json
--rw-rw-r--  2.0 unx     2259 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_invalid_language_hint_docs.json
--rw-rw-r--  2.0 unx     7775 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_country_hint_none.json
--rw-rw-r--  2.0 unx   413914 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_cancel_fail_terminal_state.json
--rw-rw-r--  2.0 unx     2592 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_whole_batch_language_hint.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_mixing_inputs.json
--rw-rw-r--  2.0 unx     2807 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_user_agent.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_no_single_input.json
--rw-rw-r--  2.0 unx     3477 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_whole_batch_language_hint_and_obj_input.json
--rw-rw-r--  2.0 unx     8455 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_document_errors.json
--rw-rw-r--  2.0 unx   125562 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_batch_size_over_limit_error.json
--rw-rw-r--  2.0 unx     2802 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_per_item_dont_use_country_hint.json
--rw-rw-r--  2.0 unx     5138 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_categories_filter.json
--rw-rw-r--  2.0 unx     1461 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_empty_credential_class.json
--rw-rw-r--  2.0 unx     5543 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_output_same_order_as_input.json
--rw-rw-r--  2.0 unx    70820 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_healthcare_fhir_bundle.json
--rw-rw-r--  2.0 unx     2974 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_input_with_some_errors.json
--rw-rw-r--  2.0 unx     2021 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_document_attribute_error_nonexistent_attribute.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_no_single_input.json
--rw-rw-r--  2.0 unx     3167 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_bad_model_version_error_multiple_tasks.json
--rw-rw-r--  2.0 unx     6720 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_rotate_subscription_key.json
--rw-rw-r--  2.0 unx     3125 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_input_with_all_errors.json
--rw-rw-r--  2.0 unx   131601 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_batch_size_over_limit.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_mixing_inputs.json
--rw-rw-r--  2.0 unx     4865 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_out_of_order_ids.json
--rw-rw-r--  2.0 unx     8415 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_document_errors.json
--rw-rw-r--  2.0 unx     8640 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_input_with_all_errors.json
--rw-rw-r--  2.0 unx     4099 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_all_successful_passing_text_document_input.json
--rw-rw-r--  2.0 unx     4316 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_input_with_some_errors.json
--rw-rw-r--  2.0 unx     1736 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_explicit_set_string_index_type.json
--rw-rw-r--  2.0 unx     1752 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_disable_service_logs.json
--rw-rw-r--  2.0 unx     1786 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_string_index_type_not_fail_v3.json
--rw-rw-r--  2.0 unx     6980 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_rotate_subscription_key.json
--rw-rw-r--  2.0 unx     1961 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_document_attribute_error_nonexistent_attribute.json
--rw-rw-r--  2.0 unx     2966 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_whole_batch_dont_use_language_hint.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_not_passing_list_for_docs.json
--rw-rw-r--  2.0 unx     3002 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_whole_batch_dont_use_language_hint.json
--rw-rw-r--  2.0 unx    68563 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_multiple_of_same_action.json
--rw-rw-r--  2.0 unx     1978 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_pass_cls.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_mixing_inputs.json
--rw-rw-r--  2.0 unx   125533 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_batch_size_over_limit_error.json
--rw-rw-r--  2.0 unx     3554 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_out_of_order_ids.json
--rw-rw-r--  2.0 unx     2258 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding.pyTestEncodingtest_emoji.json
--rw-rw-r--  2.0 unx     3176 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_no_offset_v3_linked_entity_match.json
--rw-rw-r--  2.0 unx    11614 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_normalized_text.json
--rw-rw-r--  2.0 unx     8379 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_document_errors.json
--rw-rw-r--  2.0 unx     5975 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_show_stats_and_model_version.json
--rw-rw-r--  2.0 unx     3161 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_per_item_dont_use_language_hint.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_no_single_input.json
--rw-rw-r--  2.0 unx     1842 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_bad_model_version_error.json
--rw-rw-r--  2.0 unx     5893 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_single_label_classify.json
--rw-rw-r--  2.0 unx     4150 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_output_same_order_as_input.json
--rw-rw-r--  2.0 unx     2345 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_no_offset_v3_categorized_entities.json
--rw-rw-r--  2.0 unx     2258 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding.pyTestEncodingtest_korean_nfd.json
--rw-rw-r--  2.0 unx     2443 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_whole_batch_language_hint_and_dict_per_item_hints.json
--rw-rw-r--  2.0 unx     1425 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_empty_credential_class.json
--rw-rw-r--  2.0 unx     1458 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_empty_credential_class.json
--rw-rw-r--  2.0 unx     4060 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_language_kwarg_spanish.json
--rw-rw-r--  2.0 unx    16100 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_show_stats_and_model_version_multiple_tasks_v3_1.json
--rw-rw-r--  2.0 unx     3727 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_offset.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_missing_input_records_error.json
--rw-rw-r--  2.0 unx     2553 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_document_warnings.json
--rw-rw-r--  2.0 unx     4605 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_whole_batch_dont_use_language_hint.json
--rw-rw-r--  2.0 unx     5610 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_all_successful_passing_dict.json
--rw-rw-r--  2.0 unx    20434 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_disable_service_logs.json
--rw-rw-r--  2.0 unx    11685 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_normalized_text.json
--rw-rw-r--  2.0 unx   128479 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_batch_size_over_limit_error.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_string_index_type_explicit_fails_v3.json
--rw-rw-r--  2.0 unx     1809 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_bad_model_version_error.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_mixing_inputs.json
--rw-rw-r--  2.0 unx    12473 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_cancel.json
--rw-rw-r--  2.0 unx   131637 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_batch_size_over_limit.json
--rw-rw-r--  2.0 unx     1942 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_disable_service_logs_body_param.json
--rw-rw-r--  2.0 unx     4053 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_document_attribute_error_no_result_attribute.json
--rw-rw-r--  2.0 unx     1971 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_document_attribute_error_nonexistent_attribute.json
--rw-rw-r--  2.0 unx     5885 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_show_stats_and_model_version.json
--rw-rw-r--  2.0 unx     2003 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_pass_cls.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_not_passing_list_for_docs.json
--rw-rw-r--  2.0 unx    16735 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_autodetect_lang_document.json
--rw-rw-r--  2.0 unx     2511 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_pass_cls.json
--rw-rw-r--  2.0 unx     3053 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_out_of_order_ids.json
--rw-rw-r--  2.0 unx   125526 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_batch_size_over_limit_error.json
--rw-rw-r--  2.0 unx   131658 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_batch_size_over_limit.json
--rw-rw-r--  2.0 unx     2475 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_pass_cls.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_bad_document_input.json
--rw-rw-r--  2.0 unx   125512 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_batch_size_over_limit_error.json
--rw-rw-r--  2.0 unx     2439 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_per_item_dont_use_language_hint.json
--rw-rw-r--  2.0 unx     2771 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_whole_batch_country_hint_and_dict_input.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_not_passing_list_for_docs.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_passing_none_docs.json
--rw-rw-r--  2.0 unx   131691 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_batch_size_over_limit.json
--rw-rw-r--  2.0 unx     5210 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_categories_filter.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_bad_document_input.json
--rw-rw-r--  2.0 unx     3165 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_user_agent.json
--rw-rw-r--  2.0 unx     3948 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_invalid_language_hint_method.json
--rw-rw-r--  2.0 unx     2341 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_pass_cls.json
--rw-rw-r--  2.0 unx     2476 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_whole_batch_dont_use_language_hint.json
--rw-rw-r--  2.0 unx     1962 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_document_attribute_error_no_result_attribute.json
--rw-rw-r--  2.0 unx     2258 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding.pyTestEncodingtest_korean_nfc.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_passing_none_docs.json
--rw-rw-r--  2.0 unx     3203 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_bad_model_version_error_multiple_tasks.json
--rw-rw-r--  2.0 unx     4577 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_all_successful_passing_dict_key_phrase_task.json
--rw-rw-r--  2.0 unx     1425 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_bad_credentials.json
--rw-rw-r--  2.0 unx     5631 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_custom_text.pyTestCustomTexttest_multi_label_classify.json
--rw-rw-r--  2.0 unx     1924 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_explicit_set_string_index_type_body_param.json
--rw-rw-r--  2.0 unx     2780 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_too_many_documents.json
--rw-rw-r--  2.0 unx     7741 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_all_successful_passing_dict_sentiment_task.json
--rw-rw-r--  2.0 unx     5113 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_pii_action_categories_filter.json
--rw-rw-r--  2.0 unx    19405 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_analyze_works_with_v3_1.json
--rw-rw-r--  2.0 unx     1926 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_document_attribute_error_nonexistent_attribute.json
--rw-rw-r--  2.0 unx     2295 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_invalid_language_hint_docs.json
--rw-rw-r--  2.0 unx    19296 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_disable_service_logs.json
--rw-rw-r--  2.0 unx     3175 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_out_of_order_ids.json
--rw-rw-r--  2.0 unx     1404 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_bad_credentials.json
--rw-rw-r--  2.0 unx     4936 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_out_of_order_ids.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_passing_none_docs.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_string_index_type_explicit_fails_v3.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_mixing_inputs.json
--rw-rw-r--  2.0 unx     4749 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_show_stats_and_model_version_v3_1.json
--rw-rw-r--  2.0 unx     2160 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_string_index_type_not_fail_v3.json
--rw-rw-r--  2.0 unx    12385 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_client_passed_default_language_hint.json
--rw-rw-r--  2.0 unx     3168 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_input_with_some_errors.json
--rw-rw-r--  2.0 unx     2803 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_whole_batch_dont_use_country_hint.json
--rw-rw-r--  2.0 unx     1985 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_document_attribute_error_no_result_attribute.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_missing_input_records_error.json
--rw-rw-r--  2.0 unx     2612 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_whole_batch_language_hint_and_obj_input.json
--rw-rw-r--  2.0 unx     2216 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_bad_credentials.json
--rw-rw-r--  2.0 unx     2508 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_pass_cls.json
--rw-rw-r--  2.0 unx     2431 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_categories_filter_with_domain_filter.json
--rw-rw-r--  2.0 unx     2554 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_phi_domain_filter.json
--rw-rw-r--  2.0 unx    15916 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_show_stats_and_model_version_multiple_tasks.json
--rw-rw-r--  2.0 unx     4349 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_all_successful_passing_dict.json
--rw-rw-r--  2.0 unx     2060 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_string_index_type_not_fail_v3.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_passing_none_docs.json
--rw-rw-r--  2.0 unx     3025 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_out_of_order_ids.json
--rw-rw-r--  2.0 unx     2752 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_default_string_index_type_is_UnicodeCodePoint.json
--rw-rw-r--  2.0 unx     2456 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_whole_batch_language_hint_and_obj_per_item_hints.json
--rw-rw-r--  2.0 unx     6248 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_passing_dict_abstract_summary_action.json
--rw-rw-r--  2.0 unx     1788 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_disable_service_logs.json
--rw-rw-r--  2.0 unx     2271 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_disable_service_logs.json
--rw-rw-r--  2.0 unx     2407 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_whole_batch_language_hint_and_dict_per_item_hints.json
--rw-rw-r--  2.0 unx     4367 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_extract_summary_partial_results.json
--rw-rw-r--  2.0 unx     2407 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_user_agent.json
--rw-rw-r--  2.0 unx     1814 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_duplicate_ids_error.json
--rw-rw-r--  2.0 unx     5558 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_custom_text_async.pyTestCustomTextAsynctest_multi_label_classify.json
--rw-rw-r--  2.0 unx   131694 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_batch_size_over_limit.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_mixing_inputs.json
--rw-rw-r--  2.0 unx     3799 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_dynamic_classification.pyTestDynamicClassificationtest_dynamic_classification.json
--rw-rw-r--  2.0 unx     1772 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_disable_service_logs.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_missing_input_records_error.json
--rw-rw-r--  2.0 unx     2021 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_document_attribute_error_no_result_attribute.json
--rw-rw-r--  2.0 unx     2918 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_input_with_all_errors.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_mixing_inputs.json
--rw-rw-r--  2.0 unx     2955 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_language_kwarg_spanish.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_missing_input_records_error.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_not_passing_list_for_docs.json
--rw-rw-r--  2.0 unx   125567 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_batch_size_over_limit_error.json
--rw-rw-r--  2.0 unx     3503 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_whole_batch_language_hint.json
--rw-rw-r--  2.0 unx     4911 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_too_many_documents.json
--rw-rw-r--  2.0 unx     2816 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_too_many_documents.json
--rw-rw-r--  2.0 unx     2889 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_language_kwarg_spanish.json
--rw-rw-r--  2.0 unx     3914 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_show_stats_and_model_version.json
--rw-rw-r--  2.0 unx     2733 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_whole_batch_language_hint.json
--rw-rw-r--  2.0 unx     3482 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_input_with_some_errors.json
--rw-rw-r--  2.0 unx     2387 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_all_successful_passing_text_document_input.json
--rw-rw-r--  2.0 unx     2246 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_invalid_language_hint_method.json
--rw-rw-r--  2.0 unx     2505 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_invalid_language_hint_docs.json
--rw-rw-r--  2.0 unx     4352 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_input_with_some_errors.json
--rw-rw-r--  2.0 unx     2770 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_whole_batch_country_hint_and_dict_per_item_hints.json
--rw-rw-r--  2.0 unx     5184 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_pii_action_categories_filter.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_not_passing_list_for_docs.json
--rw-rw-r--  2.0 unx     1551 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_string_index_type_not_fail_v3.json
--rw-rw-r--  2.0 unx     2222 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding_async.pyTestEncodingtest_korean_nfc.json
--rw-rw-r--  2.0 unx    10314 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_cancel_fail_v3_1.json
--rw-rw-r--  2.0 unx     3164 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_whole_batch_language_hint_and_obj_per_item_hints.json
--rw-rw-r--  2.0 unx     2725 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_too_many_documents.json
--rw-rw-r--  2.0 unx     3577 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_poller_metadata.json
--rw-rw-r--  2.0 unx     3550 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding_async.pyTestEncodingtest_zalgo_text.json
--rw-rw-r--  2.0 unx     2324 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_explicit_set_string_index_type.json
--rw-rw-r--  2.0 unx     8625 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_document_errors.json
--rw-rw-r--  2.0 unx     2779 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_output_same_order_as_input.json
--rw-rw-r--  2.0 unx     2253 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_disable_service_logs.json
--rw-rw-r--  2.0 unx     2672 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_input_with_all_errors.json
--rw-rw-r--  2.0 unx     2286 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_invalid_language_hint_docs.json
--rw-rw-r--  2.0 unx     4719 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_all_successful_passing_dict.json
--rw-rw-r--  2.0 unx     2640 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_offset.json
--rw-rw-r--  2.0 unx     2492 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_disable_service_logs_body_param.json
--rw-rw-r--  2.0 unx     2047 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_document_warnings.json
--rw-rw-r--  2.0 unx     1985 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_document_attribute_error_nonexistent_attribute.json
--rw-rw-r--  2.0 unx     2847 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_offset.json
--rw-rw-r--  2.0 unx     2214 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding_async.pyTestEncodingtest_diacritics_nfc.json
--rw-rw-r--  2.0 unx     6612 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_rotate_subscription_key.json
--rw-rw-r--  2.0 unx     3200 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_whole_batch_language_hint_and_obj_per_item_hints.json
--rw-rw-r--  2.0 unx     1945 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_bad_model_version_error.json
--rw-rw-r--  2.0 unx     1750 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_string_index_type_not_fail_v3.json
--rw-rw-r--  2.0 unx     2470 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_explicit_set_string_index_type_body_param.json
--rw-rw-r--  2.0 unx     2517 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_document_warnings.json
--rw-rw-r--  2.0 unx   131692 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_batch_size_over_limit.json
--rw-rw-r--  2.0 unx     4046 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_pass_cls.json
--rw-rw-r--  2.0 unx     2205 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_default_string_index_type_is_UnicodeCodePoint.json
--rw-rw-r--  2.0 unx     2044 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_invalid_country_hint_method.json
--rw-rw-r--  2.0 unx     8675 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_document_errors.json
--rw-rw-r--  2.0 unx     5496 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_passing_only_string.json
--rw-rw-r--  2.0 unx     9427 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_client_passed_default_language_hint.json
--rw-rw-r--  2.0 unx     1973 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_pass_cls.json
--rw-rw-r--  2.0 unx     4203 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_opinion_mining_with_negated_opinion.json
--rw-rw-r--  2.0 unx     6235 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_all_successful_passing_dict.json
--rw-rw-r--  2.0 unx     2491 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_invalid_language_hint_method.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_missing_input_records_error.json
--rw-rw-r--  2.0 unx     1979 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_document_attribute_error_no_result_attribute.json
--rw-rw-r--  2.0 unx     1782 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_duplicate_ids_error.json
--rw-rw-r--  2.0 unx     2518 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_no_offset_v3_sentence_sentiment.json
--rw-rw-r--  2.0 unx     8711 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_document_errors.json
--rw-rw-r--  2.0 unx     5272 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_out_of_order_ids.json
--rw-rw-r--  2.0 unx     2352 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_country_hint_kwarg.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_not_passing_list_for_docs.json
--rw-rw-r--  2.0 unx     1979 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_document_attribute_error_no_result_attribute.json
--rw-rw-r--  2.0 unx     2290 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_default_string_index_type_is_UnicodeCodePoint.json
--rw-rw-r--  2.0 unx     2853 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_language_kwarg_spanish.json
--rw-rw-r--  2.0 unx     2222 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding_async.pyTestEncodingtest_korean_nfd.json
--rw-rw-r--  2.0 unx     7232 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_rotate_subscription_key.json
--rw-rw-r--  2.0 unx    24229 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_opinion_mining_more_than_5_documents.json
--rw-rw-r--  2.0 unx     2925 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_whole_batch_language_hint.json
--rw-rw-r--  2.0 unx     3017 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_output_same_order_as_input.json
--rw-rw-r--  2.0 unx     3586 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding.pyTestEncodingtest_zalgo_text.json
--rw-rw-r--  2.0 unx     6624 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_partial_success_for_actions.json
--rw-rw-r--  2.0 unx     2365 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_disable_service_logs_body_param.json
--rw-rw-r--  2.0 unx     1874 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_default_string_index_type_UnicodeCodePoint_body_param.json
--rw-rw-r--  2.0 unx     3161 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_input_with_all_errors.json
--rw-rw-r--  2.0 unx     2554 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_user_agent.json
--rw-rw-r--  2.0 unx     6552 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_partial_success_for_actions.json
--rw-rw-r--  2.0 unx     1759 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_bad_model_version_error.json
--rw-rw-r--  2.0 unx     4822 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_show_stats_and_model_version_v3_1.json
--rw-rw-r--  2.0 unx     1665 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_string_index_type_not_fail_v3.json
--rw-rw-r--  2.0 unx     3140 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_no_offset_v3_linked_entity_match.json
--rw-rw-r--  2.0 unx     3437 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_input_with_all_errors.json
--rw-rw-r--  2.0 unx     5559 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_multi_label_classify.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_not_passing_list_for_docs.json
--rw-rw-r--  2.0 unx    10223 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_autodetect_lang_document_custom.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_string_index_type_explicit_fails_v3.json
--rw-rw-r--  2.0 unx     5893 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_custom_text_async.pyTestCustomTextAsynctest_single_label_classify.json
--rw-rw-r--  2.0 unx   125531 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_batch_size_over_limit_error.json
--rw-rw-r--  2.0 unx     9206 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_all_successful_passing_dict_sentiment_task.json
--rw-rw-r--  2.0 unx     2115 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_disable_service_logs.json
--rw-rw-r--  2.0 unx     4505 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_all_successful_passing_dict_key_phrase_task.json
--rw-rw-r--  2.0 unx     2282 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding.pyTestEncodingtest_emoji_with_skin_tone_modifier.json
--rw-rw-r--  2.0 unx   131655 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_batch_size_over_limit.json
--rw-rw-r--  2.0 unx     1913 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_duplicate_ids_error.json
--rw-rw-r--  2.0 unx    10153 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_autodetect_lang_document_custom.json
--rw-rw-r--  2.0 unx     2252 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_empty_credential_class.json
--rw-rw-r--  2.0 unx     4338 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_show_stats_and_model_version.json
--rw-rw-r--  2.0 unx     5813 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_show_stats_and_model_version.json
--rw-rw-r--  2.0 unx     2222 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding_async.pyTestEncodingtest_emoji.json
--rw-rw-r--  2.0 unx     2635 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_dynamic_classification_async.pyTestDynamicClassificationtest_dynamic_classification_single.json
--rw-rw-r--  2.0 unx     1587 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_string_index_type_not_fail_v3.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_bad_document_input.json
--rw-rw-r--  2.0 unx     2401 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_disable_service_logs_body_param.json
--rw-rw-r--  2.0 unx     1418 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_bad_credentials.json
--rw-rw-r--  2.0 unx     1368 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_empty_credential_class.json
--rw-rw-r--  2.0 unx     2388 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_country_hint_kwarg.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_no_single_input.json
--rw-rw-r--  2.0 unx     1458 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_empty_credential_class.json
--rw-rw-r--  2.0 unx     2200 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_too_many_documents.json
--rw-rw-r--  2.0 unx     2008 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_disable_service_logs_body_param.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_mixing_inputs.json
--rw-rw-r--  2.0 unx     1979 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_document_attribute_error_nonexistent_attribute.json
--rw-rw-r--  2.0 unx   131651 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_batch_size_over_limit.json
--rw-rw-r--  2.0 unx     2007 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_document_attribute_error_nonexistent_attribute.json
--rw-rw-r--  2.0 unx     2807 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_whole_batch_country_hint_and_dict_per_item_hints.json
--rw-rw-r--  2.0 unx     3800 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_show_stats_and_model_version.json
--rw-rw-r--  2.0 unx     2331 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding_async.pyTestEncodingtest_emoji_family.json
--rw-rw-r--  2.0 unx    13677 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_multiple_of_same_action_with_partial_results.json
--rw-rw-r--  2.0 unx     3441 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_whole_batch_language_hint_and_obj_input.json
--rw-rw-r--  2.0 unx     8337 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_client_passed_default_country_hint.json
--rw-rw-r--  2.0 unx     6240 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_custom_text.pyTestCustomTexttest_poller_metadata.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_bad_document_input.json
--rw-rw-r--  2.0 unx     6168 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_custom_text_async.pyTestCustomTextAsynctest_poller_metadata.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_string_index_type_explicit_fails_v3.json
--rw-rw-r--  2.0 unx     2766 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_per_item_dont_use_country_hint.json
--rw-rw-r--  2.0 unx     3200 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_bad_model_version_error_all_tasks.json
--rw-rw-r--  2.0 unx   381994 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_cancel_fail_terminal_state.json
--rw-rw-r--  2.0 unx     5965 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_single_label_classify.json
--rw-rw-r--  2.0 unx     9459 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_custom_text.pyTestCustomTexttest_single_label_classify_cont_token.json
--rw-rw-r--  2.0 unx     3763 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_show_stats_and_model_version.json
--rw-rw-r--  2.0 unx     7249 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_client_passed_default_language_hint.json
--rw-rw-r--  2.0 unx    18649 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_passing_only_string.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_bad_document_input.json
--rw-rw-r--  2.0 unx    28338 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_autodetect_lang_document.json
--rw-rw-r--  2.0 unx     1925 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_document_attribute_error_no_result_attribute.json
--rw-rw-r--  2.0 unx     2991 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_language_kwarg_spanish.json
--rw-rw-r--  2.0 unx     2528 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_disable_service_logs_body_param.json
--rw-rw-r--  2.0 unx     2518 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_whole_batch_language_hint_and_dict_per_item_hints.json
--rw-rw-r--  2.0 unx     4239 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_opinion_mining_with_negated_opinion.json
--rw-rw-r--  2.0 unx     1367 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_bad_credentials.json
--rw-rw-r--  2.0 unx     2844 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_all_successful_passing_dict.json
--rw-rw-r--  2.0 unx     1782 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_duplicate_ids_error.json
--rw-rw-r--  2.0 unx     4313 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_all_successful_passing_text_document_input.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_mixing_inputs.json
--rw-rw-r--  2.0 unx     2554 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_no_offset_v3_sentence_sentiment.json
--rw-rw-r--  2.0 unx    12746 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_custom_partial_error.json
--rw-rw-r--  2.0 unx     4785 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_entity_action_resolutions.json
--rw-rw-r--  2.0 unx     2151 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_disable_service_logs.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_recognize_pii_entities_v3.json
--rw-rw-r--  2.0 unx     1661 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_explicit_set_string_index_type.json
--rw-rw-r--  2.0 unx     2550 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_whole_batch_dont_use_language_hint.json
--rw-rw-r--  2.0 unx     2236 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_too_many_documents.json
--rw-rw-r--  2.0 unx     5574 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_too_many_documents.json
--rw-rw-r--  2.0 unx     2239 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_too_many_documents.json
--rw-rw-r--  2.0 unx     1877 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_duplicate_ids_error.json
--rw-rw-r--  2.0 unx     2612 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_whole_batch_language_hint_and_obj_per_item_hints.json
--rw-rw-r--  2.0 unx     1957 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_document_warnings.json
--rw-rw-r--  2.0 unx     1960 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_explicit_set_string_index_type_body_param.json
--rw-rw-r--  2.0 unx     2528 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_disable_service_logs_body_param.json
--rw-rw-r--  2.0 unx     2196 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_too_many_documents.json
--rw-rw-r--  2.0 unx     4303 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_show_stats_and_model_version.json
--rw-rw-r--  2.0 unx     2961 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_whole_batch_language_hint.json
--rw-rw-r--  2.0 unx     1422 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_bad_credentials.json
--rw-rw-r--  2.0 unx     8069 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_rotate_subscription_key.json
--rw-rw-r--  2.0 unx     2327 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_language_kwarg_spanish.json
--rw-rw-r--  2.0 unx     2351 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_all_successful_passing_text_document_input.json
--rw-rw-r--  2.0 unx     2574 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_explicit_set_string_index_type_body_param.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_missing_input_records_error.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_bad_document_input.json
--rw-rw-r--  2.0 unx     1835 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_bad_model_version_error.json
--rw-rw-r--  2.0 unx     1802 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_bad_model_version_error.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_passing_none_docs.json
--rw-rw-r--  2.0 unx     2761 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_too_many_documents.json
--rw-rw-r--  2.0 unx     2576 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_whole_batch_language_hint_and_obj_input.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_missing_input_records_error.json
--rw-rw-r--  2.0 unx     2538 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_explicit_set_string_index_type_body_param.json
--rw-rw-r--  2.0 unx     1788 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_bad_model_version_error.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_passing_none_docs.json
--rw-rw-r--  2.0 unx     9694 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_sentiment_analysis_task_with_opinion_mining.json
--rw-rw-r--  2.0 unx     3763 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_dynamic_classification_async.pyTestDynamicClassificationtest_dynamic_classification.json
--rw-rw-r--  2.0 unx    12818 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_custom_partial_error.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_missing_input_records_error.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_not_passing_list_for_docs.json
--rw-rw-r--  2.0 unx     3727 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_show_stats_and_model_version.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_not_passing_list_for_docs.json
--rw-rw-r--  2.0 unx     9586 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_all_successful_passing_text_document_input_entities_task.json
--rw-rw-r--  2.0 unx     1367 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_empty_credential_class.json
--rw-rw-r--  2.0 unx     4992 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_all_successful_passing_text_document_input.json
--rw-rw-r--  2.0 unx     5965 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_custom_text.pyTestCustomTexttest_single_label_classify.json
--rw-rw-r--  2.0 unx     2169 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_default_string_index_type_is_UnicodeCodePoint.json
--rw-rw-r--  2.0 unx     3531 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_user_agent.json
--rw-rw-r--  2.0 unx     4176 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_whole_batch_language_hint_and_obj_input.json
--rw-rw-r--  2.0 unx     1741 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_disable_service_logs.json
--rw-rw-r--  2.0 unx     2044 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_invalid_country_hint_docs.json
--rw-rw-r--  2.0 unx     1738 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_default_string_index_type_is_UnicodeCodePoint.json
--rw-rw-r--  2.0 unx     4682 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_all_successful_passing_dict.json
--rw-rw-r--  2.0 unx    12493 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_client_passed_default_language_hint.json
--rw-rw-r--  2.0 unx     3826 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_poller_metadata.json
--rw-rw-r--  2.0 unx     4194 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_input_with_some_errors.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_no_single_input.json
--rw-rw-r--  2.0 unx     3448 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_default_string_index_type_is_UnicodeCodePoint_body_param.json
--rw-rw-r--  2.0 unx     6222 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_all_successful_passing_string_pii_entities_task.json
--rw-rw-r--  2.0 unx     7682 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_document_errors.json
--rw-rw-r--  2.0 unx     4115 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_whole_batch_language_hint_and_dict_input.json
--rw-rw-r--  2.0 unx    11717 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_cancel_partial_results.json
--rw-rw-r--  2.0 unx     2295 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_invalid_language_hint_method.json
--rw-rw-r--  2.0 unx     2420 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_whole_batch_language_hint_and_obj_per_item_hints.json
--rw-rw-r--  2.0 unx   134751 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_batch_size_over_limit.json
--rw-rw-r--  2.0 unx     2697 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_whole_batch_language_hint.json
--rw-rw-r--  2.0 unx     4111 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_abstract_summary_action_with_options.json
--rw-rw-r--  2.0 unx     3922 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_poller_metadata.json
--rw-rw-r--  2.0 unx     1541 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_string_index_type_not_fail_v3.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_string_index_type_explicit_fails_v3.json
--rw-rw-r--  2.0 unx     2491 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_invalid_language_hint_docs.json
--rw-rw-r--  2.0 unx     2403 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_per_item_dont_use_language_hint.json
--rw-rw-r--  2.0 unx     4569 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_whole_batch_dont_use_language_hint.json
--rw-rw-r--  2.0 unx     2888 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_whole_batch_language_hint_and_obj_per_item_hints.json
--rw-rw-r--  2.0 unx    55480 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_healthcare_action.json
--rw-rw-r--  2.0 unx     1652 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_bad_request_on_empty_document.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_passing_none_docs.json
--rw-rw-r--  2.0 unx     1962 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_document_attribute_error_nonexistent_attribute.json
--rw-rw-r--  2.0 unx    13018 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_recognize_custom_entities.json
--rw-rw-r--  2.0 unx     7718 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_document_errors.json
--rw-rw-r--  2.0 unx     1458 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_bad_credentials.json
--rw-rw-r--  2.0 unx     2655 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_user_agent.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_no_single_input.json
--rw-rw-r--  2.0 unx     3764 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_show_stats_and_model_version.json
--rw-rw-r--  2.0 unx     2471 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_default_string_index_type_UnicodeCodePoint_body_param.json
--rw-rw-r--  2.0 unx     6294 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_all_successful_passing_string_pii_entities_task.json
--rw-rw-r--  2.0 unx     2472 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_pass_cls.json
--rw-rw-r--  2.0 unx     2541 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_invalid_language_hint_docs.json
--rw-rw-r--  2.0 unx    11405 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_cancel.json
--rw-rw-r--  2.0 unx     1461 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_bad_credentials.json
--rw-rw-r--  2.0 unx   105457 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_healthcare_continuation_token.json
--rw-rw-r--  2.0 unx     2889 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_whole_batch_language_hint_and_obj_input.json
--rw-rw-r--  2.0 unx     2434 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_explicit_set_string_index_type_body_param.json
--rw-rw-r--  2.0 unx    13530 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_invalid_language_hint_method.json
--rw-rw-r--  2.0 unx     2855 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_passing_only_string.json
--rw-rw-r--  2.0 unx     2286 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_invalid_language_hint_method.json
--rw-rw-r--  2.0 unx     6441 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_rotate_subscription_key.json
--rw-rw-r--  2.0 unx     4218 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_whole_batch_language_hint_and_obj_per_item_hints.json
--rw-rw-r--  2.0 unx     2550 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_per_item_dont_use_language_hint.json
--rw-rw-r--  2.0 unx     2808 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_whole_batch_country_hint.json
--rw-rw-r--  2.0 unx     1910 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_default_string_index_type_UnicodeCodePoint_body_param.json
--rw-rw-r--  2.0 unx     2882 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_input_with_all_errors.json
--rw-rw-r--  2.0 unx     3727 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_bing_id.json
--rw-rw-r--  2.0 unx     1454 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_bad_credentials.json
--rw-rw-r--  2.0 unx     2883 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_offset.json
--rw-rw-r--  2.0 unx     1818 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_duplicate_ids_error.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_string_index_type_explicit_fails_v3.json
--rw-rw-r--  2.0 unx     1575 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_explicit_set_string_index_type_body_param.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_passing_none_docs.json
--rw-rw-r--  2.0 unx     5236 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_out_of_order_ids.json
--rw-rw-r--  2.0 unx    12947 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_custom_text_async.pyTestCustomTextAsynctest_recognize_custom_entities.json
--rw-rw-r--  2.0 unx     4781 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_passing_only_string.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_bad_document_input.json
--rw-rw-r--  2.0 unx     1821 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_duplicate_ids_error.json
--rw-rw-r--  2.0 unx     4159 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_whole_batch_language_hint_and_dict_per_item_hints.json
--rw-rw-r--  2.0 unx     3134 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_input_with_all_errors.json
--rw-rw-r--  2.0 unx     2363 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_language_kwarg_spanish.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_bad_document_input.json
--rw-rw-r--  2.0 unx     3497 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_disable_service_logs.json
--rw-rw-r--  2.0 unx     1926 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_document_attribute_error_no_result_attribute.json
--rw-rw-r--  2.0 unx     5743 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_all_successful_passing_text_document_input.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_no_single_input.json
--rw-rw-r--  2.0 unx     2246 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_invalid_language_hint_docs.json
--rw-rw-r--  2.0 unx     2232 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_too_many_documents.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_bad_document_input.json
--rw-rw-r--  2.0 unx     2838 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_whole_batch_dont_use_country_hint.json
--rw-rw-r--  2.0 unx     1838 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_bad_model_version_error.json
--rw-rw-r--  2.0 unx     9945 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_rotate_subscription_key.json
--rw-rw-r--  2.0 unx     2443 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_user_agent.json
--rw-rw-r--  2.0 unx     4195 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_whole_batch_language_hint_and_dict_per_item_hints.json
--rw-rw-r--  2.0 unx     2259 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_invalid_language_hint_method.json
--rw-rw-r--  2.0 unx     2015 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_document_attribute_error_nonexistent_attribute.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_passing_none_docs.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_missing_input_records_error.json
--rw-rw-r--  2.0 unx     2615 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_per_item_dont_use_language_hint.json
--rw-rw-r--  2.0 unx     6372 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_passing_only_string.json
--rw-rw-r--  2.0 unx     2554 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_whole_batch_language_hint_and_dict_per_item_hints.json
--rw-rw-r--  2.0 unx     2322 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_invalid_language_hint_method.json
--rw-rw-r--  2.0 unx     2455 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_invalid_language_hint_method.json
--rw-rw-r--  2.0 unx     8491 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_document_errors.json
--rw-rw-r--  2.0 unx     1925 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_default_string_index_type_UnicodeCodePoint_body_param.json
--rw-rw-r--  2.0 unx     1971 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_document_attribute_error_no_result_attribute.json
--rw-rw-r--  2.0 unx     1752 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_bad_model_version_error.json
--rw-rw-r--  2.0 unx     3098 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_input_with_all_errors.json
--rw-rw-r--  2.0 unx     2814 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_output_same_order_as_input.json
--rw-rw-r--  2.0 unx     4182 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_whole_batch_language_hint_and_obj_per_item_hints.json
--rw-rw-r--  2.0 unx     1979 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_document_attribute_error_nonexistent_attribute.json
--rw-rw-r--  2.0 unx     3791 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_show_stats_and_model_version.json
--rw-rw-r--  2.0 unx     4193 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_user_agent.json
--rw-rw-r--  2.0 unx   125566 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_batch_size_over_limit_error.json
--rw-rw-r--  2.0 unx   101261 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_multiple_of_same_action.json
--rw-rw-r--  2.0 unx     3755 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_show_stats_and_model_version.json
--rw-rw-r--  2.0 unx     7775 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_client_passed_default_language_hint.json
--rw-rw-r--  2.0 unx     2322 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_invalid_language_hint_docs.json
--rw-rw-r--  2.0 unx     3066 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_language_kwarg_english.json
--rw-rw-r--  2.0 unx   105349 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_healthcare_continuation_token.json
--rw-rw-r--  2.0 unx     3698 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_passing_only_string.json
--rw-rw-r--  2.0 unx     1418 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_empty_credential_class.json
--rw-rw-r--  2.0 unx     1454 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_empty_credential_class.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_missing_input_records_error.json
--rw-rw-r--  2.0 unx     4609 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_whole_batch_language_hint.json
--rw-rw-r--  2.0 unx     2455 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_invalid_language_hint_docs.json
--rw-rw-r--  2.0 unx    14866 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_cancel_partial_results.json
--rw-rw-r--  2.0 unx     1967 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_pass_cls.json
--rw-rw-r--  2.0 unx     2467 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_categories_filter_with_domain_filter.json
--rw-rw-r--  2.0 unx     1914 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_bad_model_version_error.json
--rw-rw-r--  2.0 unx     2943 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_whole_batch_language_hint_and_obj_input.json
--rw-rw-r--  2.0 unx   141371 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_multiple_pages_of_results_returned_successfully.json
--rw-rw-r--  2.0 unx     7472 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_client_passed_default_language_hint.json
--rw-rw-r--  2.0 unx     6106 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_pass_cls.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_not_passing_list_for_docs.json
--rw-rw-r--  2.0 unx     1913 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_document_warnings.json
--rw-rw-r--  2.0 unx     4153 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_per_item_dont_use_language_hint.json
--rw-rw-r--  2.0 unx     2008 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_invalid_country_hint_docs.json
--rw-rw-r--  2.0 unx     1763 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_duplicate_ids_error.json
--rw-rw-r--  2.0 unx     2750 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_explicit_set_string_index_type.json
--rw-rw-r--  2.0 unx     2518 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_phi_domain_filter.json
--rw-rw-r--  2.0 unx     3189 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_all_successful_passing_text_document_input.json
--rw-rw-r--  2.0 unx     3483 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_poller_metadata.json
--rw-rw-r--  2.0 unx     1948 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_document_warnings.json
--rw-rw-r--  2.0 unx     1705 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_disable_service_logs.json
--rw-rw-r--  2.0 unx     2807 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_whole_batch_country_hint_and_dict_input.json
--rw-rw-r--  2.0 unx     1576 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_default_string_index_type_UnicodeCodePoint_body_param.json
--rw-rw-r--  2.0 unx     3971 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_pass_cls.json
--rw-rw-r--  2.0 unx     1368 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_bad_credentials.json
--rw-rw-r--  2.0 unx     2596 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_opinion_mining_no_mined_opinions.json
--rw-rw-r--  2.0 unx     7580 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_client_passed_default_language_hint.json
--rw-rw-r--  2.0 unx     1806 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_bad_model_version_error.json
--rw-rw-r--  2.0 unx     2619 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_user_agent.json
--rw-rw-r--  2.0 unx     2217 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding_async.pyTestEncodingtest_diacritics_nfd.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_passing_none_docs.json
--rw-rw-r--  2.0 unx     3061 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_out_of_order_ids.json
--rw-rw-r--  2.0 unx     2253 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding.pyTestEncodingtest_diacritics_nfd.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_passing_none_docs.json
--rw-rw-r--  2.0 unx     2377 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_pass_cls.json
--rw-rw-r--  2.0 unx     1767 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_duplicate_ids_error.json
--rw-rw-r--  2.0 unx     2784 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_whole_batch_country_hint_and_obj_per_item_hints.json
--rw-rw-r--  2.0 unx     2210 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_invalid_language_hint_method.json
--rw-rw-r--  2.0 unx     2036 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_document_warnings.json
--rw-rw-r--  2.0 unx    11394 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_input_with_some_errors.json
--rw-rw-r--  2.0 unx    10351 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_cancel_fail_v3_1.json
--rw-rw-r--  2.0 unx     2819 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_passing_only_string.json
--rw-rw-r--  2.0 unx     2000 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_document_warnings.json
--rw-rw-r--  2.0 unx    15844 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_show_stats_and_model_version_multiple_tasks.json
--rw-rw-r--  2.0 unx     9693 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_all_successful_passing_text_document_input_entities_task.json
--rw-rw-r--  2.0 unx     3019 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_entity_resolutions.json
--rw-rw-r--  2.0 unx     7921 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_country_hint_none.json
--rw-rw-r--  2.0 unx     2288 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_explicit_set_string_index_type.json
--rw-rw-r--  2.0 unx     7464 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_extract_summary_action_with_options.json
--rw-rw-r--  2.0 unx     2980 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_out_of_order_ids.json
--rw-rw-r--  2.0 unx    13019 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_custom_text.pyTestCustomTexttest_recognize_custom_entities.json
--rw-rw-r--  2.0 unx     2216 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_empty_credential_class.json
--rw-rw-r--  2.0 unx     8464 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_document_errors.json
--rw-rw-r--  2.0 unx     7392 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_extract_summary_action_with_options.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_string_index_type_explicit_fails_v3.json
--rw-rw-r--  2.0 unx    11466 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_input_with_some_errors.json
--rw-rw-r--  2.0 unx     4189 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_per_item_dont_use_language_hint.json
--rw-rw-r--  2.0 unx     2981 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_output_same_order_as_input.json
--rw-rw-r--  2.0 unx     2576 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_whole_batch_language_hint_and_obj_per_item_hints.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_not_passing_list_for_docs.json
--rw-rw-r--  2.0 unx     4439 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_extract_summary_partial_results.json
--rw-rw-r--  2.0 unx     1731 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_duplicate_ids_error.json
--rw-rw-r--  2.0 unx     1404 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_empty_credential_class.json
--rw-rw-r--  2.0 unx    16026 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_show_stats_and_model_version_multiple_tasks_v3_1.json
--rw-rw-r--  2.0 unx     3401 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_input_with_all_errors.json
--rw-rw-r--  2.0 unx    15509 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_passing_only_string_v3_1.json
--rw-rw-r--  2.0 unx       39 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_string_index_type_explicit_fails_v3.json
--rw-rw-r--  2.0 unx     1795 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_bad_model_version_error.json
--rw-rw-r--  2.0 unx     4187 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_whole_batch_language_hint_and_dict_input.json
--rw-rw-r--  2.0 unx     2217 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_disable_service_logs.json
--rw-rw-r--  2.0 unx     3580 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/mock_test_responses/action_error_with_targets_language.json
--rw-rw-r--  2.0 unx     3366 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/mock_test_responses/action_error_with_targets.json
--rw-rw-r--  2.0 unx      688 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/mock_test_responses/action_job_failure_language.json
--rw-rw-r--  2.0 unx     2302 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/mock_test_responses/action_error_no_target.json
--rw-rw-r--  2.0 unx      715 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/mock_test_responses/action_job_failure.json
--rw-rw-r--  2.0 unx     4498 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/tests/mock_test_responses/action_error_no_target_language.json
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure/ai/
--rw-rw-r--  2.0 unx       81 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/__init__.py
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/
--rw-rw-r--  2.0 unx       81 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/__init__.py
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/aio/
--rw-rw-r--  2.0 unx      289 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_version.py
--rw-rw-r--  2.0 unx     4273 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_base_client.py
--rw-rw-r--  2.0 unx     1458 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_dict_mixin.py
--rw-rw-r--  2.0 unx      226 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_user_agent.py
--rw-rw-r--  2.0 unx     4112 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_validate.py
--rw-rw-r--  2.0 unx        0 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/py.typed
--rw-rw-r--  2.0 unx     5263 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/__init__.py
--rw-rw-r--  2.0 unx    19813 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_lro.py
--rw-rw-r--  2.0 unx    96423 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_text_analytics_client.py
--rw-rw-r--  2.0 unx      620 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_check.py
--rw-rw-r--  2.0 unx     2866 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_policies.py
--rw-rw-r--  2.0 unx    20011 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_response_handlers.py
--rw-rw-r--  2.0 unx   147316 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_models.py
--rw-rw-r--  2.0 unx     4295 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_request_handlers.py
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/aio/
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/
--rw-rw-r--  2.0 unx      344 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/_version.py
--rw-rw-r--  2.0 unx    78751 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/_serialization.py
--rw-rw-r--  2.0 unx      659 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/__init__.py
--rw-rw-r--  2.0 unx      395 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/models.py
--rw-rw-r--  2.0 unx     3244 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/_configuration.py
--rw-rw-r--  2.0 unx    46917 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/_operations_mixin.py
--rw-rw-r--  2.0 unx     6020 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/_text_analytics_client.py
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/operations/
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/models/
--rw-rw-r--  2.0 unx     1716 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/_vendor.py
--rw-rw-r--  2.0 unx      851 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/__init__.py
--rw-rw-r--  2.0 unx     3511 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/_configuration.py
--rw-rw-r--  2.0 unx     4498 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/_text_analytics_client.py
--rw-rw-r--  2.0 unx      673 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/_patch.py
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/operations/
--rw-rw-r--  2.0 unx     1038 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/_vendor.py
--rw-rw-r--  2.0 unx      851 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/__init__.py
--rw-rw-r--  2.0 unx     3523 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/_configuration.py
--rw-rw-r--  2.0 unx     4567 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/_text_analytics_client.py
--rw-rw-r--  2.0 unx      673 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/_patch.py
--rw-rw-r--  2.0 unx    27744 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/operations/_text_analytics_client_operations.py
--rw-rw-r--  2.0 unx      844 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/operations/__init__.py
--rw-rw-r--  2.0 unx     5794 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/operations/_patch.py
--rw-rw-r--  2.0 unx    31849 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/operations/_text_analytics_client_operations.py
--rw-rw-r--  2.0 unx      844 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/operations/__init__.py
--rw-rw-r--  2.0 unx     5657 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/operations/_patch.py
--rw-rw-r--  2.0 unx    16492 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/models/__init__.py
--rw-rw-r--  2.0 unx   360567 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/models/_models_py3.py
--rw-rw-r--  2.0 unx     3051 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/models/_patch.py
--rw-rw-r--  2.0 unx    28295 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/models/_text_analytics_client_enums.py
--rw-rw-r--  2.0 unx      558 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/aio/__init__.py
--rw-rw-r--  2.0 unx     3196 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/aio/_configuration.py
--rw-rw-r--  2.0 unx    46604 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/aio/_operations_mixin.py
--rw-rw-r--  2.0 unx     6061 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/aio/_text_analytics_client.py
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/aio/
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/operations/
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/models/
--rw-rw-r--  2.0 unx     1716 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/_vendor.py
--rw-rw-r--  2.0 unx      851 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/__init__.py
--rw-rw-r--  2.0 unx     3207 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/_configuration.py
--rw-rw-r--  2.0 unx     4443 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/_text_analytics_client.py
--rw-rw-r--  2.0 unx     1529 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/_patch.py
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/aio/operations/
--rw-rw-r--  2.0 unx     1038 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/aio/_vendor.py
--rw-rw-r--  2.0 unx      851 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/aio/__init__.py
--rw-rw-r--  2.0 unx     3219 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/aio/_configuration.py
--rw-rw-r--  2.0 unx     4512 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/aio/_text_analytics_client.py
--rw-rw-r--  2.0 unx     1529 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/aio/_patch.py
--rw-rw-r--  2.0 unx    62457 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/aio/operations/_text_analytics_client_operations.py
--rw-rw-r--  2.0 unx      844 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/aio/operations/__init__.py
--rw-rw-r--  2.0 unx      673 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/aio/operations/_patch.py
--rw-rw-r--  2.0 unx    76298 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/operations/_text_analytics_client_operations.py
--rw-rw-r--  2.0 unx      844 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/operations/__init__.py
--rw-rw-r--  2.0 unx      673 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/operations/_patch.py
--rw-rw-r--  2.0 unx     7627 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/models/__init__.py
--rw-rw-r--  2.0 unx   152552 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/models/_models_py3.py
--rw-rw-r--  2.0 unx      673 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/models/_patch.py
--rw-rw-r--  2.0 unx    15955 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/models/_text_analytics_client_enums.py
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/aio/
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/operations/
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/models/
--rw-rw-r--  2.0 unx     1296 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/_vendor.py
--rw-rw-r--  2.0 unx      851 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/__init__.py
--rw-rw-r--  2.0 unx     3207 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/_configuration.py
--rw-rw-r--  2.0 unx     4234 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/_text_analytics_client.py
--rw-rw-r--  2.0 unx     1529 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/_patch.py
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/aio/operations/
--rw-rw-r--  2.0 unx     1038 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/aio/_vendor.py
--rw-rw-r--  2.0 unx      851 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/aio/__init__.py
--rw-rw-r--  2.0 unx     3219 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/aio/_configuration.py
--rw-rw-r--  2.0 unx     4303 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/aio/_text_analytics_client.py
--rw-rw-r--  2.0 unx     1529 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/aio/_patch.py
--rw-rw-r--  2.0 unx    19513 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/aio/operations/_text_analytics_client_operations.py
--rw-rw-r--  2.0 unx      844 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/aio/operations/__init__.py
--rw-rw-r--  2.0 unx      673 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/aio/operations/_patch.py
--rw-rw-r--  2.0 unx    25147 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/operations/_text_analytics_client_operations.py
--rw-rw-r--  2.0 unx      844 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/operations/__init__.py
--rw-rw-r--  2.0 unx      673 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/operations/_patch.py
--rw-rw-r--  2.0 unx     2954 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/models/__init__.py
--rw-rw-r--  2.0 unx    55899 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/models/_models_py3.py
--rw-rw-r--  2.0 unx      673 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/models/_patch.py
--rw-rw-r--  2.0 unx     2036 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/models/_text_analytics_client_enums.py
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/aio/
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/operations/
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/models/
--rw-rw-r--  2.0 unx     1716 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/_vendor.py
--rw-rw-r--  2.0 unx      851 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/__init__.py
--rw-rw-r--  2.0 unx     3495 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/_configuration.py
--rw-rw-r--  2.0 unx     4502 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/_text_analytics_client.py
--rw-rw-r--  2.0 unx      673 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/_patch.py
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/aio/operations/
--rw-rw-r--  2.0 unx     1038 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/aio/_vendor.py
--rw-rw-r--  2.0 unx      851 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/aio/__init__.py
--rw-rw-r--  2.0 unx     3507 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/aio/_configuration.py
--rw-rw-r--  2.0 unx     4571 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/aio/_text_analytics_client.py
--rw-rw-r--  2.0 unx      673 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/aio/_patch.py
--rw-rw-r--  2.0 unx    27608 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/aio/operations/_text_analytics_client_operations.py
--rw-rw-r--  2.0 unx      844 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/aio/operations/__init__.py
--rw-rw-r--  2.0 unx     5770 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/aio/operations/_patch.py
--rw-rw-r--  2.0 unx    31681 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/operations/_text_analytics_client_operations.py
--rw-rw-r--  2.0 unx      844 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/operations/__init__.py
--rw-rw-r--  2.0 unx     5633 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/operations/_patch.py
--rw-rw-r--  2.0 unx    11099 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/models/__init__.py
--rw-rw-r--  2.0 unx   233453 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/models/_models_py3.py
--rw-rw-r--  2.0 unx      673 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/models/_patch.py
--rw-rw-r--  2.0 unx    19608 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/models/_text_analytics_client_enums.py
--rw-rw-r--  2.0 unx     3731 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/aio/_base_client_async.py
--rw-rw-r--  2.0 unx    18284 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/aio/_lro_async.py
--rw-rw-r--  2.0 unx      512 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/aio/__init__.py
--rw-rw-r--  2.0 unx     2559 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/aio/_response_handlers_async.py
--rw-rw-r--  2.0 unx    97472 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/aio/_text_analytics_client_async.py
--rw-rw-r--  2.0 unx        1 b- defN 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure_ai_textanalytics.egg-info/not-zip-safe
--rw-rw-r--  2.0 unx        1 b- defN 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure_ai_textanalytics.egg-info/dependency_links.txt
--rw-rw-r--  2.0 unx    87186 b- defN 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure_ai_textanalytics.egg-info/SOURCES.txt
--rw-rw-r--  2.0 unx       91 b- defN 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure_ai_textanalytics.egg-info/requires.txt
--rw-rw-r--  2.0 unx        6 b- defN 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure_ai_textanalytics.egg-info/top_level.txt
--rw-rw-r--  2.0 unx    74288 b- defN 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/azure_ai_textanalytics.egg-info/PKG-INFO
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/samples/text_samples/
-drwxrwxr-x  2.0 unx        0 b- stor 22-Nov-17 23:56 azure-ai-textanalytics-5.3.0b1/samples/async_samples/
--rw-rw-r--  2.0 unx     2823 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/sample_get_detailed_diagnostics_information.py
--rw-rw-r--  2.0 unx     3511 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/sample_recognize_pii_entities.py
--rw-rw-r--  2.0 unx     2980 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/sample_recognize_entities.py
--rw-rw-r--  2.0 unx     3133 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/sample_single_label_classify.py
--rw-rw-r--  2.0 unx     3525 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/sample_authentication.py
--rw-rw-r--  2.0 unx     4469 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/sample_analyze_healthcare_action.py
--rw-rw-r--  2.0 unx     2972 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/sample_recognize_custom_entities.py
--rw-rw-r--  2.0 unx     2842 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/sample_extract_key_phrases.py
--rw-rw-r--  2.0 unx     3224 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/sample_multi_label_classify.py
--rw-rw-r--  2.0 unx     5123 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/sample_analyze_healthcare_entities.py
--rw-rw-r--  2.0 unx     6714 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/sample_analyze_sentiment_with_opinion_mining.py
--rw-rw-r--  2.0 unx     5750 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/sample_analyze_actions.py
--rw-rw-r--  2.0 unx     3158 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/sample_analyze_healthcare_entities_with_cancellation.py
--rw-rw-r--  2.0 unx    14531 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/README.md
--rw-rw-r--  2.0 unx     3212 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/sample_recognize_linked_entities.py
--rw-rw-r--  2.0 unx     4114 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/sample_abstract_summary.py
--rw-rw-r--  2.0 unx     2304 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/sample_alternative_document_input.py
--rw-rw-r--  2.0 unx     2497 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/sample_dynamic_classification.py
--rw-rw-r--  2.0 unx     4621 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/sample_analyze_sentiment.py
--rw-rw-r--  2.0 unx     3838 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/sample_extract_summary.py
--rw-rw-r--  2.0 unx     3149 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/sample_model_version.py
--rw-rw-r--  2.0 unx     3125 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/sample_detect_language.py
--rw-rw-r--  2.0 unx     1761 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/text_samples/custom_entities_sample.txt
--rw-rw-r--  2.0 unx     2658 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/text_samples/custom_classify_sample.txt
--rw-rw-r--  2.0 unx     3002 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_extract_key_phrases_async.py
--rw-rw-r--  2.0 unx     2998 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_get_detailed_diagnostics_information_async.py
--rw-rw-r--  2.0 unx     6827 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_analyze_sentiment_with_opinion_mining_async.py
--rw-rw-r--  2.0 unx     3432 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_single_label_classify_async.py
--rw-rw-r--  2.0 unx     3522 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_multi_label_classify_async.py
--rw-rw-r--  2.0 unx     4075 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_extract_summary_async.py
--rw-rw-r--  2.0 unx     4753 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_analyze_sentiment_async.py
--rw-rw-r--  2.0 unx     5248 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_analyze_healthcare_action_async.py
--rw-rw-r--  2.0 unx     3788 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_authentication_async.py
--rw-rw-r--  2.0 unx     3240 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_recognize_custom_entities_async.py
--rw-rw-r--  2.0 unx     3370 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_recognize_linked_entities_async.py
--rw-rw-r--  2.0 unx     6403 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_analyze_actions_async.py
--rw-rw-r--  2.0 unx     3140 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_recognize_entities_async.py
--rw-rw-r--  2.0 unx     3284 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_detect_language_async.py
--rw-rw-r--  2.0 unx     5342 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_analyze_healthcare_entities_async.py
--rw-rw-r--  2.0 unx     3669 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_recognize_pii_entities_async.py
--rw-rw-r--  2.0 unx     2661 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_dynamic_classification_async.py
--rw-rw-r--  2.0 unx     4346 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_abstract_summary_async.py
--rw-rw-r--  2.0 unx     3448 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_model_version_async.py
--rw-rw-r--  2.0 unx     2438 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_alternative_document_input_async.py
--rw-rw-r--  2.0 unx     3317 b- defN 22-Nov-17 23:55 azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_analyze_healthcare_entities_with_cancellation_async.py
-992 files, 24381694 bytes uncompressed, 2014943 bytes compressed:  91.7%
+Zip file size: 572785 bytes, number of entries: 250
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/tests/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/samples/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure_ai_textanalytics.egg-info/
+-rw-rw-r--  2.0 unx     2668 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/setup.py
+-rw-rw-r--  2.0 unx       39 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/pyproject.toml
+-rw-rw-r--  2.0 unx    30457 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/CHANGELOG.md
+-rw-rw-r--  2.0 unx      202 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/MANIFEST.in
+-rw-rw-r--  2.0 unx     1073 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/LICENSE
+-rw-rw-r--  2.0 unx       38 b- defN 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/setup.cfg
+-rw-rw-r--  2.0 unx    50329 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/README.md
+-rw-rw-r--  2.0 unx    81737 b- defN 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/PKG-INFO
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/tests/perfstress_tests/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/tests/mock_test_responses/
+-rw-rw-r--  2.0 unx    12340 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_summarization.py
+-rw-rw-r--  2.0 unx   106541 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_analyze_async.py
+-rw-rw-r--  2.0 unx    29518 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_recognize_linked_entities.py
+-rw-rw-r--  2.0 unx    31836 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_recognize_pii_entities_async.py
+-rw-rw-r--  2.0 unx      803 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/conftest.py
+-rw-rw-r--  2.0 unx    29957 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_recognize_entities_async.py
+-rw-rw-r--  2.0 unx    16534 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_repr.py
+-rw-rw-r--  2.0 unx    40114 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_analyze_sentiment_async.py
+-rw-rw-r--  2.0 unx    28739 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_analyze_healthcare_async.py
+-rw-rw-r--  2.0 unx    30867 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_recognize_pii_entities.py
+-rw-rw-r--  2.0 unx    98912 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_analyze.py
+-rw-rw-r--  2.0 unx    27520 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_analyze_healthcare.py
+-rw-rw-r--  2.0 unx    30543 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_recognize_linked_entities_async.py
+-rw-rw-r--  2.0 unx    12615 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_summarization_async.py
+-rw-rw-r--  2.0 unx     2527 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_auth_async.py
+-rw-rw-r--  2.0 unx      420 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_dict_mixin.py
+-rw-rw-r--  2.0 unx     1529 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_multiapi.py
+-rw-rw-r--  2.0 unx     2615 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_dynamic_classification_async.py
+-rw-rw-r--  2.0 unx    28839 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_detect_language_async.py
+-rw-rw-r--  2.0 unx     3616 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_encoding_async.py
+-rw-rw-r--  2.0 unx     3419 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_encoding.py
+-rw-rw-r--  2.0 unx     2397 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_dynamic_classification.py
+-rw-rw-r--  2.0 unx     1590 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_context_manager.py
+-rw-rw-r--  2.0 unx    37605 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_analyze_sentiment.py
+-rw-rw-r--  2.0 unx    24195 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_extract_key_phrases_async.py
+-rw-rw-r--  2.0 unx     4788 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/testcase.py
+-rw-rw-r--  2.0 unx    23368 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_extract_key_phrases.py
+-rw-rw-r--  2.0 unx    28969 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_recognize_entities.py
+-rw-rw-r--  2.0 unx    27829 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_detect_language.py
+-rw-rw-r--  2.0 unx     4018 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_json_pointer.py
+-rw-rw-r--  2.0 unx     2470 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_auth.py
+-rw-rw-r--  2.0 unx     2265 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_context_manager_async.py
+-rw-rw-r--  2.0 unx    18241 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_custom_text_async.py
+-rw-rw-r--  2.0 unx    17287 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_custom_text.py
+-rw-rw-r--  2.0 unx     1518 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/test_multiapi_async.py
+-rw-rw-r--  2.0 unx     1570 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/perfstress_tests/perf_detect_language.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/perfstress_tests/__init__.py
+-rw-rw-r--  2.0 unx     3580 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/mock_test_responses/action_error_with_targets_language.json
+-rw-rw-r--  2.0 unx     2302 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/mock_test_responses/action_error_no_target.json
+-rw-rw-r--  2.0 unx      715 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/mock_test_responses/action_job_failure.json
+-rw-rw-r--  2.0 unx     3366 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/mock_test_responses/action_error_with_targets.json
+-rw-rw-r--  2.0 unx      688 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/mock_test_responses/action_job_failure_language.json
+-rw-rw-r--  2.0 unx     4498 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/tests/mock_test_responses/action_error_no_target_language.json
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure/ai/
+-rw-rw-r--  2.0 unx       65 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/__init__.py
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/
+-rw-rw-r--  2.0 unx       65 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/__init__.py
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/aio/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/
+-rw-rw-r--  2.0 unx     4115 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_validate.py
+-rw-rw-r--  2.0 unx     4402 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_base_client.py
+-rw-rw-r--  2.0 unx    20275 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_lro.py
+-rw-rw-r--  2.0 unx      226 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_user_agent.py
+-rw-rw-r--  2.0 unx     5185 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/__init__.py
+-rw-rw-r--  2.0 unx    20090 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_response_handlers.py
+-rw-rw-r--  2.0 unx     2866 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_policies.py
+-rw-rw-r--  2.0 unx     1745 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_dict_mixin.py
+-rw-rw-r--  2.0 unx   146170 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_models.py
+-rw-rw-r--  2.0 unx      620 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_check.py
+-rw-rw-r--  2.0 unx      289 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_version.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/py.typed
+-rw-rw-r--  2.0 unx     4295 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_request_handlers.py
+-rw-rw-r--  2.0 unx   109981 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_text_analytics_client.py
+-rw-rw-r--  2.0 unx      512 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/aio/__init__.py
+-rw-rw-r--  2.0 unx   111256 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/aio/_text_analytics_client_async.py
+-rw-rw-r--  2.0 unx     2559 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/aio/_response_handlers_async.py
+-rw-rw-r--  2.0 unx     3911 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/aio/_base_client_async.py
+-rw-rw-r--  2.0 unx    18731 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/aio/_lro_async.py
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/aio/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/
+-rw-rw-r--  2.0 unx    78751 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/_serialization.py
+-rw-rw-r--  2.0 unx    46917 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/_operations_mixin.py
+-rw-rw-r--  2.0 unx     3244 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/_configuration.py
+-rw-rw-r--  2.0 unx      395 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/models.py
+-rw-rw-r--  2.0 unx      659 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/__init__.py
+-rw-rw-r--  2.0 unx      344 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/_version.py
+-rw-rw-r--  2.0 unx     6020 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/_text_analytics_client.py
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/aio/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/models/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/operations/
+-rw-rw-r--  2.0 unx     3495 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/_configuration.py
+-rw-rw-r--  2.0 unx     1716 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/_vendor.py
+-rw-rw-r--  2.0 unx      851 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/__init__.py
+-rw-rw-r--  2.0 unx      673 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/_patch.py
+-rw-rw-r--  2.0 unx     4502 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/_text_analytics_client.py
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/aio/operations/
+-rw-rw-r--  2.0 unx     3507 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/aio/_configuration.py
+-rw-rw-r--  2.0 unx     1038 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/aio/_vendor.py
+-rw-rw-r--  2.0 unx      851 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/aio/__init__.py
+-rw-rw-r--  2.0 unx      673 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/aio/_patch.py
+-rw-rw-r--  2.0 unx     4571 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/aio/_text_analytics_client.py
+-rw-rw-r--  2.0 unx      844 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/aio/operations/__init__.py
+-rw-rw-r--  2.0 unx    27608 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/aio/operations/_text_analytics_client_operations.py
+-rw-rw-r--  2.0 unx     5770 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/aio/operations/_patch.py
+-rw-rw-r--  2.0 unx   233453 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/models/_models_py3.py
+-rw-rw-r--  2.0 unx    19608 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/models/_text_analytics_client_enums.py
+-rw-rw-r--  2.0 unx    11099 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/models/__init__.py
+-rw-rw-r--  2.0 unx      673 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/models/_patch.py
+-rw-rw-r--  2.0 unx      844 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/operations/__init__.py
+-rw-rw-r--  2.0 unx    31681 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/operations/_text_analytics_client_operations.py
+-rw-rw-r--  2.0 unx     5633 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/operations/_patch.py
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/aio/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/models/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/operations/
+-rw-rw-r--  2.0 unx     3207 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/_configuration.py
+-rw-rw-r--  2.0 unx     1716 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/_vendor.py
+-rw-rw-r--  2.0 unx      851 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/__init__.py
+-rw-rw-r--  2.0 unx     1529 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/_patch.py
+-rw-rw-r--  2.0 unx     4443 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/_text_analytics_client.py
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/aio/operations/
+-rw-rw-r--  2.0 unx     3219 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/aio/_configuration.py
+-rw-rw-r--  2.0 unx     1038 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/aio/_vendor.py
+-rw-rw-r--  2.0 unx      851 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/aio/__init__.py
+-rw-rw-r--  2.0 unx     1529 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/aio/_patch.py
+-rw-rw-r--  2.0 unx     4512 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/aio/_text_analytics_client.py
+-rw-rw-r--  2.0 unx      844 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/aio/operations/__init__.py
+-rw-rw-r--  2.0 unx    62457 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/aio/operations/_text_analytics_client_operations.py
+-rw-rw-r--  2.0 unx      673 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/aio/operations/_patch.py
+-rw-rw-r--  2.0 unx   152552 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/models/_models_py3.py
+-rw-rw-r--  2.0 unx    15955 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/models/_text_analytics_client_enums.py
+-rw-rw-r--  2.0 unx     7627 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/models/__init__.py
+-rw-rw-r--  2.0 unx      673 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/models/_patch.py
+-rw-rw-r--  2.0 unx      844 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/operations/__init__.py
+-rw-rw-r--  2.0 unx    76298 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/operations/_text_analytics_client_operations.py
+-rw-rw-r--  2.0 unx      673 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/operations/_patch.py
+-rw-rw-r--  2.0 unx    46604 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/aio/_operations_mixin.py
+-rw-rw-r--  2.0 unx     3196 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/aio/_configuration.py
+-rw-rw-r--  2.0 unx      558 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/aio/__init__.py
+-rw-rw-r--  2.0 unx     6061 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/aio/_text_analytics_client.py
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/aio/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/models/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/operations/
+-rw-rw-r--  2.0 unx     3207 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/_configuration.py
+-rw-rw-r--  2.0 unx     1296 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/_vendor.py
+-rw-rw-r--  2.0 unx      851 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/__init__.py
+-rw-rw-r--  2.0 unx     1529 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/_patch.py
+-rw-rw-r--  2.0 unx     4234 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/_text_analytics_client.py
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/aio/operations/
+-rw-rw-r--  2.0 unx     3219 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/aio/_configuration.py
+-rw-rw-r--  2.0 unx     1038 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/aio/_vendor.py
+-rw-rw-r--  2.0 unx      851 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/aio/__init__.py
+-rw-rw-r--  2.0 unx     1529 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/aio/_patch.py
+-rw-rw-r--  2.0 unx     4303 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/aio/_text_analytics_client.py
+-rw-rw-r--  2.0 unx      844 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/aio/operations/__init__.py
+-rw-rw-r--  2.0 unx    19513 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/aio/operations/_text_analytics_client_operations.py
+-rw-rw-r--  2.0 unx      673 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/aio/operations/_patch.py
+-rw-rw-r--  2.0 unx    55899 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/models/_models_py3.py
+-rw-rw-r--  2.0 unx     2036 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/models/_text_analytics_client_enums.py
+-rw-rw-r--  2.0 unx     2954 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/models/__init__.py
+-rw-rw-r--  2.0 unx      673 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/models/_patch.py
+-rw-rw-r--  2.0 unx      844 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/operations/__init__.py
+-rw-rw-r--  2.0 unx    25147 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/operations/_text_analytics_client_operations.py
+-rw-rw-r--  2.0 unx      673 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/operations/_patch.py
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/models/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/operations/
+-rw-rw-r--  2.0 unx     3511 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/_configuration.py
+-rw-rw-r--  2.0 unx     1716 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/_vendor.py
+-rw-rw-r--  2.0 unx      851 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/__init__.py
+-rw-rw-r--  2.0 unx      673 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/_patch.py
+-rw-rw-r--  2.0 unx     4498 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/_text_analytics_client.py
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/operations/
+-rw-rw-r--  2.0 unx     3523 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/_configuration.py
+-rw-rw-r--  2.0 unx     1038 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/_vendor.py
+-rw-rw-r--  2.0 unx      851 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/__init__.py
+-rw-rw-r--  2.0 unx      673 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/_patch.py
+-rw-rw-r--  2.0 unx     4567 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/_text_analytics_client.py
+-rw-rw-r--  2.0 unx      844 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/operations/__init__.py
+-rw-rw-r--  2.0 unx    27744 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/operations/_text_analytics_client_operations.py
+-rw-rw-r--  2.0 unx     5794 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/operations/_patch.py
+-rw-rw-r--  2.0 unx   358855 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/models/_models_py3.py
+-rw-rw-r--  2.0 unx    28250 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/models/_text_analytics_client_enums.py
+-rw-rw-r--  2.0 unx    16354 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/models/__init__.py
+-rw-rw-r--  2.0 unx    21358 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/models/_patch.py
+-rw-rw-r--  2.0 unx      844 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/operations/__init__.py
+-rw-rw-r--  2.0 unx    31849 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/operations/_text_analytics_client_operations.py
+-rw-rw-r--  2.0 unx     5657 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/operations/_patch.py
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/samples/async_samples/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/samples/text_samples/
+-rw-rw-r--  2.0 unx     5560 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/sample_recognize_entity_resolutions.py
+-rw-rw-r--  2.0 unx     3542 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/sample_authentication.py
+-rw-rw-r--  2.0 unx     2981 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/sample_recognize_entities.py
+-rw-rw-r--  2.0 unx     4003 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/sample_abstractive_summary.py
+-rw-rw-r--  2.0 unx     6763 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/sample_analyze_sentiment_with_opinion_mining.py
+-rw-rw-r--  2.0 unx     2511 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/sample_dynamic_classification.py
+-rw-rw-r--  2.0 unx     3128 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/sample_detect_language.py
+-rw-rw-r--  2.0 unx     2986 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/sample_recognize_custom_entities.py
+-rw-rw-r--  2.0 unx     5112 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/sample_analyze_healthcare_entities.py
+-rw-rw-r--  2.0 unx     2837 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/sample_get_detailed_diagnostics_information.py
+-rw-rw-r--  2.0 unx     3701 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/sample_extract_summary.py
+-rw-rw-r--  2.0 unx     4624 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/sample_analyze_sentiment.py
+-rw-rw-r--  2.0 unx     2845 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/sample_extract_key_phrases.py
+-rw-rw-r--  2.0 unx     3147 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/sample_single_label_classify.py
+-rw-rw-r--  2.0 unx     4499 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/sample_analyze_healthcare_action.py
+-rw-rw-r--  2.0 unx     3164 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/sample_model_version.py
+-rw-rw-r--  2.0 unx     3164 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/sample_analyze_healthcare_entities_with_cancellation.py
+-rw-rw-r--  2.0 unx     5780 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/sample_analyze_actions.py
+-rw-rw-r--  2.0 unx     2307 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/sample_alternative_document_input.py
+-rw-rw-r--  2.0 unx     3224 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/sample_recognize_linked_entities.py
+-rw-rw-r--  2.0 unx    15190 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/README.md
+-rw-rw-r--  2.0 unx     3595 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/sample_recognize_pii_entities.py
+-rw-rw-r--  2.0 unx     3238 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/sample_multi_label_classify.py
+-rw-rw-r--  2.0 unx     3288 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_detect_language_async.py
+-rw-rw-r--  2.0 unx     3255 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_recognize_custom_entities_async.py
+-rw-rw-r--  2.0 unx     3013 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_get_detailed_diagnostics_information_async.py
+-rw-rw-r--  2.0 unx     5279 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_analyze_healthcare_action_async.py
+-rw-rw-r--  2.0 unx     3384 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_recognize_linked_entities_async.py
+-rw-rw-r--  2.0 unx     4757 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_analyze_sentiment_async.py
+-rw-rw-r--  2.0 unx     3754 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_recognize_pii_entities_async.py
+-rw-rw-r--  2.0 unx     3143 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_recognize_entities_async.py
+-rw-rw-r--  2.0 unx     5783 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_recognize_entity_resolutions_async.py
+-rw-rw-r--  2.0 unx     2653 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_dynamic_classification_async.py
+-rw-rw-r--  2.0 unx     3464 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_model_version_async.py
+-rw-rw-r--  2.0 unx     5332 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_analyze_healthcare_entities_async.py
+-rw-rw-r--  2.0 unx     3537 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_multi_label_classify_async.py
+-rw-rw-r--  2.0 unx     4210 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_abstractive_summary_async.py
+-rw-rw-r--  2.0 unx     3447 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_single_label_classify_async.py
+-rw-rw-r--  2.0 unx     3913 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_extract_summary_async.py
+-rw-rw-r--  2.0 unx     6434 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_analyze_actions_async.py
+-rw-rw-r--  2.0 unx     3007 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_extract_key_phrases_async.py
+-rw-rw-r--  2.0 unx     6877 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_analyze_sentiment_with_opinion_mining_async.py
+-rw-rw-r--  2.0 unx     3324 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_analyze_healthcare_entities_with_cancellation_async.py
+-rw-rw-r--  2.0 unx     3806 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_authentication_async.py
+-rw-rw-r--  2.0 unx     2442 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_alternative_document_input_async.py
+-rw-rw-r--  2.0 unx     2658 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/text_samples/custom_classify_sample.txt
+-rw-rw-r--  2.0 unx     1761 b- defN 23-Mar-07 17:37 azure-ai-textanalytics-5.3.0b2/samples/text_samples/custom_entities_sample.txt
+-rw-rw-r--  2.0 unx        6 b- defN 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure_ai_textanalytics.egg-info/top_level.txt
+-rw-rw-r--  2.0 unx       91 b- defN 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure_ai_textanalytics.egg-info/requires.txt
+-rw-rw-r--  2.0 unx        1 b- defN 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure_ai_textanalytics.egg-info/dependency_links.txt
+-rw-rw-r--  2.0 unx    11079 b- defN 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure_ai_textanalytics.egg-info/SOURCES.txt
+-rw-rw-r--  2.0 unx        1 b- defN 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure_ai_textanalytics.egg-info/not-zip-safe
+-rw-rw-r--  2.0 unx    81737 b- defN 23-Mar-07 17:38 azure-ai-textanalytics-5.3.0b2/azure_ai_textanalytics.egg-info/PKG-INFO
+250 files, 3204394 bytes uncompressed, 513871 bytes compressed:  84.0%
```

## zipnote {}

```diff
@@ -1,2977 +1,751 @@
-Filename: azure-ai-textanalytics-5.3.0b1/
+Filename: azure-ai-textanalytics-5.3.0b2/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/
+Filename: azure-ai-textanalytics-5.3.0b2/tests/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/azure/
+Filename: azure-ai-textanalytics-5.3.0b2/azure/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/azure_ai_textanalytics.egg-info/
+Filename: azure-ai-textanalytics-5.3.0b2/samples/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/samples/
+Filename: azure-ai-textanalytics-5.3.0b2/azure_ai_textanalytics.egg-info/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/setup.cfg
+Filename: azure-ai-textanalytics-5.3.0b2/setup.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/CHANGELOG.md
+Filename: azure-ai-textanalytics-5.3.0b2/pyproject.toml
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/setup.py
+Filename: azure-ai-textanalytics-5.3.0b2/CHANGELOG.md
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/README.md
+Filename: azure-ai-textanalytics-5.3.0b2/MANIFEST.in
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/MANIFEST.in
+Filename: azure-ai-textanalytics-5.3.0b2/LICENSE
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/LICENSE
+Filename: azure-ai-textanalytics-5.3.0b2/setup.cfg
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/PKG-INFO
+Filename: azure-ai-textanalytics-5.3.0b2/README.md
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/perfstress_tests/
+Filename: azure-ai-textanalytics-5.3.0b2/PKG-INFO
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/
+Filename: azure-ai-textanalytics-5.3.0b2/tests/perfstress_tests/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/mock_test_responses/
+Filename: azure-ai-textanalytics-5.3.0b2/tests/mock_test_responses/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/test_recognize_pii_entities_async.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_summarization.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/test_json_pointer.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_analyze_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/test_detect_language.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_recognize_linked_entities.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/test_detect_language_async.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_recognize_pii_entities_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/test_custom_text.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/conftest.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/test_recognize_linked_entities_async.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_recognize_entities_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/test_custom_text_async.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_repr.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/test_recognize_entities.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_analyze_sentiment_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/test_analyze_sentiment_async.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_analyze_healthcare_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/test_recognize_pii_entities.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_recognize_pii_entities.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/test_recognize_entities_async.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_analyze.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/test_analyze_healthcare.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_analyze_healthcare.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/test_auth_async.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_recognize_linked_entities_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/test_analyze.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_summarization_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/test_context_manager.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_auth_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/test_extract_key_phrases.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_dict_mixin.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/test_extract_key_phrases_async.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_multiapi.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/test_dict_mixin.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_dynamic_classification_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/test_analyze_healthcare_async.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_detect_language_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/test_dynamic_classification_async.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_encoding_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/conftest.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_encoding.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/test_encoding_async.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_dynamic_classification.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/test_analyze_sentiment.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_context_manager.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/test_multiapi_async.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_analyze_sentiment.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/test_repr.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_extract_key_phrases_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/test_encoding.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/testcase.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/test_analyze_async.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_extract_key_phrases.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/testcase.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_recognize_entities.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/test_dynamic_classification.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_detect_language.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/test_multiapi.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_json_pointer.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/test_context_manager_async.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_auth.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/test_auth.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_context_manager_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/test_recognize_linked_entities.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_custom_text_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/perfstress_tests/perf_detect_language.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_custom_text.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/perfstress_tests/__init__.py
+Filename: azure-ai-textanalytics-5.3.0b2/tests/test_multiapi_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_duplicate_ids_error.json
+Filename: azure-ai-textanalytics-5.3.0b2/tests/perfstress_tests/perf_detect_language.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_bad_credentials.json
+Filename: azure-ai-textanalytics-5.3.0b2/tests/perfstress_tests/__init__.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_no_single_input.json
+Filename: azure-ai-textanalytics-5.3.0b2/tests/mock_test_responses/action_error_with_targets_language.json
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_string_index_type_not_fail_v3.json
+Filename: azure-ai-textanalytics-5.3.0b2/tests/mock_test_responses/action_error_no_target.json
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_output_same_order_as_input.json
+Filename: azure-ai-textanalytics-5.3.0b2/tests/mock_test_responses/action_job_failure.json
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_all_successful_passing_dict.json
+Filename: azure-ai-textanalytics-5.3.0b2/tests/mock_test_responses/action_error_with_targets.json
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_dynamic_classification.pyTestDynamicClassificationtest_dynamic_classification_single.json
+Filename: azure-ai-textanalytics-5.3.0b2/tests/mock_test_responses/action_job_failure_language.json
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_show_stats_and_model_version.json
+Filename: azure-ai-textanalytics-5.3.0b2/tests/mock_test_responses/action_error_no_target_language.json
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_rotate_subscription_key.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_passing_only_string.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/__init__.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_relations.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_language_kwarg_spanish.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/__init__.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_missing_input_records_error.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/aio/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_no_single_input.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_whole_batch_country_hint_and_obj_input.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_validate.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_invalid_language_hint_method.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_base_client.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_batch_size_over_limit.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_lro.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_document_attribute_error_no_result_attribute.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_user_agent.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_whole_batch_dont_use_language_hint.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/__init__.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_client_passed_default_language_hint.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_response_handlers.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding.pyTestEncodingtest_emoji_family.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_policies.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_all_successful_passing_text_document_input.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_dict_mixin.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_all_successful_passing_text_document_input.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_models.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_per_item_dont_use_language_hint.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_check.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_not_passing_list_for_docs.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_version.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_document_attribute_error_nonexistent_attribute.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/py.typed
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_whole_batch_dont_use_language_hint.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_request_handlers.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_multi_label_classify.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_text_analytics_client.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_disable_service_logs.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/aio/__init__.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_whole_batch_language_hint_and_dict_per_item_hints.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/aio/_text_analytics_client_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_input_with_some_errors.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/aio/_response_handlers_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_empty_credential_class.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/aio/_base_client_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding.pyTestEncodingtest_emoji_family_with_skin_tone_modifier.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/aio/_lro_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_entity_resolutions.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_document_attribute_error_no_result_attribute.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_user_agent.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/aio/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_whole_batch_country_hint.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_empty_credential_class.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_explicit_set_string_index_type.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/_serialization.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_out_of_order_ids.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/_operations_mixin.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_passing_dict_abstract_summary_action.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/_configuration.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_mixing_inputs.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/models.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_no_single_input.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/__init__.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_custom_text_async.pyTestCustomTextAsynctest_recognize_custom_entities_continuation_token.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/_version.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_document_errors.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/_text_analytics_client.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_disable_service_logs_body_param.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/aio/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_opinion_mining.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/models/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_recognize_pii_entities_v3.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/operations/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_out_of_order_ids.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/_configuration.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_duplicate_ids_error.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/_vendor.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_whole_batch_language_hint_and_obj_input.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/__init__.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_whole_batch_language_hint_and_dict_per_item_hints.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/_patch.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_bad_model_version_error.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/_text_analytics_client.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_explicit_set_string_index_type_body_param.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/aio/operations/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_passing_only_string.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/aio/_configuration.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_pass_cls.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/aio/_vendor.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_explicit_set_string_index_type_body_param.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/aio/__init__.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_invalid_language_hint_method.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/aio/_patch.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_pass_cls.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/aio/_text_analytics_client.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_detect_language_script.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/aio/operations/__init__.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_bad_request_on_empty_document.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/aio/operations/_text_analytics_client_operations.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_missing_input_records_error.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/aio/operations/_patch.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_all_successful_passing_dict.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/models/_models_py3.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_client_passed_default_language_hint.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/models/_text_analytics_client_enums.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_default_string_index_type_UnicodeCodePoint_body_param.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/models/__init__.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_whole_batch_language_hint.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/models/_patch.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_too_many_documents.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/operations/__init__.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_rotate_subscription_key.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/operations/_text_analytics_client_operations.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_input_with_some_errors.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/operations/_patch.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_whole_batch_language_hint.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/aio/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_whole_batch_language_hint.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/models/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_healthcare_fhir_bundle.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/operations/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_input_with_some_errors.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/_configuration.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_entity_action_resolutions.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/_vendor.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_bad_model_version_error.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/__init__.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_all_successful_passing_text_document_input.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/_patch.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_custom_text.pyTestCustomTexttest_recognize_custom_entities_continuation_token.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/_text_analytics_client.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_default_string_index_type_is_UnicodeCodePoint.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/aio/operations/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_disable_service_logs_body_param.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/aio/_configuration.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_autodetect_with_default.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/aio/_vendor.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_per_item_dont_use_language_hint.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/aio/__init__.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_bad_credentials.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/aio/_patch.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding.pyTestEncodingtest_diacritics_nfc.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/aio/_text_analytics_client.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_multiple_pages_of_results_returned_successfully.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/aio/operations/__init__.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_offset.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/aio/operations/_text_analytics_client_operations.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_custom_text_async.pyTestCustomTextAsynctest_multi_label_classify_cont_token.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/aio/operations/_patch.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_input_with_some_errors.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/models/_models_py3.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_user_agent.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/models/_text_analytics_client_enums.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_input_with_some_errors.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/models/__init__.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_analyze_continuation_token.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/models/_patch.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_too_many_documents.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/operations/__init__.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_empty_credential_class.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/operations/_text_analytics_client_operations.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_mixing_inputs.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/operations/_patch.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_document_attribute_error_nonexistent_attribute.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/aio/_operations_mixin.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_explicit_set_string_index_type.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/aio/_configuration.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_whole_batch_dont_use_language_hint.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/aio/__init__.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_redacted_text.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/aio/_text_analytics_client.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_explicit_set_string_index_type.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/aio/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_passing_only_string.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/models/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_client_passed_default_country_hint.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/operations/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_invalid_language_hint_method.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/_configuration.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_all_successful_passing_dict.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/_vendor.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_whole_batch_language_hint_and_dict_per_item_hints.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/__init__.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_default_string_index_type_UnicodeCodePoint_body_param.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/_patch.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_batch_size_over_limit.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/_text_analytics_client.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_user_agent.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/aio/operations/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_explicit_set_string_index_type.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/aio/_configuration.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_pass_cls.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/aio/_vendor.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_out_of_order_ids.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/aio/__init__.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_all_successful_passing_dict.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/aio/_patch.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_passing_dict_extract_summary_action.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/aio/_text_analytics_client.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_passing_only_string.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/aio/operations/__init__.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_all_successful_passing_dict.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/aio/operations/_text_analytics_client_operations.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_no_offset_v3_categorized_entities.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/aio/operations/_patch.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_user_agent.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/models/_models_py3.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_healthcare_assertion.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/models/_text_analytics_client_enums.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_out_of_order_ids_multiple_tasks.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/models/__init__.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_disable_service_logs_body_param.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/models/_patch.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_all_successful_passing_dict.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/operations/__init__.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_out_of_order_ids.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/operations/_text_analytics_client_operations.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_bad_credentials.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/operations/_patch.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_invalid_language_hint_docs.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_passing_only_string.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/models/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_autodetect_with_default.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/operations/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_passing_none_docs.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/_configuration.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_batch_size_over_limit_error.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/_vendor.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_whole_batch_country_hint_and_obj_input.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/__init__.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_output_same_order_as_input.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/_patch.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_default_string_index_type_UnicodeCodePoint_body_param.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/_text_analytics_client.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_passing_dict_extract_summary_action.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/operations/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_bad_document_input.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/_configuration.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_disable_service_logs_body_param.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/_vendor.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_bing_id.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/__init__.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_string_index_type_not_fail_v3.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/_patch.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_whole_batch_dont_use_language_hint.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/_text_analytics_client.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_analyze_works_with_v3_1.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/operations/__init__.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_default_string_index_type_UnicodeCodePoint_body_param.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/operations/_text_analytics_client_operations.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_batch_size_over_limit.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/operations/_patch.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_cancellation.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/models/_models_py3.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_rotate_subscription_key.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/models/_text_analytics_client_enums.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_offset.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/models/__init__.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_whole_batch_language_hint_and_obj_input.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/models/_patch.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_passing_only_string.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/operations/__init__.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_show_stats_and_model_version.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/operations/_text_analytics_client_operations.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_default_string_index_type_is_UnicodeCodePoint.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/operations/_patch.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_bad_credentials.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/async_samples/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_payload_too_large.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/text_samples/
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_duplicate_ids_error.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/sample_recognize_entity_resolutions.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_no_single_input.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/sample_authentication.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_input_with_all_errors.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/sample_recognize_entities.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_relations.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/sample_abstractive_summary.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_default_string_index_type_is_UnicodeCodePoint.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/sample_analyze_sentiment_with_opinion_mining.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_bad_document_input.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/sample_dynamic_classification.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_custom_text.pyTestCustomTexttest_multi_label_classify_cont_token.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/sample_detect_language.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_bad_model_version_error_all_tasks.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/sample_recognize_custom_entities.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_no_single_input.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/sample_analyze_healthcare_entities.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_input_with_some_errors.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/sample_get_detailed_diagnostics_information.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_whole_batch_language_hint_and_obj_input.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/sample_extract_summary.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_all_successful_passing_text_document_input.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/sample_analyze_sentiment.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_output_same_order_as_input.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/sample_extract_key_phrases.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_document_errors.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/sample_single_label_classify.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_multiple_of_same_action_with_partial_results.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/sample_analyze_healthcare_action.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_batch_size_over_limit_error.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/sample_model_version.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_no_single_input.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/sample_analyze_healthcare_entities_with_cancellation.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_rotate_subscription_key.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/sample_analyze_actions.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_whole_batch_language_hint_and_dict_input.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/sample_alternative_document_input.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_duplicate_ids_error.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/sample_recognize_linked_entities.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_passing_only_string_v3_1.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/README.md
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_output_same_order_as_input.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/sample_recognize_pii_entities.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_client_passed_default_language_hint.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/sample_multi_label_classify.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_invalid_language_hint_docs.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_detect_language_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_no_single_input.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_recognize_custom_entities_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_document_attribute_error_no_result_attribute.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_get_detailed_diagnostics_information_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_user_agent.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_analyze_healthcare_action_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_whole_batch_language_hint_and_dict_per_item_hints.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_recognize_linked_entities_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_disable_service_logs.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_analyze_sentiment_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_document_warnings.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_recognize_pii_entities_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_healthcare_assertion.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_recognize_entities_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_batch_size_over_limit_error.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_recognize_entity_resolutions_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_analyze_continuation_token.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_dynamic_classification_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_passing_only_string.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_model_version_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_recognize_custom_entities.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_analyze_healthcare_entities_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_invalid_country_hint_method.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_multi_label_classify_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_document_errors.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_abstractive_summary_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_default_string_index_type_is_UnicodeCodePoint.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_single_label_classify_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_invalid_language_hint_docs.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_extract_summary_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_language_kwarg_english.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_analyze_actions_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_opinion_mining_more_than_5_documents.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_extract_key_phrases_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_custom_text_async.pyTestCustomTextAsynctest_single_label_classify_cont_token.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_analyze_sentiment_with_opinion_mining_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_opinion_mining_no_mined_opinions.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_analyze_healthcare_entities_with_cancellation_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_bad_document_input.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_authentication_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_redacted_text.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_alternative_document_input_async.py
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_whole_batch_language_hint_and_obj_per_item_hints.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/text_samples/custom_classify_sample.txt
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_output_same_order_as_input.json
+Filename: azure-ai-textanalytics-5.3.0b2/samples/text_samples/custom_entities_sample.txt
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_whole_batch_language_hint_and_dict_input.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure_ai_textanalytics.egg-info/top_level.txt
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding_async.pyTestEncodingtest_emoji_with_skin_tone_modifier.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure_ai_textanalytics.egg-info/requires.txt
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding_async.pyTestEncodingtest_emoji_family_with_skin_tone_modifier.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure_ai_textanalytics.egg-info/dependency_links.txt
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_document_attribute_error_no_result_attribute.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure_ai_textanalytics.egg-info/SOURCES.txt
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_output_same_order_as_input.json
+Filename: azure-ai-textanalytics-5.3.0b2/azure_ai_textanalytics.egg-info/not-zip-safe
 Comment: 
 
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_cancellation.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_default_string_index_type_is_UnicodeCodePoint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_opinion_mining.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_document_errors.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_document_warnings.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_healthcare_action.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_per_item_dont_use_language_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_all_successful_passing_text_document_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_disable_service_logs_body_param.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_payload_too_large.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_out_of_order_ids_multiple_tasks.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_input_with_all_errors.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_batch_size_over_limit_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_detect_language_script.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_rotate_subscription_key.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_disable_service_logs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_explicit_set_string_index_type.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_too_many_documents.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_mixing_inputs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_sentiment_analysis_task_with_opinion_mining.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_whole_batch_country_hint_and_obj_per_item_hints.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_invalid_language_hint_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_country_hint_none.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_cancel_fail_terminal_state.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_whole_batch_language_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_mixing_inputs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_user_agent.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_no_single_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_whole_batch_language_hint_and_obj_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_document_errors.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_batch_size_over_limit_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_per_item_dont_use_country_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_categories_filter.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_empty_credential_class.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_output_same_order_as_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_healthcare_fhir_bundle.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_input_with_some_errors.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_document_attribute_error_nonexistent_attribute.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_no_single_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_bad_model_version_error_multiple_tasks.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_rotate_subscription_key.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_input_with_all_errors.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_batch_size_over_limit.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_mixing_inputs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_out_of_order_ids.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_document_errors.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_input_with_all_errors.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_all_successful_passing_text_document_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_input_with_some_errors.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_explicit_set_string_index_type.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_disable_service_logs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_string_index_type_not_fail_v3.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_rotate_subscription_key.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_document_attribute_error_nonexistent_attribute.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_whole_batch_dont_use_language_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_not_passing_list_for_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_whole_batch_dont_use_language_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_multiple_of_same_action.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_pass_cls.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_mixing_inputs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_batch_size_over_limit_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_out_of_order_ids.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding.pyTestEncodingtest_emoji.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_no_offset_v3_linked_entity_match.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_normalized_text.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_document_errors.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_show_stats_and_model_version.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_per_item_dont_use_language_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_no_single_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_bad_model_version_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_single_label_classify.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_output_same_order_as_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_no_offset_v3_categorized_entities.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding.pyTestEncodingtest_korean_nfd.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_whole_batch_language_hint_and_dict_per_item_hints.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_empty_credential_class.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_empty_credential_class.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_language_kwarg_spanish.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_show_stats_and_model_version_multiple_tasks_v3_1.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_offset.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_missing_input_records_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_document_warnings.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_whole_batch_dont_use_language_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_all_successful_passing_dict.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_disable_service_logs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_normalized_text.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_batch_size_over_limit_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_string_index_type_explicit_fails_v3.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_bad_model_version_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_mixing_inputs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_cancel.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_batch_size_over_limit.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_disable_service_logs_body_param.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_document_attribute_error_no_result_attribute.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_document_attribute_error_nonexistent_attribute.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_show_stats_and_model_version.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_pass_cls.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_not_passing_list_for_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_autodetect_lang_document.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_pass_cls.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_out_of_order_ids.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_batch_size_over_limit_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_batch_size_over_limit.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_pass_cls.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_bad_document_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_batch_size_over_limit_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_per_item_dont_use_language_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_whole_batch_country_hint_and_dict_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_not_passing_list_for_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_passing_none_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_batch_size_over_limit.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_categories_filter.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_bad_document_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_user_agent.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_invalid_language_hint_method.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_pass_cls.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_whole_batch_dont_use_language_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_document_attribute_error_no_result_attribute.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding.pyTestEncodingtest_korean_nfc.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_passing_none_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_bad_model_version_error_multiple_tasks.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_all_successful_passing_dict_key_phrase_task.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_bad_credentials.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_custom_text.pyTestCustomTexttest_multi_label_classify.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_explicit_set_string_index_type_body_param.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_too_many_documents.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_all_successful_passing_dict_sentiment_task.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_pii_action_categories_filter.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_analyze_works_with_v3_1.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_document_attribute_error_nonexistent_attribute.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_invalid_language_hint_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_disable_service_logs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_out_of_order_ids.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_bad_credentials.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_out_of_order_ids.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_passing_none_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_string_index_type_explicit_fails_v3.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_mixing_inputs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_show_stats_and_model_version_v3_1.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_string_index_type_not_fail_v3.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_client_passed_default_language_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_input_with_some_errors.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_whole_batch_dont_use_country_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_document_attribute_error_no_result_attribute.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_missing_input_records_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_whole_batch_language_hint_and_obj_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_bad_credentials.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_pass_cls.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_categories_filter_with_domain_filter.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_phi_domain_filter.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_show_stats_and_model_version_multiple_tasks.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_all_successful_passing_dict.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_string_index_type_not_fail_v3.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_passing_none_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_out_of_order_ids.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_default_string_index_type_is_UnicodeCodePoint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_whole_batch_language_hint_and_obj_per_item_hints.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_passing_dict_abstract_summary_action.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_disable_service_logs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_disable_service_logs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_whole_batch_language_hint_and_dict_per_item_hints.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_extract_summary_partial_results.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_user_agent.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_duplicate_ids_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_custom_text_async.pyTestCustomTextAsynctest_multi_label_classify.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_batch_size_over_limit.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_mixing_inputs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_dynamic_classification.pyTestDynamicClassificationtest_dynamic_classification.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_disable_service_logs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_missing_input_records_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_document_attribute_error_no_result_attribute.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_input_with_all_errors.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_mixing_inputs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_language_kwarg_spanish.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_missing_input_records_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_not_passing_list_for_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_batch_size_over_limit_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_whole_batch_language_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_too_many_documents.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_too_many_documents.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_language_kwarg_spanish.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_show_stats_and_model_version.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_whole_batch_language_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_input_with_some_errors.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_all_successful_passing_text_document_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_invalid_language_hint_method.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_invalid_language_hint_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_input_with_some_errors.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_whole_batch_country_hint_and_dict_per_item_hints.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_pii_action_categories_filter.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_not_passing_list_for_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_string_index_type_not_fail_v3.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding_async.pyTestEncodingtest_korean_nfc.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_cancel_fail_v3_1.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_whole_batch_language_hint_and_obj_per_item_hints.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_too_many_documents.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_poller_metadata.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding_async.pyTestEncodingtest_zalgo_text.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_explicit_set_string_index_type.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_document_errors.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_output_same_order_as_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_disable_service_logs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_input_with_all_errors.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_invalid_language_hint_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_all_successful_passing_dict.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_offset.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_disable_service_logs_body_param.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_document_warnings.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_document_attribute_error_nonexistent_attribute.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_offset.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding_async.pyTestEncodingtest_diacritics_nfc.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_rotate_subscription_key.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_whole_batch_language_hint_and_obj_per_item_hints.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_bad_model_version_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_string_index_type_not_fail_v3.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_explicit_set_string_index_type_body_param.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_document_warnings.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_batch_size_over_limit.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_pass_cls.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_default_string_index_type_is_UnicodeCodePoint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_invalid_country_hint_method.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_document_errors.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_passing_only_string.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_client_passed_default_language_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_pass_cls.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_opinion_mining_with_negated_opinion.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_all_successful_passing_dict.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_invalid_language_hint_method.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_missing_input_records_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_document_attribute_error_no_result_attribute.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_duplicate_ids_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_no_offset_v3_sentence_sentiment.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_document_errors.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_out_of_order_ids.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_country_hint_kwarg.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_not_passing_list_for_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_document_attribute_error_no_result_attribute.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_default_string_index_type_is_UnicodeCodePoint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_language_kwarg_spanish.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding_async.pyTestEncodingtest_korean_nfd.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_rotate_subscription_key.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_opinion_mining_more_than_5_documents.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_whole_batch_language_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_output_same_order_as_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding.pyTestEncodingtest_zalgo_text.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_partial_success_for_actions.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_disable_service_logs_body_param.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_default_string_index_type_UnicodeCodePoint_body_param.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_input_with_all_errors.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_user_agent.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_partial_success_for_actions.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_bad_model_version_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_show_stats_and_model_version_v3_1.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_string_index_type_not_fail_v3.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_no_offset_v3_linked_entity_match.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_input_with_all_errors.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_multi_label_classify.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_not_passing_list_for_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_autodetect_lang_document_custom.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_string_index_type_explicit_fails_v3.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_custom_text_async.pyTestCustomTextAsynctest_single_label_classify.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_batch_size_over_limit_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_all_successful_passing_dict_sentiment_task.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_disable_service_logs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_all_successful_passing_dict_key_phrase_task.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding.pyTestEncodingtest_emoji_with_skin_tone_modifier.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_batch_size_over_limit.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_duplicate_ids_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_autodetect_lang_document_custom.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_empty_credential_class.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_show_stats_and_model_version.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_show_stats_and_model_version.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding_async.pyTestEncodingtest_emoji.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_dynamic_classification_async.pyTestDynamicClassificationtest_dynamic_classification_single.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_string_index_type_not_fail_v3.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_bad_document_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_disable_service_logs_body_param.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_bad_credentials.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_empty_credential_class.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_country_hint_kwarg.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_no_single_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_empty_credential_class.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_too_many_documents.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_disable_service_logs_body_param.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_mixing_inputs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_document_attribute_error_nonexistent_attribute.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_batch_size_over_limit.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_document_attribute_error_nonexistent_attribute.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_whole_batch_country_hint_and_dict_per_item_hints.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_show_stats_and_model_version.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding_async.pyTestEncodingtest_emoji_family.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_multiple_of_same_action_with_partial_results.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_whole_batch_language_hint_and_obj_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_client_passed_default_country_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_custom_text.pyTestCustomTexttest_poller_metadata.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_bad_document_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_custom_text_async.pyTestCustomTextAsynctest_poller_metadata.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_string_index_type_explicit_fails_v3.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_per_item_dont_use_country_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_bad_model_version_error_all_tasks.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_cancel_fail_terminal_state.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_single_label_classify.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_custom_text.pyTestCustomTexttest_single_label_classify_cont_token.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_show_stats_and_model_version.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_client_passed_default_language_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_passing_only_string.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_bad_document_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_autodetect_lang_document.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_document_attribute_error_no_result_attribute.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_language_kwarg_spanish.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_disable_service_logs_body_param.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_whole_batch_language_hint_and_dict_per_item_hints.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_opinion_mining_with_negated_opinion.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_bad_credentials.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_all_successful_passing_dict.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_duplicate_ids_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_all_successful_passing_text_document_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_mixing_inputs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_no_offset_v3_sentence_sentiment.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_custom_partial_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_entity_action_resolutions.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_disable_service_logs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_recognize_pii_entities_v3.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_explicit_set_string_index_type.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_whole_batch_dont_use_language_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_too_many_documents.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_too_many_documents.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_too_many_documents.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_duplicate_ids_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_whole_batch_language_hint_and_obj_per_item_hints.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_document_warnings.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_explicit_set_string_index_type_body_param.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_disable_service_logs_body_param.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_too_many_documents.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_show_stats_and_model_version.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_whole_batch_language_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_bad_credentials.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_rotate_subscription_key.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_language_kwarg_spanish.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_all_successful_passing_text_document_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_explicit_set_string_index_type_body_param.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_missing_input_records_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_bad_document_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_bad_model_version_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_bad_model_version_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_passing_none_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_too_many_documents.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_whole_batch_language_hint_and_obj_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_missing_input_records_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_explicit_set_string_index_type_body_param.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_bad_model_version_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_passing_none_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_sentiment_analysis_task_with_opinion_mining.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_dynamic_classification_async.pyTestDynamicClassificationtest_dynamic_classification.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_custom_partial_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_missing_input_records_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_not_passing_list_for_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_show_stats_and_model_version.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_not_passing_list_for_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_all_successful_passing_text_document_input_entities_task.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_empty_credential_class.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_all_successful_passing_text_document_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_custom_text.pyTestCustomTexttest_single_label_classify.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_default_string_index_type_is_UnicodeCodePoint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_user_agent.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_whole_batch_language_hint_and_obj_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_disable_service_logs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_invalid_country_hint_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_default_string_index_type_is_UnicodeCodePoint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_all_successful_passing_dict.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_client_passed_default_language_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_poller_metadata.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_input_with_some_errors.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_no_single_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_default_string_index_type_is_UnicodeCodePoint_body_param.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_all_successful_passing_string_pii_entities_task.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_document_errors.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_whole_batch_language_hint_and_dict_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_cancel_partial_results.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_invalid_language_hint_method.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_whole_batch_language_hint_and_obj_per_item_hints.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_batch_size_over_limit.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_whole_batch_language_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_abstract_summary_action_with_options.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_poller_metadata.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_string_index_type_not_fail_v3.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_string_index_type_explicit_fails_v3.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_invalid_language_hint_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_per_item_dont_use_language_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_whole_batch_dont_use_language_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_whole_batch_language_hint_and_obj_per_item_hints.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_healthcare_action.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_bad_request_on_empty_document.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_passing_none_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_document_attribute_error_nonexistent_attribute.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_recognize_custom_entities.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_document_errors.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_bad_credentials.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_user_agent.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_no_single_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_show_stats_and_model_version.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_default_string_index_type_UnicodeCodePoint_body_param.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_all_successful_passing_string_pii_entities_task.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_pass_cls.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_invalid_language_hint_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_cancel.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_bad_credentials.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_healthcare_continuation_token.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_whole_batch_language_hint_and_obj_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_explicit_set_string_index_type_body_param.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_invalid_language_hint_method.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_passing_only_string.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_invalid_language_hint_method.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_rotate_subscription_key.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_whole_batch_language_hint_and_obj_per_item_hints.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_per_item_dont_use_language_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_whole_batch_country_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_default_string_index_type_UnicodeCodePoint_body_param.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_input_with_all_errors.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_bing_id.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_bad_credentials.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_offset.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_duplicate_ids_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_string_index_type_explicit_fails_v3.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_explicit_set_string_index_type_body_param.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_passing_none_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_out_of_order_ids.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_custom_text_async.pyTestCustomTextAsynctest_recognize_custom_entities.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_passing_only_string.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_bad_document_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_duplicate_ids_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_whole_batch_language_hint_and_dict_per_item_hints.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_input_with_all_errors.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_language_kwarg_spanish.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_bad_document_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_disable_service_logs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_document_attribute_error_no_result_attribute.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_all_successful_passing_text_document_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_no_single_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_invalid_language_hint_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_too_many_documents.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_bad_document_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_whole_batch_dont_use_country_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_bad_model_version_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_rotate_subscription_key.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_user_agent.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_whole_batch_language_hint_and_dict_per_item_hints.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_invalid_language_hint_method.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_document_attribute_error_nonexistent_attribute.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_passing_none_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_missing_input_records_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_per_item_dont_use_language_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_passing_only_string.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_whole_batch_language_hint_and_dict_per_item_hints.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_invalid_language_hint_method.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_invalid_language_hint_method.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_document_errors.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_default_string_index_type_UnicodeCodePoint_body_param.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_document_attribute_error_no_result_attribute.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_bad_model_version_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_input_with_all_errors.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_output_same_order_as_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_whole_batch_language_hint_and_obj_per_item_hints.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_document_attribute_error_nonexistent_attribute.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_show_stats_and_model_version.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_user_agent.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_batch_size_over_limit_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_multiple_of_same_action.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_show_stats_and_model_version.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_client_passed_default_language_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_invalid_language_hint_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_language_kwarg_english.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_healthcare_continuation_token.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_passing_only_string.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_empty_credential_class.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_empty_credential_class.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_missing_input_records_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_whole_batch_language_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_invalid_language_hint_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_cancel_partial_results.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_pass_cls.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_categories_filter_with_domain_filter.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_bad_model_version_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_whole_batch_language_hint_and_obj_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_multiple_pages_of_results_returned_successfully.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_client_passed_default_language_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_pass_cls.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities_async.pyTestRecognizeEntitiestest_not_passing_list_for_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_document_warnings.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_per_item_dont_use_language_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_invalid_country_hint_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_duplicate_ids_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_explicit_set_string_index_type.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_phi_domain_filter.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_all_successful_passing_text_document_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_poller_metadata.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_document_warnings.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_disable_service_logs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_whole_batch_country_hint_and_dict_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_default_string_index_type_UnicodeCodePoint_body_param.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_pass_cls.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_bad_credentials.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_opinion_mining_no_mined_opinions.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases.pyTestExtractKeyPhrasestest_client_passed_default_language_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_bad_model_version_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_user_agent.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding_async.pyTestEncodingtest_diacritics_nfd.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_passing_none_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_out_of_order_ids.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_encoding.pyTestEncodingtest_diacritics_nfd.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_passing_none_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_pass_cls.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_duplicate_ids_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_whole_batch_country_hint_and_obj_per_item_hints.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_invalid_language_hint_method.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_document_warnings.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare_async.pyTestHealthtest_input_with_some_errors.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_cancel_fail_v3_1.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_passing_only_string.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_document_warnings.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_show_stats_and_model_version_multiple_tasks.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_all_successful_passing_text_document_input_entities_task.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_entities.pyTestRecognizeEntitiestest_entity_resolutions.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_country_hint_none.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities_async.pyTestRecognizeLinkedEntitiestest_explicit_set_string_index_type.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_extract_summary_action_with_options.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_out_of_order_ids.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_custom_text.pyTestCustomTexttest_recognize_custom_entities.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_empty_credential_class.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_document_errors.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_extract_summary_action_with_options.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities.pyTestRecognizePIIEntitiestest_string_index_type_explicit_fails_v3.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_input_with_some_errors.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_per_item_dont_use_language_hint.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_pii_entities_async.pyTestRecognizePIIEntitiestest_output_same_order_as_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_extract_key_phrases_async.pyTestExtractKeyPhrasestest_whole_batch_language_hint_and_obj_per_item_hints.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_recognize_linked_entities.pyTestRecognizeLinkedEntitiestest_not_passing_list_for_docs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze.pyTestAnalyzetest_extract_summary_partial_results.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_duplicate_ids_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_empty_credential_class.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_async.pyTestAnalyzeAsynctest_show_stats_and_model_version_multiple_tasks_v3_1.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_input_with_all_errors.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_passing_only_string_v3_1.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_string_index_type_explicit_fails_v3.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_bad_model_version_error.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_healthcare.pyTestHealthtest_whole_batch_language_hint_and_dict_input.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_disable_service_logs.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/mock_test_responses/action_error_with_targets_language.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/mock_test_responses/action_error_with_targets.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/mock_test_responses/action_job_failure_language.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/mock_test_responses/action_error_no_target.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/mock_test_responses/action_job_failure.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/tests/mock_test_responses/action_error_no_target_language.json
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/__init__.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/__init__.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/aio/
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_version.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_base_client.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_dict_mixin.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_user_agent.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_validate.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/py.typed
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/__init__.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_lro.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_text_analytics_client.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_check.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_policies.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_response_handlers.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_models.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_request_handlers.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/aio/
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/_version.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/_serialization.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/__init__.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/models.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/_configuration.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/_operations_mixin.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/_text_analytics_client.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/operations/
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/models/
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/_vendor.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/__init__.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/_configuration.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/_text_analytics_client.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/_patch.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/operations/
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/_vendor.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/__init__.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/_configuration.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/_text_analytics_client.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/_patch.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/operations/_text_analytics_client_operations.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/operations/__init__.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/operations/_patch.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/operations/_text_analytics_client_operations.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/operations/__init__.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/operations/_patch.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/models/__init__.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/models/_models_py3.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/models/_patch.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/models/_text_analytics_client_enums.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/aio/__init__.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/aio/_configuration.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/aio/_operations_mixin.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/aio/_text_analytics_client.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/aio/
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/operations/
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/models/
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/_vendor.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/__init__.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/_configuration.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/_text_analytics_client.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/_patch.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/aio/operations/
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/aio/_vendor.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/aio/__init__.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/aio/_configuration.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/aio/_text_analytics_client.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/aio/_patch.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/aio/operations/_text_analytics_client_operations.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/aio/operations/__init__.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/aio/operations/_patch.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/operations/_text_analytics_client_operations.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/operations/__init__.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/operations/_patch.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/models/__init__.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/models/_models_py3.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/models/_patch.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/models/_text_analytics_client_enums.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/aio/
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/operations/
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/models/
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/_vendor.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/__init__.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/_configuration.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/_text_analytics_client.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/_patch.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/aio/operations/
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/aio/_vendor.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/aio/__init__.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/aio/_configuration.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/aio/_text_analytics_client.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/aio/_patch.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/aio/operations/_text_analytics_client_operations.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/aio/operations/__init__.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/aio/operations/_patch.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/operations/_text_analytics_client_operations.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/operations/__init__.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/operations/_patch.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/models/__init__.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/models/_models_py3.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/models/_patch.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/models/_text_analytics_client_enums.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/aio/
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/operations/
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/models/
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/_vendor.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/__init__.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/_configuration.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/_text_analytics_client.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/_patch.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/aio/operations/
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/aio/_vendor.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/aio/__init__.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/aio/_configuration.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/aio/_text_analytics_client.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/aio/_patch.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/aio/operations/_text_analytics_client_operations.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/aio/operations/__init__.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/aio/operations/_patch.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/operations/_text_analytics_client_operations.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/operations/__init__.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/operations/_patch.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/models/__init__.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/models/_models_py3.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/models/_patch.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/models/_text_analytics_client_enums.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/aio/_base_client_async.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/aio/_lro_async.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/aio/__init__.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/aio/_response_handlers_async.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/aio/_text_analytics_client_async.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure_ai_textanalytics.egg-info/not-zip-safe
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure_ai_textanalytics.egg-info/dependency_links.txt
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure_ai_textanalytics.egg-info/SOURCES.txt
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure_ai_textanalytics.egg-info/requires.txt
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure_ai_textanalytics.egg-info/top_level.txt
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/azure_ai_textanalytics.egg-info/PKG-INFO
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/text_samples/
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/async_samples/
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/sample_get_detailed_diagnostics_information.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/sample_recognize_pii_entities.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/sample_recognize_entities.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/sample_single_label_classify.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/sample_authentication.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/sample_analyze_healthcare_action.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/sample_recognize_custom_entities.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/sample_extract_key_phrases.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/sample_multi_label_classify.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/sample_analyze_healthcare_entities.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/sample_analyze_sentiment_with_opinion_mining.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/sample_analyze_actions.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/sample_analyze_healthcare_entities_with_cancellation.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/README.md
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/sample_recognize_linked_entities.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/sample_abstract_summary.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/sample_alternative_document_input.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/sample_dynamic_classification.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/sample_analyze_sentiment.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/sample_extract_summary.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/sample_model_version.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/sample_detect_language.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/text_samples/custom_entities_sample.txt
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/text_samples/custom_classify_sample.txt
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_extract_key_phrases_async.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_get_detailed_diagnostics_information_async.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_analyze_sentiment_with_opinion_mining_async.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_single_label_classify_async.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_multi_label_classify_async.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_extract_summary_async.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_analyze_sentiment_async.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_analyze_healthcare_action_async.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_authentication_async.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_recognize_custom_entities_async.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_recognize_linked_entities_async.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_analyze_actions_async.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_recognize_entities_async.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_detect_language_async.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_analyze_healthcare_entities_async.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_recognize_pii_entities_async.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_dynamic_classification_async.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_abstract_summary_async.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_model_version_async.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_alternative_document_input_async.py
-Comment: 
-
-Filename: azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_analyze_healthcare_entities_with_cancellation_async.py
+Filename: azure-ai-textanalytics-5.3.0b2/azure_ai_textanalytics.egg-info/PKG-INFO
 Comment: 
 
 Zip file comment:
```

## Comparing `azure-ai-textanalytics-5.3.0b1/CHANGELOG.md` & `azure-ai-textanalytics-5.3.0b2/CHANGELOG.md`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,30 @@
 # Release History
 
+## 5.3.0b2 (2023-03-07)
+
+This version of the client library defaults to the service API version `2022-10-01-preview`.
+
+### Features Added
+
+- Added `begin_extract_summary` client method to perform extractive summarization on documents.
+- Added `begin_abstractive_summary` client method to perform abstractive summarization on documents.
+
+### Breaking Changes
+
+- Removed models `BaseResolution` and `BooleanResolution`.
+- Removed enum value `BooleanResolution` from `ResolutionKind`.
+- Renamed model `AbstractSummaryAction` to `AbstractiveSummaryAction`.
+- Renamed model `AbstractSummaryResult` to `AbstractiveSummaryResult`.
+- Removed keyword argument `autodetect_default_language` from long-running operation APIs.
+
+### Other Changes
+
+ - Improved static typing in the client library. 
+
 ## 5.3.0b1 (2022-11-17)
 
 This version of the client library defaults to the service API version `2022-10-01-preview`.
 
 ### Features Added
 - Added the Extractive Summarization feature and related models: `ExtractSummaryAction`, `ExtractSummaryResult`, and `SummarySentence`.
   Access the feature through the `begin_analyze_actions` API.
```

## Comparing `azure-ai-textanalytics-5.3.0b1/setup.py` & `azure-ai-textanalytics-5.3.0b2/setup.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/README.md` & `azure-ai-textanalytics-5.3.0b2/README.md`

 * *Files 13% similar despite different names*

```diff
@@ -26,38 +26,15 @@
 - You must have an [Azure subscription][azure_subscription] and a
   [Cognitive Services or Language service resource][ta_or_cs_resource] to use this package.
 
 #### Create a Cognitive Services or Language service resource
 
 The Language service supports both [multi-service and single-service access][multi_and_single_service].
 Create a Cognitive Services resource if you plan to access multiple cognitive services under a single endpoint/key. For Language service access only, create a Language service resource.
-
-You can create the resource using
-
-**Option 1:** [Azure Portal][azure_portal_create_ta_resource]
-
-**Option 2:** [Azure CLI][azure_cli_create_ta_resource].
-Below is an example of how you can create a Language service resource using the CLI:
-
-```bash
-# Create a new resource group to hold the Language service resource -
-# if using an existing resource group, skip this step
-az group create --name my-resource-group --location westus2
-```
-
-```bash
-# Create text analytics
-az cognitiveservices account create \
-    --name text-analytics-resource \
-    --resource-group my-resource-group \
-    --kind TextAnalytics \
-    --sku F0 \
-    --location westus2 \
-    --yes
-```
+You can create the resource using the [Azure Portal][azure_portal_create_ta_resource] or [Azure CLI][azure_cli] following the steps in [this document][azure_cli_create_ta_resource].
 
 Interaction with the service using the client library begins with a [client](#textanalyticsclient "TextAnalyticsClient").
 To create a client object, you will need the Cognitive Services or Language service `endpoint` to
 your resource and a `credential` that allows you access:
 
 ```python
 from azure.core.credentials import AzureKeyCredential
@@ -74,22 +51,36 @@
 
 Install the Azure Text Analytics client library for Python with [pip][pip]:
 
 ```bash
 pip install azure-ai-textanalytics --pre
 ```
 
+<!-- SNIPPET:sample_authentication.create_ta_client_with_key -->
+
+```python
+import os
+from azure.core.credentials import AzureKeyCredential
+from azure.ai.textanalytics import TextAnalyticsClient
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
+key = os.environ["AZURE_LANGUAGE_KEY"]
+
+text_analytics_client = TextAnalyticsClient(endpoint, AzureKeyCredential(key))
+```
+
+<!-- END SNIPPET -->
+
 > Note that `5.2.X` and newer targets the Azure Cognitive Service for Language APIs. These APIs include the text analysis and natural language processing features found in the previous versions of the Text Analytics client library.
 In addition, the service API has changed from semantic to date-based versioning. This version of the client library defaults to the latest supported API version, which currently is `2022-10-01-preview`.
 
 This table shows the relationship between SDK versions and supported API versions of the service
 
 | SDK version  | Supported API version of service  |
 | ------------ | --------------------------------- |
-| 5.3.0b1 - Latest beta release | 3.0, 3.1, 2022-05-01, 2022-10-01-preview (default) |
+| 5.3.0b2 - Latest beta release | 3.0, 3.1, 2022-05-01, 2022-10-01-preview (default) |
 | 5.2.X - Latest stable release | 3.0, 3.1, 2022-05-01 (default) |
 | 5.1.0  | 3.0, 3.1 (default) |
 | 5.0.0  | 3.0 |
 
 API version can be selected by passing the [api_version][text_analytics_client] keyword argument into the client.
 For the latest Language service features, consider selecting the most recent beta API version. For production scenarios, the latest stable version is recommended. Setting to an older version may result in reduced feature compatibility.
 
@@ -114,22 +105,28 @@
 `az cognitiveservices account keys list --name "resource-name" --resource-group "resource-group-name"`
 
 #### Create a TextAnalyticsClient with an API Key Credential
 
 Once you have the value for the API key, you can pass it as a string into an instance of [AzureKeyCredential][azure-key-credential]. Use the key as the credential parameter
 to authenticate the client:
 
+<!-- SNIPPET:sample_authentication.create_ta_client_with_key -->
+
 ```python
+import os
 from azure.core.credentials import AzureKeyCredential
 from azure.ai.textanalytics import TextAnalyticsClient
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
+key = os.environ["AZURE_LANGUAGE_KEY"]
 
-credential = AzureKeyCredential("<api_key>")
-text_analytics_client = TextAnalyticsClient(endpoint="https://<resource-name>.cognitiveservices.azure.com/", credential=credential)
+text_analytics_client = TextAnalyticsClient(endpoint, AzureKeyCredential(key))
 ```
 
+<!-- END SNIPPET -->
+
 #### Create a TextAnalyticsClient with an Azure Active Directory Credential
 
 To use an [Azure Active Directory (AAD) token credential][cognitive_authentication_aad],
 provide an instance of the desired credential type obtained from the
 [azure-identity][azure_identity_credentials] library.
 Note that regional endpoints do not support AAD authentication. Create a [custom subdomain][custom_subdomain]
 name for your resource in order to use this type of authentication.
@@ -145,22 +142,29 @@
 can be used to authenticate the client:
 
 Set the values of the client ID, tenant ID, and client secret of the AAD application as environment variables:
 AZURE_CLIENT_ID, AZURE_TENANT_ID, AZURE_CLIENT_SECRET
 
 Use the returned token credential to authenticate the client:
 
+<!-- SNIPPET:sample_authentication.create_ta_client_with_aad -->
+
 ```python
+import os
 from azure.ai.textanalytics import TextAnalyticsClient
 from azure.identity import DefaultAzureCredential
 
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
 credential = DefaultAzureCredential()
-text_analytics_client = TextAnalyticsClient(endpoint="https://<resource-name>.cognitiveservices.azure.com/", credential=credential)
+
+text_analytics_client = TextAnalyticsClient(endpoint, credential=credential)
 ```
 
+<!-- END SNIPPET -->
+
 ## Key concepts
 
 ### TextAnalyticsClient
 
 The Text Analytics client library provides a [TextAnalyticsClient][text_analytics_client] to do analysis on [batches of documents](#examples "Examples").
 It provides both synchronous and asynchronous operations to access a specific use of text analysis, such as language detection or key phrase extraction.
 
@@ -223,15 +227,14 @@
         print(f"Sentiment is {result.sentiment}")
     elif result.kind == "KeyPhraseExtraction":
         print(f"Key phrases: {result.key_phrases}")
     elif result.is_error is True:
         print(f"Document error: {result.code}, {result.message}")
 ```
 
-
 ### Long-Running Operations
 
 Long-running operations are operations which consist of an initial request sent to the service to start an operation,
 followed by polling the service at intervals to determine whether the operation has completed or failed, and if it has
 succeeded, to get the result.
 
 Methods that support healthcare analysis, custom text analysis, or multiple analyses are modeled as long-running operations.
@@ -251,257 +254,337 @@
 - [Detect Language](#detect-language "Detect language")
 - [Healthcare Entities Analysis](#healthcare-entities-analysis "Healthcare Entities Analysis")
 - [Multiple Analysis](#multiple-analysis "Multiple analysis")
 - [Custom Entity Recognition][recognize_custom_entities_sample]
 - [Custom Single Label Classification][single_label_classify_sample]
 - [Custom Multi Label Classification][multi_label_classify_sample]
 - [Extractive Summarization][extract_summary_sample]
-- [Abstractive Summarization][abstract_summary_sample]
+- [Abstractive Summarization][abstractive_summary_sample]
 - [Dynamic Classification][dynamic_classification_sample]
 
-### Analyze sentiment
+### Analyze Sentiment
 
 [analyze_sentiment][analyze_sentiment] looks at its input text and determines whether its sentiment is positive, negative, neutral or mixed. It's response includes per-sentence sentiment analysis and confidence scores.
 
+<!-- SNIPPET:sample_analyze_sentiment.analyze_sentiment -->
+
 ```python
+import os
 from azure.core.credentials import AzureKeyCredential
 from azure.ai.textanalytics import TextAnalyticsClient
 
-credential = AzureKeyCredential("<api_key>")
-endpoint="https://<resource-name>.cognitiveservices.azure.com/"
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
+key = os.environ["AZURE_LANGUAGE_KEY"]
 
-text_analytics_client = TextAnalyticsClient(endpoint, credential)
+text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))
 
 documents = [
-    "I did not like the restaurant. The food was somehow both too spicy and underseasoned. Additionally, I thought the location was too far away from the playhouse.",
-    "The restaurant was decorated beautifully. The atmosphere was unlike any other restaurant I've been to.",
-    "The food was yummy. :)"
+    """I had the best day of my life. I decided to go sky-diving and it made me appreciate my whole life so much more.
+    I developed a deep-connection with my instructor as well, and I feel as if I've made a life-long friend in her.""",
+    """This was a waste of my time. All of the views on this drop are extremely boring, all I saw was grass. 0/10 would
+    not recommend to any divers, even first timers.""",
+    """This was pretty good! The sights were ok, and I had fun with my instructors! Can't complain too much about my experience""",
+    """I only have one word for my experience: WOW!!! I can't believe I have had such a wonderful skydiving company right
+    in my backyard this whole time! I will definitely be a repeat customer, and I want to take my grandmother skydiving too,
+    I know she'll love it!"""
 ]
 
-response = text_analytics_client.analyze_sentiment(documents, language="en")
-result = [doc for doc in response if not doc.is_error]
 
-for doc in result:
+result = text_analytics_client.analyze_sentiment(documents, show_opinion_mining=True)
+docs = [doc for doc in result if not doc.is_error]
+
+print("Let's visualize the sentiment of each of these documents")
+for idx, doc in enumerate(docs):
+    print(f"Document text: {documents[idx]}")
     print(f"Overall sentiment: {doc.sentiment}")
-    print(
-        f"Scores: positive={doc.confidence_scores.positive}; "
-        f"neutral={doc.confidence_scores.neutral}; "
-        f"negative={doc.confidence_scores.negative}\n"
-    )
 ```
 
+<!-- END SNIPPET -->
+
 The returned response is a heterogeneous list of result and error objects: list[[AnalyzeSentimentResult][analyze_sentiment_result], [DocumentError][document_error]]
 
 Please refer to the service documentation for a conceptual discussion of [sentiment analysis][sentiment_analysis]. To see how to conduct more granular analysis into the opinions related to individual aspects (such as attributes of a product or service) in a text, see [here][opinion_mining_sample].
 
-### Recognize entities
+### Recognize Entities
 
 [recognize_entities][recognize_entities] recognizes and categories entities in its input text as people, places, organizations, date/time, quantities, percentages, currencies, and more.
 
+<!-- SNIPPET:sample_recognize_entities.recognize_entities -->
+
 ```python
+import os
+import typing
 from azure.core.credentials import AzureKeyCredential
 from azure.ai.textanalytics import TextAnalyticsClient
 
-credential = AzureKeyCredential("<api_key>")
-endpoint="https://<resource-name>.cognitiveservices.azure.com/"
-
-text_analytics_client = TextAnalyticsClient(endpoint, credential)
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
+key = os.environ["AZURE_LANGUAGE_KEY"]
 
-documents = [
-    """
-    Microsoft was founded by Bill Gates and Paul Allen. Its headquarters are located in Redmond. Redmond is a
-    city in King County, Washington, United States, located 15 miles east of Seattle.
-    """,
-    "Jeff bought three dozen eggs because there was a 50% discount."
+text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))
+reviews = [
+    """I work for Foo Company, and we hired Contoso for our annual founding ceremony. The food
+    was amazing and we all can't say enough good words about the quality and the level of service.""",
+    """We at the Foo Company re-hired Contoso after all of our past successes with the company.
+    Though the food was still great, I feel there has been a quality drop since their last time
+    catering for us. Is anyone else running into the same problem?""",
+    """Bar Company is over the moon about the service we received from Contoso, the best sliders ever!!!!"""
 ]
 
-response = text_analytics_client.recognize_entities(documents, language="en")
-result = [doc for doc in response if not doc.is_error]
+result = text_analytics_client.recognize_entities(reviews)
+result = [review for review in result if not review.is_error]
+organization_to_reviews: typing.Dict[str, typing.List[str]] = {}
+
+for idx, review in enumerate(result):
+    for entity in review.entities:
+        print(f"Entity '{entity.text}' has category '{entity.category}'")
+        if entity.category == 'Organization':
+            organization_to_reviews.setdefault(entity.text, [])
+            organization_to_reviews[entity.text].append(reviews[idx])
 
-for doc in result:
-    for entity in doc.entities:
-        print(f"Entity: {entity.text}")
-        print(f"...Category: {entity.category}")
-        print(f"...Confidence Score: {entity.confidence_score}")
-        print(f"...Offset: {entity.offset}")
+for organization, reviews in organization_to_reviews.items():
+    print(
+        "\n\nOrganization '{}' has left us the following review(s): {}".format(
+            organization, "\n\n".join(reviews)
+        )
+    )
 ```
 
+<!-- END SNIPPET -->
+
 The returned response is a heterogeneous list of result and error objects: list[[RecognizeEntitiesResult][recognize_entities_result], [DocumentError][document_error]]
 
 Please refer to the service documentation for a conceptual discussion of [named entity recognition][named_entity_recognition]
 and [supported types][named_entity_categories].
 
-### Recognize linked entities
+### Recognize Linked Entities
 
 [recognize_linked_entities][recognize_linked_entities] recognizes and disambiguates the identity of each entity found in its input text (for example,
 determining whether an occurrence of the word Mars refers to the planet, or to the
 Roman god of war). Recognized entities are associated with URLs to a well-known knowledge base, like Wikipedia.
 
+<!-- SNIPPET:sample_recognize_linked_entities.recognize_linked_entities -->
+
 ```python
+import os
 from azure.core.credentials import AzureKeyCredential
 from azure.ai.textanalytics import TextAnalyticsClient
 
-credential = AzureKeyCredential("<api_key>")
-endpoint="https://<resource-name>.cognitiveservices.azure.com/"
-
-text_analytics_client = TextAnalyticsClient(endpoint, credential)
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
+key = os.environ["AZURE_LANGUAGE_KEY"]
 
+text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))
 documents = [
-    "Microsoft was founded by Bill Gates and Paul Allen. Its headquarters are located in Redmond.",
-    "Easter Island, a Chilean territory, is a remote volcanic island in Polynesia."
+    """
+    Microsoft was founded by Bill Gates with some friends he met at Harvard. One of his friends,
+    Steve Ballmer, eventually became CEO after Bill Gates as well. Steve Ballmer eventually stepped
+    down as CEO of Microsoft, and was succeeded by Satya Nadella.
+    Microsoft originally moved its headquarters to Bellevue, Washington in January 1979, but is now
+    headquartered in Redmond.
+    """
 ]
 
-response = text_analytics_client.recognize_linked_entities(documents, language="en")
-result = [doc for doc in response if not doc.is_error]
+result = text_analytics_client.recognize_linked_entities(documents)
+docs = [doc for doc in result if not doc.is_error]
 
-for doc in result:
+print(
+    "Let's map each entity to it's Wikipedia article. I also want to see how many times each "
+    "entity is mentioned in a document\n\n"
+)
+entity_to_url = {}
+for doc in docs:
     for entity in doc.entities:
-        print(f"Entity: {entity.name}")
-        print(f"...URL: {entity.url}")
-        print(f"...Data Source: {entity.data_source}")
-        print("...Entity matches:")
-        for match in entity.matches:
-            print(f"......Entity match text: {match.text}")
-            print(f"......Confidence Score: {match.confidence_score}")
-            print(f"......Offset: {match.offset}")
+        print("Entity '{}' has been mentioned '{}' time(s)".format(
+            entity.name, len(entity.matches)
+        ))
+        if entity.data_source == "Wikipedia":
+            entity_to_url[entity.name] = entity.url
 ```
 
+<!-- END SNIPPET -->
+
 The returned response is a heterogeneous list of result and error objects: list[[RecognizeLinkedEntitiesResult][recognize_linked_entities_result], [DocumentError][document_error]]
 
 Please refer to the service documentation for a conceptual discussion of [entity linking][linked_entity_recognition]
 and [supported types][linked_entities_categories].
 
-### Recognize PII entities
+### Recognize PII Entities
 
 [recognize_pii_entities][recognize_pii_entities] recognizes and categorizes Personally Identifiable Information (PII) entities in its input text, such as
 Social Security Numbers, bank account information, credit card numbers, and more.
 
+<!-- SNIPPET:sample_recognize_pii_entities.recognize_pii_entities -->
+
 ```python
+import os
 from azure.core.credentials import AzureKeyCredential
 from azure.ai.textanalytics import TextAnalyticsClient
 
-credential = AzureKeyCredential("<api_key>")
-endpoint="https://<resource-name>.cognitiveservices.azure.com/"
-
-text_analytics_client = TextAnalyticsClient(endpoint, credential)
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
+key = os.environ["AZURE_LANGUAGE_KEY"]
 
+text_analytics_client = TextAnalyticsClient(
+    endpoint=endpoint, credential=AzureKeyCredential(key)
+)
 documents = [
-    """
-    We have an employee called Parker who cleans up after customers. The employee's
-    SSN is 859-98-0987, and their phone number is 555-555-5555.
-    """
+    """Parker Doe has repaid all of their loans as of 2020-04-25.
+    Their SSN is 859-98-0987. To contact them, use their phone number
+    555-555-5555. They are originally from Brazil and have Brazilian CPF number 998.214.865-68"""
 ]
-response = text_analytics_client.recognize_pii_entities(documents, language="en")
-result = [doc for doc in response if not doc.is_error]
-for idx, doc in enumerate(result):
+
+result = text_analytics_client.recognize_pii_entities(documents)
+docs = [doc for doc in result if not doc.is_error]
+
+print(
+    "Let's compare the original document with the documents after redaction. "
+    "I also want to comb through all of the entities that got redacted"
+)
+for idx, doc in enumerate(docs):
     print(f"Document text: {documents[idx]}")
     print(f"Redacted document text: {doc.redacted_text}")
     for entity in doc.entities:
-        print(f"...Entity: {entity.text}")
-        print(f"......Category: {entity.category}")
-        print(f"......Confidence Score: {entity.confidence_score}")
-        print(f"......Offset: {entity.offset}")
+        print("...Entity '{}' with category '{}' got redacted".format(
+            entity.text, entity.category
+        ))
 ```
 
+<!-- END SNIPPET -->
+
 The returned response is a heterogeneous list of result and error objects: list[[RecognizePiiEntitiesResult][recognize_pii_entities_result], [DocumentError][document_error]]
 
 Please refer to the service documentation for [supported PII entity types][pii_entity_categories].
 
 Note: The Recognize PII Entities service is available in API version v3.1 and newer.
 
-### Extract key phrases
+### Extract Key Phrases
 
 [extract_key_phrases][extract_key_phrases] determines the main talking points in its input text. For example, for the input text "The food was delicious and there were wonderful staff", the API returns: "food" and "wonderful staff".
 
+<!-- SNIPPET:sample_extract_key_phrases.extract_key_phrases -->
+
 ```python
+import os
 from azure.core.credentials import AzureKeyCredential
 from azure.ai.textanalytics import TextAnalyticsClient
 
-credential = AzureKeyCredential("<api_key>")
-endpoint="https://<resource-name>.cognitiveservices.azure.com/"
-
-text_analytics_client = TextAnalyticsClient(endpoint, credential)
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
+key = os.environ["AZURE_LANGUAGE_KEY"]
 
-documents = [
-    "Redmond is a city in King County, Washington, United States, located 15 miles east of Seattle.",
+text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))
+articles = [
+    """
+    Washington, D.C. Autumn in DC is a uniquely beautiful season. The leaves fall from the trees
+    in a city chock-full of forests, leaving yellow leaves on the ground and a clearer view of the
+    blue sky above...
+    """,
     """
-    I need to take my cat to the veterinarian. He has been sick recently, and I need to take him
-    before I travel to South America for the summer.
+    Redmond, WA. In the past few days, Microsoft has decided to further postpone the start date of
+    its United States workers, due to the pandemic that rages with no end in sight...
     """,
+    """
+    Redmond, WA. Employees at Microsoft can be excited about the new coffee shop that will open on campus
+    once workers no longer have to work remotely...
+    """
 ]
 
-response = text_analytics_client.extract_key_phrases(documents, language="en")
-result = [doc for doc in response if not doc.is_error]
-
-for doc in result:
-    print(doc.key_phrases)
+result = text_analytics_client.extract_key_phrases(articles)
+for idx, doc in enumerate(result):
+    if not doc.is_error:
+        print("Key phrases in article #{}: {}".format(
+            idx + 1,
+            ", ".join(doc.key_phrases)
+        ))
 ```
 
+<!-- END SNIPPET -->
+
 The returned response is a heterogeneous list of result and error objects: list[[ExtractKeyPhrasesResult][extract_key_phrases_result], [DocumentError][document_error]]
 
 Please refer to the service documentation for a conceptual discussion of [key phrase extraction][key_phrase_extraction].
 
-### Detect language
+### Detect Language
 
 [detect_language][detect_language] determines the language of its input text, including the confidence score of the predicted language.
 
+<!-- SNIPPET:sample_detect_language.detect_language -->
+
 ```python
+import os
 from azure.core.credentials import AzureKeyCredential
 from azure.ai.textanalytics import TextAnalyticsClient
 
-credential = AzureKeyCredential("<api_key>")
-endpoint="https://<resource-name>.cognitiveservices.azure.com/"
-
-text_analytics_client = TextAnalyticsClient(endpoint, credential)
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
+key = os.environ["AZURE_LANGUAGE_KEY"]
 
+text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))
 documents = [
     """
-    This whole document is written in English. In order for the whole document to be written
-    in English, every sentence also has to be written in English, which it is.
+    The concierge Paulette was extremely helpful. Sadly when we arrived the elevator was broken, but with Paulette's help we barely noticed this inconvenience.
+    She arranged for our baggage to be brought up to our room with no extra charge and gave us a free meal to refurbish all of the calories we lost from
+    walking up the stairs :). Can't say enough good things about my experience!
     """,
-    "Il documento scritto in italiano.",
-    "Dies ist in deutsche Sprache verfasst."
+    """
+    最近由于工作压力太大，我们决定去富酒店度假。那儿的温泉实在太舒服了，我跟我丈夫都完全恢复了工作前的青春精神！加油！
+    """
 ]
 
-response = text_analytics_client.detect_language(documents)
-result = [doc for doc in response if not doc.is_error]
+result = text_analytics_client.detect_language(documents)
+reviewed_docs = [doc for doc in result if not doc.is_error]
 
-for doc in result:
-    print(f"Language detected: {doc.primary_language.name}")
-    print(f"ISO6391 name: {doc.primary_language.iso6391_name}")
-    print(f"Confidence score: {doc.primary_language.confidence_score}\n")
+print("Let's see what language each review is in!")
+
+for idx, doc in enumerate(reviewed_docs):
+    print("Review #{} is in '{}', which has ISO639-1 name '{}'\n".format(
+        idx, doc.primary_language.name, doc.primary_language.iso6391_name
+    ))
 ```
 
+<!-- END SNIPPET -->
+
 The returned response is a heterogeneous list of result and error objects: list[[DetectLanguageResult][detect_language_result], [DocumentError][document_error]]
 
 Please refer to the service documentation for a conceptual discussion of [language detection][language_detection]
 and [language and regional support][language_and_regional_support].
 
 ### Healthcare Entities Analysis
 
 [Long-running operation](#long-running-operations) [begin_analyze_healthcare_entities][analyze_healthcare_entities] extracts entities recognized within the healthcare domain, and identifies relationships between entities within the input document and links to known sources of information in various well known databases, such as UMLS, CHV, MSH, etc.
 
+<!-- SNIPPET:sample_analyze_healthcare_entities.analyze_healthcare_entities -->
+
 ```python
+import os
+import typing
 from azure.core.credentials import AzureKeyCredential
-from azure.ai.textanalytics import TextAnalyticsClient
+from azure.ai.textanalytics import TextAnalyticsClient, HealthcareEntityRelation
 
-credential = AzureKeyCredential("<api_key>")
-endpoint="https://<resource-name>.cognitiveservices.azure.com/"
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
+key = os.environ["AZURE_LANGUAGE_KEY"]
 
-text_analytics_client = TextAnalyticsClient(endpoint, credential)
+text_analytics_client = TextAnalyticsClient(
+    endpoint=endpoint,
+    credential=AzureKeyCredential(key),
+)
 
-documents = ["Subject is taking 100mg of ibuprofen twice daily"]
+documents = [
+    """
+    Patient needs to take 100 mg of ibuprofen, and 3 mg of potassium. Also needs to take
+    10 mg of Zocor.
+    """,
+    """
+    Patient needs to take 50 mg of ibuprofen, and 2 mg of Coumadin.
+    """
+]
 
 poller = text_analytics_client.begin_analyze_healthcare_entities(documents)
 result = poller.result()
 
 docs = [doc for doc in result if not doc.is_error]
 
-print("Results of Healthcare Entities Analysis:")
-for idx, doc in enumerate(docs):
+print("Let's first visualize the outputted healthcare result:")
+for doc in docs:
     for entity in doc.entities:
         print(f"Entity: {entity.text}")
         print(f"...Normalized Text: {entity.normalized_text}")
         print(f"...Category: {entity.category}")
         print(f"...Subcategory: {entity.subcategory}")
         print(f"...Offset: {entity.offset}")
         print(f"...Confidence score: {entity.confidence_score}")
@@ -516,16 +599,25 @@
             print(f"......Certainty: {entity.assertion.certainty}")
             print(f"......Association: {entity.assertion.association}")
     for relation in doc.entity_relations:
         print(f"Relation of type: {relation.relation_type} has the following roles")
         for role in relation.roles:
             print(f"...Role '{role.name}' with entity '{role.entity.text}'")
     print("------------------------------------------")
+
+print("Now, let's get all of medication dosage relations from the documents")
+dosage_of_medication_relations = [
+    entity_relation
+    for doc in docs
+    for entity_relation in doc.entity_relations if entity_relation.relation_type == HealthcareEntityRelation.DOSAGE_OF_MEDICATION
+]
 ```
 
+<!-- END SNIPPET -->
+
 Note: Healthcare Entities Analysis is only available with API version v3.1 and newer.
 
 ### Multiple Analysis
 
 [Long-running operation](#long-running-operations) [begin_analyze_actions][analyze_actions] performs multiple analyses over one set of documents in a single request. Currently it is supported using any combination of the following Language APIs in a single request:
 
 - Entities Recognition
@@ -536,65 +628,119 @@
 - Custom Entity Recognition (API version 2022-05-01 and newer)
 - Custom Single Label Classification (API version 2022-05-01 and newer)
 - Custom Multi Label Classification (API version 2022-05-01 and newer)
 - Healthcare Entities Analysis (API version 2022-05-01 and newer)
 - Extractive Summarization (API version 2022-10-01-preview and newer)
 - Abstractive Summarization (API version 2022-10-01-preview and newer)
 
+<!-- SNIPPET:sample_analyze_actions.analyze -->
+
 ```python
+import os
 from azure.core.credentials import AzureKeyCredential
 from azure.ai.textanalytics import (
     TextAnalyticsClient,
     RecognizeEntitiesAction,
+    RecognizeLinkedEntitiesAction,
+    RecognizePiiEntitiesAction,
+    ExtractKeyPhrasesAction,
     AnalyzeSentimentAction,
 )
 
-credential = AzureKeyCredential("<api_key>")
-endpoint="https://<resource-name>.cognitiveservices.azure.com/"
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
+key = os.environ["AZURE_LANGUAGE_KEY"]
 
-text_analytics_client = TextAnalyticsClient(endpoint, credential)
+text_analytics_client = TextAnalyticsClient(
+    endpoint=endpoint,
+    credential=AzureKeyCredential(key),
+)
 
-documents = ["Microsoft was founded by Bill Gates and Paul Allen."]
+documents = [
+    'We went to Contoso Steakhouse located at midtown NYC last week for a dinner party, and we adore the spot! '
+    'They provide marvelous food and they have a great menu. The chief cook happens to be the owner (I think his name is John Doe) '
+    'and he is super nice, coming out of the kitchen and greeted us all.'
+    ,
+
+    'We enjoyed very much dining in the place! '
+    'The Sirloin steak I ordered was tender and juicy, and the place was impeccably clean. You can even pre-order from their '
+    'online menu at www.contososteakhouse.com, call 312-555-0176 or send email to order@contososteakhouse.com! '
+    'The only complaint I have is the food didn\'t come fast enough. Overall I highly recommend it!'
+]
 
 poller = text_analytics_client.begin_analyze_actions(
     documents,
     display_name="Sample Text Analysis",
     actions=[
         RecognizeEntitiesAction(),
-        AnalyzeSentimentAction()
-    ]
+        RecognizePiiEntitiesAction(),
+        ExtractKeyPhrasesAction(),
+        RecognizeLinkedEntitiesAction(),
+        AnalyzeSentimentAction(),
+    ],
 )
 
-# returns multiple actions results in the same order as the inputted actions
 document_results = poller.result()
 for doc, action_results in zip(documents, document_results):
     print(f"\nDocument text: {doc}")
     for result in action_results:
         if result.kind == "EntityRecognition":
             print("...Results of Recognize Entities Action:")
             for entity in result.entities:
                 print(f"......Entity: {entity.text}")
                 print(f".........Category: {entity.category}")
                 print(f".........Confidence Score: {entity.confidence_score}")
                 print(f".........Offset: {entity.offset}")
 
+        elif result.kind == "PiiEntityRecognition":
+            print("...Results of Recognize PII Entities action:")
+            for pii_entity in result.entities:
+                print(f"......Entity: {pii_entity.text}")
+                print(f".........Category: {pii_entity.category}")
+                print(f".........Confidence Score: {pii_entity.confidence_score}")
+
+        elif result.kind == "KeyPhraseExtraction":
+            print("...Results of Extract Key Phrases action:")
+            print(f"......Key Phrases: {result.key_phrases}")
+
+        elif result.kind == "EntityLinking":
+            print("...Results of Recognize Linked Entities action:")
+            for linked_entity in result.entities:
+                print(f"......Entity name: {linked_entity.name}")
+                print(f".........Data source: {linked_entity.data_source}")
+                print(f".........Data source language: {linked_entity.language}")
+                print(
+                    f".........Data source entity ID: {linked_entity.data_source_entity_id}"
+                )
+                print(f".........Data source URL: {linked_entity.url}")
+                print(".........Document matches:")
+                for match in linked_entity.matches:
+                    print(f"............Match text: {match.text}")
+                    print(f"............Confidence Score: {match.confidence_score}")
+                    print(f"............Offset: {match.offset}")
+                    print(f"............Length: {match.length}")
+
         elif result.kind == "SentimentAnalysis":
             print("...Results of Analyze Sentiment action:")
             print(f"......Overall sentiment: {result.sentiment}")
-            print(f"......Scores: positive={result.confidence_scores.positive}; "
-                  f"neutral={result.confidence_scores.neutral}; "
-                  f"negative={result.confidence_scores.negative}\n")
+            print(
+                f"......Scores: positive={result.confidence_scores.positive}; \
+                neutral={result.confidence_scores.neutral}; \
+                negative={result.confidence_scores.negative} \n"
+            )
 
         elif result.is_error is True:
-            print(f"......Is an error with code '{result.code}' "
-                  f"and message '{result.message}'")
+            print(
+                f"...Is an error with code '{result.error.code}' and message '{result.error.message}'"
+            )
 
     print("------------------------------------------")
 ```
 
+<!-- END SNIPPET -->
+
 The returned response is an object encapsulating multiple iterables, each representing results of individual analyses.
 
 Note: Multiple analysis is available in API version v3.1 and newer.
 
 ## Optional Configuration
 
 Optional keyword arguments can be passed in at the client and per-operation level.
@@ -666,20 +812,21 @@
 - Detect language: [sample_detect_language.py][detect_language_sample] ([async version][detect_language_sample_async])
 - Healthcare Entities Analysis: [sample_analyze_healthcare_entities.py][analyze_healthcare_entities_sample] ([async version][analyze_healthcare_entities_sample_async])
 - Multiple Analysis: [sample_analyze_actions.py][analyze_sample] ([async version][analyze_sample_async])
 - Custom Entity Recognition: [sample_recognize_custom_entities.py][recognize_custom_entities_sample] ([async_version][recognize_custom_entities_sample_async])
 - Custom Single Label Classification: [sample_single_label_classify.py][single_label_classify_sample] ([async_version][single_label_classify_sample_async])
 - Custom Multi Label Classification: [sample_multi_label_classify.py][multi_label_classify_sample] ([async_version][multi_label_classify_sample_async])
 - Extractive text summarization: [sample_extract_summary.py][extract_summary_sample] ([async version][extract_summary_sample_async])
-- Abstractive text summarization: [sample_abstract_summary.py][abstract_summary_sample] ([async version][abstract_summary_sample_async])
+- Abstractive text summarization: [sample_abstractive_summary.py][abstractive_summary_sample] ([async version][abstractive_summary_sample_async])
 - Dynamic Classification: [sample_dynamic_classification.py][dynamic_classification_sample] ([async_version][dynamic_classification_sample_async])
 
 Advanced scenarios
 
 - Opinion Mining: [sample_analyze_sentiment_with_opinion_mining.py][opinion_mining_sample] ([async_version][opinion_mining_sample_async])
+- NER resolutions: [sample_recognize_entity_resolutions.py][recognize_entity_resolutions_sample] ([async_version][recognize_entity_resolutions_sample_async])
 
 ### Additional documentation
 
 For more extensive documentation on Azure Cognitive Service for Language, see the [Language Service documentation][language_product_documentation] on docs.microsoft.com.
 
 ## Contributing
 
@@ -696,15 +843,16 @@
 [ta_ref_docs]: https://aka.ms/azsdk-python-textanalytics-ref-docs
 [ta_samples]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples
 [language_product_documentation]: https://docs.microsoft.com/azure/cognitive-services/language-service
 [azure_subscription]: https://azure.microsoft.com/free/
 [ta_or_cs_resource]: https://docs.microsoft.com/azure/cognitive-services/cognitive-services-apis-create-account?tabs=multiservice%2Cwindows
 [pip]: https://pypi.org/project/pip/
 [azure_portal_create_ta_resource]: https://ms.portal.azure.com/#create/Microsoft.CognitiveServicesTextAnalytics
-[azure_cli_create_ta_resource]: https://docs.microsoft.com/azure/cognitive-services/cognitive-services-apis-create-account-cli?tabs=windows
+[azure_cli]: https://docs.microsoft.com/cli/azure
+[azure_cli_create_ta_resource]: https://learn.microsoft.com/azure/cognitive-services/cognitive-services-apis-create-account-cli
 [multi_and_single_service]: https://docs.microsoft.com/azure/cognitive-services/cognitive-services-apis-create-account?tabs=multiservice%2Cwindows
 [azure_cli_endpoint_lookup]: https://docs.microsoft.com/cli/azure/cognitiveservices/account?view=azure-cli-latest#az-cognitiveservices-account-show
 [azure_portal_get_endpoint]: https://docs.microsoft.com/azure/cognitive-services/cognitive-services-apis-create-account?tabs=multiservice%2Cwindows#get-the-keys-for-your-resource
 [cognitive_authentication]: https://docs.microsoft.com/azure/cognitive-services/authentication
 [cognitive_authentication_api_key]: https://docs.microsoft.com/azure/cognitive-services/cognitive-services-apis-create-account?tabs=multiservice%2Cwindows#get-the-keys-for-your-resource
 [install_azure_identity]: https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/identity/azure-identity#install-the-package
 [register_aad_app]: https://docs.microsoft.com/azure/cognitive-services/authentication#assign-a-role-to-a-service-principal
@@ -736,15 +884,15 @@
 [detect_language]: https://aka.ms/azsdk-python-textanalytics-detectlanguage
 [language_detection]: https://docs.microsoft.com/azure/cognitive-services/language-service/language-detection/overview
 [language_and_regional_support]: https://docs.microsoft.com/azure/cognitive-services/language-service/language-detection/language-support
 [sentiment_analysis]: https://docs.microsoft.com/azure/cognitive-services/language-service/sentiment-opinion-mining/overview
 [key_phrase_extraction]: https://docs.microsoft.com/azure/cognitive-services/language-service/key-phrase-extraction/overview
 [linked_entities_categories]: https://aka.ms/taner
 [linked_entity_recognition]: https://docs.microsoft.com/azure/cognitive-services/language-service/entity-linking/overview
-[pii_entity_categories]: https://aka.ms/tanerpii
+[pii_entity_categories]: https://aka.ms/azsdk/language/pii
 [named_entity_recognition]: https://docs.microsoft.com/azure/cognitive-services/language-service/named-entity-recognition/overview
 [named_entity_categories]: https://aka.ms/taner
 [azure_core_ref_docs]: https://aka.ms/azsdk-python-core-policies
 [azure_core]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/core/azure-core/README.md
 [azure_identity]: https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/identity/azure-identity
 [python_logging]: https://docs.python.org/3/library/logging.html
 [sample_authentication]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_authentication.py
@@ -772,15 +920,17 @@
 [single_label_classify_sample]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_single_label_classify.py
 [single_label_classify_sample_async]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/async_samples/sample_single_label_classify_async.py
 [multi_label_classify_sample]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_multi_label_classify.py
 [multi_label_classify_sample_async]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/async_samples/sample_multi_label_classify_async.py
 [healthcare_action_sample]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_analyze_healthcare_action.py
 [extract_summary_sample]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_extract_summary.py
 [extract_summary_sample_async]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/async_samples/sample_extract_summary_async.py
-[abstract_summary_sample]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_abstract_summary.py
-[abstract_summary_sample_async]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/async_samples/sample_abstract_summary_async.py
+[abstractive_summary_sample]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_abstractive_summary.py
+[abstractive_summary_sample_async]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/async_samples/sample_abstractive_summary_async.py
 [dynamic_classification_sample]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_dynamic_classification.py
 [dynamic_classification_sample_async]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/async_samples/sample_dynamic_classification_async.py
+[recognize_entity_resolutions_sample]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_recognize_entity_resolutions.py
+[recognize_entity_resolutions_sample_async]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/async_samples/sample_recognize_entity_resolutions_async.py
 [cla]: https://cla.microsoft.com
 [code_of_conduct]: https://opensource.microsoft.com/codeofconduct/
 [coc_faq]: https://opensource.microsoft.com/codeofconduct/faq/
 [coc_contact]: mailto:opencode@microsoft.com
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## Comparing `azure-ai-textanalytics-5.3.0b1/LICENSE` & `azure-ai-textanalytics-5.3.0b2/LICENSE`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/PKG-INFO` & `azure-ai-textanalytics-5.3.0b2/PKG-INFO`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: azure-ai-textanalytics
-Version: 5.3.0b1
+Version: 5.3.0b2
 Summary: Microsoft Azure Text Analytics Client Library for Python
 Home-page: https://github.com/Azure/azure-sdk-for-python
 Author: Microsoft Corporation
 Author-email: azpysdkhelp@microsoft.com
 License: MIT License
 Keywords: azure,azure sdk,text analytics,cognitive services,natural language processing
 Classifier: Development Status :: 4 - Beta
@@ -49,38 +49,15 @@
 - You must have an [Azure subscription][azure_subscription] and a
   [Cognitive Services or Language service resource][ta_or_cs_resource] to use this package.
 
 #### Create a Cognitive Services or Language service resource
 
 The Language service supports both [multi-service and single-service access][multi_and_single_service].
 Create a Cognitive Services resource if you plan to access multiple cognitive services under a single endpoint/key. For Language service access only, create a Language service resource.
-
-You can create the resource using
-
-**Option 1:** [Azure Portal][azure_portal_create_ta_resource]
-
-**Option 2:** [Azure CLI][azure_cli_create_ta_resource].
-Below is an example of how you can create a Language service resource using the CLI:
-
-```bash
-# Create a new resource group to hold the Language service resource -
-# if using an existing resource group, skip this step
-az group create --name my-resource-group --location westus2
-```
-
-```bash
-# Create text analytics
-az cognitiveservices account create \
-    --name text-analytics-resource \
-    --resource-group my-resource-group \
-    --kind TextAnalytics \
-    --sku F0 \
-    --location westus2 \
-    --yes
-```
+You can create the resource using the [Azure Portal][azure_portal_create_ta_resource] or [Azure CLI][azure_cli] following the steps in [this document][azure_cli_create_ta_resource].
 
 Interaction with the service using the client library begins with a [client](#textanalyticsclient "TextAnalyticsClient").
 To create a client object, you will need the Cognitive Services or Language service `endpoint` to
 your resource and a `credential` that allows you access:
 
 ```python
 from azure.core.credentials import AzureKeyCredential
@@ -97,22 +74,36 @@
 
 Install the Azure Text Analytics client library for Python with [pip][pip]:
 
 ```bash
 pip install azure-ai-textanalytics --pre
 ```
 
+<!-- SNIPPET:sample_authentication.create_ta_client_with_key -->
+
+```python
+import os
+from azure.core.credentials import AzureKeyCredential
+from azure.ai.textanalytics import TextAnalyticsClient
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
+key = os.environ["AZURE_LANGUAGE_KEY"]
+
+text_analytics_client = TextAnalyticsClient(endpoint, AzureKeyCredential(key))
+```
+
+<!-- END SNIPPET -->
+
 > Note that `5.2.X` and newer targets the Azure Cognitive Service for Language APIs. These APIs include the text analysis and natural language processing features found in the previous versions of the Text Analytics client library.
 In addition, the service API has changed from semantic to date-based versioning. This version of the client library defaults to the latest supported API version, which currently is `2022-10-01-preview`.
 
 This table shows the relationship between SDK versions and supported API versions of the service
 
 | SDK version  | Supported API version of service  |
 | ------------ | --------------------------------- |
-| 5.3.0b1 - Latest beta release | 3.0, 3.1, 2022-05-01, 2022-10-01-preview (default) |
+| 5.3.0b2 - Latest beta release | 3.0, 3.1, 2022-05-01, 2022-10-01-preview (default) |
 | 5.2.X - Latest stable release | 3.0, 3.1, 2022-05-01 (default) |
 | 5.1.0  | 3.0, 3.1 (default) |
 | 5.0.0  | 3.0 |
 
 API version can be selected by passing the [api_version][text_analytics_client] keyword argument into the client.
 For the latest Language service features, consider selecting the most recent beta API version. For production scenarios, the latest stable version is recommended. Setting to an older version may result in reduced feature compatibility.
 
@@ -137,22 +128,28 @@
 `az cognitiveservices account keys list --name "resource-name" --resource-group "resource-group-name"`
 
 #### Create a TextAnalyticsClient with an API Key Credential
 
 Once you have the value for the API key, you can pass it as a string into an instance of [AzureKeyCredential][azure-key-credential]. Use the key as the credential parameter
 to authenticate the client:
 
+<!-- SNIPPET:sample_authentication.create_ta_client_with_key -->
+
 ```python
+import os
 from azure.core.credentials import AzureKeyCredential
 from azure.ai.textanalytics import TextAnalyticsClient
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
+key = os.environ["AZURE_LANGUAGE_KEY"]
 
-credential = AzureKeyCredential("<api_key>")
-text_analytics_client = TextAnalyticsClient(endpoint="https://<resource-name>.cognitiveservices.azure.com/", credential=credential)
+text_analytics_client = TextAnalyticsClient(endpoint, AzureKeyCredential(key))
 ```
 
+<!-- END SNIPPET -->
+
 #### Create a TextAnalyticsClient with an Azure Active Directory Credential
 
 To use an [Azure Active Directory (AAD) token credential][cognitive_authentication_aad],
 provide an instance of the desired credential type obtained from the
 [azure-identity][azure_identity_credentials] library.
 Note that regional endpoints do not support AAD authentication. Create a [custom subdomain][custom_subdomain]
 name for your resource in order to use this type of authentication.
@@ -168,22 +165,29 @@
 can be used to authenticate the client:
 
 Set the values of the client ID, tenant ID, and client secret of the AAD application as environment variables:
 AZURE_CLIENT_ID, AZURE_TENANT_ID, AZURE_CLIENT_SECRET
 
 Use the returned token credential to authenticate the client:
 
+<!-- SNIPPET:sample_authentication.create_ta_client_with_aad -->
+
 ```python
+import os
 from azure.ai.textanalytics import TextAnalyticsClient
 from azure.identity import DefaultAzureCredential
 
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
 credential = DefaultAzureCredential()
-text_analytics_client = TextAnalyticsClient(endpoint="https://<resource-name>.cognitiveservices.azure.com/", credential=credential)
+
+text_analytics_client = TextAnalyticsClient(endpoint, credential=credential)
 ```
 
+<!-- END SNIPPET -->
+
 ## Key concepts
 
 ### TextAnalyticsClient
 
 The Text Analytics client library provides a [TextAnalyticsClient][text_analytics_client] to do analysis on [batches of documents](#examples "Examples").
 It provides both synchronous and asynchronous operations to access a specific use of text analysis, such as language detection or key phrase extraction.
 
@@ -246,15 +250,14 @@
         print(f"Sentiment is {result.sentiment}")
     elif result.kind == "KeyPhraseExtraction":
         print(f"Key phrases: {result.key_phrases}")
     elif result.is_error is True:
         print(f"Document error: {result.code}, {result.message}")
 ```
 
-
 ### Long-Running Operations
 
 Long-running operations are operations which consist of an initial request sent to the service to start an operation,
 followed by polling the service at intervals to determine whether the operation has completed or failed, and if it has
 succeeded, to get the result.
 
 Methods that support healthcare analysis, custom text analysis, or multiple analyses are modeled as long-running operations.
@@ -274,257 +277,337 @@
 - [Detect Language](#detect-language "Detect language")
 - [Healthcare Entities Analysis](#healthcare-entities-analysis "Healthcare Entities Analysis")
 - [Multiple Analysis](#multiple-analysis "Multiple analysis")
 - [Custom Entity Recognition][recognize_custom_entities_sample]
 - [Custom Single Label Classification][single_label_classify_sample]
 - [Custom Multi Label Classification][multi_label_classify_sample]
 - [Extractive Summarization][extract_summary_sample]
-- [Abstractive Summarization][abstract_summary_sample]
+- [Abstractive Summarization][abstractive_summary_sample]
 - [Dynamic Classification][dynamic_classification_sample]
 
-### Analyze sentiment
+### Analyze Sentiment
 
 [analyze_sentiment][analyze_sentiment] looks at its input text and determines whether its sentiment is positive, negative, neutral or mixed. It's response includes per-sentence sentiment analysis and confidence scores.
 
+<!-- SNIPPET:sample_analyze_sentiment.analyze_sentiment -->
+
 ```python
+import os
 from azure.core.credentials import AzureKeyCredential
 from azure.ai.textanalytics import TextAnalyticsClient
 
-credential = AzureKeyCredential("<api_key>")
-endpoint="https://<resource-name>.cognitiveservices.azure.com/"
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
+key = os.environ["AZURE_LANGUAGE_KEY"]
 
-text_analytics_client = TextAnalyticsClient(endpoint, credential)
+text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))
 
 documents = [
-    "I did not like the restaurant. The food was somehow both too spicy and underseasoned. Additionally, I thought the location was too far away from the playhouse.",
-    "The restaurant was decorated beautifully. The atmosphere was unlike any other restaurant I've been to.",
-    "The food was yummy. :)"
+    """I had the best day of my life. I decided to go sky-diving and it made me appreciate my whole life so much more.
+    I developed a deep-connection with my instructor as well, and I feel as if I've made a life-long friend in her.""",
+    """This was a waste of my time. All of the views on this drop are extremely boring, all I saw was grass. 0/10 would
+    not recommend to any divers, even first timers.""",
+    """This was pretty good! The sights were ok, and I had fun with my instructors! Can't complain too much about my experience""",
+    """I only have one word for my experience: WOW!!! I can't believe I have had such a wonderful skydiving company right
+    in my backyard this whole time! I will definitely be a repeat customer, and I want to take my grandmother skydiving too,
+    I know she'll love it!"""
 ]
 
-response = text_analytics_client.analyze_sentiment(documents, language="en")
-result = [doc for doc in response if not doc.is_error]
 
-for doc in result:
+result = text_analytics_client.analyze_sentiment(documents, show_opinion_mining=True)
+docs = [doc for doc in result if not doc.is_error]
+
+print("Let's visualize the sentiment of each of these documents")
+for idx, doc in enumerate(docs):
+    print(f"Document text: {documents[idx]}")
     print(f"Overall sentiment: {doc.sentiment}")
-    print(
-        f"Scores: positive={doc.confidence_scores.positive}; "
-        f"neutral={doc.confidence_scores.neutral}; "
-        f"negative={doc.confidence_scores.negative}\n"
-    )
 ```
 
+<!-- END SNIPPET -->
+
 The returned response is a heterogeneous list of result and error objects: list[[AnalyzeSentimentResult][analyze_sentiment_result], [DocumentError][document_error]]
 
 Please refer to the service documentation for a conceptual discussion of [sentiment analysis][sentiment_analysis]. To see how to conduct more granular analysis into the opinions related to individual aspects (such as attributes of a product or service) in a text, see [here][opinion_mining_sample].
 
-### Recognize entities
+### Recognize Entities
 
 [recognize_entities][recognize_entities] recognizes and categories entities in its input text as people, places, organizations, date/time, quantities, percentages, currencies, and more.
 
+<!-- SNIPPET:sample_recognize_entities.recognize_entities -->
+
 ```python
+import os
+import typing
 from azure.core.credentials import AzureKeyCredential
 from azure.ai.textanalytics import TextAnalyticsClient
 
-credential = AzureKeyCredential("<api_key>")
-endpoint="https://<resource-name>.cognitiveservices.azure.com/"
-
-text_analytics_client = TextAnalyticsClient(endpoint, credential)
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
+key = os.environ["AZURE_LANGUAGE_KEY"]
 
-documents = [
-    """
-    Microsoft was founded by Bill Gates and Paul Allen. Its headquarters are located in Redmond. Redmond is a
-    city in King County, Washington, United States, located 15 miles east of Seattle.
-    """,
-    "Jeff bought three dozen eggs because there was a 50% discount."
+text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))
+reviews = [
+    """I work for Foo Company, and we hired Contoso for our annual founding ceremony. The food
+    was amazing and we all can't say enough good words about the quality and the level of service.""",
+    """We at the Foo Company re-hired Contoso after all of our past successes with the company.
+    Though the food was still great, I feel there has been a quality drop since their last time
+    catering for us. Is anyone else running into the same problem?""",
+    """Bar Company is over the moon about the service we received from Contoso, the best sliders ever!!!!"""
 ]
 
-response = text_analytics_client.recognize_entities(documents, language="en")
-result = [doc for doc in response if not doc.is_error]
+result = text_analytics_client.recognize_entities(reviews)
+result = [review for review in result if not review.is_error]
+organization_to_reviews: typing.Dict[str, typing.List[str]] = {}
+
+for idx, review in enumerate(result):
+    for entity in review.entities:
+        print(f"Entity '{entity.text}' has category '{entity.category}'")
+        if entity.category == 'Organization':
+            organization_to_reviews.setdefault(entity.text, [])
+            organization_to_reviews[entity.text].append(reviews[idx])
 
-for doc in result:
-    for entity in doc.entities:
-        print(f"Entity: {entity.text}")
-        print(f"...Category: {entity.category}")
-        print(f"...Confidence Score: {entity.confidence_score}")
-        print(f"...Offset: {entity.offset}")
+for organization, reviews in organization_to_reviews.items():
+    print(
+        "\n\nOrganization '{}' has left us the following review(s): {}".format(
+            organization, "\n\n".join(reviews)
+        )
+    )
 ```
 
+<!-- END SNIPPET -->
+
 The returned response is a heterogeneous list of result and error objects: list[[RecognizeEntitiesResult][recognize_entities_result], [DocumentError][document_error]]
 
 Please refer to the service documentation for a conceptual discussion of [named entity recognition][named_entity_recognition]
 and [supported types][named_entity_categories].
 
-### Recognize linked entities
+### Recognize Linked Entities
 
 [recognize_linked_entities][recognize_linked_entities] recognizes and disambiguates the identity of each entity found in its input text (for example,
 determining whether an occurrence of the word Mars refers to the planet, or to the
 Roman god of war). Recognized entities are associated with URLs to a well-known knowledge base, like Wikipedia.
 
+<!-- SNIPPET:sample_recognize_linked_entities.recognize_linked_entities -->
+
 ```python
+import os
 from azure.core.credentials import AzureKeyCredential
 from azure.ai.textanalytics import TextAnalyticsClient
 
-credential = AzureKeyCredential("<api_key>")
-endpoint="https://<resource-name>.cognitiveservices.azure.com/"
-
-text_analytics_client = TextAnalyticsClient(endpoint, credential)
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
+key = os.environ["AZURE_LANGUAGE_KEY"]
 
+text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))
 documents = [
-    "Microsoft was founded by Bill Gates and Paul Allen. Its headquarters are located in Redmond.",
-    "Easter Island, a Chilean territory, is a remote volcanic island in Polynesia."
+    """
+    Microsoft was founded by Bill Gates with some friends he met at Harvard. One of his friends,
+    Steve Ballmer, eventually became CEO after Bill Gates as well. Steve Ballmer eventually stepped
+    down as CEO of Microsoft, and was succeeded by Satya Nadella.
+    Microsoft originally moved its headquarters to Bellevue, Washington in January 1979, but is now
+    headquartered in Redmond.
+    """
 ]
 
-response = text_analytics_client.recognize_linked_entities(documents, language="en")
-result = [doc for doc in response if not doc.is_error]
+result = text_analytics_client.recognize_linked_entities(documents)
+docs = [doc for doc in result if not doc.is_error]
 
-for doc in result:
+print(
+    "Let's map each entity to it's Wikipedia article. I also want to see how many times each "
+    "entity is mentioned in a document\n\n"
+)
+entity_to_url = {}
+for doc in docs:
     for entity in doc.entities:
-        print(f"Entity: {entity.name}")
-        print(f"...URL: {entity.url}")
-        print(f"...Data Source: {entity.data_source}")
-        print("...Entity matches:")
-        for match in entity.matches:
-            print(f"......Entity match text: {match.text}")
-            print(f"......Confidence Score: {match.confidence_score}")
-            print(f"......Offset: {match.offset}")
+        print("Entity '{}' has been mentioned '{}' time(s)".format(
+            entity.name, len(entity.matches)
+        ))
+        if entity.data_source == "Wikipedia":
+            entity_to_url[entity.name] = entity.url
 ```
 
+<!-- END SNIPPET -->
+
 The returned response is a heterogeneous list of result and error objects: list[[RecognizeLinkedEntitiesResult][recognize_linked_entities_result], [DocumentError][document_error]]
 
 Please refer to the service documentation for a conceptual discussion of [entity linking][linked_entity_recognition]
 and [supported types][linked_entities_categories].
 
-### Recognize PII entities
+### Recognize PII Entities
 
 [recognize_pii_entities][recognize_pii_entities] recognizes and categorizes Personally Identifiable Information (PII) entities in its input text, such as
 Social Security Numbers, bank account information, credit card numbers, and more.
 
+<!-- SNIPPET:sample_recognize_pii_entities.recognize_pii_entities -->
+
 ```python
+import os
 from azure.core.credentials import AzureKeyCredential
 from azure.ai.textanalytics import TextAnalyticsClient
 
-credential = AzureKeyCredential("<api_key>")
-endpoint="https://<resource-name>.cognitiveservices.azure.com/"
-
-text_analytics_client = TextAnalyticsClient(endpoint, credential)
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
+key = os.environ["AZURE_LANGUAGE_KEY"]
 
+text_analytics_client = TextAnalyticsClient(
+    endpoint=endpoint, credential=AzureKeyCredential(key)
+)
 documents = [
-    """
-    We have an employee called Parker who cleans up after customers. The employee's
-    SSN is 859-98-0987, and their phone number is 555-555-5555.
-    """
+    """Parker Doe has repaid all of their loans as of 2020-04-25.
+    Their SSN is 859-98-0987. To contact them, use their phone number
+    555-555-5555. They are originally from Brazil and have Brazilian CPF number 998.214.865-68"""
 ]
-response = text_analytics_client.recognize_pii_entities(documents, language="en")
-result = [doc for doc in response if not doc.is_error]
-for idx, doc in enumerate(result):
+
+result = text_analytics_client.recognize_pii_entities(documents)
+docs = [doc for doc in result if not doc.is_error]
+
+print(
+    "Let's compare the original document with the documents after redaction. "
+    "I also want to comb through all of the entities that got redacted"
+)
+for idx, doc in enumerate(docs):
     print(f"Document text: {documents[idx]}")
     print(f"Redacted document text: {doc.redacted_text}")
     for entity in doc.entities:
-        print(f"...Entity: {entity.text}")
-        print(f"......Category: {entity.category}")
-        print(f"......Confidence Score: {entity.confidence_score}")
-        print(f"......Offset: {entity.offset}")
+        print("...Entity '{}' with category '{}' got redacted".format(
+            entity.text, entity.category
+        ))
 ```
 
+<!-- END SNIPPET -->
+
 The returned response is a heterogeneous list of result and error objects: list[[RecognizePiiEntitiesResult][recognize_pii_entities_result], [DocumentError][document_error]]
 
 Please refer to the service documentation for [supported PII entity types][pii_entity_categories].
 
 Note: The Recognize PII Entities service is available in API version v3.1 and newer.
 
-### Extract key phrases
+### Extract Key Phrases
 
 [extract_key_phrases][extract_key_phrases] determines the main talking points in its input text. For example, for the input text "The food was delicious and there were wonderful staff", the API returns: "food" and "wonderful staff".
 
+<!-- SNIPPET:sample_extract_key_phrases.extract_key_phrases -->
+
 ```python
+import os
 from azure.core.credentials import AzureKeyCredential
 from azure.ai.textanalytics import TextAnalyticsClient
 
-credential = AzureKeyCredential("<api_key>")
-endpoint="https://<resource-name>.cognitiveservices.azure.com/"
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
+key = os.environ["AZURE_LANGUAGE_KEY"]
 
-text_analytics_client = TextAnalyticsClient(endpoint, credential)
-
-documents = [
-    "Redmond is a city in King County, Washington, United States, located 15 miles east of Seattle.",
+text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))
+articles = [
+    """
+    Washington, D.C. Autumn in DC is a uniquely beautiful season. The leaves fall from the trees
+    in a city chock-full of forests, leaving yellow leaves on the ground and a clearer view of the
+    blue sky above...
+    """,
     """
-    I need to take my cat to the veterinarian. He has been sick recently, and I need to take him
-    before I travel to South America for the summer.
+    Redmond, WA. In the past few days, Microsoft has decided to further postpone the start date of
+    its United States workers, due to the pandemic that rages with no end in sight...
     """,
+    """
+    Redmond, WA. Employees at Microsoft can be excited about the new coffee shop that will open on campus
+    once workers no longer have to work remotely...
+    """
 ]
 
-response = text_analytics_client.extract_key_phrases(documents, language="en")
-result = [doc for doc in response if not doc.is_error]
-
-for doc in result:
-    print(doc.key_phrases)
+result = text_analytics_client.extract_key_phrases(articles)
+for idx, doc in enumerate(result):
+    if not doc.is_error:
+        print("Key phrases in article #{}: {}".format(
+            idx + 1,
+            ", ".join(doc.key_phrases)
+        ))
 ```
 
+<!-- END SNIPPET -->
+
 The returned response is a heterogeneous list of result and error objects: list[[ExtractKeyPhrasesResult][extract_key_phrases_result], [DocumentError][document_error]]
 
 Please refer to the service documentation for a conceptual discussion of [key phrase extraction][key_phrase_extraction].
 
-### Detect language
+### Detect Language
 
 [detect_language][detect_language] determines the language of its input text, including the confidence score of the predicted language.
 
+<!-- SNIPPET:sample_detect_language.detect_language -->
+
 ```python
+import os
 from azure.core.credentials import AzureKeyCredential
 from azure.ai.textanalytics import TextAnalyticsClient
 
-credential = AzureKeyCredential("<api_key>")
-endpoint="https://<resource-name>.cognitiveservices.azure.com/"
-
-text_analytics_client = TextAnalyticsClient(endpoint, credential)
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
+key = os.environ["AZURE_LANGUAGE_KEY"]
 
+text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))
 documents = [
     """
-    This whole document is written in English. In order for the whole document to be written
-    in English, every sentence also has to be written in English, which it is.
+    The concierge Paulette was extremely helpful. Sadly when we arrived the elevator was broken, but with Paulette's help we barely noticed this inconvenience.
+    She arranged for our baggage to be brought up to our room with no extra charge and gave us a free meal to refurbish all of the calories we lost from
+    walking up the stairs :). Can't say enough good things about my experience!
     """,
-    "Il documento scritto in italiano.",
-    "Dies ist in deutsche Sprache verfasst."
+    """
+    最近由于工作压力太大，我们决定去富酒店度假。那儿的温泉实在太舒服了，我跟我丈夫都完全恢复了工作前的青春精神！加油！
+    """
 ]
 
-response = text_analytics_client.detect_language(documents)
-result = [doc for doc in response if not doc.is_error]
+result = text_analytics_client.detect_language(documents)
+reviewed_docs = [doc for doc in result if not doc.is_error]
+
+print("Let's see what language each review is in!")
 
-for doc in result:
-    print(f"Language detected: {doc.primary_language.name}")
-    print(f"ISO6391 name: {doc.primary_language.iso6391_name}")
-    print(f"Confidence score: {doc.primary_language.confidence_score}\n")
+for idx, doc in enumerate(reviewed_docs):
+    print("Review #{} is in '{}', which has ISO639-1 name '{}'\n".format(
+        idx, doc.primary_language.name, doc.primary_language.iso6391_name
+    ))
 ```
 
+<!-- END SNIPPET -->
+
 The returned response is a heterogeneous list of result and error objects: list[[DetectLanguageResult][detect_language_result], [DocumentError][document_error]]
 
 Please refer to the service documentation for a conceptual discussion of [language detection][language_detection]
 and [language and regional support][language_and_regional_support].
 
 ### Healthcare Entities Analysis
 
 [Long-running operation](#long-running-operations) [begin_analyze_healthcare_entities][analyze_healthcare_entities] extracts entities recognized within the healthcare domain, and identifies relationships between entities within the input document and links to known sources of information in various well known databases, such as UMLS, CHV, MSH, etc.
 
+<!-- SNIPPET:sample_analyze_healthcare_entities.analyze_healthcare_entities -->
+
 ```python
+import os
+import typing
 from azure.core.credentials import AzureKeyCredential
-from azure.ai.textanalytics import TextAnalyticsClient
+from azure.ai.textanalytics import TextAnalyticsClient, HealthcareEntityRelation
 
-credential = AzureKeyCredential("<api_key>")
-endpoint="https://<resource-name>.cognitiveservices.azure.com/"
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
+key = os.environ["AZURE_LANGUAGE_KEY"]
 
-text_analytics_client = TextAnalyticsClient(endpoint, credential)
+text_analytics_client = TextAnalyticsClient(
+    endpoint=endpoint,
+    credential=AzureKeyCredential(key),
+)
 
-documents = ["Subject is taking 100mg of ibuprofen twice daily"]
+documents = [
+    """
+    Patient needs to take 100 mg of ibuprofen, and 3 mg of potassium. Also needs to take
+    10 mg of Zocor.
+    """,
+    """
+    Patient needs to take 50 mg of ibuprofen, and 2 mg of Coumadin.
+    """
+]
 
 poller = text_analytics_client.begin_analyze_healthcare_entities(documents)
 result = poller.result()
 
 docs = [doc for doc in result if not doc.is_error]
 
-print("Results of Healthcare Entities Analysis:")
-for idx, doc in enumerate(docs):
+print("Let's first visualize the outputted healthcare result:")
+for doc in docs:
     for entity in doc.entities:
         print(f"Entity: {entity.text}")
         print(f"...Normalized Text: {entity.normalized_text}")
         print(f"...Category: {entity.category}")
         print(f"...Subcategory: {entity.subcategory}")
         print(f"...Offset: {entity.offset}")
         print(f"...Confidence score: {entity.confidence_score}")
@@ -539,16 +622,25 @@
             print(f"......Certainty: {entity.assertion.certainty}")
             print(f"......Association: {entity.assertion.association}")
     for relation in doc.entity_relations:
         print(f"Relation of type: {relation.relation_type} has the following roles")
         for role in relation.roles:
             print(f"...Role '{role.name}' with entity '{role.entity.text}'")
     print("------------------------------------------")
+
+print("Now, let's get all of medication dosage relations from the documents")
+dosage_of_medication_relations = [
+    entity_relation
+    for doc in docs
+    for entity_relation in doc.entity_relations if entity_relation.relation_type == HealthcareEntityRelation.DOSAGE_OF_MEDICATION
+]
 ```
 
+<!-- END SNIPPET -->
+
 Note: Healthcare Entities Analysis is only available with API version v3.1 and newer.
 
 ### Multiple Analysis
 
 [Long-running operation](#long-running-operations) [begin_analyze_actions][analyze_actions] performs multiple analyses over one set of documents in a single request. Currently it is supported using any combination of the following Language APIs in a single request:
 
 - Entities Recognition
@@ -559,65 +651,119 @@
 - Custom Entity Recognition (API version 2022-05-01 and newer)
 - Custom Single Label Classification (API version 2022-05-01 and newer)
 - Custom Multi Label Classification (API version 2022-05-01 and newer)
 - Healthcare Entities Analysis (API version 2022-05-01 and newer)
 - Extractive Summarization (API version 2022-10-01-preview and newer)
 - Abstractive Summarization (API version 2022-10-01-preview and newer)
 
+<!-- SNIPPET:sample_analyze_actions.analyze -->
+
 ```python
+import os
 from azure.core.credentials import AzureKeyCredential
 from azure.ai.textanalytics import (
     TextAnalyticsClient,
     RecognizeEntitiesAction,
+    RecognizeLinkedEntitiesAction,
+    RecognizePiiEntitiesAction,
+    ExtractKeyPhrasesAction,
     AnalyzeSentimentAction,
 )
 
-credential = AzureKeyCredential("<api_key>")
-endpoint="https://<resource-name>.cognitiveservices.azure.com/"
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
+key = os.environ["AZURE_LANGUAGE_KEY"]
 
-text_analytics_client = TextAnalyticsClient(endpoint, credential)
+text_analytics_client = TextAnalyticsClient(
+    endpoint=endpoint,
+    credential=AzureKeyCredential(key),
+)
 
-documents = ["Microsoft was founded by Bill Gates and Paul Allen."]
+documents = [
+    'We went to Contoso Steakhouse located at midtown NYC last week for a dinner party, and we adore the spot! '
+    'They provide marvelous food and they have a great menu. The chief cook happens to be the owner (I think his name is John Doe) '
+    'and he is super nice, coming out of the kitchen and greeted us all.'
+    ,
+
+    'We enjoyed very much dining in the place! '
+    'The Sirloin steak I ordered was tender and juicy, and the place was impeccably clean. You can even pre-order from their '
+    'online menu at www.contososteakhouse.com, call 312-555-0176 or send email to order@contososteakhouse.com! '
+    'The only complaint I have is the food didn\'t come fast enough. Overall I highly recommend it!'
+]
 
 poller = text_analytics_client.begin_analyze_actions(
     documents,
     display_name="Sample Text Analysis",
     actions=[
         RecognizeEntitiesAction(),
-        AnalyzeSentimentAction()
-    ]
+        RecognizePiiEntitiesAction(),
+        ExtractKeyPhrasesAction(),
+        RecognizeLinkedEntitiesAction(),
+        AnalyzeSentimentAction(),
+    ],
 )
 
-# returns multiple actions results in the same order as the inputted actions
 document_results = poller.result()
 for doc, action_results in zip(documents, document_results):
     print(f"\nDocument text: {doc}")
     for result in action_results:
         if result.kind == "EntityRecognition":
             print("...Results of Recognize Entities Action:")
             for entity in result.entities:
                 print(f"......Entity: {entity.text}")
                 print(f".........Category: {entity.category}")
                 print(f".........Confidence Score: {entity.confidence_score}")
                 print(f".........Offset: {entity.offset}")
 
+        elif result.kind == "PiiEntityRecognition":
+            print("...Results of Recognize PII Entities action:")
+            for pii_entity in result.entities:
+                print(f"......Entity: {pii_entity.text}")
+                print(f".........Category: {pii_entity.category}")
+                print(f".........Confidence Score: {pii_entity.confidence_score}")
+
+        elif result.kind == "KeyPhraseExtraction":
+            print("...Results of Extract Key Phrases action:")
+            print(f"......Key Phrases: {result.key_phrases}")
+
+        elif result.kind == "EntityLinking":
+            print("...Results of Recognize Linked Entities action:")
+            for linked_entity in result.entities:
+                print(f"......Entity name: {linked_entity.name}")
+                print(f".........Data source: {linked_entity.data_source}")
+                print(f".........Data source language: {linked_entity.language}")
+                print(
+                    f".........Data source entity ID: {linked_entity.data_source_entity_id}"
+                )
+                print(f".........Data source URL: {linked_entity.url}")
+                print(".........Document matches:")
+                for match in linked_entity.matches:
+                    print(f"............Match text: {match.text}")
+                    print(f"............Confidence Score: {match.confidence_score}")
+                    print(f"............Offset: {match.offset}")
+                    print(f"............Length: {match.length}")
+
         elif result.kind == "SentimentAnalysis":
             print("...Results of Analyze Sentiment action:")
             print(f"......Overall sentiment: {result.sentiment}")
-            print(f"......Scores: positive={result.confidence_scores.positive}; "
-                  f"neutral={result.confidence_scores.neutral}; "
-                  f"negative={result.confidence_scores.negative}\n")
+            print(
+                f"......Scores: positive={result.confidence_scores.positive}; \
+                neutral={result.confidence_scores.neutral}; \
+                negative={result.confidence_scores.negative} \n"
+            )
 
         elif result.is_error is True:
-            print(f"......Is an error with code '{result.code}' "
-                  f"and message '{result.message}'")
+            print(
+                f"...Is an error with code '{result.error.code}' and message '{result.error.message}'"
+            )
 
     print("------------------------------------------")
 ```
 
+<!-- END SNIPPET -->
+
 The returned response is an object encapsulating multiple iterables, each representing results of individual analyses.
 
 Note: Multiple analysis is available in API version v3.1 and newer.
 
 ## Optional Configuration
 
 Optional keyword arguments can be passed in at the client and per-operation level.
@@ -689,20 +835,21 @@
 - Detect language: [sample_detect_language.py][detect_language_sample] ([async version][detect_language_sample_async])
 - Healthcare Entities Analysis: [sample_analyze_healthcare_entities.py][analyze_healthcare_entities_sample] ([async version][analyze_healthcare_entities_sample_async])
 - Multiple Analysis: [sample_analyze_actions.py][analyze_sample] ([async version][analyze_sample_async])
 - Custom Entity Recognition: [sample_recognize_custom_entities.py][recognize_custom_entities_sample] ([async_version][recognize_custom_entities_sample_async])
 - Custom Single Label Classification: [sample_single_label_classify.py][single_label_classify_sample] ([async_version][single_label_classify_sample_async])
 - Custom Multi Label Classification: [sample_multi_label_classify.py][multi_label_classify_sample] ([async_version][multi_label_classify_sample_async])
 - Extractive text summarization: [sample_extract_summary.py][extract_summary_sample] ([async version][extract_summary_sample_async])
-- Abstractive text summarization: [sample_abstract_summary.py][abstract_summary_sample] ([async version][abstract_summary_sample_async])
+- Abstractive text summarization: [sample_abstractive_summary.py][abstractive_summary_sample] ([async version][abstractive_summary_sample_async])
 - Dynamic Classification: [sample_dynamic_classification.py][dynamic_classification_sample] ([async_version][dynamic_classification_sample_async])
 
 Advanced scenarios
 
 - Opinion Mining: [sample_analyze_sentiment_with_opinion_mining.py][opinion_mining_sample] ([async_version][opinion_mining_sample_async])
+- NER resolutions: [sample_recognize_entity_resolutions.py][recognize_entity_resolutions_sample] ([async_version][recognize_entity_resolutions_sample_async])
 
 ### Additional documentation
 
 For more extensive documentation on Azure Cognitive Service for Language, see the [Language Service documentation][language_product_documentation] on docs.microsoft.com.
 
 ## Contributing
 
@@ -719,15 +866,16 @@
 [ta_ref_docs]: https://aka.ms/azsdk-python-textanalytics-ref-docs
 [ta_samples]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples
 [language_product_documentation]: https://docs.microsoft.com/azure/cognitive-services/language-service
 [azure_subscription]: https://azure.microsoft.com/free/
 [ta_or_cs_resource]: https://docs.microsoft.com/azure/cognitive-services/cognitive-services-apis-create-account?tabs=multiservice%2Cwindows
 [pip]: https://pypi.org/project/pip/
 [azure_portal_create_ta_resource]: https://ms.portal.azure.com/#create/Microsoft.CognitiveServicesTextAnalytics
-[azure_cli_create_ta_resource]: https://docs.microsoft.com/azure/cognitive-services/cognitive-services-apis-create-account-cli?tabs=windows
+[azure_cli]: https://docs.microsoft.com/cli/azure
+[azure_cli_create_ta_resource]: https://learn.microsoft.com/azure/cognitive-services/cognitive-services-apis-create-account-cli
 [multi_and_single_service]: https://docs.microsoft.com/azure/cognitive-services/cognitive-services-apis-create-account?tabs=multiservice%2Cwindows
 [azure_cli_endpoint_lookup]: https://docs.microsoft.com/cli/azure/cognitiveservices/account?view=azure-cli-latest#az-cognitiveservices-account-show
 [azure_portal_get_endpoint]: https://docs.microsoft.com/azure/cognitive-services/cognitive-services-apis-create-account?tabs=multiservice%2Cwindows#get-the-keys-for-your-resource
 [cognitive_authentication]: https://docs.microsoft.com/azure/cognitive-services/authentication
 [cognitive_authentication_api_key]: https://docs.microsoft.com/azure/cognitive-services/cognitive-services-apis-create-account?tabs=multiservice%2Cwindows#get-the-keys-for-your-resource
 [install_azure_identity]: https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/identity/azure-identity#install-the-package
 [register_aad_app]: https://docs.microsoft.com/azure/cognitive-services/authentication#assign-a-role-to-a-service-principal
@@ -759,15 +907,15 @@
 [detect_language]: https://aka.ms/azsdk-python-textanalytics-detectlanguage
 [language_detection]: https://docs.microsoft.com/azure/cognitive-services/language-service/language-detection/overview
 [language_and_regional_support]: https://docs.microsoft.com/azure/cognitive-services/language-service/language-detection/language-support
 [sentiment_analysis]: https://docs.microsoft.com/azure/cognitive-services/language-service/sentiment-opinion-mining/overview
 [key_phrase_extraction]: https://docs.microsoft.com/azure/cognitive-services/language-service/key-phrase-extraction/overview
 [linked_entities_categories]: https://aka.ms/taner
 [linked_entity_recognition]: https://docs.microsoft.com/azure/cognitive-services/language-service/entity-linking/overview
-[pii_entity_categories]: https://aka.ms/tanerpii
+[pii_entity_categories]: https://aka.ms/azsdk/language/pii
 [named_entity_recognition]: https://docs.microsoft.com/azure/cognitive-services/language-service/named-entity-recognition/overview
 [named_entity_categories]: https://aka.ms/taner
 [azure_core_ref_docs]: https://aka.ms/azsdk-python-core-policies
 [azure_core]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/core/azure-core/README.md
 [azure_identity]: https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/identity/azure-identity
 [python_logging]: https://docs.python.org/3/library/logging.html
 [sample_authentication]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_authentication.py
@@ -795,26 +943,49 @@
 [single_label_classify_sample]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_single_label_classify.py
 [single_label_classify_sample_async]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/async_samples/sample_single_label_classify_async.py
 [multi_label_classify_sample]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_multi_label_classify.py
 [multi_label_classify_sample_async]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/async_samples/sample_multi_label_classify_async.py
 [healthcare_action_sample]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_analyze_healthcare_action.py
 [extract_summary_sample]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_extract_summary.py
 [extract_summary_sample_async]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/async_samples/sample_extract_summary_async.py
-[abstract_summary_sample]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_abstract_summary.py
-[abstract_summary_sample_async]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/async_samples/sample_abstract_summary_async.py
+[abstractive_summary_sample]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_abstractive_summary.py
+[abstractive_summary_sample_async]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/async_samples/sample_abstractive_summary_async.py
 [dynamic_classification_sample]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_dynamic_classification.py
 [dynamic_classification_sample_async]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/async_samples/sample_dynamic_classification_async.py
+[recognize_entity_resolutions_sample]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_recognize_entity_resolutions.py
+[recognize_entity_resolutions_sample_async]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/async_samples/sample_recognize_entity_resolutions_async.py
 [cla]: https://cla.microsoft.com
 [code_of_conduct]: https://opensource.microsoft.com/codeofconduct/
 [coc_faq]: https://opensource.microsoft.com/codeofconduct/faq/
 [coc_contact]: mailto:opencode@microsoft.com
 
 
 # Release History
 
+## 5.3.0b2 (2023-03-07)
+
+This version of the client library defaults to the service API version `2022-10-01-preview`.
+
+### Features Added
+
+- Added `begin_extract_summary` client method to perform extractive summarization on documents.
+- Added `begin_abstractive_summary` client method to perform abstractive summarization on documents.
+
+### Breaking Changes
+
+- Removed models `BaseResolution` and `BooleanResolution`.
+- Removed enum value `BooleanResolution` from `ResolutionKind`.
+- Renamed model `AbstractSummaryAction` to `AbstractiveSummaryAction`.
+- Renamed model `AbstractSummaryResult` to `AbstractiveSummaryResult`.
+- Removed keyword argument `autodetect_default_language` from long-running operation APIs.
+
+### Other Changes
+
+ - Improved static typing in the client library. 
+
 ## 5.3.0b1 (2022-11-17)
 
 This version of the client library defaults to the service API version `2022-10-01-preview`.
 
 ### Features Added
 - Added the Extractive Summarization feature and related models: `ExtractSummaryAction`, `ExtractSummaryResult`, and `SummarySentence`.
   Access the feature through the `begin_analyze_actions` API.
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/test_recognize_pii_entities_async.py` & `azure-ai-textanalytics-5.3.0b2/tests/test_recognize_pii_entities_async.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/test_json_pointer.py` & `azure-ai-textanalytics-5.3.0b2/tests/test_json_pointer.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/test_detect_language.py` & `azure-ai-textanalytics-5.3.0b2/tests/test_detect_language.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/test_detect_language_async.py` & `azure-ai-textanalytics-5.3.0b2/tests/test_detect_language_async.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/test_custom_text.py` & `azure-ai-textanalytics-5.3.0b2/tests/test_custom_text.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/test_recognize_linked_entities_async.py` & `azure-ai-textanalytics-5.3.0b2/tests/test_recognize_linked_entities_async.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/test_custom_text_async.py` & `azure-ai-textanalytics-5.3.0b2/tests/test_custom_text_async.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/test_recognize_entities.py` & `azure-ai-textanalytics-5.3.0b2/tests/test_recognize_entities.py`

 * *Files 0% similar despite different names*

```diff
@@ -35,17 +35,16 @@
     @TextAnalyticsClientPreparer()
     @recorded_by_proxy
     def test_all_successful_passing_dict(self, client):
         docs = [{"id": "1", "language": "en", "text": "Microsoft was founded by Bill Gates and Paul Allen on April 4, 1975."},
                 {"id": "2", "language": "es", "text": "Microsoft fue fundado por Bill Gates y Paul Allen el 4 de abril de 1975."},
                 {"id": "3", "language": "de", "text": "Microsoft wurde am 4. April 1975 von Bill Gates und Paul Allen gegründet."}]
 
-        response = client.recognize_entities(docs, model_version="2020-02-01", show_stats=True)
+        response = client.recognize_entities(docs, show_stats=True)
         for doc in response:
-            # assert len(doc.entities) == 4 commenting out because of service error
             assert doc.id is not None
             assert doc.statistics is not None
             for entity in doc.entities:
                 assert entity.text is not None
                 assert entity.category is not None
                 assert entity.offset is not None
                 assert entity.confidence_score is not None
@@ -56,17 +55,16 @@
     def test_all_successful_passing_text_document_input(self, client):
         docs = [
             TextDocumentInput(id="1", text="Microsoft was founded by Bill Gates and Paul Allen on April 4, 1975.", language="en"),
             TextDocumentInput(id="2", text="Microsoft fue fundado por Bill Gates y Paul Allen el 4 de abril de 1975.", language="es"),
             TextDocumentInput(id="3", text="Microsoft wurde am 4. April 1975 von Bill Gates und Paul Allen gegründet.", language="de")
         ]
 
-        response = client.recognize_entities(docs, model_version="2020-02-01")
+        response = client.recognize_entities(docs)
         for doc in response:
-            # assert len(doc.entities) == 4 commenting out because of service error
             for entity in doc.entities:
                 assert entity.text is not None
                 assert entity.category is not None
                 assert entity.offset is not None
                 assert entity.confidence_score is not None
 
     @TextAnalyticsPreparer()
```

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/test_analyze_sentiment_async.py` & `azure-ai-textanalytics-5.3.0b2/tests/test_analyze_sentiment_async.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/test_recognize_pii_entities.py` & `azure-ai-textanalytics-5.3.0b2/tests/test_recognize_pii_entities.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/test_recognize_entities_async.py` & `azure-ai-textanalytics-5.3.0b2/tests/test_recognize_entities_async.py`

 * *Files 2% similar despite different names*

```diff
@@ -39,17 +39,16 @@
     @TextAnalyticsClientPreparer()
     @recorded_by_proxy_async
     async def test_all_successful_passing_dict(self, client):
         docs = [{"id": "1", "language": "en", "text": "Microsoft was founded by Bill Gates and Paul Allen on April 4, 1975."},
                 {"id": "2", "language": "es", "text": "Microsoft fue fundado por Bill Gates y Paul Allen el 4 de abril de 1975."},
                 {"id": "3", "language": "de", "text": "Microsoft wurde am 4. April 1975 von Bill Gates und Paul Allen gegründet."}]
 
-        response = await client.recognize_entities(docs, model_version="2020-02-01", show_stats=True)
+        response = await client.recognize_entities(docs, show_stats=True)
         for doc in response:
-            # assert len(doc.entities) == 4 commenting out because of service error
             assert doc.id is not None
             assert doc.statistics is not None
             for entity in doc.entities:
                 assert entity.text is not None
                 assert entity.category is not None
                 assert entity.offset is not None
                 assert entity.confidence_score is not None
@@ -60,17 +59,16 @@
     async def test_all_successful_passing_text_document_input(self, client):
         docs = [
             TextDocumentInput(id="1", text="Microsoft was founded by Bill Gates and Paul Allen on April 4, 1975.", language="en"),
             TextDocumentInput(id="2", text="Microsoft fue fundado por Bill Gates y Paul Allen el 4 de abril de 1975.", language="es"),
             TextDocumentInput(id="3", text="Microsoft wurde am 4. April 1975 von Bill Gates und Paul Allen gegründet.", language="de")
         ]
 
-        response = await client.recognize_entities(docs, model_version="2020-02-01")
+        response = await client.recognize_entities(docs)
         for doc in response:
-            # assert len(doc.entities) == 4 commenting out because of service error
             for entity in doc.entities:
                 assert entity.text is not None
                 assert entity.category is not None
                 assert entity.offset is not None
                 assert entity.confidence_score is not None
 
     @TextAnalyticsPreparer()
```

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/test_analyze_healthcare.py` & `azure-ai-textanalytics-5.3.0b2/tests/test_analyze_healthcare.py`

 * *Files 1% similar despite different names*

```diff
@@ -169,15 +169,14 @@
             assert stats['validDocumentsCount'] == 4
             assert stats['erroneousDocumentsCount'] == 1
             assert stats['transactionsCount'] == 4
 
         response = client.begin_analyze_healthcare_entities(
             docs,
             show_stats=True,
-            model_version="2021-01-11",
             polling_interval=self._interval(),
             raw_response_hook = callback,
         ).result()
 
         num_error = 0
         for doc in response:
             if doc.is_error:
```

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/test_auth_async.py` & `azure-ai-textanalytics-5.3.0b2/tests/test_auth_async.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/test_analyze.py` & `azure-ai-textanalytics-5.3.0b2/tests/test_analyze.py`

 * *Files 0% similar despite different names*

```diff
@@ -38,15 +38,15 @@
     MultiLabelClassifyAction,
     RecognizeCustomEntitiesAction,
     ClassifyDocumentResult,
     RecognizeCustomEntitiesResult,
     AnalyzeHealthcareEntitiesAction,
     ExtractSummaryAction,
     ExtractSummaryResult,
-    AbstractSummaryAction,
+    AbstractiveSummaryAction,
 )
 
 # pre-apply the client_cls positional argument so it needn't be explicitly passed below
 TextAnalyticsClientPreparer = functools.partial(_TextAnalyticsClientPreparer, TextAnalyticsClient)
 
 TextAnalyticsCustomPreparer = functools.partial(
     TextAnalyticsPreparer,
@@ -191,15 +191,15 @@
                 else:
                     food_target = sentence.mined_opinions[0].target
                     service_target = sentence.mined_opinions[1].target
                     self.validateConfidenceScores(food_target.confidence_scores)
                     assert 4 == food_target.offset
 
                     assert 'service' == service_target.text
-                    assert 'positive' == service_target.sentiment
+                    assert 'negative' == service_target.sentiment
                     assert 0.0 == service_target.confidence_scores.neutral
                     self.validateConfidenceScores(service_target.confidence_scores)
                     assert 13 == service_target.offset
 
                     food_opinion = sentence.mined_opinions[0].assessments[0]
                     service_opinion = sentence.mined_opinions[1].assessments[0]
                     self.assertOpinionsEqual(food_opinion, service_opinion)
@@ -1965,15 +1965,15 @@
             "much away, too easily, and that's a dangerous strategy at this time, Davis said in a BBC radio "
             "interview Monday morning. Johnson's resignation came Monday afternoon local time, just before the "
             "Prime Minister was due to make a scheduled statement in Parliament. This afternoon, the Prime Minister "
             "accepted the resignation of Boris Johnson as Foreign Secretary, a statement from Downing Street said."}]
 
         response = client.begin_analyze_actions(
             docs,
-            actions=[AbstractSummaryAction()],
+            actions=[AbstractiveSummaryAction()],
             show_stats=True,
             polling_interval=self._interval(),
         ).result()
 
         document_results = list(response)
 
         for document_result in document_results:
@@ -1996,15 +1996,15 @@
         actions=[
             RecognizeEntitiesAction(),
             ExtractKeyPhrasesAction(),
             RecognizePiiEntitiesAction(),
             # RecognizeLinkedEntitiesAction(),  # https://dev.azure.com/msazure/Cognitive%20Services/_workitems/edit/15859145
             AnalyzeSentimentAction(),
             AnalyzeHealthcareEntitiesAction(),  # https://dev.azure.com/msazure/Cognitive%20Services/_workitems/edit/16040765
-            ExtractSummaryAction(),
+            # ExtractSummaryAction(),  https://github.com/Azure/azure-sdk-for-python/issues/27727
         ]
         poller = client.begin_analyze_actions(
             docs,
             actions,
             polling_interval=self._interval(),
         )
 
@@ -2067,28 +2067,27 @@
                 else:
                     assert doc.detected_language.iso6391_name == "es"
 
     @pytest.mark.skipif(not is_public_cloud(), reason='Usgov and China Cloud are not supported')
     @TextAnalyticsPreparer()
     @TextAnalyticsClientPreparer()
     @recorded_by_proxy
-    def test_autodetect_with_default(self, client):
+    def test_autodetect_language(self, client):
         docs = ["hello world"]
         actions=[
             RecognizeEntitiesAction(),
             ExtractKeyPhrasesAction(),
             RecognizePiiEntitiesAction(),
             RecognizeLinkedEntitiesAction(),
             AnalyzeSentimentAction(),
         ]
         poller = client.begin_analyze_actions(
             docs,
             actions,
             language="auto",
-            autodetect_default_language="es",
             polling_interval=self._interval(),
         )
 
         result = list(poller.result())
         for res in result:
             for doc in res:
                 assert doc.detected_language.iso6391_name == "en"
```

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/test_context_manager.py` & `azure-ai-textanalytics-5.3.0b2/tests/test_context_manager.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/test_extract_key_phrases.py` & `azure-ai-textanalytics-5.3.0b2/tests/test_extract_key_phrases.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/test_extract_key_phrases_async.py` & `azure-ai-textanalytics-5.3.0b2/tests/test_extract_key_phrases_async.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/test_analyze_healthcare_async.py` & `azure-ai-textanalytics-5.3.0b2/tests/test_analyze_healthcare_async.py`

 * *Files 0% similar despite different names*

```diff
@@ -188,15 +188,14 @@
             assert stats['erroneousDocumentsCount'] == 1
             assert stats['transactionsCount'] == 4
 
         async with client:
             response = await (await client.begin_analyze_healthcare_entities(
                 docs,
                 show_stats=True,
-                model_version="2021-01-11",
                 polling_interval=self._interval(),
                 raw_response_hook=callback,
             )).result()
 
         assert response
         assert not hasattr(response, "statistics")
```

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/test_dynamic_classification_async.py` & `azure-ai-textanalytics-5.3.0b2/tests/test_dynamic_classification_async.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/conftest.py` & `azure-ai-textanalytics-5.3.0b2/tests/conftest.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/test_encoding_async.py` & `azure-ai-textanalytics-5.3.0b2/tests/test_encoding_async.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/test_analyze_sentiment.py` & `azure-ai-textanalytics-5.3.0b2/tests/test_analyze_sentiment.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/test_multiapi_async.py` & `azure-ai-textanalytics-5.3.0b2/tests/test_multiapi_async.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/test_repr.py` & `azure-ai-textanalytics-5.3.0b2/tests/test_repr.py`

 * *Files 10% similar despite different names*

```diff
@@ -24,18 +24,18 @@
     assert repr(model) == model_repr
     return model, model_repr
 
 @pytest.fixture
 def text_analytics_warning():
     model = _models.TextAnalyticsWarning(
         code="LongWordsInDocument",
-        message="The document contains very long words."
+        message="warning"
     )
     model_repr = (
-        "TextAnalyticsWarning(code=LongWordsInDocument, message=The document contains very long words.)"
+        "TextAnalyticsWarning(code=LongWordsInDocument, message=warning)"
     )
     assert repr(model) == model_repr
     return model, model_repr
 
 @pytest.fixture
 def text_analytics_error():
     model = _models.TextAnalyticsError(
@@ -67,19 +67,20 @@
 def categorized_entity():
     model = _models.CategorizedEntity(
         text="Bill Gates",
         category="Person",
         subcategory="Age",
         length=10,
         offset=0,
-        confidence_score=0.899
+        confidence_score=0.899,
+        resolutions=[]
     )
     model_repr = (
         "CategorizedEntity(text=Bill Gates, category=Person, subcategory=Age, "
-        "length=10, offset=0, confidence_score=0.899)"
+        "length=10, offset=0, confidence_score=0.899, resolutions=[])"
     )
     assert repr(model) == model_repr
     return model, model_repr
 
 """recognize PII entities models"""
 @pytest.fixture
 def pii_entity():
@@ -142,15 +143,16 @@
     assert repr(model) == model_repr
     return model, model_repr
 
 @pytest.fixture
 def target_assessment_confidence_score():
     model = _models.SentimentConfidenceScores(
         positive=0.5,
-        negative=0.5
+        negative=0.5,
+        neutral=0.0
     )
     model_repr = "SentimentConfidenceScores(positive=0.5, neutral=0.0, negative=0.5)"
     assert repr(model) == model_repr
     return model, model_repr
 
 @pytest.fixture
 def target_sentiment(target_assessment_confidence_score):
@@ -192,23 +194,23 @@
     model_repr = f"MinedOpinion(target={target_sentiment[1]}, assessments=[{assessment_sentiment[1]}])"
     assert repr(model) == model_repr
     return model, model_repr
 
 @pytest.fixture
 def sentence_sentiment(sentiment_confidence_scores, mined_opinion):
     model = _models.SentenceSentiment(
-        text="This is a sentence.",
+        text="sentence.",
         sentiment="neutral",
         confidence_scores=sentiment_confidence_scores[0],
         length=19,
         offset=0,
         mined_opinions=[mined_opinion[0]]
     )
     model_repr = (
-        "SentenceSentiment(text=This is a sentence., sentiment=neutral, confidence_scores={}, "\
+        "SentenceSentiment(text=sentence., sentiment=neutral, confidence_scores={}, "
         "length=19, offset=0, mined_opinions=[{}])".format(
             sentiment_confidence_scores[1], mined_opinion[1]
         )
     )
     assert repr(model) == model_repr
     return model, model_repr
 
@@ -216,50 +218,47 @@
 def recognize_pii_entities_result(pii_entity, text_analytics_warning, text_document_statistics):
     model = _models.RecognizePiiEntitiesResult(
         id="1",
         entities=[pii_entity[0]],
         redacted_text="***********",
         warnings=[text_analytics_warning[0]],
         statistics=text_document_statistics[0],
-        is_error=False
     )
     model_repr = "RecognizePiiEntitiesResult(id=1, entities=[{}], redacted_text=***********, warnings=[{}], " \
-    "statistics={}, is_error=False)".format(
+    "statistics={}, is_error=False, kind=PiiEntityRecognition)".format(
         pii_entity[1], text_analytics_warning[1], text_document_statistics[1]
     )
 
     assert repr(model) == model_repr
     return model, model_repr
 
 @pytest.fixture
 def recognize_entities_result(categorized_entity, text_analytics_warning, text_document_statistics):
     model = _models.RecognizeEntitiesResult(
         id="1",
         entities=[categorized_entity[0]],
         warnings=[text_analytics_warning[0]],
         statistics=text_document_statistics[0],
-        is_error=False
     )
-    model_repr = "RecognizeEntitiesResult(id=1, entities=[{}], warnings=[{}], statistics={}, is_error=False)".format(
+    model_repr = "RecognizeEntitiesResult(id=1, entities=[{}], warnings=[{}], statistics={}, is_error=False, kind=EntityRecognition)".format(
         categorized_entity[1], text_analytics_warning[1], text_document_statistics[1]
     )
 
     assert repr(model) == model_repr
     return model, model_repr
 
 @pytest.fixture
 def extract_key_phrases_result(text_analytics_warning, text_document_statistics):
     model = _models.ExtractKeyPhrasesResult(
         id="1",
         key_phrases=["dog", "cat", "bird"],
         warnings=[text_analytics_warning[0]],
         statistics=text_document_statistics[0],
-        is_error=False
     )
-    model_repr = "ExtractKeyPhrasesResult(id=1, key_phrases=['dog', 'cat', 'bird'], warnings=[{}], statistics={}, is_error=False)".format(
+    model_repr = "ExtractKeyPhrasesResult(id=1, key_phrases=['dog', 'cat', 'bird'], warnings=[{}], statistics={}, is_error=False, kind=KeyPhraseExtraction)".format(
         text_analytics_warning[1], text_document_statistics[1]
     )
 
     assert repr(model) == model_repr
     return model, model_repr
 
 @pytest.fixture
@@ -352,17 +351,16 @@
 
         assert repr(model) == model_repr
 
     def test_document_error(self, text_analytics_error):
         model = _models.DocumentError(
             id="1",
             error=text_analytics_error[0],
-            is_error=True
         )
-        model_repr = f"DocumentError(id=1, error={text_analytics_error[1]}, is_error=True)"
+        model_repr = f"DocumentError(id=1, error={text_analytics_error[1]}, is_error=True, kind=DocumentError)"
 
         assert repr(model) == model_repr
 
     def test_text_document_batch_statistics(self):
         model = _models.TextDocumentBatchStatistics(
             document_count=1,
             valid_document_count=2,
@@ -378,32 +376,30 @@
 
     def test_detect_language_result(self, detected_language, text_analytics_warning, text_document_statistics):
         model = _models.DetectLanguageResult(
             id="1",
             primary_language=detected_language[0],
             warnings=[text_analytics_warning[0]],
             statistics=text_document_statistics[0],
-            is_error=False
         )
-        model_repr = "DetectLanguageResult(id=1, primary_language={}, warnings=[{}], statistics={}, is_error=False)".format(
+        model_repr = "DetectLanguageResult(id=1, primary_language={}, warnings=[{}], statistics={}, is_error=False, kind=LanguageDetection)".format(
             detected_language[1], text_analytics_warning[1], text_document_statistics[1]
         )
 
         assert repr(model) == model_repr
 
     def test_recognized_linked_entities_result(self, linked_entity, text_analytics_warning, text_document_statistics, detected_language):
         model = _models.RecognizeLinkedEntitiesResult(
             id="1",
             entities=[linked_entity[0]],
             warnings=[text_analytics_warning[0]],
             statistics=text_document_statistics[0],
-            detected_language=detected_language[0],
-            is_error=False
+            detected_language=detected_language[0]
         )
-        model_repr = "RecognizeLinkedEntitiesResult(id=1, entities=[{}], warnings=[{}], statistics={}, detected_language={}, is_error=False)".format(
+        model_repr = "RecognizeLinkedEntitiesResult(id=1, entities=[{}], warnings=[{}], statistics={}, detected_language={}, is_error=False, kind=EntityLinking)".format(
             linked_entity[1], text_analytics_warning[1], text_document_statistics[1], detected_language[1]
         )
 
         assert repr(model) == model_repr
 
     def test_analyze_sentiment_result(
         self, text_analytics_warning, text_document_statistics, sentiment_confidence_scores, sentence_sentiment, detected_language
@@ -412,19 +408,18 @@
             id="1",
             sentiment="positive",
             warnings=[text_analytics_warning[0]],
             statistics=text_document_statistics[0],
             confidence_scores=sentiment_confidence_scores[0],
             sentences=[sentence_sentiment[0]],
             detected_language=detected_language[0],
-            is_error=False
         )
         model_repr = (
             "AnalyzeSentimentResult(id=1, sentiment=positive, warnings=[{}], statistics={}, confidence_scores={}, "
-            "sentences=[{}], detected_language={}, is_error=False)".format(
+            "sentences=[{}], detected_language={}, is_error=False, kind=SentimentAnalysis)".format(
                 text_analytics_warning[1], text_document_statistics[1], sentiment_confidence_scores[1], sentence_sentiment[1], detected_language[1]
             )
         )
 
         assert repr(model) == model_repr
 
     def test_inner_error_takes_precedence(self):
@@ -443,23 +438,23 @@
         assert error.code == "UnsupportedLanguageCode"
         assert error.message == "Supplied language not supported. Pass in one of: de,en,es,fr,it,ja,ko,nl,pt-PT,zh-Hans,zh-Hant"
 
     def test_analyze_healthcare_entities_result_item(
         self, healthcare_entity, healthcare_relation, text_analytics_warning, text_document_statistics
     ):
         model = _models.AnalyzeHealthcareEntitiesResult(
-            id=1,
+            id="1",
             entities=[healthcare_entity[0]],
             entity_relations=[healthcare_relation[0]],
             warnings=[text_analytics_warning[0]],
             statistics=text_document_statistics[0],
-            fhir_bundle="{}",
-            is_error=False
+            detected_language="",
+            fhir_bundle={},
         )
 
         model_repr = (
-            "AnalyzeHealthcareEntitiesResult(id=1, entities=[{}], entity_relations=[{}], warnings=[{}], statistics={}, fhir_bundle={}, is_error=False)".format(
+            "AnalyzeHealthcareEntitiesResult(id=1, entities=[{}], entity_relations=[{}], warnings=[{}], statistics={}, fhir_bundle={}, detected_language='en', is_error=False, kind=Healthcare)".format(
                 healthcare_entity[1], healthcare_relation[1], text_analytics_warning[1], text_document_statistics[1], "{}"
             )
         )
 
-        assert repr(model) == model_repr[:1024]
+        assert repr(model)[:1024] == model_repr[:1024]
```

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/test_encoding.py` & `azure-ai-textanalytics-5.3.0b2/tests/test_encoding.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/test_analyze_async.py` & `azure-ai-textanalytics-5.3.0b2/tests/test_analyze_async.py`

 * *Files 0% similar despite different names*

```diff
@@ -40,15 +40,15 @@
     MultiLabelClassifyAction,
     RecognizeCustomEntitiesAction,
     ClassifyDocumentResult,
     RecognizeCustomEntitiesResult,
     AnalyzeHealthcareEntitiesAction,
     ExtractSummaryAction,
     ExtractSummaryResult,
-    AbstractSummaryAction,
+    AbstractiveSummaryAction,
 )
 
 # pre-apply the client_cls positional argument so it needn't be explicitly passed below
 TextAnalyticsClientPreparer = functools.partial(_TextAnalyticsClientPreparer, TextAnalyticsClient)
 
 TextAnalyticsCustomPreparer = functools.partial(
     TextAnalyticsPreparer,
@@ -238,15 +238,15 @@
                 else:
                     food_target = sentence.mined_opinions[0].target
                     service_target = sentence.mined_opinions[1].target
                     self.validateConfidenceScores(food_target.confidence_scores)
                     assert 4 == food_target.offset
 
                     assert 'service' == service_target.text
-                    assert 'positive' == service_target.sentiment
+                    assert 'negative' == service_target.sentiment
                     assert 0.0 == service_target.confidence_scores.neutral
                     self.validateConfidenceScores(service_target.confidence_scores)
                     assert 13 == service_target.offset
 
                     food_opinion = sentence.mined_opinions[0].assessments[0]
                     service_opinion = sentence.mined_opinions[1].assessments[0]
                     self.assertOpinionsEqual(food_opinion, service_opinion)
@@ -2087,15 +2087,15 @@
             "much away, too easily, and that's a dangerous strategy at this time, Davis said in a BBC radio "
             "interview Monday morning. Johnson's resignation came Monday afternoon local time, just before the "
             "Prime Minister was due to make a scheduled statement in Parliament. This afternoon, the Prime Minister "
             "accepted the resignation of Boris Johnson as Foreign Secretary, a statement from Downing Street said."}]
 
         poller = await client.begin_analyze_actions(
             docs,
-            actions=[AbstractSummaryAction()],
+            actions=[AbstractiveSummaryAction()],
             show_stats=True,
             polling_interval=self._interval(),
         )
         document_results = await poller.result()
         async for document_result in document_results:
             for result in document_result:
                 assert result.statistics is not None
@@ -2116,15 +2116,15 @@
         actions=[
             RecognizeEntitiesAction(),
             ExtractKeyPhrasesAction(),
             RecognizePiiEntitiesAction(),
             # RecognizeLinkedEntitiesAction(),  # https://dev.azure.com/msazure/Cognitive%20Services/_workitems/edit/15859145
             AnalyzeSentimentAction(),
             AnalyzeHealthcareEntitiesAction(),  # https://dev.azure.com/msazure/Cognitive%20Services/_workitems/edit/16040765
-            ExtractSummaryAction(),
+            # ExtractSummaryAction(),  https://github.com/Azure/azure-sdk-for-python/issues/27727
         ]
         async with client:
             poller = await client.begin_analyze_actions(
                 docs,
                 actions,
                 polling_interval=self._interval(),
             )
@@ -2189,28 +2189,27 @@
                     else:
                         assert doc.detected_language.iso6391_name == "es"
 
     @pytest.mark.skipif(not is_public_cloud(), reason='Usgov and China Cloud are not supported')
     @TextAnalyticsPreparer()
     @TextAnalyticsClientPreparer()
     @recorded_by_proxy_async
-    async def test_autodetect_with_default(self, client):
+    async def test_autodetect_language(self, client):
         docs = ["hello world"]
         actions=[
             RecognizeEntitiesAction(),
             ExtractKeyPhrasesAction(),
             RecognizePiiEntitiesAction(),
             RecognizeLinkedEntitiesAction(),
             AnalyzeSentimentAction(),
         ]
         poller = await client.begin_analyze_actions(
             docs,
             actions,
             language="auto",
-            autodetect_default_language="es",
             polling_interval=self._interval(),
         )
 
         result = await poller.result()
         async for res in result:
             for doc in res:
                 assert doc.detected_language.iso6391_name == "en"
```

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/testcase.py` & `azure-ai-textanalytics-5.3.0b2/tests/testcase.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/test_dynamic_classification.py` & `azure-ai-textanalytics-5.3.0b2/tests/test_dynamic_classification.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/test_multiapi.py` & `azure-ai-textanalytics-5.3.0b2/tests/test_multiapi.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/test_context_manager_async.py` & `azure-ai-textanalytics-5.3.0b2/tests/test_context_manager_async.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/test_auth.py` & `azure-ai-textanalytics-5.3.0b2/tests/test_auth.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/test_recognize_linked_entities.py` & `azure-ai-textanalytics-5.3.0b2/tests/test_recognize_linked_entities.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/perfstress_tests/perf_detect_language.py` & `azure-ai-textanalytics-5.3.0b2/tests/perfstress_tests/perf_detect_language.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language.pyTestDetectLanguagetest_output_same_order_as_input.json` & `azure-ai-textanalytics-5.3.0b2/tests/mock_test_responses/action_error_with_targets.json`

 * *Files 20% similar despite different names*

### Pretty-printed

 * *Differences: {'replace': "OrderedDict([('jobId', '59678d1c-109e-4d93-a42f-05eb5e063525'), "*

 * *            "('lastUpdateDateTime', '2021-10-21T23:02:34Z'), ('createdDateTime', "*

 * *            "'2021-10-21T23:02:27Z'), ('expirationDateTime', '2021-10-22T23:02:27Z'), ('status', "*

 * *            "'partiallyCompleted'), ('errors', [OrderedDict([('code', 'InvalidRequest'), "*

 * *            "('message', 'Some error2'), ('target', '#/tasks/piiEntityRecognitionTasks/0')]), "*

 * *            "OrderedDict([('code', 'InvalidRequest'), ('message' […]*

```diff
@@ -1,116 +1,131 @@
 {
-    "Entries": [
+    "createdDateTime": "2021-10-21T23:02:27Z",
+    "errors": [
         {
-            "RequestBody": {
-                "analysisInput": {
-                    "documents": [
-                        {
-                            "countryHint": "US",
-                            "id": "1",
-                            "text": "one"
-                        },
-                        {
-                            "countryHint": "US",
-                            "id": "2",
-                            "text": "two"
-                        },
-                        {
-                            "countryHint": "US",
-                            "id": "3",
-                            "text": "three"
-                        },
-                        {
-                            "countryHint": "US",
-                            "id": "4",
-                            "text": "four"
-                        },
-                        {
-                            "countryHint": "US",
-                            "id": "5",
-                            "text": "five"
-                        }
-                    ]
-                },
-                "kind": "LanguageDetection",
-                "parameters": {}
+            "code": "InvalidRequest",
+            "message": "Some error2",
+            "target": "#/tasks/piiEntityRecognitionTasks/0"
+        },
+        {
+            "code": "InvalidRequest",
+            "message": "Some error5",
+            "target": "#/tasks/piiEntityRecognitionTasks/1"
+        },
+        {
+            "code": "InvalidRequest",
+            "message": "Some error0",
+            "target": "#/tasks/entityRecognitionTasks/0"
+        },
+        {
+            "code": "InvalidRequest",
+            "message": "Some error1",
+            "target": "#/tasks/keyPhraseExtractionTasks/0"
+        },
+        {
+            "code": "InvalidRequest",
+            "message": "Some error3",
+            "target": "#/tasks/entityLinkingTasks/0"
+        },
+        {
+            "code": "InvalidRequest",
+            "message": "Some error4",
+            "target": "#/tasks/sentimentAnalysisTasks/0"
+        }
+    ],
+    "expirationDateTime": "2021-10-22T23:02:27Z",
+    "jobId": "59678d1c-109e-4d93-a42f-05eb5e063525",
+    "lastUpdateDateTime": "2021-10-21T23:02:34Z",
+    "status": "partiallyCompleted",
+    "tasks": {
+        "completed": 1,
+        "entityLinkingTasks": [
+            {
+                "lastUpdateDateTime": "2021-03-03T22:39:37.1716697Z",
+                "state": "failed",
+                "taskName": "3"
+            }
+        ],
+        "entityRecognitionPiiTasks": [
+            {
+                "lastUpdateDateTime": "2021-03-03T22:39:37.1716697Z",
+                "state": "failed",
+                "taskName": "2"
             },
-            "RequestHeaders": {
-                "Accept": "application/json",
-                "Accept-Encoding": "gzip, deflate",
-                "Connection": "keep-alive",
-                "Content-Length": "330",
-                "Content-Type": "application/json",
-                "User-Agent": "azsdk-python-ai-textanalytics/5.3.0b1 Python/3.10.0 (Windows-10-10.0.22621-SP0)"
+            {
+                "lastUpdateDateTime": "2021-03-03T22:39:37.1716697Z",
+                "state": "failed",
+                "taskName": "5"
+            }
+        ],
+        "entityRecognitionTasks": [
+            {
+                "lastUpdateDateTime": "2021-03-03T22:39:37.1716697Z",
+                "state": "failed",
+                "taskName": "0"
+            }
+        ],
+        "failed": 6,
+        "inProgress": 0,
+        "keyPhraseExtractionTasks": [
+            {
+                "lastUpdateDateTime": "2021-03-03T22:39:37.1716697Z",
+                "state": "failed",
+                "taskName": "1"
+            }
+        ],
+        "sentimentAnalysisTasks": [
+            {
+                "lastUpdateDateTime": "2021-03-03T22:39:37.1716697Z",
+                "state": "failed",
+                "taskName": "4"
             },
-            "RequestMethod": "POST",
-            "RequestUri": "https://fakeendpoint.cognitiveservices.azure.com/language/:analyze-text?api-version=2022-10-01-preview",
-            "ResponseBody": {
-                "kind": "LanguageDetectionResults",
+            {
+                "lastUpdateDateTime": "2021-03-03T22:39:37.1716697Z",
                 "results": {
                     "documents": [
                         {
-                            "detectedLanguage": {
-                                "confidenceScore": 1.0,
-                                "iso6391Name": "en",
-                                "name": "English"
+                            "confidenceScores": {
+                                "negative": 0.98,
+                                "neutral": 0.02,
+                                "positive": 0.0
                             },
                             "id": "1",
+                            "sentences": [
+                                {
+                                    "confidenceScores": {
+                                        "negative": 0.98,
+                                        "neutral": 0.02,
+                                        "positive": 0.0
+                                    },
+                                    "length": 2,
+                                    "offset": 0,
+                                    "sentiment": "negative",
+                                    "text": ":("
+                                }
+                            ],
+                            "sentiment": "negative",
                             "warnings": []
-                        },
-                        {
-                            "detectedLanguage": {
-                                "confidenceScore": 1.0,
-                                "iso6391Name": "en",
-                                "name": "English"
-                            },
-                            "id": "2",
-                            "warnings": []
-                        },
-                        {
-                            "detectedLanguage": {
-                                "confidenceScore": 1.0,
-                                "iso6391Name": "en",
-                                "name": "English"
-                            },
-                            "id": "3",
-                            "warnings": []
-                        },
-                        {
-                            "detectedLanguage": {
-                                "confidenceScore": 1.0,
-                                "iso6391Name": "en",
-                                "name": "English"
-                            },
-                            "id": "4",
-                            "warnings": []
-                        },
+                        }
+                    ],
+                    "errors": [
                         {
-                            "detectedLanguage": {
-                                "confidenceScore": 1.0,
-                                "iso6391Name": "en",
-                                "name": "English"
+                            "error": {
+                                "code": "InvalidArgument",
+                                "innererror": {
+                                    "code": "InvalidDocument",
+                                    "message": "Document text is empty."
+                                },
+                                "message": "Invalid document in request."
                             },
-                            "id": "5",
-                            "warnings": []
+                            "id": "2"
                         }
                     ],
-                    "errors": [],
-                    "modelVersion": "2021-11-20"
-                }
-            },
-            "ResponseHeaders": {
-                "Content-Length": "621",
-                "Content-Type": "application/json; charset=utf-8",
-                "Date": "Mon, 17 Oct 2022 19:02:28 GMT",
-                "Set-Cookie": ".AspNetCore.Mvc.CookieTempDataProvider=; expires=Thu, 01 Jan 1970 00:00:00 GMT; path=/; samesite=lax; httponly",
-                "Strict-Transport-Security": "max-age=31536000; includeSubDomains; preload",
-                "X-Content-Type-Options": "nosniff",
-                "apim-request-id": "8af27035-0fe6-4d2d-ad98-b6dfa487c02b",
-                "csp-billing-usage": "CognitiveServices.TextAnalytics.BatchScoring=5,CognitiveServices.TextAnalytics.TextRecords=5",
-                "x-envoy-upstream-service-time": "8"
-            },
-            "StatusCode": 200
-        }
-    ],
-    "Variables": {}
+                    "modelVersion": "2020-04-01"
+                },
+                "state": "succeeded",
+                "taskName": "6"
+            }
+        ],
+        "total": 7
+    }
 }
```

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/recordings/test_detect_language_async.pyTestDetectLanguagetest_passing_only_string.json` & `azure-ai-textanalytics-5.3.0b2/tests/mock_test_responses/action_error_with_targets_language.json`

 * *Files 24% similar despite different names*

### Pretty-printed

 * *Differences: {'replace': "OrderedDict([('jobId', '59678d1c-109e-4d93-a42f-05eb5e063525'), "*

 * *            "('lastUpdateDateTime', '2021-10-21T23:02:34Z'), ('createdDateTime', "*

 * *            "'2021-10-21T23:02:27Z'), ('expirationDateTime', '2021-10-22T23:02:27Z'), ('status', "*

 * *            "'partiallySucceeded'), ('errors', [OrderedDict([('code', 'InvalidRequest'), "*

 * *            "('message', 'Some error0'), ('target', '#/tasks/items/0')]), OrderedDict([('code', "*

 * *            "'InvalidRequest'), ('message', 'Some error1'), (' […]*

```diff
@@ -1,118 +1,135 @@
 {
-    "Entries": [
+    "createdDateTime": "2021-10-21T23:02:27Z",
+    "errors": [
         {
-            "RequestBody": {
-                "analysisInput": {
-                    "documents": [
-                        {
-                            "countryHint": "US",
-                            "id": "0",
-                            "text": "I should take my cat to the veterinarian."
-                        },
-                        {
-                            "countryHint": "US",
-                            "id": "1",
-                            "text": "Este es un document escrito en Espa\u00f1ol."
-                        },
-                        {
-                            "countryHint": "US",
-                            "id": "2",
-                            "text": "\u732b\u306f\u5e78\u305b"
-                        },
-                        {
-                            "countryHint": "US",
-                            "id": "3",
-                            "text": "Fahrt nach Stuttgart und dann zum Hotel zu Fu."
-                        },
-                        {
-                            "countryHint": "US",
-                            "id": "4",
-                            "text": ""
-                        }
-                    ]
-                },
-                "kind": "LanguageDetection",
-                "parameters": {}
+            "code": "InvalidRequest",
+            "message": "Some error0",
+            "target": "#/tasks/items/0"
+        },
+        {
+            "code": "InvalidRequest",
+            "message": "Some error1",
+            "target": "#/tasks/items/1"
+        },
+        {
+            "code": "InvalidRequest",
+            "message": "Some error2",
+            "target": "#/tasks/items/2"
+        },
+        {
+            "code": "InvalidRequest",
+            "message": "Some error3",
+            "target": "#/tasks/items/3"
+        },
+        {
+            "code": "InvalidRequest",
+            "message": "Some error4",
+            "target": "#/tasks/items/4"
+        },
+        {
+            "code": "InvalidRequest",
+            "message": "Some error5",
+            "target": "#/tasks/items/5"
+        },
+        {
+            "code": "InvalidRequest",
+            "message": "Some error6",
+            "target": "#/tasks/items/6"
+        }
+    ],
+    "expirationDateTime": "2021-10-22T23:02:27Z",
+    "jobId": "59678d1c-109e-4d93-a42f-05eb5e063525",
+    "lastUpdateDateTime": "2021-10-21T23:02:34Z",
+    "status": "partiallySucceeded",
+    "tasks": {
+        "completed": 1,
+        "failed": 6,
+        "inProgress": 0,
+        "items": [
+            {
+                "kind": "EntityRecognitionLROResults",
+                "lastUpdateDateTime": "2022-04-18T19:43:22.3181205Z",
+                "status": "failed",
+                "taskName": "0"
+            },
+            {
+                "kind": "KeyPhraseExtractionLROResults",
+                "lastUpdateDateTime": "2022-04-18T19:43:09.0888005Z",
+                "status": "failed",
+                "taskName": "1"
             },
-            "RequestHeaders": {
-                "Accept": "application/json",
-                "Accept-Encoding": "gzip, deflate",
-                "Content-Length": "466",
-                "Content-Type": "application/json",
-                "User-Agent": "azsdk-python-ai-textanalytics/5.3.0b1 Python/3.10.0 (Windows-10-10.0.22621-SP0)"
-            },
-            "RequestMethod": "POST",
-            "RequestUri": "https://fakeendpoint.cognitiveservices.azure.com/language/:analyze-text?api-version=2022-10-01-preview",
-            "ResponseBody": {
-                "kind": "LanguageDetectionResults",
+            {
+                "kind": "PiiEntityRecognitionLROResults",
+                "lastUpdateDateTime": "2022-04-18T19:43:18.5390244Z",
+                "status": "failed",
+                "taskName": "2"
+            },
+            {
+                "kind": "EntityLinkingLROResults",
+                "lastUpdateDateTime": "2022-04-18T19:43:10.4261716Z",
+                "status": "failed",
+                "taskName": "3"
+            },
+            {
+                "kind": "SentimentAnalysisLROResults",
+                "lastUpdateDateTime": "2022-04-18T19:43:11.4597392Z",
+                "status": "failed",
+                "taskName": "4"
+            },
+            {
+                "kind": "PiiEntityRecognitionLROResults",
+                "lastUpdateDateTime": "2022-04-18T19:43:18.5390244Z",
+                "status": "failed",
+                "taskName": "5"
+            },
+            {
+                "kind": "SentimentAnalysisLROResults",
+                "lastUpdateDateTime": "2022-04-18T19:43:11.4597392Z",
                 "results": {
                     "documents": [
                         {
-                            "detectedLanguage": {
-                                "confidenceScore": 0.96,
-                                "iso6391Name": "en",
-                                "name": "English"
-                            },
-                            "id": "0",
-                            "warnings": []
-                        },
-                        {
-                            "detectedLanguage": {
-                                "confidenceScore": 1.0,
-                                "iso6391Name": "en",
-                                "name": "English"
+                            "confidenceScores": {
+                                "negative": 0.54,
+                                "neutral": 0.45,
+                                "positive": 0.01
                             },
                             "id": "1",
-                            "warnings": []
-                        },
-                        {
-                            "detectedLanguage": {
-                                "confidenceScore": 1.0,
-                                "iso6391Name": "ja",
-                                "name": "Japanese"
-                            },
-                            "id": "2",
-                            "warnings": []
-                        },
-                        {
-                            "detectedLanguage": {
-                                "confidenceScore": 0.99,
-                                "iso6391Name": "de",
-                                "name": "German"
-                            },
-                            "id": "3",
+                            "sentences": [
+                                {
+                                    "confidenceScores": {
+                                        "negative": 0.54,
+                                        "neutral": 0.45,
+                                        "positive": 0.01
+                                    },
+                                    "length": 38,
+                                    "offset": 0,
+                                    "sentiment": "negative",
+                                    "text": "I did not like the hotel we stayed at."
+                                }
+                            ],
+                            "sentiment": "negative",
                             "warnings": []
                         }
                     ],
                     "errors": [
                         {
                             "error": {
                                 "code": "InvalidArgument",
                                 "innererror": {
                                     "code": "InvalidDocument",
                                     "message": "Document text is empty."
                                 },
-                                "message": "Invalid Document in request."
+                                "message": "Invalid document in request."
                             },
-                            "id": "4"
+                            "id": "2"
                         }
                     ],
-                    "modelVersion": "2021-11-20"
-                }
-            },
-            "ResponseHeaders": {
-                "Content-Length": "681",
-                "Content-Type": "application/json; charset=utf-8",
-                "Date": "Mon, 17 Oct 2022 19:02:42 GMT",
-                "Set-Cookie": ".AspNetCore.Mvc.CookieTempDataProvider=; expires=Thu, 01 Jan 1970 00:00:00 GMT; path=/; samesite=lax; httponly",
-                "Strict-Transport-Security": "max-age=31536000; includeSubDomains; preload",
-                "X-Content-Type-Options": "nosniff",
-                "apim-request-id": "0068c7c1-093a-42e9-a28d-6b78e1efcae4",
-                "csp-billing-usage": "CognitiveServices.TextAnalytics.BatchScoring=4,CognitiveServices.TextAnalytics.TextRecords=4",
-                "x-envoy-upstream-service-time": "9"
-            },
-            "StatusCode": 200
-        }
-    ],
-    "Variables": {}
+                    "modelVersion": "2019-10-01"
+                },
+                "status": "succeeded",
+                "taskName": "6"
+            }
+        ],
+        "total": 7
+    }
 }
```

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment_async.pyTestAnalyzeSentimenttest_default_string_index_type_UnicodeCodePoint_body_param.json` & `azure-ai-textanalytics-5.3.0b2/tests/mock_test_responses/action_error_no_target.json`

 * *Files 26% similar despite different names*

### Pretty-printed

 * *Differences: {'replace': "OrderedDict([('jobId', '59678d1c-109e-4d93-a42f-05eb5e063525'), "*

 * *            "('lastUpdateDateTime', '2021-10-21T23:02:34Z'), ('createdDateTime', "*

 * *            "'2021-10-21T23:02:27Z'), ('expirationDateTime', '2021-10-22T23:02:27Z'), ('status', "*

 * *            "'partiallyCompleted'), ('errors', [OrderedDict([('code', 'InternalServerError'), "*

 * *            "('message', '1 out of 3 job tasks failed. Failed job tasks : "*

 * *            "v3.1/entities/general.')])]), ('tasks', OrderedDict([('completed', […]*

```diff
@@ -1,75 +1,96 @@
 {
-    "Entries": [
+    "createdDateTime": "2021-10-21T23:02:27Z",
+    "errors": [
         {
-            "RequestBody": {
-                "analysisInput": {
+            "code": "InternalServerError",
+            "message": "1 out of 3 job tasks failed. Failed job tasks : v3.1/entities/general."
+        }
+    ],
+    "expirationDateTime": "2021-10-22T23:02:27Z",
+    "jobId": "59678d1c-109e-4d93-a42f-05eb5e063525",
+    "lastUpdateDateTime": "2021-10-21T23:02:34Z",
+    "status": "partiallyCompleted",
+    "tasks": {
+        "completed": 2,
+        "entityLinkingTasks": [
+            {
+                "lastUpdateDateTime": "2022-02-10T22:55:40.7352591Z",
+                "results": {
                     "documents": [
                         {
+                            "entities": [],
+                            "id": "56",
+                            "warnings": []
+                        },
+                        {
+                            "entities": [],
                             "id": "0",
-                            "language": "en",
-                            "text": "Hello world"
+                            "warnings": []
+                        },
+                        {
+                            "entities": [],
+                            "id": "19",
+                            "warnings": []
+                        },
+                        {
+                            "entities": [],
+                            "id": "1",
+                            "warnings": []
                         }
-                    ]
+                    ],
+                    "errors": [],
+                    "modelVersion": "2021-06-01"
                 },
-                "kind": "SentimentAnalysis",
-                "parameters": {
-                    "stringIndexType": "UnicodeCodePoint"
-                }
-            },
-            "RequestHeaders": {
-                "Accept": "application/json",
-                "Accept-Encoding": "gzip, deflate",
-                "Content-Length": "172",
-                "Content-Type": "application/json",
-                "User-Agent": "azsdk-python-ai-textanalytics/5.3.0b1 Python/3.10.0 (Windows-10-10.0.22621-SP0)"
-            },
-            "RequestMethod": "POST",
-            "RequestUri": "https://fakeendpoint.cognitiveservices.azure.com/language/:analyze-text?api-version=2022-05-01",
-            "ResponseBody": {
-                "kind": "SentimentAnalysisResults",
+                "state": "succeeded",
+                "taskName": "3"
+            }
+        ],
+        "entityRecognitionPiiTasks": [
+            {
+                "lastUpdateDateTime": "2022-02-10T22:55:42.4490584Z",
                 "results": {
                     "documents": [
                         {
-                            "confidenceScores": {
-                                "negative": 0.01,
-                                "neutral": 0.74,
-                                "positive": 0.25
-                            },
+                            "entities": [],
+                            "id": "56",
+                            "redactedText": ":)",
+                            "warnings": []
+                        },
+                        {
+                            "entities": [],
                             "id": "0",
-                            "sentences": [
-                                {
-                                    "confidenceScores": {
-                                        "negative": 0.01,
-                                        "neutral": 0.74,
-                                        "positive": 0.25
-                                    },
-                                    "length": 11,
-                                    "offset": 0,
-                                    "sentiment": "neutral",
-                                    "text": "Hello world"
-                                }
-                            ],
-                            "sentiment": "neutral",
+                            "redactedText": ":(",
+                            "warnings": []
+                        },
+                        {
+                            "entities": [],
+                            "id": "19",
+                            "redactedText": ":P",
+                            "warnings": []
+                        },
+                        {
+                            "entities": [],
+                            "id": "1",
+                            "redactedText": ":D",
                             "warnings": []
                         }
                     ],
                     "errors": [],
-                    "modelVersion": "2022-10-01"
-                }
-            },
-            "ResponseHeaders": {
-                "Content-Length": "366",
-                "Content-Type": "application/json; charset=utf-8",
-                "Date": "Mon, 17 Oct 2022 19:00:54 GMT",
-                "Set-Cookie": ".AspNetCore.Mvc.CookieTempDataProvider=; expires=Thu, 01 Jan 1970 00:00:00 GMT; path=/; samesite=lax; httponly",
-                "Strict-Transport-Security": "max-age=31536000; includeSubDomains; preload",
-                "X-Content-Type-Options": "nosniff",
-                "apim-request-id": "c4420aef-ac19-4fda-8d4a-37f453abaaee",
-                "csp-billing-usage": "CognitiveServices.TextAnalytics.BatchScoring=1,CognitiveServices.TextAnalytics.TextRecords=1",
-                "x-envoy-upstream-service-time": "17"
-            },
-            "StatusCode": 200
-        }
-    ],
-    "Variables": {}
+                    "modelVersion": "2021-01-15"
+                },
+                "state": "succeeded",
+                "taskName": "2"
+            }
+        ],
+        "entityRecognitionTasks": [
+            {
+                "lastUpdateDateTime": "2021-03-03T22:39:37.1716697Z",
+                "state": "failed",
+                "taskName": "0"
+            }
+        ],
+        "failed": 1,
+        "inProgress": 0,
+        "total": 3
+    }
 }
```

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/recordings/test_analyze_sentiment.pyTestAnalyzeSentimenttest_output_same_order_as_input.json` & `azure-ai-textanalytics-5.3.0b2/tests/mock_test_responses/action_error_no_target_language.json`

 * *Files 23% similar despite different names*

### Pretty-printed

 * *Differences: {'replace': "OrderedDict([('jobId', '5e7abd01-cb10-4232-afde-dfcf0eb01dbc'), "*

 * *            "('lastUpdateDateTime', '2022-04-18T19:43:22Z'), ('createdDateTime', "*

 * *            "'2022-04-18T19:43:08Z'), ('expirationDateTime', '2022-04-19T19:43:08Z'), ('status', "*

 * *            "'partiallySucceeded'), ('errors', [OrderedDict([('code', 'InternalServerError'), "*

 * *            "('message', '1 out of 5 job tasks failed. Failed job tasks : "*

 * *            "keyphrasescomposite.')])]), ('tasks', OrderedDict([('completed', 4 […]*

```diff
@@ -1,188 +1,122 @@
 {
-    "Entries": [
+    "createdDateTime": "2022-04-18T19:43:08Z",
+    "errors": [
         {
-            "RequestBody": {
-                "analysisInput": {
+            "code": "InternalServerError",
+            "message": "1 out of 5 job tasks failed. Failed job tasks : keyphrasescomposite."
+        }
+    ],
+    "expirationDateTime": "2022-04-19T19:43:08Z",
+    "jobId": "5e7abd01-cb10-4232-afde-dfcf0eb01dbc",
+    "lastUpdateDateTime": "2022-04-18T19:43:22Z",
+    "status": "partiallySucceeded",
+    "tasks": {
+        "completed": 4,
+        "failed": 1,
+        "inProgress": 0,
+        "items": [
+            {
+                "kind": "EntityRecognitionLROResults",
+                "lastUpdateDateTime": "2022-04-18T19:43:22.3181205Z",
+                "results": {
                     "documents": [
                         {
+                            "entities": [
+                                {
+                                    "category": "Location",
+                                    "confidenceScore": 0.76,
+                                    "length": 5,
+                                    "offset": 19,
+                                    "text": "hotel"
+                                }
+                            ],
                             "id": "1",
-                            "language": "en",
-                            "text": "one"
-                        },
-                        {
-                            "id": "2",
-                            "language": "en",
-                            "text": "two"
-                        },
-                        {
-                            "id": "3",
-                            "language": "en",
-                            "text": "three"
-                        },
-                        {
-                            "id": "4",
-                            "language": "en",
-                            "text": "four"
-                        },
-                        {
-                            "id": "5",
-                            "language": "en",
-                            "text": "five"
+                            "warnings": []
                         }
-                    ]
+                    ],
+                    "errors": [],
+                    "modelVersion": "2020-04-01"
                 },
-                "kind": "SentimentAnalysis",
-                "parameters": {
-                    "stringIndexType": "UnicodeCodePoint"
-                }
+                "status": "succeeded",
+                "taskName": "0"
             },
-            "RequestHeaders": {
-                "Accept": "application/json",
-                "Accept-Encoding": "gzip, deflate",
-                "Connection": "keep-alive",
-                "Content-Length": "352",
-                "Content-Type": "application/json",
-                "User-Agent": "azsdk-python-ai-textanalytics/5.3.0b1 Python/3.10.0 (Windows-10-10.0.22621-SP0)"
+            {
+                "kind": "KeyPhraseExtractionLROResults",
+                "lastUpdateDateTime": "2022-04-18T19:43:09.0888005Z",
+                "status": "failed",
+                "taskName": "1"
             },
-            "RequestMethod": "POST",
-            "RequestUri": "https://fakeendpoint.cognitiveservices.azure.com/language/:analyze-text?api-version=2022-10-01-preview",
-            "ResponseBody": {
-                "kind": "SentimentAnalysisResults",
+            {
+                "kind": "PiiEntityRecognitionLROResults",
+                "lastUpdateDateTime": "2022-04-18T19:43:18.5390244Z",
                 "results": {
                     "documents": [
                         {
-                            "confidenceScores": {
-                                "negative": 0.0,
-                                "neutral": 0.95,
-                                "positive": 0.05
-                            },
+                            "entities": [],
                             "id": "1",
-                            "sentences": [
-                                {
-                                    "confidenceScores": {
-                                        "negative": 0.0,
-                                        "neutral": 0.95,
-                                        "positive": 0.05
-                                    },
-                                    "length": 3,
-                                    "offset": 0,
-                                    "sentiment": "neutral",
-                                    "text": "one"
-                                }
-                            ],
-                            "sentiment": "neutral",
+                            "redactedText": "I did not like the hotel we stayed at.",
                             "warnings": []
-                        },
-                        {
-                            "confidenceScores": {
-                                "negative": 0.01,
-                                "neutral": 0.96,
-                                "positive": 0.04
-                            },
-                            "id": "2",
-                            "sentences": [
-                                {
-                                    "confidenceScores": {
-                                        "negative": 0.01,
-                                        "neutral": 0.96,
-                                        "positive": 0.04
-                                    },
-                                    "length": 3,
-                                    "offset": 0,
-                                    "sentiment": "neutral",
-                                    "text": "two"
-                                }
-                            ],
-                            "sentiment": "neutral",
-                            "warnings": []
-                        },
+                        }
+                    ],
+                    "errors": [],
+                    "modelVersion": "2021-01-15"
+                },
+                "status": "succeeded",
+                "taskName": "2"
+            },
+            {
+                "kind": "EntityLinkingLROResults",
+                "lastUpdateDateTime": "2022-04-18T19:43:10.4261716Z",
+                "results": {
+                    "documents": [
                         {
-                            "confidenceScores": {
-                                "negative": 0.0,
-                                "neutral": 0.99,
-                                "positive": 0.0
-                            },
-                            "id": "3",
-                            "sentences": [
-                                {
-                                    "confidenceScores": {
-                                        "negative": 0.0,
-                                        "neutral": 0.99,
-                                        "positive": 0.0
-                                    },
-                                    "length": 5,
-                                    "offset": 0,
-                                    "sentiment": "neutral",
-                                    "text": "three"
-                                }
-                            ],
-                            "sentiment": "neutral",
+                            "entities": [],
+                            "id": "1",
                             "warnings": []
-                        },
+                        }
+                    ],
+                    "errors": [],
+                    "modelVersion": "2021-06-01"
+                },
+                "status": "succeeded",
+                "taskName": "3"
+            },
+            {
+                "kind": "SentimentAnalysisLROResults",
+                "lastUpdateDateTime": "2022-04-18T19:43:11.4597392Z",
+                "results": {
+                    "documents": [
                         {
                             "confidenceScores": {
-                                "negative": 0.0,
-                                "neutral": 0.98,
+                                "negative": 0.54,
+                                "neutral": 0.45,
                                 "positive": 0.01
                             },
-                            "id": "4",
+                            "id": "1",
                             "sentences": [
                                 {
                                     "confidenceScores": {
-                                        "negative": 0.0,
-                                        "neutral": 0.98,
+                                        "negative": 0.54,
+                                        "neutral": 0.45,
                                         "positive": 0.01
                                     },
-                                    "length": 4,
+                                    "length": 38,
                                     "offset": 0,
-                                    "sentiment": "neutral",
-                                    "text": "four"
+                                    "sentiment": "negative",
+                                    "text": "I did not like the hotel we stayed at."
                                 }
                             ],
-                            "sentiment": "neutral",
-                            "warnings": []
-                        },
-                        {
-                            "confidenceScores": {
-                                "negative": 0.01,
-                                "neutral": 0.93,
-                                "positive": 0.06
-                            },
-                            "id": "5",
-                            "sentences": [
-                                {
-                                    "confidenceScores": {
-                                        "negative": 0.01,
-                                        "neutral": 0.93,
-                                        "positive": 0.06
-                                    },
-                                    "length": 4,
-                                    "offset": 0,
-                                    "sentiment": "neutral",
-                                    "text": "five"
-                                }
-                            ],
-                            "sentiment": "neutral",
+                            "sentiment": "negative",
                             "warnings": []
                         }
                     ],
                     "errors": [],
-                    "modelVersion": "2022-10-01"
-                }
-            },
-            "ResponseHeaders": {
-                "Content-Length": "1377",
-                "Content-Type": "application/json; charset=utf-8",
-                "Date": "Mon, 17 Oct 2022 19:00:21 GMT",
-                "Set-Cookie": ".AspNetCore.Mvc.CookieTempDataProvider=; expires=Thu, 01 Jan 1970 00:00:00 GMT; path=/; samesite=lax; httponly",
-                "Strict-Transport-Security": "max-age=31536000; includeSubDomains; preload",
-                "X-Content-Type-Options": "nosniff",
-                "apim-request-id": "be87cf68-7765-4cca-9dc1-421abb0f0614",
-                "csp-billing-usage": "CognitiveServices.TextAnalytics.BatchScoring=5,CognitiveServices.TextAnalytics.TextRecords=5",
-                "x-envoy-upstream-service-time": "32"
-            },
-            "StatusCode": 200
-        }
-    ],
-    "Variables": {}
+                    "modelVersion": "2019-10-01"
+                },
+                "status": "succeeded",
+                "taskName": "4"
+            }
+        ],
+        "total": 5
+    }
 }
```

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/mock_test_responses/action_job_failure_language.json` & `azure-ai-textanalytics-5.3.0b2/tests/mock_test_responses/action_job_failure_language.json`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/tests/mock_test_responses/action_job_failure.json` & `azure-ai-textanalytics-5.3.0b2/tests/mock_test_responses/action_job_failure.json`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_base_client.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_base_client.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 # ------------------------------------
 # Copyright (c) Microsoft Corporation.
 # Licensed under the MIT License.
 # ------------------------------------
 
-from typing import Union, Any
+from typing import Union, Any, Optional
 from enum import Enum
 from azure.core import CaseInsensitiveEnumMeta
 from azure.core.pipeline.policies import AzureKeyCredentialPolicy, HttpLoggingPolicy
 from azure.core.credentials import AzureKeyCredential, TokenCredential
 from ._generated import TextAnalyticsClient as _TextAnalyticsClient
 from ._policies import TextAnalyticsResponseHookPolicy, QuotaExceededPolicy
 from ._user_agent import USER_AGENT
@@ -44,14 +44,16 @@
 
 
 class TextAnalyticsClientBase:
     def __init__(
         self,
         endpoint: str,
         credential: Union[AzureKeyCredential, TokenCredential],
+        *,
+        api_version: Optional[Union[str, TextAnalyticsApiVersion]] = None,
         **kwargs: Any
     ) -> None:
         http_logging_policy = HttpLoggingPolicy(**kwargs)
         http_logging_policy.allowed_header_names.update(
             {
                 "Operation-Location",
                 "apim-request-id",
@@ -78,17 +80,17 @@
             }
         )
         try:
             endpoint = endpoint.rstrip("/")
         except AttributeError:
             raise ValueError("Parameter 'endpoint' must be a string.")
 
-        self._api_version = kwargs.pop("api_version", DEFAULT_API_VERSION)
+        self._api_version = api_version if api_version is not None else DEFAULT_API_VERSION
         if hasattr(self._api_version, "value"):
-            self._api_version = self._api_version.value
+            self._api_version = self._api_version.value  # type: ignore
         self._client = _TextAnalyticsClient(
             endpoint=endpoint,
             credential=credential,  # type: ignore
             api_version=self._api_version,
             sdk_moniker=USER_AGENT,
             authentication_policy=kwargs.pop("authentication_policy", _authentication_policy(credential)),
             custom_hook_policy=kwargs.pop("custom_hook_policy", TextAnalyticsResponseHookPolicy(**kwargs)),
```

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_validate.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_validate.py`

 * *Files 0% similar despite different names*

```diff
@@ -26,15 +26,15 @@
     if actions is None:
         return
 
     actions_version_mapping = {
         "2022-10-01-preview":
         [
             "ExtractSummaryAction",
-            "AbstractSummaryAction",
+            "AbstractiveSummaryAction",
         ],
         "2022-05-01":
         [
             "RecognizeCustomEntitiesAction",
             "SingleLabelClassifyAction",
             "MultiLabelClassifyAction",
             "AnalyzeHealthcareEntitiesAction"
```

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/__init__.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -56,27 +56,25 @@
     ClassifyDocumentResult,
     ClassificationCategory,
     AnalyzeHealthcareEntitiesAction,
     TextAnalysisKind,
     ExtractSummaryAction,
     ExtractSummaryResult,
     SummarySentence,
-    AbstractSummaryResult,
+    AbstractiveSummaryResult,
     AbstractiveSummary,
     SummaryContext,
-    AbstractSummaryAction,
+    AbstractiveSummaryAction,
     DynamicClassificationResult,
 )
 from ._generated.models import (
     HealthcareDocumentType,
-    BaseResolution,
     ResolutionKind,
     AgeResolution,
     AreaResolution,
-    BooleanResolution,
     CurrencyResolution,
     DateTimeResolution,
     InformationResolution,
     LengthResolution,
     NumberResolution,
     NumericRangeResolution,
     OrdinalResolution,
@@ -158,19 +156,17 @@
     "AnalyzeHealthcareEntitiesAction",
     "TextAnalysisLROPoller",
     "TextAnalysisKind",
     "ExtractSummaryAction",
     "ExtractSummaryResult",
     "SummarySentence",
     "HealthcareDocumentType",
-    "BaseResolution",
     "ResolutionKind",
     "AgeResolution",
     "AreaResolution",
-    "BooleanResolution",
     "CurrencyResolution",
     "DateTimeResolution",
     "InformationResolution",
     "LengthResolution",
     "NumberResolution",
     "NumericRangeResolution",
     "OrdinalResolution",
@@ -187,17 +183,17 @@
     "NumberKind",
     "RangeKind",
     "RelativeTo",
     "SpeedUnit",
     "TemperatureUnit",
     "VolumeUnit",
     "WeightUnit",
-    "AbstractSummaryResult",
+    "AbstractiveSummaryResult",
     "AbstractiveSummary",
     "SummaryContext",
-    "AbstractSummaryAction",
+    "AbstractiveSummaryAction",
     "ClassificationType",
     "DynamicClassificationResult",
     "DateTimeSubKind",
 ]
 
 __version__ = VERSION
```

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_lro.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_lro.py`

 * *Files 6% similar despite different names*

```diff
@@ -2,14 +2,15 @@
 # Copyright (c) Microsoft Corporation.
 # Licensed under the MIT License.
 # ------------------------------------
 
 import base64
 import functools
 import json
+import datetime
 from urllib.parse import urlencode
 from typing import Any, Mapping, Optional, Callable, TypeVar, cast
 from typing_extensions import Protocol, runtime_checkable
 from azure.core.exceptions import HttpResponseError
 from azure.core.tracing.decorator import distributed_trace
 from azure.core.polling import LROPoller
 from azure.core.polling.base_polling import (
@@ -113,52 +114,52 @@
         :raises ~azure.core.exceptions.HttpResponseError: When the operation has already reached a terminal state.
         """
         ...
 
 
 class TextAnalyticsOperationResourcePolling(OperationResourcePolling):
     def __init__(
-        self, operation_location_header="operation-location", show_stats=False
-    ):
+        self, operation_location_header: str ="operation-location", show_stats: Optional[bool] = False
+    ) -> None:
         super().__init__(
             operation_location_header=operation_location_header
         )
         self._show_stats = show_stats
         self._query_params = {"showStats": show_stats}
 
-    def get_polling_url(self):
+    def get_polling_url(self) -> str:
         if not self._show_stats:
             return super().get_polling_url()
 
         # language api compat
         delimiter = "&" if super().get_polling_url().find("?") != -1 else "?"
 
         return (
             super().get_polling_url()
             + delimiter
             + urlencode(self._query_params)
         )
 
 
 class TextAnalyticsLROPollingMethod(LROBasePolling):
-    def finished(self):
+    def finished(self) -> bool:
         """Is this polling finished?
 
         :rtype: bool
         """
         return TextAnalyticsLROPollingMethod._finished(self.status())
 
     @staticmethod
-    def _finished(status):
+    def _finished(status) -> bool:
         if hasattr(status, "value"):
             status = status.value
         return str(status).lower() in _FINISHED
 
     @staticmethod
-    def _failed(status):
+    def _failed(status) -> bool:
         if hasattr(status, "value"):
             status = status.value
         return str(status).lower() in _FAILED
 
     @staticmethod
     def _raise_if_bad_http_status_and_method(response):
         """Check response status code is valid.
@@ -183,14 +184,16 @@
         :param callable update_cmd: The function to call to retrieve the
          latest status of the long running operation.
         :raises: OperationFailed if operation status 'Failed' or 'Canceled'.
         :raises: BadStatus if response status invalid.
         :raises: BadResponse if response invalid.
         """
 
+        if not self.finished():
+            self.update_status()
         while not self.finished():
             self._delay()
             self.update_status()
 
         if TextAnalyticsLROPollingMethod._failed(self.status()):
             try:
                 job = json.loads(self._pipeline_response.http_response.text())
@@ -206,62 +209,61 @@
             self._pipeline_response = self.request_status(final_get_url)
             TextAnalyticsLROPollingMethod._raise_if_bad_http_status_and_method(
                 self._pipeline_response.http_response
             )
 
 
 class AnalyzeHealthcareEntitiesLROPollingMethod(TextAnalyticsLROPollingMethod):
-    def __init__(self, *args, **kwargs):
+    def __init__(self, *args: Any, **kwargs: Any) -> None:
         self._doc_id_order = kwargs.pop("doc_id_order", None)
         self._show_stats = kwargs.pop("show_stats", None)
         self._text_analytics_client = kwargs.pop("text_analytics_client")
         super().__init__(*args, **kwargs)
 
     @property
     def _current_body(self):
         from ._generated.models import JobState
         return JobState.deserialize(self._pipeline_response)
 
     @property
-    def created_on(self):
+    def created_on(self) -> Optional[datetime.datetime]:
         if not self._current_body:
             return None
         return self._current_body.created_date_time
 
     @property
-    def expires_on(self):
+    def expires_on(self) -> Optional[datetime.datetime]:
         if not self._current_body:
             return None
         return self._current_body.expiration_date_time
 
     @property
-    def last_modified_on(self):
+    def last_modified_on(self) -> Optional[datetime.datetime]:
         if not self._current_body:
             return None
         return self._current_body.last_update_date_time
 
     @property
-    def id(self):
+    def id(self) -> str:
         if self._current_body and self._current_body.job_id is not None:
             return self._current_body.job_id
         return self._get_id_from_headers()
 
     def _get_id_from_headers(self) -> str:
         return self._initial_response.http_response.headers[
             "Operation-Location"
         ].split("/jobs/")[1].split("?")[0]
 
     @property
-    def display_name(self):
+    def display_name(self) -> Optional[str]:
         if not self._current_body:
             return None
         return self._current_body.display_name
 
-    def get_continuation_token(self):
-        # type: () -> str
+    def get_continuation_token(self) -> str:
         import pickle
         self._initial_response.context.options["doc_id_order"] = self._doc_id_order
         self._initial_response.context.options["show_stats"] = self._show_stats
         return base64.b64encode(pickle.dumps(self._initial_response)).decode('ascii')
 
 
 class AnalyzeHealthcareEntitiesLROPoller(LROPoller[PollingReturnType]):
@@ -284,15 +286,15 @@
             "id": self.polling_method().id,
             "created_on": self.polling_method().created_on,
             "expires_on": self.polling_method().expires_on,
             "display_name": self.polling_method().display_name,
             "last_modified_on": self.polling_method().last_modified_on,
         }
 
-    def __getattr__(self, item):
+    def __getattr__(self, item: str) -> Any:
         attrs = [
             "created_on",
             "expires_on",
             "display_name",
             "last_modified_on",
             "id"
         ]
@@ -371,87 +373,86 @@
         except HttpResponseError as error:
             from ._response_handlers import process_http_response_error
 
             process_http_response_error(error)
 
 
 class AnalyzeActionsLROPollingMethod(TextAnalyticsLROPollingMethod):
-    def __init__(self, *args, **kwargs):
+    def __init__(self, *args: Any, **kwargs: Any) -> None:
         self._doc_id_order = kwargs.pop("doc_id_order", None)
         self._task_id_order = kwargs.pop("task_id_order", None)
         self._show_stats = kwargs.pop("show_stats", None)
-        self._text_analytics_client = kwargs.pop("text_analytics_client", None)
+        self._text_analytics_client = kwargs.pop("text_analytics_client")
         super().__init__(*args, **kwargs)
 
     @property
     def _current_body(self):
         from ._generated.models import JobState
         return JobState.deserialize(self._pipeline_response)
 
     @property
-    def created_on(self):
+    def created_on(self) -> Optional[datetime.datetime]:
         if not self._current_body:
             return None
         return self._current_body.created_date_time
 
     @property
-    def expires_on(self):
+    def expires_on(self) -> Optional[datetime.datetime]:
         if not self._current_body:
             return None
         return self._current_body.expiration_date_time
 
     @property
-    def display_name(self):
+    def display_name(self) -> Optional[str]:
         if not self._current_body:
             return None
         return self._current_body.display_name
 
     @property
-    def actions_failed_count(self):
+    def actions_failed_count(self) -> Optional[int]:
         if not self._current_body:
             return None
         return self._current_body.additional_properties.get("tasks", {}).get("failed", None)
 
     @property
-    def actions_in_progress_count(self):
+    def actions_in_progress_count(self) -> Optional[int]:
         if not self._current_body:
             return None
         return self._current_body.additional_properties.get("tasks", {}).get("inProgress", None)
 
     @property
-    def actions_succeeded_count(self):
+    def actions_succeeded_count(self) -> Optional[int]:
         if not self._current_body:
             return None
         return self._current_body.additional_properties.get("tasks", {}).get("completed", None)
 
     @property
-    def last_modified_on(self):
+    def last_modified_on(self) -> Optional[datetime.datetime]:
         if not self._current_body:
             return None
         return self._current_body.last_update_date_time
 
     @property
-    def total_actions_count(self):
+    def total_actions_count(self) -> Optional[int]:
         if not self._current_body:
             return None
         return self._current_body.additional_properties.get("tasks", {}).get("total", None)
 
     @property
-    def id(self):
+    def id(self) -> str:
         if self._current_body and self._current_body.job_id is not None:
             return self._current_body.job_id
         return self._get_id_from_headers()
 
     def _get_id_from_headers(self) -> str:
         return self._initial_response.http_response.headers[
             "Operation-Location"
         ].split("/jobs/")[1].split("?")[0]
 
-    def get_continuation_token(self):
-        # type: () -> str
+    def get_continuation_token(self) -> str:
         import pickle
         self._initial_response.context.options["doc_id_order"] = self._doc_id_order
         self._initial_response.context.options["task_id_order"] = self._task_id_order
         self._initial_response.context.options["show_stats"] = self._show_stats
         return base64.b64encode(pickle.dumps(self._initial_response)).decode('ascii')
 
 
@@ -479,15 +480,15 @@
             "last_modified_on": self.polling_method().last_modified_on,
             "actions_failed_count": self.polling_method().actions_failed_count,
             "actions_in_progress_count": self.polling_method().actions_in_progress_count,
             "actions_succeeded_count": self.polling_method().actions_succeeded_count,
             "total_actions_count": self.polling_method().total_actions_count,
         }
 
-    def __getattr__(self, item):
+    def __getattr__(self, item: str) -> Any:
         attrs = [
             "created_on",
             "expires_on",
             "display_name",
             "actions_failed_count",
             "actions_in_progress_count",
             "actions_succeeded_count",
```

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_text_analytics_client.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/aio/_text_analytics_client_async.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,167 +1,174 @@
 # ------------------------------------
 # Copyright (c) Microsoft Corporation.
 # Licensed under the MIT License.
 # ------------------------------------
 # pylint: disable=too-many-lines
 
-from typing import (
-    Union,
-    Any,
-    List,
-    Dict,
-    cast,
-)
-from azure.core.paging import ItemPaged
-from azure.core.tracing.decorator import distributed_trace
+from typing import Union, Any, List, Dict, cast, Optional
+from azure.core.async_paging import AsyncItemPaged
+from azure.core.tracing.decorator_async import distributed_trace_async
 from azure.core.exceptions import HttpResponseError
 from azure.core.credentials import AzureKeyCredential
-from azure.core.credentials import TokenCredential
-from ._base_client import TextAnalyticsClientBase
-from ._lro import AnalyzeActionsLROPoller, AnalyzeHealthcareEntitiesLROPoller, TextAnalysisLROPoller
-from ._request_handlers import (
+from azure.core.credentials_async import AsyncTokenCredential
+from .._base_client import TextAnalyticsApiVersion
+from ._base_client_async import AsyncTextAnalyticsClientBase
+from .._request_handlers import (
     _validate_input,
     _determine_action_type,
 )
-from ._validate import validate_multiapi_args, check_for_unsupported_actions_types
-from ._response_handlers import (
+from .._validate import validate_multiapi_args, check_for_unsupported_actions_types
+from .._response_handlers import (
     process_http_response_error,
     entities_result,
     linked_entities_result,
     key_phrases_result,
     sentiment_result,
     language_result,
     pii_entities_result,
-    healthcare_paged_result,
-    analyze_paged_result,
     _get_result_from_continuation_token,
     dynamic_classification_result,
 )
-
-from ._lro import (
-    TextAnalyticsOperationResourcePolling,
-    AnalyzeActionsLROPollingMethod,
-    AnalyzeHealthcareEntitiesLROPollingMethod,
-)
-from ._models import (
+from ._response_handlers_async import healthcare_paged_result, analyze_paged_result
+from .._generated.models import HealthcareDocumentType, ClassificationType
+from .._models import (
     DetectLanguageInput,
     TextDocumentInput,
     DetectLanguageResult,
     RecognizeEntitiesResult,
     RecognizeLinkedEntitiesResult,
     ExtractKeyPhrasesResult,
     AnalyzeSentimentResult,
     DocumentError,
     RecognizePiiEntitiesResult,
     RecognizeEntitiesAction,
     RecognizePiiEntitiesAction,
-    RecognizeLinkedEntitiesAction,
     ExtractKeyPhrasesAction,
+    _AnalyzeActionsType,
+    RecognizeLinkedEntitiesAction,
     AnalyzeSentimentAction,
     AnalyzeHealthcareEntitiesResult,
     RecognizeCustomEntitiesAction,
     RecognizeCustomEntitiesResult,
     SingleLabelClassifyAction,
     MultiLabelClassifyAction,
     ClassifyDocumentResult,
     AnalyzeHealthcareEntitiesAction,
-    _AnalyzeActionsType,
     ExtractSummaryAction,
     ExtractSummaryResult,
-    AbstractSummaryAction,
-    AbstractSummaryResult,
+    AbstractiveSummaryAction,
+    AbstractiveSummaryResult,
     DynamicClassificationResult,
+    PiiEntityDomain,
+    PiiEntityCategory,
+)
+from .._check import is_language_api, string_index_type_compatibility
+from .._lro import TextAnalyticsOperationResourcePolling
+from ._lro_async import (
+    AsyncAnalyzeHealthcareEntitiesLROPollingMethod,
+    AsyncAnalyzeActionsLROPollingMethod,
+    AsyncAnalyzeHealthcareEntitiesLROPoller,
+    AsyncAnalyzeActionsLROPoller,
+    AsyncTextAnalysisLROPoller,
 )
-from ._check import is_language_api, string_index_type_compatibility
 
 
-AnalyzeActionsResponse = TextAnalysisLROPoller[
-    ItemPaged[
+AsyncAnalyzeActionsResponse = AsyncTextAnalysisLROPoller[
+    AsyncItemPaged[
         List[
             Union[
                 RecognizeEntitiesResult,
                 RecognizeLinkedEntitiesResult,
                 RecognizePiiEntitiesResult,
                 ExtractKeyPhrasesResult,
                 AnalyzeSentimentResult,
                 RecognizeCustomEntitiesResult,
                 ClassifyDocumentResult,
                 AnalyzeHealthcareEntitiesResult,
                 ExtractSummaryResult,
-                AbstractSummaryResult,
+                AbstractiveSummaryResult,
                 DocumentError,
             ]
         ]
     ]
 ]
 
 
-class TextAnalyticsClient(TextAnalyticsClientBase):
+class TextAnalyticsClient(AsyncTextAnalyticsClientBase):
     """The Language service API is a suite of natural language processing (NLP) skills built with the best-in-class
     Microsoft machine learning algorithms. The API can be used to analyze unstructured text for
     tasks such as sentiment analysis, key phrase extraction, entities recognition,
     and language detection, and more.
 
     Further documentation can be found in
     https://docs.microsoft.com/azure/cognitive-services/language-service/overview
 
     :param str endpoint: Supported Cognitive Services or Language resource
         endpoints (protocol and hostname, for example: 'https://<resource-name>.cognitiveservices.azure.com').
     :param credential: Credentials needed for the client to connect to Azure.
         This can be the an instance of AzureKeyCredential if using a Cognitive Services/Language API key
         or a token credential from :mod:`azure.identity`.
-    :type credential: ~azure.core.credentials.AzureKeyCredential or ~azure.core.credentials.TokenCredential
+    :type credential: ~azure.core.credentials.AzureKeyCredential or ~azure.core.credentials_async.AsyncTokenCredential
     :keyword str default_country_hint: Sets the default country_hint to use for all operations.
         Defaults to "US". If you don't want to use a country hint, pass the string "none".
     :keyword str default_language: Sets the default language to use for all operations.
         Defaults to "en".
     :keyword api_version: The API version of the service to use for requests. It defaults to the
         latest service version. Setting to an older version may result in reduced feature compatibility.
     :paramtype api_version: str or ~azure.ai.textanalytics.TextAnalyticsApiVersion
 
     .. admonition:: Example:
 
-        .. literalinclude:: ../samples/sample_authentication.py
-            :start-after: [START create_ta_client_with_key]
-            :end-before: [END create_ta_client_with_key]
+        .. literalinclude:: ../samples/async_samples/sample_authentication_async.py
+            :start-after: [START create_ta_client_with_key_async]
+            :end-before: [END create_ta_client_with_key_async]
             :language: python
             :dedent: 4
             :caption: Creating the TextAnalyticsClient with endpoint and API key.
 
-        .. literalinclude:: ../samples/sample_authentication.py
-            :start-after: [START create_ta_client_with_aad]
-            :end-before: [END create_ta_client_with_aad]
+        .. literalinclude:: ../samples/async_samples/sample_authentication_async.py
+            :start-after: [START create_ta_client_with_aad_async]
+            :end-before: [END create_ta_client_with_aad_async]
             :language: python
             :dedent: 4
             :caption: Creating the TextAnalyticsClient with endpoint and token credential from Azure Active Directory.
     """
 
     def __init__(
         self,
         endpoint: str,
-        credential: Union[AzureKeyCredential, TokenCredential],
-        **kwargs: Any
+        credential: Union[AzureKeyCredential, AsyncTokenCredential],
+        *,
+        default_language: Optional[str] = None,
+        default_country_hint: Optional[str] = None,
+        api_version: Optional[Union[str, TextAnalyticsApiVersion]] = None,
+        **kwargs: Any,
     ) -> None:
         super().__init__(
-            endpoint=endpoint, credential=credential, **kwargs
+            endpoint=endpoint, credential=credential, api_version=api_version, **kwargs
         )
-        self._default_language = kwargs.pop("default_language", "en")
-        self._default_country_hint = kwargs.pop("default_country_hint", "US")
-        self._string_index_type_default = (
-            None if kwargs.get("api_version") == "v3.0" else "UnicodeCodePoint"
+        self._default_language = default_language if default_language is not None else "en"
+        self._default_country_hint = default_country_hint if default_country_hint is not None else "US"
+        self._string_code_unit = (
+            None if api_version == "v3.0" else "UnicodeCodePoint"
         )
 
-    @distributed_trace
+    @distributed_trace_async
     @validate_multiapi_args(
         version_method_added="v3.0",
         args_mapping={"v3.1": ["disable_service_logs"]}
     )
-    def detect_language(
+    async def detect_language(
         self,
         documents: Union[List[str], List[DetectLanguageInput], List[Dict[str, str]]],
+        *,
+        country_hint: Optional[str] = None,
+        disable_service_logs: Optional[bool] = None,
+        model_version: Optional[str] = None,
+        show_stats: Optional[bool] = None,
         **kwargs: Any,
     ) -> List[Union[DetectLanguageResult, DocumentError]]:
         """Detect language for a batch of documents.
 
         Returns the detected language and a numeric score between zero and
         one. Scores close to one indicate 100% certainty that the identified
         language is true. See https://aka.ms/talangs for the list of enabled languages.
@@ -175,63 +182,60 @@
             `{"id": "1", "country_hint": "us", "text": "hello world"}`.
         :type documents:
             list[str] or list[~azure.ai.textanalytics.DetectLanguageInput] or list[dict[str, str]]
         :keyword str country_hint: Country of origin hint for the entire batch. Accepts two
             letter country codes specified by ISO 3166-1 alpha-2. Per-document
             country hints will take precedence over whole batch hints. Defaults to
             "US". If you don't want to use a country hint, pass the string "none".
-        :keyword str model_version: Version of the model used on the service side for scoring,
-            e.g. "latest", "2019-10-01". If a model version
+        :keyword str model_version: This value indicates which model will
+            be used for scoring, e.g. "latest", "2019-10-01". If a model-version
             is not specified, the API will default to the latest, non-preview version.
             See here for more info: https://aka.ms/text-analytics-model-versioning
         :keyword bool show_stats: If set to true, response will contain document
             level statistics in the `statistics` field of the document-level response.
         :keyword bool disable_service_logs: If set to true, you opt-out of having your text input
             logged on the service side for troubleshooting. By default, the Language service logs your
             input text for 48 hours, solely to allow for troubleshooting issues in providing you with
             the service's natural language processing functions. Setting this parameter to true,
             disables input logging and may limit our ability to remediate issues that occur. Please see
             Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
             additional details, and Microsoft Responsible AI principles at
             https://www.microsoft.com/ai/responsible-ai.
-        :return: The combined list of :class:`~azure.ai.textanalytics.DetectLanguageResult` and
-            :class:`~azure.ai.textanalytics.DocumentError` in the order the original documents were
-            passed in.
+        :return: The combined list of :class:`~azure.ai.textanalytics.DetectLanguageResult`
+            and :class:`~azure.ai.textanalytics.DocumentError` in the order the original documents
+            were passed in.
         :rtype: list[~azure.ai.textanalytics.DetectLanguageResult or ~azure.ai.textanalytics.DocumentError]
         :raises ~azure.core.exceptions.HttpResponseError or TypeError or ValueError:
 
         .. versionadded:: v3.1
             The *disable_service_logs* keyword argument.
 
         .. admonition:: Example:
 
-            .. literalinclude:: ../samples/sample_detect_language.py
-                :start-after: [START detect_language]
-                :end-before: [END detect_language]
+            .. literalinclude:: ../samples/async_samples/sample_detect_language_async.py
+                :start-after: [START detect_language_async]
+                :end-before: [END detect_language_async]
                 :language: python
                 :dedent: 4
                 :caption: Detecting language in a batch of documents.
         """
-        country_hint_arg = kwargs.pop("country_hint", None)
-        country_hint = (
-            country_hint_arg
-            if country_hint_arg is not None
+
+        country_hint_arg = (
+            country_hint
+            if country_hint is not None
             else self._default_country_hint
         )
-        docs = _validate_input(documents, "country_hint", country_hint)
-        model_version = kwargs.pop("model_version", None)
-        show_stats = kwargs.pop("show_stats", None)
-        disable_service_logs = kwargs.pop("disable_service_logs", None)
+        docs = _validate_input(documents, "country_hint", country_hint_arg)
 
         try:
             if is_language_api(self._api_version):
                 models = self._client.models(api_version=self._api_version)
                 return cast(
                     List[Union[DetectLanguageResult, DocumentError]],
-                    self._client.analyze_text(
+                    await self._client.analyze_text(
                         body=models.AnalyzeTextLanguageDetectionInput(
                             analysis_input={"documents": docs},
                             parameters=models.LanguageDetectionTaskParameters(
                                 logging_opt_out=disable_service_logs,
                                 model_version=model_version
                             )
                         ),
@@ -240,49 +244,55 @@
                         **kwargs
                     )
                 )
 
             # api_versions 3.0, 3.1
             return cast(
                 List[Union[DetectLanguageResult, DocumentError]],
-                self._client.languages(
+                await self._client.languages(
                     documents=docs,
                     model_version=model_version,
                     show_stats=show_stats,
                     logging_opt_out=disable_service_logs,
                     cls=kwargs.pop("cls", language_result),
                     **kwargs
                 )
             )
         except HttpResponseError as error:
             return process_http_response_error(error)
 
-    @distributed_trace
+    @distributed_trace_async
     @validate_multiapi_args(
         version_method_added="v3.0",
         args_mapping={"v3.1": ["string_index_type", "disable_service_logs"]}
     )
-    def recognize_entities(
+    async def recognize_entities(
         self,
         documents: Union[List[str], List[TextDocumentInput], List[Dict[str, str]]],
+        *,
+        disable_service_logs: Optional[bool] = None,
+        language: Optional[str] = None,
+        model_version: Optional[str] = None,
+        show_stats: Optional[bool] = None,
+        string_index_type: Optional[str] = None,
         **kwargs: Any,
     ) -> List[Union[RecognizeEntitiesResult, DocumentError]]:
         """Recognize entities for a batch of documents.
 
         Identifies and categorizes entities in your text as people, places,
         organizations, date/time, quantities, percentages, currencies, and more.
         For the list of supported entity types, check: https://aka.ms/taner
 
         See https://aka.ms/azsdk/textanalytics/data-limits for service data limits.
 
         :param documents: The set of documents to process as part of this batch.
             If you wish to specify the ID and language on a per-item basis you must
-            use as input a list[:class:`~azure.ai.textanalytics.TextDocumentInput`] or a list
-            of dict representations of :class:`~azure.ai.textanalytics.TextDocumentInput`,
-            like `{"id": "1", "language": "en", "text": "hello world"}`.
+            use as input a list[:class:`~azure.ai.textanalytics.TextDocumentInput`] or a list of
+            dict representations of :class:`~azure.ai.textanalytics.TextDocumentInput`, like
+            `{"id": "1", "language": "en", "text": "hello world"}`.
         :type documents:
             list[str] or list[~azure.ai.textanalytics.TextDocumentInput] or list[dict[str, str]]
         :keyword str language: The 2 letter ISO 639-1 representation of language for the
             entire batch. For example, use "en" for English; "es" for Spanish etc.
             If not set, uses "en" for English as default. Per-document language will
             take precedence over whole batch language. See https://aka.ms/talangs for
             supported languages in Language API.
@@ -301,89 +311,94 @@
             input text for 48 hours, solely to allow for troubleshooting issues in providing you with
             the service's natural language processing functions. Setting this parameter to true,
             disables input logging and may limit our ability to remediate issues that occur. Please see
             Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
             additional details, and Microsoft Responsible AI principles at
             https://www.microsoft.com/ai/responsible-ai.
         :return: The combined list of :class:`~azure.ai.textanalytics.RecognizeEntitiesResult` and
-            :class:`~azure.ai.textanalytics.DocumentError` in the order the original documents
-            were passed in.
+            :class:`~azure.ai.textanalytics.DocumentError` in the order the original documents were
+            passed in.
         :rtype: list[~azure.ai.textanalytics.RecognizeEntitiesResult or ~azure.ai.textanalytics.DocumentError]
         :raises ~azure.core.exceptions.HttpResponseError or TypeError or ValueError:
 
         .. versionadded:: v3.1
             The *disable_service_logs* and *string_index_type* keyword arguments.
 
         .. admonition:: Example:
 
-            .. literalinclude:: ../samples/sample_recognize_entities.py
-                :start-after: [START recognize_entities]
-                :end-before: [END recognize_entities]
+            .. literalinclude:: ../samples/async_samples/sample_recognize_entities_async.py
+                :start-after: [START recognize_entities_async]
+                :end-before: [END recognize_entities_async]
                 :language: python
                 :dedent: 4
                 :caption: Recognize entities in a batch of documents.
         """
-        language_arg = kwargs.pop("language", None)
-        language = language_arg if language_arg is not None else self._default_language
-        docs = _validate_input(documents, "language", language)
-        model_version = kwargs.pop("model_version", None)
-        show_stats = kwargs.pop("show_stats", None)
-        string_index_type = kwargs.pop("string_index_type", self._string_index_type_default)
-        disable_service_logs = kwargs.pop("disable_service_logs", None)
+
+        language_arg = language if language is not None else self._default_language
+        docs = _validate_input(documents, "language", language_arg)
+        string_index_type_arg = string_index_type if string_index_type is not None else self._string_code_unit
 
         try:
             if is_language_api(self._api_version):
                 models = self._client.models(api_version=self._api_version)
                 return cast(
                     List[Union[RecognizeEntitiesResult, DocumentError]],
-                    self._client.analyze_text(
+                    await self._client.analyze_text(
                         body=models.AnalyzeTextEntityRecognitionInput(
                             analysis_input={"documents": docs},
                             parameters=models.EntitiesTaskParameters(
                                 logging_opt_out=disable_service_logs,
                                 model_version=model_version,
-                                string_index_type=string_index_type_compatibility(string_index_type)
+                                string_index_type=string_index_type_compatibility(string_index_type_arg)
                             )
                         ),
                         show_stats=show_stats,
                         cls=kwargs.pop("cls", entities_result),
                         **kwargs
                     )
                 )
 
             # api_versions 3.0, 3.1
             return cast(
                 List[Union[RecognizeEntitiesResult, DocumentError]],
-                self._client.entities_recognition_general(
+                await self._client.entities_recognition_general(
                     documents=docs,
                     model_version=model_version,
                     show_stats=show_stats,
-                    string_index_type=string_index_type,
+                    string_index_type=string_index_type_arg,
                     logging_opt_out=disable_service_logs,
                     cls=kwargs.pop("cls", entities_result),
                     **kwargs,
                 )
             )
         except HttpResponseError as error:
             return process_http_response_error(error)
 
-    @distributed_trace
+    @distributed_trace_async
     @validate_multiapi_args(
         version_method_added="v3.1"
     )
-    def recognize_pii_entities(
+    async def recognize_pii_entities(
         self,
         documents: Union[List[str], List[TextDocumentInput], List[Dict[str, str]]],
+        *,
+        categories_filter: Optional[List[Union[str, PiiEntityCategory]]] = None,
+        disable_service_logs: Optional[bool] = None,
+        domain_filter: Optional[Union[str, PiiEntityDomain]] = None,
+        language: Optional[str] = None,
+        model_version: Optional[str] = None,
+        show_stats: Optional[bool] = None,
+        string_index_type: Optional[str] = None,
         **kwargs: Any,
     ) -> List[Union[RecognizePiiEntitiesResult, DocumentError]]:
         """Recognize entities containing personal information for a batch of documents.
 
         Returns a list of personal information entities ("SSN",
         "Bank Account", etc) in the document.  For the list of supported entity types,
-        check https://aka.ms/tanerpii
+        check https://aka.ms/azsdk/language/pii
 
         See https://aka.ms/azsdk/textanalytics/data-limits for service data limits.
 
         :param documents: The set of documents to process as part of this batch.
             If you wish to specify the ID and language on a per-item basis you must
             use as input a list[:class:`~azure.ai.textanalytics.TextDocumentInput`] or a list of
             dict representations of :class:`~azure.ai.textanalytics.TextDocumentInput`, like
@@ -399,15 +414,15 @@
             be used for scoring, e.g. "latest", "2019-10-01". If a model-version
             is not specified, the API will default to the latest, non-preview version.
             See here for more info: https://aka.ms/text-analytics-model-versioning
         :keyword bool show_stats: If set to true, response will contain document
             level statistics in the `statistics` field of the document-level response.
         :keyword domain_filter: Filters the response entities to ones only included in the specified domain.
             I.e., if set to 'phi', will only return entities in the Protected Healthcare Information domain.
-            See https://aka.ms/tanerpii for more information.
+            See https://aka.ms/azsdk/language/pii for more information.
         :paramtype domain_filter: str or ~azure.ai.textanalytics.PiiEntityDomain
         :keyword categories_filter: Instead of filtering over all PII entity categories, you can pass in a list of
             the specific PII entity categories you want to filter out. For example, if you only want to filter out
             U.S. social security numbers in a document, you can pass in
             `[PiiEntityCategory.US_SOCIAL_SECURITY_NUMBER]` for this kwarg.
         :paramtype categories_filter: list[str or ~azure.ai.textanalytics.PiiEntityCategory]
         :keyword str string_index_type: Specifies the method used to interpret string offsets.
@@ -435,72 +450,72 @@
             .. literalinclude:: ../samples/sample_recognize_pii_entities.py
                 :start-after: [START recognize_pii_entities]
                 :end-before: [END recognize_pii_entities]
                 :language: python
                 :dedent: 4
                 :caption: Recognize personally identifiable information entities in a batch of documents.
         """
-        language_arg = kwargs.pop("language", None)
-        language = language_arg if language_arg is not None else self._default_language
-        docs = _validate_input(documents, "language", language)
-        model_version = kwargs.pop("model_version", None)
-        show_stats = kwargs.pop("show_stats", None)
-        domain_filter = kwargs.pop("domain_filter", None)
-        categories_filter = kwargs.pop("categories_filter", None)
-        string_index_type = kwargs.pop("string_index_type", self._string_index_type_default)
-        disable_service_logs = kwargs.pop("disable_service_logs", None)
+        language_arg = language if language is not None else self._default_language
+        docs = _validate_input(documents, "language", language_arg)
+        string_index_type_arg = string_index_type if string_index_type is not None else self._string_code_unit
 
         try:
             if is_language_api(self._api_version):
                 models = self._client.models(api_version=self._api_version)
                 return cast(
                     List[Union[RecognizePiiEntitiesResult, DocumentError]],
-                    self._client.analyze_text(
+                    await self._client.analyze_text(
                         body=models.AnalyzeTextPiiEntitiesRecognitionInput(
                             analysis_input={"documents": docs},
                             parameters=models.PiiTaskParameters(
                                 logging_opt_out=disable_service_logs,
                                 model_version=model_version,
                                 domain=domain_filter,
                                 pii_categories=categories_filter,
-                                string_index_type=string_index_type_compatibility(string_index_type)
+                                string_index_type=string_index_type_compatibility(string_index_type_arg)
                             )
                         ),
                         show_stats=show_stats,
                         cls=kwargs.pop("cls", pii_entities_result),
                         **kwargs
                     )
                 )
 
             # api_versions 3.0, 3.1
             return cast(
                 List[Union[RecognizePiiEntitiesResult, DocumentError]],
-                self._client.entities_recognition_pii(
+                await self._client.entities_recognition_pii(
                     documents=docs,
                     model_version=model_version,
                     show_stats=show_stats,
                     domain=domain_filter,
                     pii_categories=categories_filter,
                     logging_opt_out=disable_service_logs,
-                    string_index_type=string_index_type,
+                    string_index_type=string_index_type_arg,
                     cls=kwargs.pop("cls", pii_entities_result),
                     **kwargs
                 )
             )
         except HttpResponseError as error:
             return process_http_response_error(error)
 
-    @distributed_trace
+    @distributed_trace_async
     @validate_multiapi_args(
         version_method_added="v3.0",
         args_mapping={"v3.1": ["string_index_type", "disable_service_logs"]}
     )
-    def recognize_linked_entities(
+    async def recognize_linked_entities(
         self,
         documents: Union[List[str], List[TextDocumentInput], List[Dict[str, str]]],
+        *,
+        disable_service_logs: Optional[bool] = None,
+        language: Optional[str] = None,
+        model_version: Optional[str] = None,
+        show_stats: Optional[bool] = None,
+        string_index_type: Optional[str] = None,
         **kwargs: Any,
     ) -> List[Union[RecognizeLinkedEntitiesResult, DocumentError]]:
         """Recognize linked entities from a well-known knowledge base for a batch of documents.
 
         Identifies and disambiguates the identity of each entity found in text (for example,
         determining whether an occurrence of the word Mars refers to the planet, or to the
         Roman god of war). Recognized entities are associated with URLs to a well-known
@@ -545,300 +560,76 @@
         :raises ~azure.core.exceptions.HttpResponseError or TypeError or ValueError:
 
         .. versionadded:: v3.1
             The *disable_service_logs* and *string_index_type* keyword arguments.
 
         .. admonition:: Example:
 
-            .. literalinclude:: ../samples/sample_recognize_linked_entities.py
-                :start-after: [START recognize_linked_entities]
-                :end-before: [END recognize_linked_entities]
+            .. literalinclude:: ../samples/async_samples/sample_recognize_linked_entities_async.py
+                :start-after: [START recognize_linked_entities_async]
+                :end-before: [END recognize_linked_entities_async]
                 :language: python
                 :dedent: 4
                 :caption: Recognize linked entities in a batch of documents.
         """
-        language_arg = kwargs.pop("language", None)
-        language = language_arg if language_arg is not None else self._default_language
-        docs = _validate_input(documents, "language", language)
-        model_version = kwargs.pop("model_version", None)
-        show_stats = kwargs.pop("show_stats", None)
-        disable_service_logs = kwargs.pop("disable_service_logs", None)
-        string_index_type = kwargs.pop("string_index_type", self._string_index_type_default)
+
+        language_arg = language if language is not None else self._default_language
+        docs = _validate_input(documents, "language", language_arg)
+        string_index_type_arg = string_index_type if string_index_type is not None else self._string_code_unit
 
         try:
             if is_language_api(self._api_version):
                 models = self._client.models(api_version=self._api_version)
                 return cast(
                     List[Union[RecognizeLinkedEntitiesResult, DocumentError]],
-                    self._client.analyze_text(
+                    await self._client.analyze_text(
                         body=models.AnalyzeTextEntityLinkingInput(
                             analysis_input={"documents": docs},
                             parameters=models.EntityLinkingTaskParameters(
                                 logging_opt_out=disable_service_logs,
                                 model_version=model_version,
-                                string_index_type=string_index_type_compatibility(string_index_type)
+                                string_index_type=string_index_type_compatibility(string_index_type_arg)
                             )
                         ),
                         show_stats=show_stats,
                         cls=kwargs.pop("cls", linked_entities_result),
                         **kwargs
                     )
                 )
 
             # api_versions 3.0, 3.1
             return cast(
                 List[Union[RecognizeLinkedEntitiesResult, DocumentError]],
-                self._client.entities_linking(
+                await self._client.entities_linking(
                     documents=docs,
                     logging_opt_out=disable_service_logs,
                     model_version=model_version,
-                    string_index_type=string_index_type,
+                    string_index_type=string_index_type_arg,
                     show_stats=show_stats,
                     cls=kwargs.pop("cls", linked_entities_result),
                     **kwargs
                 )
             )
         except HttpResponseError as error:
             return process_http_response_error(error)
 
-    # pylint: disable=unused-argument
-    def _healthcare_result_callback(
-        self, raw_response, deserialized, doc_id_order, task_id_order=None, show_stats=False, bespoke=False
-    ):
-        if deserialized is None:
-            models = self._client.models(api_version=self._api_version)
-            response_cls = \
-                models.AnalyzeTextJobState if is_language_api(self._api_version) else models.HealthcareJobState
-            deserialized = response_cls.deserialize(raw_response)
-        return healthcare_paged_result(
-            doc_id_order,
-            self._client.analyze_text_job_status if is_language_api(self._api_version) else self._client.health_status,
-            raw_response,
-            deserialized,
-            show_stats=show_stats,
-        )
-
-    @distributed_trace
-    @validate_multiapi_args(
-        version_method_added="v3.1",
-        args_mapping={
-            "2022-10-01-preview": ["fhir_version", "document_type", "autodetect_default_language"],
-            "2022-05-01": ["display_name"]
-        }
-    )
-    def begin_analyze_healthcare_entities(
-        self,
-        documents: Union[List[str], List[TextDocumentInput], List[Dict[str, str]]],
-        **kwargs: Any,
-    ) -> AnalyzeHealthcareEntitiesLROPoller[ItemPaged[Union[AnalyzeHealthcareEntitiesResult, DocumentError]]]:
-        """Analyze healthcare entities and identify relationships between these entities in a batch of documents.
-
-        Entities are associated with references that can be found in existing knowledge bases,
-        such as UMLS, CHV, MSH, etc.
-
-        We also extract the relations found between entities, for example in "The subject took 100 mg of ibuprofen",
-        we would extract the relationship between the "100 mg" dosage and the "ibuprofen" medication.
-
-        :param documents: The set of documents to process as part of this batch.
-            If you wish to specify the ID and language on a per-item basis you must
-            use as input a list[:class:`~azure.ai.textanalytics.TextDocumentInput`] or a list of
-            dict representations of :class:`~azure.ai.textanalytics.TextDocumentInput`, like
-            `{"id": "1", "language": "en", "text": "hello world"}`.
-        :type documents:
-            list[str] or list[~azure.ai.textanalytics.TextDocumentInput] or list[dict[str, str]]
-        :keyword str model_version: This value indicates which model will
-            be used for scoring, e.g. "latest", "2019-10-01". If a model-version
-            is not specified, the API will default to the latest, non-preview version.
-            See here for more info: https://aka.ms/text-analytics-model-versioning
-        :keyword bool show_stats: If set to true, response will contain document level statistics.
-        :keyword str language: The 2 letter ISO 639-1 representation of language for the
-            entire batch. For example, use "en" for English; "es" for Spanish etc.
-            For automatic language detection, use "auto" (Only supported by API version
-            2022-10-01-preview and newer). If not set, uses "en" for English as default.
-            Per-document language will take precedence over whole batch language.
-            See https://aka.ms/talangs for supported languages in Language API.
-        :keyword str autodetect_default_language: Default/fallback language to use for documents requesting
-            automatic language detection.
-        :keyword str display_name: An optional display name to set for the requested analysis.
-        :keyword str string_index_type: Specifies the method used to interpret string offsets.
-            `UnicodeCodePoint`, the Python encoding, is the default. To override the Python default,
-            you can also pass in `Utf16CodeUnit` or `TextElement_v8`. For additional information
-            see https://aka.ms/text-analytics-offsets
-        :keyword int polling_interval: Waiting time between two polls for LRO operations
-            if no Retry-After header is present. Defaults to 5 seconds.
-        :keyword str continuation_token:
-            Call `continuation_token()` on the poller object to save the long-running operation (LRO)
-            state into an opaque token. Pass the value as the `continuation_token` keyword argument
-            to restart the LRO from a saved state.
-        :keyword bool disable_service_logs: Defaults to true, meaning that the Language service will not log your
-            input text on the service side for troubleshooting. If set to False, the Language service logs your
-            input text for 48 hours, solely to allow for troubleshooting issues in providing you with
-            the service's natural language processing functions. Please see
-            Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
-            additional details, and Microsoft Responsible AI principles at
-            https://www.microsoft.com/ai/responsible-ai.
-        :keyword str fhir_version: The FHIR Spec version that the result will use to format the fhir_bundle
-            on the result object. For additional information see https://www.hl7.org/fhir/overview.html.
-            The only acceptable values to pass in are None and "4.0.1". The default value is None.
-        :keyword document_type: Document type that can be provided as input for Fhir Documents. Expect to
-            have fhir_version provided when used. Behavior of using None enum is the same as not using the
-            document_type parameter. Known values are: "None", "ClinicalTrial", "DischargeSummary",
-            "ProgressNote", "HistoryAndPhysical", "Consult", "Imaging", "Pathology", and "ProcedureNote".
-        :paramtype document_type: str or ~azure.ai.textanalytics.HealthcareDocumentType
-        :return: An instance of an AnalyzeHealthcareEntitiesLROPoller. Call `result()` on the this
-            object to return a heterogeneous pageable of
-            :class:`~azure.ai.textanalytics.AnalyzeHealthcareEntitiesResult` and
-            :class:`~azure.ai.textanalytics.DocumentError`.
-        :rtype:
-            ~azure.ai.textanalytics.AnalyzeHealthcareEntitiesLROPoller[~azure.core.paging.ItemPaged[
-            ~azure.ai.textanalytics.AnalyzeHealthcareEntitiesResult or ~azure.ai.textanalytics.DocumentError]]
-        :raises ~azure.core.exceptions.HttpResponseError or TypeError or ValueError:
-
-        .. versionadded:: v3.1
-            The *begin_analyze_healthcare_entities* client method.
-        .. versionadded:: 2022-05-01
-            The *display_name* keyword argument.
-        .. versionadded:: 2022-10-01-preview
-            The *fhir_version*, *document_type*, and *autodetect_default_language* keyword arguments.
-
-        .. admonition:: Example:
-
-            .. literalinclude:: ../samples/sample_analyze_healthcare_entities.py
-                :start-after: [START analyze_healthcare_entities]
-                :end-before: [END analyze_healthcare_entities]
-                :language: python
-                :dedent: 4
-                :caption: Recognize healthcare entities in a batch of documents.
-        """
-        language_arg = kwargs.pop("language", None)
-        language = language_arg if language_arg is not None else self._default_language
-        model_version = kwargs.pop("model_version", None)
-        show_stats = kwargs.pop("show_stats", None)
-        polling_interval = kwargs.pop("polling_interval", 5)
-        continuation_token = kwargs.pop("continuation_token", None)
-        string_index_type = kwargs.pop("string_index_type", self._string_index_type_default)
-        disable_service_logs = kwargs.pop("disable_service_logs", None)
-        display_name = kwargs.pop("display_name", None)
-        fhir_version = kwargs.pop("fhir_version", None)
-        document_type = kwargs.pop("document_type", None)
-        autodetect_default_language = kwargs.pop("autodetect_default_language", None)
-
-        if continuation_token:
-            return cast(
-                AnalyzeHealthcareEntitiesLROPoller[
-                    ItemPaged[Union[AnalyzeHealthcareEntitiesResult, DocumentError]]
-                ],
-                _get_result_from_continuation_token(
-                    self._client._client,  # pylint: disable=protected-access
-                    continuation_token,
-                    AnalyzeHealthcareEntitiesLROPoller,
-                    AnalyzeHealthcareEntitiesLROPollingMethod(
-                        text_analytics_client=self._client,
-                        timeout=polling_interval,
-                        **kwargs
-                    ),
-                    self._healthcare_result_callback
-                )
-            )
-
-        docs = _validate_input(documents, "language", language)
-        doc_id_order = [doc.get("id") for doc in docs]
-        my_cls = kwargs.pop(
-            "cls",
-            lambda pipeline_response, deserialized, _: self._healthcare_result_callback(
-                pipeline_response, deserialized, doc_id_order, show_stats=show_stats
-            ),
-        )
-        models = self._client.models(api_version=self._api_version)
-
-        try:
-            if is_language_api(self._api_version):
-                docs = models.MultiLanguageAnalysisInput(
-                    documents=_validate_input(documents, "language", language)
-                )
-                return cast(
-                    AnalyzeHealthcareEntitiesLROPoller[
-                        ItemPaged[Union[AnalyzeHealthcareEntitiesResult, DocumentError]]
-                    ],
-                    self._client.begin_analyze_text_submit_job(  # type: ignore
-                        body=models.AnalyzeTextJobsInput(
-                            analysis_input=docs,
-                            display_name=display_name,
-                            default_language=autodetect_default_language,
-                            tasks=[
-                                models.HealthcareLROTask(
-                                    task_name="0",
-                                    parameters=models.HealthcareTaskParameters(
-                                        model_version=model_version,
-                                        logging_opt_out=disable_service_logs,
-                                        string_index_type=string_index_type_compatibility(string_index_type),
-                                        fhir_version=fhir_version,
-                                        document_type=document_type,
-                                    )
-                                )
-                            ]
-                        ),
-                        cls=my_cls,
-                        polling=AnalyzeHealthcareEntitiesLROPollingMethod(
-                            text_analytics_client=self._client,
-                            timeout=polling_interval,
-                            show_stats=show_stats,
-                            doc_id_order=doc_id_order,
-                            lro_algorithms=[
-                                TextAnalyticsOperationResourcePolling(
-                                    show_stats=show_stats,
-                                )
-                            ],
-                            **kwargs
-                        ),
-                        continuation_token=continuation_token,
-                        poller_cls=AnalyzeHealthcareEntitiesLROPoller,
-                        **kwargs
-                    )
-                )
-
-            # v3.1
-            return cast(
-                AnalyzeHealthcareEntitiesLROPoller[
-                    ItemPaged[Union[AnalyzeHealthcareEntitiesResult, DocumentError]]
-                ],
-                self._client.begin_health(
-                    docs,
-                    model_version=model_version,
-                    string_index_type=string_index_type,
-                    logging_opt_out=disable_service_logs,
-                    cls=my_cls,
-                    polling=AnalyzeHealthcareEntitiesLROPollingMethod(
-                        text_analytics_client=self._client,
-                        timeout=polling_interval,
-                        doc_id_order=doc_id_order,
-                        show_stats=show_stats,
-                        lro_algorithms=[
-                            TextAnalyticsOperationResourcePolling(
-                                show_stats=show_stats,
-                            )
-                        ],
-                        **kwargs
-                    ),
-                    continuation_token=continuation_token,
-                    **kwargs
-                )
-            )
-        except HttpResponseError as error:
-            return process_http_response_error(error)
-
-    @distributed_trace
+    @distributed_trace_async
     @validate_multiapi_args(
         version_method_added="v3.0",
         args_mapping={"v3.1": ["disable_service_logs"]}
     )
-    def extract_key_phrases(
+    async def extract_key_phrases(
         self,
         documents: Union[List[str], List[TextDocumentInput], List[Dict[str, str]]],
-        **kwargs: Any,
+        *,
+        disable_service_logs: Optional[bool] = None,
+        language: Optional[str] = None,
+        model_version: Optional[str] = None,
+        show_stats: Optional[bool] = None,
+        **kwargs: Any
     ) -> List[Union[ExtractKeyPhrasesResult, DocumentError]]:
         """Extract key phrases from a batch of documents.
 
         Returns a list of strings denoting the key phrases in the input
         text. For example, for the input text "The food was delicious and there
         were wonderful staff", the API returns the main talking points: "food"
         and "wonderful staff"
@@ -878,34 +669,31 @@
         :raises ~azure.core.exceptions.HttpResponseError or TypeError or ValueError:
 
         .. versionadded:: v3.1
             The *disable_service_logs* keyword argument.
 
         .. admonition:: Example:
 
-            .. literalinclude:: ../samples/sample_extract_key_phrases.py
-                :start-after: [START extract_key_phrases]
-                :end-before: [END extract_key_phrases]
+            .. literalinclude:: ../samples/async_samples/sample_extract_key_phrases_async.py
+                :start-after: [START extract_key_phrases_async]
+                :end-before: [END extract_key_phrases_async]
                 :language: python
                 :dedent: 4
                 :caption: Extract the key phrases in a batch of documents.
         """
-        language_arg = kwargs.pop("language", None)
-        language = language_arg if language_arg is not None else self._default_language
-        docs = _validate_input(documents, "language", language)
-        model_version = kwargs.pop("model_version", None)
-        show_stats = kwargs.pop("show_stats", None)
-        disable_service_logs = kwargs.pop("disable_service_logs", None)
+
+        language_arg = language if language is not None else self._default_language
+        docs = _validate_input(documents, "language", language_arg)
 
         try:
             if is_language_api(self._api_version):
                 models = self._client.models(api_version=self._api_version)
                 return cast(
                     List[Union[ExtractKeyPhrasesResult, DocumentError]],
-                    self._client.analyze_text(
+                    await self._client.analyze_text(
                         body=models.AnalyzeTextKeyPhraseExtractionInput(
                             analysis_input={"documents": docs},
                             parameters=models.KeyPhraseTaskParameters(
                                 logging_opt_out=disable_service_logs,
                                 model_version=model_version,
                             )
                         ),
@@ -914,48 +702,55 @@
                         **kwargs
                     )
                 )
 
             # api_versions 3.0, 3.1
             return cast(
                 List[Union[ExtractKeyPhrasesResult, DocumentError]],
-                self._client.key_phrases(
+                await self._client.key_phrases(
                     documents=docs,
                     model_version=model_version,
                     show_stats=show_stats,
                     logging_opt_out=disable_service_logs,
                     cls=kwargs.pop("cls", key_phrases_result),
                     **kwargs
                 )
             )
         except HttpResponseError as error:
             return process_http_response_error(error)
 
-    @distributed_trace
+    @distributed_trace_async
     @validate_multiapi_args(
         version_method_added="v3.0",
         args_mapping={"v3.1": ["show_opinion_mining", "disable_service_logs", "string_index_type"]}
     )
-    def analyze_sentiment(
+    async def analyze_sentiment(
         self,
         documents: Union[List[str], List[TextDocumentInput], List[Dict[str, str]]],
+        *,
+        disable_service_logs: Optional[bool] = None,
+        language: Optional[str] = None,
+        model_version: Optional[str] = None,
+        show_opinion_mining: Optional[bool] = None,
+        show_stats: Optional[bool] = None,
+        string_index_type: Optional[str] = None,
         **kwargs: Any,
     ) -> List[Union[AnalyzeSentimentResult, DocumentError]]:
         """Analyze sentiment for a batch of documents. Turn on opinion mining with `show_opinion_mining`.
 
         Returns a sentiment prediction, as well as sentiment scores for
         each sentiment class (Positive, Negative, and Neutral) for the document
         and each sentence within it.
 
         See https://aka.ms/azsdk/textanalytics/data-limits for service data limits.
 
         :param documents: The set of documents to process as part of this batch.
             If you wish to specify the ID and language on a per-item basis you must
             use as input a list[:class:`~azure.ai.textanalytics.TextDocumentInput`] or a list of
-            dict representations of  :class:`~azure.ai.textanalytics.TextDocumentInput`, like
+            dict representations of :class:`~azure.ai.textanalytics.TextDocumentInput`, like
             `{"id": "1", "language": "en", "text": "hello world"}`.
         :type documents:
             list[str] or list[~azure.ai.textanalytics.TextDocumentInput] or list[dict[str, str]]
         :keyword bool show_opinion_mining: Whether to mine the opinions of a sentence and conduct more
             granular analysis around the aspects of a product or service (also known as
             aspect-based sentiment analysis). If set to true, the returned
             :class:`~azure.ai.textanalytics.SentenceSentiment` objects
@@ -991,68 +786,291 @@
         :raises ~azure.core.exceptions.HttpResponseError or TypeError or ValueError:
 
         .. versionadded:: v3.1
             The *show_opinion_mining*, *disable_service_logs*, and *string_index_type* keyword arguments.
 
         .. admonition:: Example:
 
-            .. literalinclude:: ../samples/sample_analyze_sentiment.py
-                :start-after: [START analyze_sentiment]
-                :end-before: [END analyze_sentiment]
+            .. literalinclude:: ../samples/async_samples/sample_analyze_sentiment_async.py
+                :start-after: [START analyze_sentiment_async]
+                :end-before: [END analyze_sentiment_async]
                 :language: python
                 :dedent: 4
                 :caption: Analyze sentiment in a batch of documents.
         """
-        language_arg = kwargs.pop("language", None)
-        language = language_arg if language_arg is not None else self._default_language
-        docs = _validate_input(documents, "language", language)
-        model_version = kwargs.pop("model_version", None)
-        show_stats = kwargs.pop("show_stats", None)
-        show_opinion_mining = kwargs.pop("show_opinion_mining", None)
-        disable_service_logs = kwargs.pop("disable_service_logs", None)
-        string_index_type = kwargs.pop("string_index_type", self._string_index_type_default)
+
+        language_arg = language if language is not None else self._default_language
+        docs = _validate_input(documents, "language", language_arg)
+        string_index_type_arg = string_index_type if string_index_type is not None else self._string_code_unit
 
         try:
             if is_language_api(self._api_version):
                 models = self._client.models(api_version=self._api_version)
                 return cast(
                     List[Union[AnalyzeSentimentResult, DocumentError]],
-                    self._client.analyze_text(
+                    await self._client.analyze_text(
                         body=models.AnalyzeTextSentimentAnalysisInput(
                             analysis_input={"documents": docs},
                             parameters=models.SentimentAnalysisTaskParameters(
                                 logging_opt_out=disable_service_logs,
                                 model_version=model_version,
-                                string_index_type=string_index_type_compatibility(string_index_type),
+                                string_index_type=string_index_type_compatibility(string_index_type_arg),
                                 opinion_mining=show_opinion_mining,
                             )
                         ),
                         show_stats=show_stats,
                         cls=kwargs.pop("cls", sentiment_result),
                         **kwargs
                     )
                 )
 
             # api_versions 3.0, 3.1
             return cast(
                 List[Union[AnalyzeSentimentResult, DocumentError]],
-                self._client.sentiment(
+                await self._client.sentiment(
                     documents=docs,
                     logging_opt_out=disable_service_logs,
                     model_version=model_version,
-                    string_index_type=string_index_type,
+                    string_index_type=string_index_type_arg,
                     opinion_mining=show_opinion_mining,
                     show_stats=show_stats,
                     cls=kwargs.pop("cls", sentiment_result),
                     **kwargs
                 )
             )
         except HttpResponseError as error:
             return process_http_response_error(error)
 
+    # pylint: disable=unused-argument
+    def _healthcare_result_callback(
+        self, raw_response, deserialized, doc_id_order, task_id_order=None, show_stats=False, bespoke=False
+    ):
+        if deserialized is None:
+            models = self._client.models(api_version=self._api_version)
+            response_cls = \
+                models.AnalyzeTextJobState if is_language_api(self._api_version) else models.HealthcareJobState
+            deserialized = response_cls.deserialize(raw_response)
+        return healthcare_paged_result(
+            doc_id_order,
+            self._client.analyze_text_job_status if is_language_api(self._api_version) else self._client.health_status,
+            raw_response,
+            deserialized,
+            show_stats=show_stats,
+        )
+
+    @distributed_trace_async
+    @validate_multiapi_args(
+        version_method_added="v3.1",
+        args_mapping={
+            "2022-10-01-preview": ["fhir_version", "document_type"],
+            "2022-05-01": ["display_name"]
+        }
+    )
+    async def begin_analyze_healthcare_entities(
+        self,
+        documents: Union[List[str], List[TextDocumentInput], List[Dict[str, str]]],
+        *,
+        continuation_token: Optional[str] = None,
+        disable_service_logs: Optional[bool] = None,
+        display_name: Optional[str] = None,
+        document_type: Optional[Union[str, HealthcareDocumentType]] = None,
+        fhir_version: Optional[str] = None,
+        language: Optional[str] = None,
+        model_version: Optional[str] = None,
+        polling_interval: Optional[int] = None,
+        show_stats: Optional[bool] = None,
+        string_index_type: Optional[str] = None,
+        **kwargs: Any,
+    ) -> AsyncAnalyzeHealthcareEntitiesLROPoller[
+        AsyncItemPaged[Union[AnalyzeHealthcareEntitiesResult, DocumentError]]
+    ]:
+        """Analyze healthcare entities and identify relationships between these entities in a batch of documents.
+
+        Entities are associated with references that can be found in existing knowledge bases,
+        such as UMLS, CHV, MSH, etc.
+
+        We also extract the relations found between entities, for example in "The subject took 100 mg of ibuprofen",
+        we would extract the relationship between the "100 mg" dosage and the "ibuprofen" medication.
+
+        :param documents: The set of documents to process as part of this batch.
+            If you wish to specify the ID and language on a per-item basis you must
+            use as input a list[:class:`~azure.ai.textanalytics.TextDocumentInput`] or a list of
+            dict representations of :class:`~azure.ai.textanalytics.TextDocumentInput`, like
+            `{"id": "1", "language": "en", "text": "hello world"}`.
+        :type documents:
+            list[str] or list[~azure.ai.textanalytics.TextDocumentInput] or list[dict[str, str]]
+        :keyword str model_version: This value indicates which model will
+            be used for scoring, e.g. "latest", "2019-10-01". If a model-version
+            is not specified, the API will default to the latest, non-preview version.
+            See here for more info: https://aka.ms/text-analytics-model-versioning
+        :keyword bool show_stats: If set to true, response will contain document level statistics.
+        :keyword str language: The 2 letter ISO 639-1 representation of language for the
+            entire batch. For example, use "en" for English; "es" for Spanish etc.
+            For automatic language detection, use "auto" (Only supported by API version
+            2022-10-01-preview and newer). If not set, uses "en" for English as default.
+            Per-document language will take precedence over whole batch language.
+            See https://aka.ms/talangs for supported languages in Language API.
+        :keyword str display_name: An optional display name to set for the requested analysis.
+        :keyword str string_index_type: Specifies the method used to interpret string offsets.
+            Can be one of 'UnicodeCodePoint' (default), 'Utf16CodeUnit', or 'TextElement_v8'.
+            For additional information see https://aka.ms/text-analytics-offsets
+        :keyword int polling_interval: Waiting time between two polls for LRO operations
+            if no Retry-After header is present. Defaults to 5 seconds.
+        :keyword str continuation_token:
+            Call `continuation_token()` on the poller object to save the long-running operation (LRO)
+            state into an opaque token. Pass the value as the `continuation_token` keyword argument
+            to restart the LRO from a saved state.
+        :keyword bool disable_service_logs: Defaults to true, meaning that the Language service will not log your
+            input text on the service side for troubleshooting. If set to False, the Language service logs your
+            input text for 48 hours, solely to allow for troubleshooting issues in providing you with
+            the Text Analytics natural language processing functions. Please see
+            Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
+            additional details, and Microsoft Responsible AI principles at
+            https://www.microsoft.com/ai/responsible-ai.
+        :keyword str fhir_version: The FHIR Spec version that the result will use to format the fhir_bundle
+            on the result object. For additional information see https://www.hl7.org/fhir/overview.html.
+            The only acceptable values to pass in are None and "4.0.1". The default value is None.
+        :keyword document_type: Document type that can be provided as input for Fhir Documents. Expect to
+            have fhir_version provided when used. Behavior of using None enum is the same as not using the
+            document_type parameter. Known values are: "None", "ClinicalTrial", "DischargeSummary",
+            "ProgressNote", "HistoryAndPhysical", "Consult", "Imaging", "Pathology", and "ProcedureNote".
+        :paramtype document_type: str or ~azure.ai.textanalytics.HealthcareDocumentType
+        :return: An instance of an AsyncAnalyzeHealthcareEntitiesLROPoller. Call `result()` on the poller
+            object to return a heterogeneous pageable of
+            :class:`~azure.ai.textanalytics.AnalyzeHealthcareEntitiesResult` and
+            :class:`~azure.ai.textanalytics.DocumentError`.
+        :rtype:
+            ~azure.ai.textanalytics.aio.AsyncAnalyzeHealthcareEntitiesLROPoller[~azure.core.async_paging.AsyncItemPaged[
+            ~azure.ai.textanalytics.AnalyzeHealthcareEntitiesResult or ~azure.ai.textanalytics.DocumentError]]
+        :raises ~azure.core.exceptions.HttpResponseError or TypeError or ValueError:
+
+        .. versionadded:: v3.1
+            The *begin_analyze_healthcare_entities* client method.
+        .. versionadded:: 2022-05-01
+            The *display_name* keyword argument.
+        .. versionadded:: 2022-10-01-preview
+            The *fhir_version* and *document_type* keyword arguments.
+
+        .. admonition:: Example:
+
+            .. literalinclude:: ../samples/async_samples/sample_analyze_healthcare_entities_async.py
+                :start-after: [START analyze_healthcare_entities_async]
+                :end-before: [END analyze_healthcare_entities_async]
+                :language: python
+                :dedent: 4
+                :caption: Analyze healthcare entities in a batch of documents.
+        """
+
+        language_arg = language if language is not None else self._default_language
+        polling_interval_arg = polling_interval if polling_interval is not None else 5
+        string_index_type_arg = string_index_type if string_index_type is not None else self._string_code_unit
+
+        if continuation_token:
+            return cast(
+                AsyncAnalyzeHealthcareEntitiesLROPoller[
+                    AsyncItemPaged[Union[AnalyzeHealthcareEntitiesResult, DocumentError]]
+                ],
+                _get_result_from_continuation_token(
+                    self._client._client,  # pylint: disable=protected-access
+                    continuation_token,
+                    AsyncAnalyzeHealthcareEntitiesLROPoller,
+                    AsyncAnalyzeHealthcareEntitiesLROPollingMethod(
+                        text_analytics_client=self._client,
+                        timeout=polling_interval_arg,
+                        **kwargs
+                    ),
+                    self._healthcare_result_callback
+                )
+            )
+
+        docs = _validate_input(documents, "language", language_arg)
+        doc_id_order = [doc.get("id") for doc in docs]
+        my_cls = kwargs.pop(
+            "cls",
+            lambda pipeline_response, deserialized, _: self._healthcare_result_callback(
+                pipeline_response, deserialized, doc_id_order, show_stats=show_stats
+            ),
+        )
+        models = self._client.models(api_version=self._api_version)
+
+        try:
+            if is_language_api(self._api_version):
+                input_docs = models.MultiLanguageAnalysisInput(
+                    documents=docs
+                )
+                return cast(
+                    AsyncAnalyzeHealthcareEntitiesLROPoller[
+                        AsyncItemPaged[Union[AnalyzeHealthcareEntitiesResult, DocumentError]]
+                    ],
+                    await self._client.begin_analyze_text_submit_job(  # type: ignore
+                        body=models.AnalyzeTextJobsInput(
+                            analysis_input=input_docs,
+                            display_name=display_name,
+                            tasks=[
+                                models.HealthcareLROTask(
+                                    task_name="0",
+                                    parameters=models.HealthcareTaskParameters(
+                                        model_version=model_version,
+                                        logging_opt_out=disable_service_logs,
+                                        string_index_type=string_index_type_compatibility(string_index_type_arg),
+                                        fhir_version=fhir_version,
+                                        document_type=document_type,
+                                    )
+                                )
+                            ]
+                        ),
+                        cls=my_cls,
+                        polling=AsyncAnalyzeHealthcareEntitiesLROPollingMethod(
+                            text_analytics_client=self._client,
+                            timeout=polling_interval_arg,
+                            show_stats=show_stats,
+                            doc_id_order=doc_id_order,
+                            lro_algorithms=[
+                                TextAnalyticsOperationResourcePolling(
+                                    show_stats=show_stats,
+                                )
+                            ],
+                            **kwargs
+                        ),
+                        continuation_token=continuation_token,
+                        poller_cls=AsyncAnalyzeHealthcareEntitiesLROPoller,
+                        **kwargs
+                    )
+                )
+
+            # v3.1
+            return cast(
+                AsyncAnalyzeHealthcareEntitiesLROPoller[
+                    AsyncItemPaged[Union[AnalyzeHealthcareEntitiesResult, DocumentError]]
+                ],
+                await self._client.begin_health(
+                    docs,
+                    model_version=model_version,
+                    string_index_type=string_index_type_arg,
+                    logging_opt_out=disable_service_logs,
+                    cls=my_cls,
+                    polling=AsyncAnalyzeHealthcareEntitiesLROPollingMethod(
+                        text_analytics_client=self._client,
+                        doc_id_order=doc_id_order,
+                        show_stats=show_stats,
+                        timeout=polling_interval_arg,
+                        lro_algorithms=[
+                            TextAnalyticsOperationResourcePolling(
+                                show_stats=show_stats,
+                            )
+                        ],
+                        **kwargs,
+                    ),
+                    continuation_token=continuation_token,
+                    **kwargs,
+                )
+            )
+        except HttpResponseError as error:
+            return process_http_response_error(error)
+
     def _analyze_result_callback(
         self, raw_response, deserialized, doc_id_order, task_id_order=None, show_stats=False, bespoke=False
     ):
 
         if deserialized is None:
             models = self._client.models(api_version=self._api_version)
             response_cls = models.AnalyzeTextJobState if is_language_api(self._api_version) else models.AnalyzeJobState
@@ -1063,55 +1081,58 @@
             self._client.analyze_text_job_status if is_language_api(self._api_version) else self._client.analyze_status,
             raw_response,
             deserialized,
             show_stats=show_stats,
             bespoke=bespoke
         )
 
-    @distributed_trace
+    @distributed_trace_async
     @validate_multiapi_args(
         version_method_added="v3.1",
         custom_wrapper=check_for_unsupported_actions_types,
-        args_mapping={
-            "2022-10-01-preview": ["autodetect_default_language"],
-        }
     )
-    def begin_analyze_actions(
+    async def begin_analyze_actions(
         self,
         documents: Union[List[str], List[TextDocumentInput], List[Dict[str, str]]],
         actions: List[
             Union[
                 RecognizeEntitiesAction,
                 RecognizeLinkedEntitiesAction,
                 RecognizePiiEntitiesAction,
                 ExtractKeyPhrasesAction,
                 AnalyzeSentimentAction,
                 RecognizeCustomEntitiesAction,
                 SingleLabelClassifyAction,
                 MultiLabelClassifyAction,
                 AnalyzeHealthcareEntitiesAction,
                 ExtractSummaryAction,
-                AbstractSummaryAction,
+                AbstractiveSummaryAction,
             ]
         ],
+        *,
+        continuation_token: Optional[str] = None,
+        display_name: Optional[str] = None,
+        language: Optional[str] = None,
+        polling_interval: Optional[int] = None,
+        show_stats: Optional[bool] = None,
         **kwargs: Any,
-    ) -> TextAnalysisLROPoller[
-        ItemPaged[
+    ) -> AsyncTextAnalysisLROPoller[
+        AsyncItemPaged[
             List[
                 Union[
                     RecognizeEntitiesResult,
                     RecognizeLinkedEntitiesResult,
                     RecognizePiiEntitiesResult,
                     ExtractKeyPhrasesResult,
                     AnalyzeSentimentResult,
                     RecognizeCustomEntitiesResult,
                     ClassifyDocumentResult,
                     AnalyzeHealthcareEntitiesResult,
                     ExtractSummaryResult,
-                    AbstractSummaryResult,
+                    AbstractiveSummaryResult,
                     DocumentError,
                 ]
             ]
         ]
     ]:
         """Start a long-running operation to perform a variety of text analysis actions over a batch of documents.
 
@@ -1135,116 +1156,109 @@
         :param actions: A heterogeneous list of actions to perform on the input documents.
             Each action object encapsulates the parameters used for the particular action type.
             The action results will be in the same order of the input actions.
         :type actions:
             list[RecognizeEntitiesAction or RecognizePiiEntitiesAction or ExtractKeyPhrasesAction or
             RecognizeLinkedEntitiesAction or AnalyzeSentimentAction or
             RecognizeCustomEntitiesAction or SingleLabelClassifyAction or
-            MultiLabelClassifyAction or AnalyzeHealthcareEntitiesAction or ExtractSummaryAction
-            or AbstractSummaryAction]
+            MultiLabelClassifyAction or AnalyzeHealthcareEntitiesAction or
+            AbstractiveSummaryAction or ExtractSummaryAction]
         :keyword str display_name: An optional display name to set for the requested analysis.
         :keyword str language: The 2 letter ISO 639-1 representation of language for the
             entire batch. For example, use "en" for English; "es" for Spanish etc.
             For automatic language detection, use "auto" (Only supported by API version
             2022-10-01-preview and newer). If not set, uses "en" for English as default.
             Per-document language will take precedence over whole batch language.
             See https://aka.ms/talangs for supported languages in Language API.
-        :keyword str autodetect_default_language: Default/fallback language to use for documents requesting
-            automatic language detection.
         :keyword bool show_stats: If set to true, response will contain document level statistics.
         :keyword int polling_interval: Waiting time between two polls for LRO operations
             if no Retry-After header is present. Defaults to 5 seconds.
         :keyword str continuation_token:
             Call `continuation_token()` on the poller object to save the long-running operation (LRO)
             state into an opaque token. Pass the value as the `continuation_token` keyword argument
             to restart the LRO from a saved state.
-        :return: An instance of an TextAnalysisLROPoller. Call `result()` on the poller
+        :return: An instance of an AsyncTextAnalysisLROPoller. Call `result()` on the poller
             object to return a pageable heterogeneous list of lists. This list of lists is first ordered
             by the documents you input, then ordered by the actions you input. For example,
             if you have documents input ["Hello", "world"], and actions
             :class:`~azure.ai.textanalytics.RecognizeEntitiesAction` and
             :class:`~azure.ai.textanalytics.AnalyzeSentimentAction`, when iterating over the list of lists,
             you will first iterate over the action results for the "Hello" document, getting the
             :class:`~azure.ai.textanalytics.RecognizeEntitiesResult` of "Hello",
             then the :class:`~azure.ai.textanalytics.AnalyzeSentimentResult` of "Hello".
             Then, you will get the :class:`~azure.ai.textanalytics.RecognizeEntitiesResult` and
             :class:`~azure.ai.textanalytics.AnalyzeSentimentResult` of "world".
         :rtype:
-            ~azure.ai.textanalytics.TextAnalysisLROPoller[~azure.core.paging.ItemPaged[
+            ~azure.ai.textanalytics.aio.AsyncTextAnalysisLROPoller[~azure.core.async_paging.AsyncItemPaged[
             list[RecognizeEntitiesResult or RecognizeLinkedEntitiesResult or RecognizePiiEntitiesResult or
             ExtractKeyPhrasesResult or AnalyzeSentimentResult or RecognizeCustomEntitiesResult
             or ClassifyDocumentResult or AnalyzeHealthcareEntitiesResult or ExtractSummaryResult
-            or AbstractSummaryResult or DocumentError]]]
+            or AbstractiveSummaryResult or DocumentError]]]
         :raises ~azure.core.exceptions.HttpResponseError or TypeError or ValueError:
 
         .. versionadded:: v3.1
             The *begin_analyze_actions* client method.
         .. versionadded:: 2022-05-01
             The *RecognizeCustomEntitiesAction*, *SingleLabelClassifyAction*,
             *MultiLabelClassifyAction*, and *AnalyzeHealthcareEntitiesAction* input options and the
             corresponding *RecognizeCustomEntitiesResult*, *ClassifyDocumentResult*,
             and *AnalyzeHealthcareEntitiesResult* result objects
         .. versionadded:: 2022-10-01-preview
             The *ExtractSummaryAction* and *AbstractSummaryAction* input options and the corresponding
             *ExtractSummaryResult* and *AbstractSummaryResult* result objects.
-            The *autodetect_default_language* keyword argument.
 
         .. admonition:: Example:
 
-            .. literalinclude:: ../samples/sample_analyze_actions.py
-                :start-after: [START analyze]
-                :end-before: [END analyze]
+            .. literalinclude:: ../samples/async_samples/sample_analyze_actions_async.py
+                :start-after: [START analyze_async]
+                :end-before: [END analyze_async]
                 :language: python
                 :dedent: 4
-                :caption: Start a long-running operation to perform a variety of text analysis
-                    actions over a batch of documents.
+                :caption: Start a long-running operation to perform a variety of text analysis actions over
+                    a batch of documents.
         """
 
-        continuation_token = kwargs.pop("continuation_token", None)
-        display_name = kwargs.pop("display_name", None)
-        language_arg = kwargs.pop("language", None)
-        show_stats = kwargs.pop("show_stats", None)
-        polling_interval = kwargs.pop("polling_interval", 5)
-        language = language_arg if language_arg is not None else self._default_language
+        language_arg = language if language is not None else self._default_language
+        polling_interval_arg = polling_interval if polling_interval is not None else 5
         bespoke = kwargs.pop("bespoke", False)
-        autodetect_default_language = kwargs.pop("autodetect_default_language", None)
 
         if continuation_token:
             return cast(
-                AnalyzeActionsResponse,
+                AsyncAnalyzeActionsResponse,
                 _get_result_from_continuation_token(
                     self._client._client,  # pylint: disable=protected-access
                     continuation_token,
-                    AnalyzeActionsLROPoller,
-                    AnalyzeActionsLROPollingMethod(
+                    AsyncAnalyzeActionsLROPoller,
+                    AsyncAnalyzeActionsLROPollingMethod(
                         text_analytics_client=self._client,
-                        timeout=polling_interval,
+                        timeout=polling_interval_arg,
                         **kwargs
                     ),
                     self._analyze_result_callback,
                     bespoke
                 )
             )
 
         models = self._client.models(api_version=self._api_version)
 
         input_model_cls = \
             models.MultiLanguageAnalysisInput if is_language_api(self._api_version) else models.MultiLanguageBatchInput
         docs = input_model_cls(
-            documents=_validate_input(documents, "language", language)
+            documents=_validate_input(documents, "language", language_arg)
         )
         doc_id_order = [doc.get("id") for doc in docs.documents]
         try:
             generated_tasks = [
                 action._to_generated(self._api_version, str(idx))  # pylint: disable=protected-access
                 for idx, action in enumerate(actions)
             ]
         except AttributeError as e:
             raise TypeError("Unsupported action type in list.") from e
         task_order = [(_determine_action_type(a), a.task_name) for a in generated_tasks]
+
         response_cls = kwargs.pop(
             "cls",
             lambda pipeline_response, deserialized, _:
                 self._analyze_result_callback(
                     pipeline_response,
                     deserialized,
                     doc_id_order,
@@ -1253,26 +1267,25 @@
                     bespoke=bespoke
                 ),
         )
 
         try:
             if is_language_api(self._api_version):
                 return cast(
-                    AnalyzeActionsResponse,
-                    self._client.begin_analyze_text_submit_job(
+                    AsyncAnalyzeActionsResponse,
+                    await self._client.begin_analyze_text_submit_job(
                         body=models.AnalyzeTextJobsInput(
                             analysis_input=docs,
                             display_name=display_name,
-                            default_language=autodetect_default_language,
                             tasks=generated_tasks
                         ),
                         cls=response_cls,
-                        polling=AnalyzeActionsLROPollingMethod(
+                        polling=AsyncAnalyzeActionsLROPollingMethod(
                             text_analytics_client=self._client,
-                            timeout=polling_interval,
+                            timeout=polling_interval_arg,
                             show_stats=show_stats,
                             doc_id_order=doc_id_order,
                             task_id_order=task_order,
                             lro_algorithms=[
                                 TextAnalyticsOperationResourcePolling(
                                     show_stats=show_stats,
                                 )
@@ -1307,52 +1320,57 @@
                     if _determine_action_type(a) == _AnalyzeActionsType.ANALYZE_SENTIMENT
                 ],
             )
             analyze_body = models.AnalyzeBatchInput(
                 display_name=display_name, tasks=analyze_tasks, analysis_input=docs
             )
             return cast(
-                AnalyzeActionsResponse,
-                self._client.begin_analyze(
+                AsyncAnalyzeActionsResponse,
+                await self._client.begin_analyze(
                     body=analyze_body,
                     cls=response_cls,
-                    polling=AnalyzeActionsLROPollingMethod(
+                    polling=AsyncAnalyzeActionsLROPollingMethod(
                         text_analytics_client=self._client,
-                        timeout=polling_interval,
+                        timeout=polling_interval_arg,
                         show_stats=show_stats,
                         doc_id_order=doc_id_order,
                         task_id_order=task_order,
                         lro_algorithms=[
                             TextAnalyticsOperationResourcePolling(
                                 show_stats=show_stats,
                             )
                         ],
-                        **kwargs
+                        **kwargs,
                     ),
                     continuation_token=continuation_token,
-                    **kwargs
+                    **kwargs,
                 )
             )
         except HttpResponseError as error:
             return process_http_response_error(error)
 
-    @distributed_trace
+    @distributed_trace_async
     @validate_multiapi_args(
         version_method_added="2022-05-01",
-        args_mapping={
-            "2022-10-01-preview": ["autodetect_default_language"],
-        }
     )
-    def begin_recognize_custom_entities(
+    async def begin_recognize_custom_entities(
         self,
         documents: Union[List[str], List[TextDocumentInput], List[Dict[str, str]]],
         project_name: str,
         deployment_name: str,
+        *,
+        continuation_token: Optional[str] = None,
+        disable_service_logs: Optional[bool] = None,
+        display_name: Optional[str] = None,
+        language: Optional[str] = None,
+        polling_interval: Optional[int] = None,
+        show_stats: Optional[bool] = None,
+        string_index_type: Optional[str] = None,
         **kwargs: Any,
-    ) -> TextAnalysisLROPoller[ItemPaged[Union[RecognizeCustomEntitiesResult, DocumentError]]]:
+    ) -> AsyncTextAnalysisLROPoller[AsyncItemPaged[Union[RecognizeCustomEntitiesResult, DocumentError]]]:
         """Start a long-running custom named entity recognition operation.
 
         For information on regional support of custom features and how to train a model to
         recognize custom entities, see https://aka.ms/azsdk/textanalytics/customentityrecognition
 
         :param documents: The set of documents to process as part of this batch.
             If you wish to specify the ID and language on a per-item basis you must
@@ -1365,16 +1383,14 @@
         :param str deployment_name: This field indicates the deployment name for the model.
         :keyword str language: The 2 letter ISO 639-1 representation of language for the
             entire batch. For example, use "en" for English; "es" for Spanish etc.
             For automatic language detection, use "auto" (Only supported by API version
             2022-10-01-preview and newer). If not set, uses "en" for English as default.
             Per-document language will take precedence over whole batch language.
             See https://aka.ms/talangs for supported languages in Language API.
-        :keyword str autodetect_default_language: Default/fallback language to use for documents requesting
-            automatic language detection.
         :keyword bool show_stats: If set to true, response will contain document level statistics.
         :keyword bool disable_service_logs: If set to true, you opt-out of having your text input
             logged on the service side for troubleshooting. By default, the Language service logs your
             input text for 48 hours, solely to allow for troubleshooting issues in providing you with
             the service's natural language processing functions. Setting this parameter to true,
             disables input logging and may limit our ability to remediate issues that occur. Please see
             Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
@@ -1387,98 +1403,101 @@
         :keyword int polling_interval: Waiting time between two polls for LRO operations
             if no Retry-After header is present. Defaults to 5 seconds.
         :keyword str continuation_token:
             Call `continuation_token()` on the poller object to save the long-running operation (LRO)
             state into an opaque token. Pass the value as the `continuation_token` keyword argument
             to restart the LRO from a saved state.
         :keyword str display_name: An optional display name to set for the requested analysis.
-        :return: An instance of an TextAnalysisLROPoller. Call `result()` on the this
+        :return: An instance of an AsyncTextAnalysisLROPoller. Call `result()` on the this
             object to return a heterogeneous pageable of
             :class:`~azure.ai.textanalytics.RecognizeCustomEntitiesResult` and
             :class:`~azure.ai.textanalytics.DocumentError`.
         :rtype:
-            ~azure.ai.textanalytics.TextAnalysisLROPoller[~azure.core.paging.ItemPaged[
+            ~azure.ai.textanalytics.aio.AsyncTextAnalysisLROPoller[~azure.core.async_paging.AsyncItemPaged[
             ~azure.ai.textanalytics.RecognizeCustomEntitiesResult or ~azure.ai.textanalytics.DocumentError]]
         :raises ~azure.core.exceptions.HttpResponseError:
 
         .. versionadded:: 2022-05-01
             The *begin_recognize_custom_entities* client method.
-        .. versionadded:: 2022-10-01-preview
-            The *autodetect_default_language* keyword argument.
 
         .. admonition:: Example:
 
-            .. literalinclude:: ../samples/sample_recognize_custom_entities.py
-                :start-after: [START recognize_custom_entities]
-                :end-before: [END recognize_custom_entities]
+            .. literalinclude:: ../samples/async_samples/sample_recognize_custom_entities_async.py
+                :start-after: [START recognize_custom_entities_async]
+                :end-before: [END recognize_custom_entities_async]
                 :language: python
                 :dedent: 4
                 :caption: Recognize custom entities in a batch of documents.
         """
 
-        continuation_token = kwargs.pop("continuation_token", None)
-        string_index_type = kwargs.pop("string_index_type", self._string_index_type_default)
-        disable_service_logs = kwargs.pop("disable_service_logs", None)
-        polling_interval = kwargs.pop("polling_interval", 5)
+        polling_interval_arg = polling_interval if polling_interval is not None else 5
+        string_index_type_arg = string_index_type if string_index_type is not None else self._string_code_unit
 
         if continuation_token:
             return cast(
-                TextAnalysisLROPoller[ItemPaged[Union[RecognizeCustomEntitiesResult, DocumentError]]],
+                AsyncTextAnalysisLROPoller[AsyncItemPaged[Union[RecognizeCustomEntitiesResult, DocumentError]]],
                 _get_result_from_continuation_token(
                     self._client._client,  # pylint: disable=protected-access
                     continuation_token,
-                    AnalyzeActionsLROPoller,
-                    AnalyzeActionsLROPollingMethod(
+                    AsyncAnalyzeActionsLROPoller,
+                    AsyncAnalyzeActionsLROPollingMethod(
                         text_analytics_client=self._client,
-                        timeout=polling_interval,
+                        timeout=polling_interval_arg,
                         **kwargs
                     ),
                     self._analyze_result_callback,
                     bespoke=True
                 )
             )
 
         try:
             return cast(
-                TextAnalysisLROPoller[
-                    ItemPaged[Union[RecognizeCustomEntitiesResult, DocumentError]]
+                AsyncTextAnalysisLROPoller[
+                    AsyncItemPaged[Union[RecognizeCustomEntitiesResult, DocumentError]]
                 ],
-                self.begin_analyze_actions(
+                await self.begin_analyze_actions(
                     documents,
                     actions=[
                         RecognizeCustomEntitiesAction(
                             project_name=project_name,
                             deployment_name=deployment_name,
-                            string_index_type=string_index_type,
+                            string_index_type=string_index_type_arg,
                             disable_service_logs=disable_service_logs
                         )
                     ],
-                    polling_interval=polling_interval,
+                    display_name=display_name,
+                    show_stats=show_stats,
+                    language=language,
+                    polling_interval=polling_interval_arg,
                     bespoke=True,
                     **kwargs
                 )
             )
 
         except HttpResponseError as error:
             return process_http_response_error(error)
 
-    @distributed_trace
+    @distributed_trace_async
     @validate_multiapi_args(
         version_method_added="2022-05-01",
-        args_mapping={
-            "2022-10-01-preview": ["autodetect_default_language"],
-        }
     )
-    def begin_single_label_classify(
+    async def begin_single_label_classify(
         self,
         documents: Union[List[str], List[TextDocumentInput], List[Dict[str, str]]],
         project_name: str,
         deployment_name: str,
+        *,
+        continuation_token: Optional[str] = None,
+        disable_service_logs: Optional[bool] = None,
+        display_name: Optional[str] = None,
+        language: Optional[str] = None,
+        polling_interval: Optional[int] = None,
+        show_stats: Optional[bool] = None,
         **kwargs: Any,
-    ) -> TextAnalysisLROPoller[ItemPaged[Union[ClassifyDocumentResult, DocumentError]]]:
+    ) -> AsyncTextAnalysisLROPoller[AsyncItemPaged[Union[ClassifyDocumentResult, DocumentError]]]:
         """Start a long-running custom single label classification operation.
 
         For information on regional support of custom features and how to train a model to
         classify your documents, see https://aka.ms/azsdk/textanalytics/customfunctionalities
 
         :param documents: The set of documents to process as part of this batch.
             If you wish to specify the ID and language on a per-item basis you must
@@ -1491,16 +1510,14 @@
         :param str deployment_name: This field indicates the deployment name for the model.
         :keyword str language: The 2 letter ISO 639-1 representation of language for the
             entire batch. For example, use "en" for English; "es" for Spanish etc.
             For automatic language detection, use "auto" (Only supported by API version
             2022-10-01-preview and newer). If not set, uses "en" for English as default.
             Per-document language will take precedence over whole batch language.
             See https://aka.ms/talangs for supported languages in Language API.
-        :keyword str autodetect_default_language: Default/fallback language to use for documents requesting
-            automatic language detection.
         :keyword bool show_stats: If set to true, response will contain document level statistics.
         :keyword bool disable_service_logs: If set to true, you opt-out of having your text input
             logged on the service side for troubleshooting. By default, the Language service logs your
             input text for 48 hours, solely to allow for troubleshooting issues in providing you with
             the service's natural language processing functions. Setting this parameter to true,
             disables input logging and may limit our ability to remediate issues that occur. Please see
             Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
@@ -1509,96 +1526,99 @@
         :keyword int polling_interval: Waiting time between two polls for LRO operations
             if no Retry-After header is present. Defaults to 5 seconds.
         :keyword str continuation_token:
             Call `continuation_token()` on the poller object to save the long-running operation (LRO)
             state into an opaque token. Pass the value as the `continuation_token` keyword argument
             to restart the LRO from a saved state.
         :keyword str display_name: An optional display name to set for the requested analysis.
-        :return: An instance of an TextAnalysisLROPoller. Call `result()` on the this
+        :return: An instance of an AsyncTextAnalysisLROPoller. Call `result()` on the this
             object to return a heterogeneous pageable of
             :class:`~azure.ai.textanalytics.ClassifyDocumentResult` and
             :class:`~azure.ai.textanalytics.DocumentError`.
         :rtype:
-            ~azure.ai.textanalytics.TextAnalysisLROPoller[~azure.core.paging.ItemPaged[
+            ~azure.ai.textanalytics.aio.AsyncTextAnalysisLROPoller[~azure.core.async_paging.AsyncItemPaged[
             ~azure.ai.textanalytics.ClassifyDocumentResult or ~azure.ai.textanalytics.DocumentError]]
         :raises ~azure.core.exceptions.HttpResponseError:
 
         .. versionadded:: 2022-05-01
             The *begin_single_label_classify* client method.
-        .. versionadded:: 2022-10-01-preview
-            The *autodetect_default_language* keyword argument.
 
         .. admonition:: Example:
 
-            .. literalinclude:: ../samples/sample_single_label_classify.py
-                :start-after: [START single_label_classify]
-                :end-before: [END single_label_classify]
+            .. literalinclude:: ../samples/async_samples/sample_single_label_classify_async.py
+                :start-after: [START single_label_classify_async]
+                :end-before: [END single_label_classify_async]
                 :language: python
                 :dedent: 4
                 :caption: Perform single label classification on a batch of documents.
         """
 
-        continuation_token = kwargs.pop("continuation_token", None)
-        disable_service_logs = kwargs.pop("disable_service_logs", None)
-        polling_interval = kwargs.pop("polling_interval", 5)
+        polling_interval_arg = polling_interval if polling_interval is not None else 5
 
         if continuation_token:
             return cast(
-                TextAnalysisLROPoller[ItemPaged[Union[ClassifyDocumentResult, DocumentError]]],
+                AsyncTextAnalysisLROPoller[AsyncItemPaged[Union[ClassifyDocumentResult, DocumentError]]],
                 _get_result_from_continuation_token(
                     self._client._client,  # pylint: disable=protected-access
                     continuation_token,
-                    AnalyzeActionsLROPoller,
-                    AnalyzeActionsLROPollingMethod(
+                    AsyncAnalyzeActionsLROPoller,
+                    AsyncAnalyzeActionsLROPollingMethod(
                         text_analytics_client=self._client,
-                        timeout=polling_interval,
+                        timeout=polling_interval_arg,
                         **kwargs
                     ),
                     self._analyze_result_callback,
                     bespoke=True
                 )
             )
 
         try:
             return cast(
-                TextAnalysisLROPoller[
-                    ItemPaged[Union[ClassifyDocumentResult, DocumentError]]
+                AsyncTextAnalysisLROPoller[
+                    AsyncItemPaged[Union[ClassifyDocumentResult, DocumentError]]
                 ],
-                self.begin_analyze_actions(
+                await self.begin_analyze_actions(
                     documents,
                     actions=[
                         SingleLabelClassifyAction(
                             project_name=project_name,
                             deployment_name=deployment_name,
                             disable_service_logs=disable_service_logs
                         )
                     ],
-                    polling_interval=polling_interval,
+                    polling_interval=polling_interval_arg,
+                    display_name=display_name,
+                    show_stats=show_stats,
+                    language=language,
                     bespoke=True,
                     **kwargs
                 )
             )
 
         except HttpResponseError as error:
             return process_http_response_error(error)
 
-    @distributed_trace
+    @distributed_trace_async
     @validate_multiapi_args(
         version_method_added="2022-05-01",
-        args_mapping={
-            "2022-10-01-preview": ["autodetect_default_language"],
-        }
     )
-    def begin_multi_label_classify(
+    async def begin_multi_label_classify(
         self,
         documents: Union[List[str], List[TextDocumentInput], List[Dict[str, str]]],
         project_name: str,
         deployment_name: str,
+        *,
+        continuation_token: Optional[str] = None,
+        disable_service_logs: Optional[bool] = None,
+        display_name: Optional[str] = None,
+        language: Optional[str] = None,
+        polling_interval: Optional[int] = None,
+        show_stats: Optional[bool] = None,
         **kwargs: Any,
-    ) -> TextAnalysisLROPoller[ItemPaged[Union[ClassifyDocumentResult, DocumentError]]]:
+    ) -> AsyncTextAnalysisLROPoller[AsyncItemPaged[Union[ClassifyDocumentResult, DocumentError]]]:
         """Start a long-running custom multi label classification operation.
 
         For information on regional support of custom features and how to train a model to
         classify your documents, see https://aka.ms/azsdk/textanalytics/customfunctionalities
 
         :param documents: The set of documents to process as part of this batch.
             If you wish to specify the ID and language on a per-item basis you must
@@ -1611,16 +1631,14 @@
         :param str deployment_name: This field indicates the deployment name for the model.
         :keyword str language: The 2 letter ISO 639-1 representation of language for the
             entire batch. For example, use "en" for English; "es" for Spanish etc.
             For automatic language detection, use "auto" (Only supported by API version
             2022-10-01-preview and newer). If not set, uses "en" for English as default.
             Per-document language will take precedence over whole batch language.
             See https://aka.ms/talangs for supported languages in Language API.
-        :keyword str autodetect_default_language: Default/fallback language to use for documents requesting
-            automatic language detection.
         :keyword bool show_stats: If set to true, response will contain document level statistics.
         :keyword bool disable_service_logs: If set to true, you opt-out of having your text input
             logged on the service side for troubleshooting. By default, the Language service logs your
             input text for 48 hours, solely to allow for troubleshooting issues in providing you with
             the service's natural language processing functions. Setting this parameter to true,
             disables input logging and may limit our ability to remediate issues that occur. Please see
             Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
@@ -1629,90 +1647,95 @@
         :keyword int polling_interval: Waiting time between two polls for LRO operations
             if no Retry-After header is present. Defaults to 5 seconds.
         :keyword str continuation_token:
             Call `continuation_token()` on the poller object to save the long-running operation (LRO)
             state into an opaque token. Pass the value as the `continuation_token` keyword argument
             to restart the LRO from a saved state.
         :keyword str display_name: An optional display name to set for the requested analysis.
-        :return: An instance of an TextAnalysisLROPoller. Call `result()` on the this
+        :return: An instance of an AsyncTextAnalysisLROPoller. Call `result()` on the this
             object to return a heterogeneous pageable of
             :class:`~azure.ai.textanalytics.ClassifyDocumentResult` and
             :class:`~azure.ai.textanalytics.DocumentError`.
         :rtype:
-            ~azure.ai.textanalytics.TextAnalysisLROPoller[~azure.core.paging.ItemPaged[
+            ~azure.ai.textanalytics.aio.AsyncTextAnalysisLROPoller[~azure.core.async_paging.AsyncItemPaged[
             ~azure.ai.textanalytics.ClassifyDocumentResult or ~azure.ai.textanalytics.DocumentError]]
         :raises ~azure.core.exceptions.HttpResponseError:
 
         .. versionadded:: 2022-05-01
             The *begin_multi_label_classify* client method.
-        .. versionadded:: 2022-10-01-preview
-            The *autodetect_default_language* keyword argument.
 
         .. admonition:: Example:
 
-            .. literalinclude:: ../samples/sample_multi_label_classify.py
-                :start-after: [START multi_label_classify]
-                :end-before: [END multi_label_classify]
+            .. literalinclude:: ../samples/async_samples/sample_multi_label_classify_async.py
+                :start-after: [START multi_label_classify_async]
+                :end-before: [END multi_label_classify_async]
                 :language: python
                 :dedent: 4
                 :caption: Perform multi label classification on a batch of documents.
         """
 
-        continuation_token = kwargs.pop("continuation_token", None)
-        disable_service_logs = kwargs.pop("disable_service_logs", None)
-        polling_interval = kwargs.pop("polling_interval", 5)
+        polling_interval_arg = polling_interval if polling_interval is not None else 5
 
         if continuation_token:
             return cast(
-                TextAnalysisLROPoller[ItemPaged[Union[ClassifyDocumentResult, DocumentError]]],
+                AsyncTextAnalysisLROPoller[AsyncItemPaged[Union[ClassifyDocumentResult, DocumentError]]],
                 _get_result_from_continuation_token(
                     self._client._client,  # pylint: disable=protected-access
                     continuation_token,
-                    AnalyzeActionsLROPoller,
-                    AnalyzeActionsLROPollingMethod(
+                    AsyncAnalyzeActionsLROPoller,
+                    AsyncAnalyzeActionsLROPollingMethod(
                         text_analytics_client=self._client,
-                        timeout=polling_interval,
+                        timeout=polling_interval_arg,
                         **kwargs
                     ),
                     self._analyze_result_callback,
                     bespoke=True
                 )
             )
 
         try:
             return cast(
-                TextAnalysisLROPoller[
-                    ItemPaged[Union[ClassifyDocumentResult, DocumentError]]
+                AsyncTextAnalysisLROPoller[
+                    AsyncItemPaged[Union[ClassifyDocumentResult, DocumentError]]
                 ],
-                self.begin_analyze_actions(
+                await self.begin_analyze_actions(
                     documents,
                     actions=[
                         MultiLabelClassifyAction(
                             project_name=project_name,
                             deployment_name=deployment_name,
                             disable_service_logs=disable_service_logs
                         )
                     ],
-                    polling_interval=polling_interval,
+                    polling_interval=polling_interval_arg,
+                    display_name=display_name,
+                    show_stats=show_stats,
+                    language=language,
                     bespoke=True,
                     **kwargs
                 )
             )
 
         except HttpResponseError as error:
             return process_http_response_error(error)
 
-    @distributed_trace
+    @distributed_trace_async
     @validate_multiapi_args(
         version_method_added="2022-10-01-preview",
     )
-    def dynamic_classification(
+    async def dynamic_classification(
         self,
         documents: Union[List[str], List[TextDocumentInput], List[Dict[str, str]]],
         categories: List[str],
+        *,
+        classification_type: Optional[Union[str, ClassificationType]] = None,
+        disable_service_logs: Optional[bool] = None,
+        language: Optional[str] = None,
+        model_version: Optional[str] = None,
+        show_stats: Optional[bool] = None,
         **kwargs: Any,
     ) -> List[Union[DynamicClassificationResult, DocumentError]]:
         """Perform dynamic classification on a batch of documents.
 
         On the fly classification of the input documents into one or multiple categories.
         Assigns either one or multiple categories per document. This type of classification
         doesn't require model training.
@@ -1760,34 +1783,30 @@
         :raises ~azure.core.exceptions.HttpResponseError:
 
         .. versionadded:: 2022-10-01-preview
             The *dynamic_classification* client method.
 
         .. admonition:: Example:
 
-            .. literalinclude:: ../samples/sample_dynamic_classification.py
-                :start-after: [START dynamic_classification]
-                :end-before: [END dynamic_classification]
+            .. literalinclude:: ../samples/async_samples/sample_dynamic_classification_async.py
+                :start-after: [START dynamic_classification_async]
+                :end-before: [END dynamic_classification_async]
                 :language: python
                 :dedent: 4
                 :caption: Perform dynamic classification on a batch of documents.
         """
-        language_arg = kwargs.pop("language", None)
-        language = language_arg if language_arg is not None else self._default_language
-        docs = _validate_input(documents, "language", language)
-        model_version = kwargs.pop("model_version", None)
-        show_stats = kwargs.pop("show_stats", None)
-        disable_service_logs = kwargs.pop("disable_service_logs", None)
-        classification_type = kwargs.pop("classification_type", None)
+
+        language_arg = language if language is not None else self._default_language
+        docs = _validate_input(documents, "language", language_arg)
 
         try:
             models = self._client.models(api_version=self._api_version)
             return cast(
                 List[Union[DynamicClassificationResult, DocumentError]],
-                self._client.analyze_text(
+                await self._client.analyze_text(
                     body=models.AnalyzeTextDynamicClassificationInput(
                         analysis_input={"documents": docs},
                         parameters=models.DynamicClassificationTaskParameters(
                             categories=categories,
                             logging_opt_out=disable_service_logs,
                             model_version=model_version,
                             classification_type=classification_type,
@@ -1796,7 +1815,263 @@
                     show_stats=show_stats,
                     cls=kwargs.pop("cls", dynamic_classification_result),
                     **kwargs
                 )
             )
         except HttpResponseError as error:
             return process_http_response_error(error)
+
+    @distributed_trace_async
+    @validate_multiapi_args(
+        version_method_added="2022-10-01-preview"
+    )
+    async def begin_extract_summary(
+        self,
+        documents: Union[List[str], List[TextDocumentInput], List[Dict[str, str]]],
+        *,
+        continuation_token: Optional[str] = None,
+        disable_service_logs: Optional[bool] = None,
+        display_name: Optional[str] = None,
+        language: Optional[str] = None,
+        polling_interval: Optional[int] = None,
+        show_stats: Optional[bool] = None,
+        model_version: Optional[str] = None,
+        string_index_type: Optional[str] = None,
+        max_sentence_count: Optional[int] = None,
+        order_by: Optional[str] = None,
+        **kwargs: Any,
+    ) -> AsyncTextAnalysisLROPoller[AsyncItemPaged[Union[ExtractSummaryResult, DocumentError]]]:
+        """Start a long-running extractive summarization operation.
+
+        For a conceptual discussion of extractive summarization, see the service documentation:
+        https://learn.microsoft.com/azure/cognitive-services/language-service/summarization/overview
+
+        :param documents: The set of documents to process as part of this batch.
+            If you wish to specify the ID and language on a per-item basis you must
+            use as input a list[:class:`~azure.ai.textanalytics.TextDocumentInput`] or a list of
+            dict representations of :class:`~azure.ai.textanalytics.TextDocumentInput`, like
+            `{"id": "1", "language": "en", "text": "hello world"}`.
+        :type documents:
+            list[str] or list[~azure.ai.textanalytics.TextDocumentInput] or list[dict[str, str]]
+        :keyword str language: The 2 letter ISO 639-1 representation of language for the
+            entire batch. For example, use "en" for English; "es" for Spanish etc.
+            For automatic language detection, use "auto" (Only supported by API version
+            2022-10-01-preview and newer). If not set, uses "en" for English as default.
+            Per-document language will take precedence over whole batch language.
+            See https://aka.ms/talangs for supported languages in Language API.
+        :keyword bool show_stats: If set to true, response will contain document level statistics.
+        :keyword Optional[int] max_sentence_count: Maximum number of sentences to return. Defaults to 3.
+        :keyword Optional[str] order_by:  Possible values include: "Offset", "Rank". Default value: "Offset".
+        :keyword Optional[str] model_version: The model version to use for the analysis.
+        :keyword Optional[str] string_index_type: Specifies the method used to interpret string offsets.
+        :keyword bool disable_service_logs: If set to true, you opt-out of having your text input
+            logged on the service side for troubleshooting. By default, the Language service logs your
+            input text for 48 hours, solely to allow for troubleshooting issues in providing you with
+            the service's natural language processing functions. Setting this parameter to true,
+            disables input logging and may limit our ability to remediate issues that occur. Please see
+            Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
+            additional details, and Microsoft Responsible AI principles at
+            https://www.microsoft.com/ai/responsible-ai.
+        :keyword int polling_interval: Waiting time between two polls for LRO operations
+            if no Retry-After header is present. Defaults to 5 seconds.
+        :keyword str continuation_token:
+            Call `continuation_token()` on the poller object to save the long-running operation (LRO)
+            state into an opaque token. Pass the value as the `continuation_token` keyword argument
+            to restart the LRO from a saved state.
+        :keyword str display_name: An optional display name to set for the requested analysis.
+        :return: An instance of an AsyncTextAnalysisLROPoller. Call `result()` on the this
+            object to return a heterogeneous pageable of
+            :class:`~azure.ai.textanalytics.ExtractSummaryResult` and
+            :class:`~azure.ai.textanalytics.DocumentError`.
+        :rtype:
+            ~azure.ai.textanalytics.AsyncTextAnalysisLROPoller[~azure.core.async_paging.AsyncItemPaged[
+            ~azure.ai.textanalytics.ExtractSummaryResult or ~azure.ai.textanalytics.DocumentError]]
+        :raises ~azure.core.exceptions.HttpResponseError:
+
+        .. versionadded:: 2022-10-01-preview
+            The *begin_extract_summary* client method.
+
+        .. admonition:: Example:
+
+            .. literalinclude:: ../samples/async_samples/sample_extract_summary_async.py
+                :start-after: [START extract_summary_async]
+                :end-before: [END extract_summary_async]
+                :language: python
+                :dedent: 4
+                :caption: Perform extractive summarization on a batch of documents.
+        """
+
+        polling_interval_arg = polling_interval if polling_interval is not None else 5
+        string_index_type_arg = string_index_type if string_index_type is not None else self._string_code_unit
+
+        if continuation_token:
+            return cast(
+                AsyncTextAnalysisLROPoller[AsyncItemPaged[Union[ExtractSummaryResult, DocumentError]]],
+                _get_result_from_continuation_token(
+                    self._client._client,  # pylint: disable=protected-access
+                    continuation_token,
+                    AsyncAnalyzeActionsLROPoller,
+                    AsyncAnalyzeActionsLROPollingMethod(
+                        text_analytics_client=self._client,
+                        timeout=polling_interval_arg,
+                        **kwargs
+                    ),
+                    self._analyze_result_callback,
+                    bespoke=True
+                )
+            )
+
+        try:
+            return cast(
+                AsyncTextAnalysisLROPoller[
+                    AsyncItemPaged[Union[ExtractSummaryResult, DocumentError]]
+                ],
+                await self.begin_analyze_actions(
+                    documents,
+                    actions=[
+                        ExtractSummaryAction(
+                            model_version=model_version,
+                            string_index_type=string_index_type_arg,
+                            max_sentence_count=max_sentence_count,
+                            order_by=order_by,
+                            disable_service_logs=disable_service_logs,
+                        )
+                    ],
+                    polling_interval=polling_interval_arg,
+                    display_name=display_name,
+                    show_stats=show_stats,
+                    language=language,
+                    bespoke=True,
+                    **kwargs
+                )
+            )
+
+        except HttpResponseError as error:
+            return process_http_response_error(error)
+
+    @distributed_trace_async
+    @validate_multiapi_args(
+        version_method_added="2022-10-01-preview"
+    )
+    async def begin_abstractive_summary(
+        self,
+        documents: Union[List[str], List[TextDocumentInput], List[Dict[str, str]]],
+        *,
+        continuation_token: Optional[str] = None,
+        disable_service_logs: Optional[bool] = None,
+        display_name: Optional[str] = None,
+        language: Optional[str] = None,
+        polling_interval: Optional[int] = None,
+        show_stats: Optional[bool] = None,
+        model_version: Optional[str] = None,
+        string_index_type: Optional[str] = None,
+        sentence_count: Optional[int] = None,
+        **kwargs: Any,
+    ) -> AsyncTextAnalysisLROPoller[AsyncItemPaged[Union[AbstractiveSummaryResult, DocumentError]]]:
+        """Start a long-running abstractive summarization operation.
+
+        For a conceptual discussion of abstractive summarization, see the service documentation:
+        https://learn.microsoft.com/azure/cognitive-services/language-service/summarization/overview
+
+        .. note:: The abstractive summarization feature is part of a gated preview. Request access here:
+            https://aka.ms/applyforgatedsummarizationfeatures
+
+        :param documents: The set of documents to process as part of this batch.
+            If you wish to specify the ID and language on a per-item basis you must
+            use as input a list[:class:`~azure.ai.textanalytics.TextDocumentInput`] or a list of
+            dict representations of :class:`~azure.ai.textanalytics.TextDocumentInput`, like
+            `{"id": "1", "language": "en", "text": "hello world"}`.
+        :type documents:
+            list[str] or list[~azure.ai.textanalytics.TextDocumentInput] or list[dict[str, str]]
+        :keyword str language: The 2 letter ISO 639-1 representation of language for the
+            entire batch. For example, use "en" for English; "es" for Spanish etc.
+            For automatic language detection, use "auto" (Only supported by API version
+            2022-10-01-preview and newer). If not set, uses "en" for English as default.
+            Per-document language will take precedence over whole batch language.
+            See https://aka.ms/talangs for supported languages in Language API.
+        :keyword bool show_stats: If set to true, response will contain document level statistics.
+        :keyword Optional[int] sentence_count: It controls the approximate number of sentences in the output summaries.
+        :keyword Optional[str] model_version: The model version to use for the analysis.
+        :keyword Optional[str] string_index_type: Specifies the method used to interpret string offsets.
+        :keyword bool disable_service_logs: If set to true, you opt-out of having your text input
+            logged on the service side for troubleshooting. By default, the Language service logs your
+            input text for 48 hours, solely to allow for troubleshooting issues in providing you with
+            the service's natural language processing functions. Setting this parameter to true,
+            disables input logging and may limit our ability to remediate issues that occur. Please see
+            Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
+            additional details, and Microsoft Responsible AI principles at
+            https://www.microsoft.com/ai/responsible-ai.
+        :keyword int polling_interval: Waiting time between two polls for LRO operations
+            if no Retry-After header is present. Defaults to 5 seconds.
+        :keyword str continuation_token:
+            Call `continuation_token()` on the poller object to save the long-running operation (LRO)
+            state into an opaque token. Pass the value as the `continuation_token` keyword argument
+            to restart the LRO from a saved state.
+        :keyword str display_name: An optional display name to set for the requested analysis.
+        :return: An instance of an AsyncTextAnalysisLROPoller. Call `result()` on the this
+            object to return a heterogeneous pageable of
+            :class:`~azure.ai.textanalytics.AbstractiveSummaryResult` and
+            :class:`~azure.ai.textanalytics.DocumentError`.
+        :rtype:
+            ~azure.ai.textanalytics.AsyncTextAnalysisLROPoller[~azure.core.async_paging.AsyncItemPaged[
+            ~azure.ai.textanalytics.AbstractiveSummaryResult or ~azure.ai.textanalytics.DocumentError]]
+        :raises ~azure.core.exceptions.HttpResponseError:
+
+        .. versionadded:: 2022-10-01-preview
+            The *begin_abstractive_summary* client method.
+
+        .. admonition:: Example:
+
+            .. literalinclude:: ../samples/async_samples/sample_abstractive_summary_async.py
+                :start-after: [START abstractive_summary_async]
+                :end-before: [END abstractive_summary_async]
+                :language: python
+                :dedent: 4
+                :caption: Perform abstractive summarization on a batch of documents.
+        """
+
+        polling_interval_arg = polling_interval if polling_interval is not None else 5
+        string_index_type_arg = string_index_type if string_index_type is not None else self._string_code_unit
+
+        if continuation_token:
+            return cast(
+                AsyncTextAnalysisLROPoller[AsyncItemPaged[Union[AbstractiveSummaryResult, DocumentError]]],
+                _get_result_from_continuation_token(
+                    self._client._client,  # pylint: disable=protected-access
+                    continuation_token,
+                    AsyncAnalyzeActionsLROPoller,
+                    AsyncAnalyzeActionsLROPollingMethod(
+                        text_analytics_client=self._client,
+                        timeout=polling_interval_arg,
+                        **kwargs
+                    ),
+                    self._analyze_result_callback,
+                    bespoke=True
+                )
+            )
+
+        try:
+            return cast(
+                AsyncTextAnalysisLROPoller[
+                    AsyncItemPaged[Union[AbstractiveSummaryResult, DocumentError]]
+                ],
+                await self.begin_analyze_actions(
+                    documents,
+                    actions=[
+                        AbstractiveSummaryAction(
+                            model_version=model_version,
+                            string_index_type=string_index_type_arg,
+                            sentence_count=sentence_count,
+                            disable_service_logs=disable_service_logs,
+                        )
+                    ],
+                    polling_interval=polling_interval_arg,
+                    display_name=display_name,
+                    show_stats=show_stats,
+                    language=language,
+                    bespoke=True,
+                    **kwargs
+                )
+            )
+
+        except HttpResponseError as error:
+            return process_http_response_error(error)
```

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_check.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_check.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_policies.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_policies.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_response_handlers.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_response_handlers.py`

 * *Files 0% similar despite different names*

```diff
@@ -33,15 +33,15 @@
     PiiEntity,
     AnalyzeHealthcareEntitiesResult,
     _AnalyzeActionsType,
     RecognizeCustomEntitiesResult,
     ClassifyDocumentResult,
     ActionPointerKind,
     ExtractSummaryResult,
-    AbstractSummaryResult,
+    AbstractiveSummaryResult,
     DynamicClassificationResult,
 )
 
 
 class CSODataV4Format(ODataV4Format):
     def __init__(self, odata_error):
         try:
@@ -133,15 +133,15 @@
     return choose_wrapper
 
 
 @prepare_result
 def abstract_summary_result(
     summary, results, *args, **kwargs
 ):  # pylint: disable=unused-argument
-    return AbstractSummaryResult._from_generated(  # pylint: disable=protected-access
+    return AbstractiveSummaryResult._from_generated(  # pylint: disable=protected-access
         summary
     )
 
 
 @prepare_result
 def language_result(language, results):  # pylint: disable=unused-argument
     return DetectLanguageResult(
@@ -405,15 +405,18 @@
     )
 
 
 def pad_result(tasks_obj, doc_id_order):
     return [
         DocumentError(
             id=doc_id,
-            error=TextAnalyticsError(message=f"No result for document. Action returned status '{tasks_obj.status}'.")
+            error=TextAnalyticsError(
+                code=None,  # type: ignore
+                message=f"No result for document. Action returned status '{tasks_obj.status}'."
+            )
         ) for doc_id in doc_id_order
     ]
 
 
 def get_ordered_errors(tasks_obj, task_name, doc_id_order):
     # throw exception if error missing a target
     missing_target = any([error for error in tasks_obj.errors if error.target is None])
```

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_models.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_models.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,26 +1,44 @@
 # pylint: disable=too-many-lines
 # ------------------------------------
 # Copyright (c) Microsoft Corporation.
 # Licensed under the MIT License.
 # ------------------------------------
+# pylint: disable=unused-argument
 import re
 from enum import Enum
+from typing import Optional, List, Dict, Any, Union
 from typing_extensions import Literal
 from azure.core import CaseInsensitiveEnumMeta
 from ._generated.models import (
     LanguageInput,
     MultiLanguageInput,
+    AgeResolution,
+    AreaResolution,
+    CurrencyResolution,
+    DateTimeResolution,
+    InformationResolution,
+    LengthResolution,
+    NumberResolution,
+    NumericRangeResolution,
+    OrdinalResolution,
+    SpeedResolution,
+    TemperatureResolution,
+    TemporalSpanResolution,
+    VolumeResolution,
+    WeightResolution,
+    HealthcareDocumentType,
 )
 from ._generated.v3_0 import models as _v3_0_models
 from ._generated.v3_1 import models as _v3_1_models
 from ._generated.v2022_10_01_preview import models as _v2022_10_01_preview_models
 from ._check import is_language_api, string_index_type_compatibility
 from ._dict_mixin import DictMixin
 
+STRING_INDEX_TYPE_DEFAULT = "UnicodeCodePoint"
 
 def _get_indices(relation):
     return [int(s) for s in re.findall(r"\d+", relation)]
 
 
 class TextAnalysisKind(str, Enum, metaclass=CaseInsensitiveEnumMeta):
     """Enumeration of supported Text Analysis kinds.
@@ -308,38 +326,39 @@
     TREATMENT_NAME = "TreatmentName"
 
 
 class PiiEntityDomain(str, Enum, metaclass=CaseInsensitiveEnumMeta):
     """The different domains of PII entities that users can filter by"""
 
     PROTECTED_HEALTH_INFORMATION = (
-        "phi"  # See https://aka.ms/tanerpii for more information.
+        "phi"  # See https://aka.ms/azsdk/language/pii for more information.
     )
 
 
 class DetectedLanguage(DictMixin):
     """DetectedLanguage contains the predicted language found in text,
     its confidence score, and its ISO 639-1 representation.
 
-    :ivar name: Long name of a detected language (e.g. English,
-        French).
-    :vartype name: str
-    :ivar iso6391_name: A two letter representation of the detected
-        language according to the ISO 639-1 standard (e.g. en, fr).
-    :vartype iso6391_name: str
-    :ivar confidence_score: A confidence score between 0 and 1. Scores close
-        to 1 indicate 100% certainty that the identified language is true.
-    :vartype confidence_score: float
-    :ivar Optional[str] script: Identifies the script of the input document. Possible values: "Latin".
-
     .. versionadded:: 2022-10-01-preview
         The *script* property.
     """
 
-    def __init__(self, **kwargs):
+    name: str
+    """Long name of a detected language (e.g. English,
+        French)."""
+    iso6391_name: str
+    """A two letter representation of the detected
+        language according to the ISO 639-1 standard (e.g. en, fr)."""
+    confidence_score: float
+    """A confidence score between 0 and 1. Scores close
+        to 1 indicate 100% certainty that the identified language is true."""
+    script: Optional[str] = None
+    """Identifies the script of the input document. Possible value is 'Latin'."""
+
+    def __init__(self, **kwargs: Any) -> None:
         self.name = kwargs.get("name", None)
         self.iso6391_name = kwargs.get("iso6391_name", None)
         self.confidence_score = kwargs.get("confidence_score", None)
         self.script = kwargs.get("script", None)
 
     @classmethod
     def _from_generated(cls, language):
@@ -347,168 +366,161 @@
         return cls(
             name=language.name,
             iso6391_name=language.iso6391_name,
             confidence_score=language.confidence_score,
             script=script
         )
 
-    def __repr__(self):
-        return "DetectedLanguage(name={}, iso6391_name={}, confidence_score={}, script={})".format(
-            self.name, self.iso6391_name, self.confidence_score, self.script
-        )[:1024]
+    def __repr__(self) -> str:
+        return (
+            f"DetectedLanguage(name={self.name}, iso6391_name={self.iso6391_name}, "
+            f"confidence_score={self.confidence_score}, script={self.script})"[:1024]
+        )
 
 
 class RecognizeEntitiesResult(DictMixin):
     """RecognizeEntitiesResult is a result object which contains
     the recognized entities from a particular document.
 
-    :ivar id: Unique, non-empty document identifier that matches the
-        document id that was passed in with the request. If not specified
-        in the request, an id is assigned for the document.
-    :vartype id: str
-    :ivar entities: Recognized entities in the document.
-    :vartype entities:
-        list[~azure.ai.textanalytics.CategorizedEntity]
-    :ivar warnings: Warnings encountered while processing document. Results will still be returned
-        if there are warnings, but they may not be fully accurate.
-    :vartype warnings: list[~azure.ai.textanalytics.TextAnalyticsWarning]
-    :ivar statistics: If `show_stats=True` was specified in the request this
-        field will contain information about the document payload.
-    :vartype statistics:
-        Optional[~azure.ai.textanalytics.TextDocumentStatistics]
-    :ivar detected_language: If 'language' is set to 'auto' for the document in the request this
-        field will contain the DetectedLanguage for the document.
-    :vartype detected_language: Optional[~azure.ai.textanalytics.DetectedLanguage]
-    :ivar bool is_error: Boolean check for error item when iterating over list of
-        results. Always False for an instance of a RecognizeEntitiesResult.
-    :ivar str kind: The text analysis kind - "EntityRecognition".
-
     .. versionadded:: 2022-10-01-preview
         The *detected_language* property.
     """
 
-    def __init__(self, **kwargs):
+    id: str  # pylint: disable=redefined-builtin
+    """Unique, non-empty document identifier that matches the
+        document id that was passed in with the request. If not specified
+        in the request, an id is assigned for the document."""
+    entities: List["CategorizedEntity"]
+    """Recognized entities in the document."""
+    warnings: List["TextAnalyticsWarning"]
+    """Warnings encountered while processing document. Results will still be returned
+        if there are warnings, but they may not be fully accurate."""
+    statistics: Optional["TextDocumentStatistics"] = None
+    """If `show_stats=True` was specified in the request this
+        field will contain information about the document payload."""
+    detected_language: Optional[DetectedLanguage] = None
+    """If 'language' is set to 'auto' for the document in the request this
+        field will contain the DetectedLanguage for the document."""
+    is_error: Literal[False] = False
+    """Boolean check for error item when iterating over list of
+        results. Always False for an instance of a RecognizeEntitiesResult."""
+    kind: Literal["EntityRecognition"] = "EntityRecognition"
+    """The text analysis kind - "EntityRecognition"."""
+
+    def __init__(self, **kwargs: Any) -> None:
         self.id = kwargs.get("id", None)
         self.entities = kwargs.get("entities", None)
         self.warnings = kwargs.get("warnings", [])
         self.statistics = kwargs.get("statistics", None)
         self.detected_language = kwargs.get("detected_language", None)
         self.is_error: Literal[False] = False
         self.kind: Literal["EntityRecognition"] = "EntityRecognition"
 
-    def __repr__(self):
-        return "RecognizeEntitiesResult(id={}, entities={}, warnings={}, statistics={}, " \
-               "detected_language={}, is_error={})".format(
-            self.id,
-            repr(self.entities),
-            repr(self.warnings),
-            repr(self.statistics),
-            repr(self.detected_language),
-            self.is_error,
-        )[
-            :1024
-        ]
+    def __repr__(self) -> str:
+        return (
+            f"RecognizeEntitiesResult(id={self.id}, entities={repr(self.entities)}, "
+            f"warnings={repr(self.warnings)}, statistics={repr(self.statistics)}, "
+            f"detected_language={repr(self.detected_language)}, is_error={self.is_error}, "
+            f"kind={self.kind})"[:1024]
+        )
 
 
 class RecognizePiiEntitiesResult(DictMixin):
     """RecognizePiiEntitiesResult is a result object which contains
     the recognized Personally Identifiable Information (PII) entities
     from a particular document.
 
-    :ivar str id: Unique, non-empty document identifier that matches the
-        document id that was passed in with the request. If not specified
-        in the request, an id is assigned for the document.
-    :ivar entities: Recognized PII entities in the document.
-    :vartype entities:
-        list[~azure.ai.textanalytics.PiiEntity]
-    :ivar str redacted_text: Returns the text of the input document with all of the PII information
-        redacted out.
-    :ivar warnings: Warnings encountered while processing document. Results will still be returned
-        if there are warnings, but they may not be fully accurate.
-    :vartype warnings: list[~azure.ai.textanalytics.TextAnalyticsWarning]
-    :ivar statistics: If `show_stats=True` was specified in the request this
-        field will contain information about the document payload.
-    :vartype statistics:
-        Optional[~azure.ai.textanalytics.TextDocumentStatistics]
-    :ivar detected_language: If 'language' is set to 'auto' for the document in the request this
-        field will contain the DetectedLanguage for the document.
-    :vartype detected_language: Optional[~azure.ai.textanalytics.DetectedLanguage]
-    :ivar bool is_error: Boolean check for error item when iterating over list of
-        results. Always False for an instance of a RecognizePiiEntitiesResult.
-    :ivar str kind: The text analysis kind - "PiiEntityRecognition".
-
     .. versionadded:: 2022-10-01-preview
         The *detected_language* property.
     """
 
-    def __init__(self, **kwargs):
+    id: str  # pylint: disable=redefined-builtin
+    """Unique, non-empty document identifier that matches the
+        document id that was passed in with the request. If not specified
+        in the request, an id is assigned for the document."""
+    entities: List["PiiEntity"]
+    """Recognized PII entities in the document."""
+    redacted_text: str
+    """Returns the text of the input document with all of the PII information
+        redacted out."""
+    warnings: List["TextAnalyticsWarning"]
+    """Warnings encountered while processing document. Results will still be returned
+        if there are warnings, but they may not be fully accurate."""
+    statistics: Optional["TextDocumentStatistics"] = None
+    """If `show_stats=True` was specified in the request this
+        field will contain information about the document payload."""
+    detected_language: Optional[DetectedLanguage] = None
+    """If 'language' is set to 'auto' for the document in the request this
+        field will contain the DetectedLanguage for the document."""
+    is_error: Literal[False] = False
+    """Boolean check for error item when iterating over list of
+        results. Always False for an instance of a RecognizePiiEntitiesResult."""
+    kind: Literal["PiiEntityRecognition"] = "PiiEntityRecognition"
+    """The text analysis kind - "PiiEntityRecognition"."""
+
+    def __init__(self, **kwargs: Any) -> None:
         self.id = kwargs.get("id", None)
         self.entities = kwargs.get("entities", None)
         self.redacted_text = kwargs.get("redacted_text", None)
         self.warnings = kwargs.get("warnings", [])
         self.statistics = kwargs.get("statistics", None)
         self.detected_language = kwargs.get('detected_language', None)
         self.is_error: Literal[False] = False
         self.kind: Literal["PiiEntityRecognition"] = "PiiEntityRecognition"
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         return (
-            "RecognizePiiEntitiesResult(id={}, entities={}, redacted_text={}, warnings={}, "
-            "statistics={}, detected_language={}, is_error={})".format(
-                self.id,
-                repr(self.entities),
-                self.redacted_text,
-                repr(self.warnings),
-                repr(self.statistics),
-                repr(self.detected_language),
-                self.is_error,
-            )[:1024]
+            f"RecognizePiiEntitiesResult(id={self.id}, entities={repr(self.entities)}, "
+            f"redacted_text={self.redacted_text}, warnings={repr(self.warnings)}, "
+            f"statistics={repr(self.statistics)}, detected_language={repr(self.detected_language)}, "
+            f"is_error={self.is_error}, kind={self.kind})"[:1024]
         )
 
 
 class AnalyzeHealthcareEntitiesResult(DictMixin):
     """
     AnalyzeHealthcareEntitiesResult contains the Healthcare entities from a
     particular document.
 
-    :ivar str id: Unique, non-empty document identifier that matches the
+    .. versionadded:: 2022-10-01-preview
+        The *fhir_bundle* and *detected_language* properties.
+    """
+
+    id: str  # pylint: disable=redefined-builtin
+    """Unique, non-empty document identifier that matches the
         document id that was passed in with the request. If not specified
-        in the request, an id is assigned for the document.
-    :ivar entities: Identified Healthcare entities in the document, i.e. in
+        in the request, an id is assigned for the document."""
+    entities: List["HealthcareEntity"]
+    """Identified Healthcare entities in the document, i.e. in
         the document "The subject took ibuprofen", "ibuprofen" is an identified entity
-        from the document.
-    :vartype entities:
-        list[~azure.ai.textanalytics.HealthcareEntity]
-    :ivar entity_relations: Identified Healthcare relations between entities. For example, in the
+        from the document."""
+    entity_relations: List["HealthcareRelation"]
+    """Identified Healthcare relations between entities. For example, in the
         document "The subject took 100mg of ibuprofen", we would identify the relationship
-        between the dosage of 100mg and the medication ibuprofen.
-    :vartype entity_relations: list[~azure.ai.textanalytics.HealthcareRelation]
-    :ivar warnings: Warnings encountered while processing document. Results will still be returned
-        if there are warnings, but they may not be fully accurate.
-    :vartype warnings: list[~azure.ai.textanalytics.TextAnalyticsWarning]
-    :ivar statistics: If show_stats=true was specified in the request this
-        field will contain information about the document payload.
-    :vartype statistics:
-        Optional[~azure.ai.textanalytics.TextDocumentStatistics]
-    :ivar fhir_bundle: If `fhir_version` is passed, this will contain a
+        between the dosage of 100mg and the medication ibuprofen."""
+    warnings: List["TextAnalyticsWarning"]
+    """Warnings encountered while processing document. Results will still be returned
+        if there are warnings, but they may not be fully accurate."""
+    statistics: Optional["TextDocumentStatistics"] = None
+    """If show_stats=true was specified in the request this
+        field will contain information about the document payload."""
+    fhir_bundle: Optional[Dict[str, Any]] = None
+    """If `fhir_version` is passed, this will contain a
         FHIR compatible object for consumption in other Healthcare tools. For additional
-        information see https://www.hl7.org/fhir/overview.html.
-    :vartype fhir_bundle: Optional[dict[str, any]]
-    :ivar detected_language: If 'language' is set to 'auto' for the document in the request this
-        field will contain the detected language for the document.
-    :vartype detected_language: Optional[str]
-    :ivar bool is_error: Boolean check for error item when iterating over list of
-        results. Always False for an instance of a AnalyzeHealthcareEntitiesResult.
-    :ivar str kind: The text analysis kind - "Healthcare".
+        information see https://www.hl7.org/fhir/overview.html."""
+    detected_language: Optional[str] = None
+    """If 'language' is set to 'auto' for the document in the request this
+        field will contain the detected language for the document."""
+    is_error: Literal[False] = False
+    """Boolean check for error item when iterating over list of
+        results. Always False for an instance of a AnalyzeHealthcareEntitiesResult."""
+    kind: Literal["Healthcare"] = "Healthcare"
+    """The text analysis kind - "Healthcare"."""
 
-    .. versionadded:: 2022-10-01-preview
-        The *fhir_bundle* and *detected_language* properties.
-    """
-
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         self.id = kwargs.get("id", None)
         self.entities = kwargs.get("entities", None)
         self.entity_relations = kwargs.get("entity_relations", None)
         self.warnings = kwargs.get("warnings", [])
         self.statistics = kwargs.get("statistics", None)
         self.fhir_bundle = kwargs.get("fhir_bundle", None)
         self.detected_language = kwargs.get('detected_language', None)
@@ -546,54 +558,47 @@
             fhir_bundle=fhir_bundle,
             detected_language=detected_language  # https://github.com/Azure/azure-sdk-for-python/issues/27171
             # detected_language=DetectedLanguage._from_generated(  # pylint: disable=protected-access
             #     healthcare_result.detected_language
             # ) if hasattr(healthcare_result, "detected_language") and healthcare_result.detected_language else None
         )
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         return (
-            "AnalyzeHealthcareEntitiesResult(id={}, entities={}, entity_relations={}, warnings={}, "
-            "statistics={}, fhir_bundle={}, detected_language={}, is_error={})".format(
-                self.id,
-                repr(self.entities),
-                repr(self.entity_relations),
-                repr(self.warnings),
-                repr(self.statistics),
-                self.fhir_bundle,
-                self.detected_language,
-                self.is_error,
-            )[:1024]
+            f"AnalyzeHealthcareEntitiesResult(id={self.id}, entities={repr(self.entities)}, "
+            f"entity_relations={repr(self.entity_relations)}, warnings={repr(self.warnings)}, "
+            f"statistics={repr(self.statistics)}, fhir_bundle={self.fhir_bundle}, "
+            f"detected_language={self.detected_language}, is_error={self.is_error}, kind={self.kind})"[:1024]
         )
 
 
 class HealthcareRelation(DictMixin):
     """HealthcareRelation is a result object which represents a relation detected in a document.
 
     Every HealthcareRelation is an entity graph of a certain relation type,
     where all entities are connected and have specific roles within the relation context.
 
-    :ivar relation_type: The type of relation, i.e. the relationship between "100mg" and
-        "ibuprofen" in the document "The subject took 100 mg of ibuprofen" is "DosageOfMedication".
-        Possible values found in :class:`~azure.ai.textanalytics.HealthcareEntityRelation`
-    :vartype relation_type: str
-    :ivar roles: The roles present in this relation. I.e., in the document
-        "The subject took 100 mg of ibuprofen", the present roles are "Dosage" and "Medication".
-    :vartype roles: list[~azure.ai.textanalytics.HealthcareRelationRole]
-    :ivar confidence_score: Confidence score between 0 and 1 of the extracted relation.
-    :vartype confidence_score: Optional[float]
-
     .. versionadded:: 2022-10-01-preview
         The *confidence_score* property.
     """
 
-    def __init__(self, **kwargs):
-        self.relation_type = kwargs.get("relation_type")
-        self.roles = kwargs.get("roles")
-        self.confidence_score = kwargs.get("confidence_score")
+    relation_type: str
+    """The type of relation, i.e. the relationship between "100mg" and
+        "ibuprofen" in the document "The subject took 100 mg of ibuprofen" is "DosageOfMedication".
+        Possible values found in :class:`~azure.ai.textanalytics.HealthcareEntityRelation`"""
+    roles: List["HealthcareRelationRole"]
+    """The roles present in this relation. I.e., in the document
+        "The subject took 100 mg of ibuprofen", the present roles are "Dosage" and "Medication"."""
+    confidence_score: Optional[float] = None
+    """Confidence score between 0 and 1 of the extracted relation."""
+
+    def __init__(self, **kwargs: Any) -> None:
+        self.relation_type = kwargs.get("relation_type", None)
+        self.roles = kwargs.get("roles", None)
+        self.confidence_score = kwargs.get("confidence_score", None)
 
     @classmethod
     def _from_generated(cls, healthcare_relation_result, entities):
         roles = [
             HealthcareRelationRole._from_generated(  # pylint: disable=protected-access
                 r, entities
             )
@@ -603,42 +608,39 @@
             if hasattr(healthcare_relation_result, "confidence_score") else None
         return cls(
             relation_type=healthcare_relation_result.relation_type,
             roles=roles,
             confidence_score=confidence_score,
         )
 
-    def __repr__(self):
-        return "HealthcareRelation(relation_type={}, roles={}, confidence_score={})".format(
-            self.relation_type,
-            repr(self.roles),
-            self.confidence_score,
-        )[:1024]
+    def __repr__(self) -> str:
+        return f"HealthcareRelation(relation_type={self.relation_type}, roles={repr(self.roles)}, " \
+               f"confidence_score={self.confidence_score})"[:1024]
 
 
 class HealthcareRelationRole(DictMixin):
     """A model representing a role in a relation.
 
     For example, in "The subject took 100 mg of ibuprofen",
     "100 mg" is a dosage entity fulfilling the role "Dosage"
     in the extracted relation "DosageOfMedication".
+    """
 
-    :ivar name: The role of the entity in the relationship. I.e., in the relation
+    name: str
+    """The role of the entity in the relationship. I.e., in the relation
         "The subject took 100 mg of ibuprofen", the dosage entity "100 mg" has role
-        "Dosage".
-    :vartype name: str
-    :ivar entity: The entity that is present in the relationship. For example, in
+        "Dosage"."""
+    entity: "HealthcareEntity"
+    """The entity that is present in the relationship. For example, in
         "The subject took 100 mg of ibuprofen", this property holds the dosage entity
-        of "100 mg".
-    :vartype entity: ~azure.ai.textanalytics.HealthcareEntity
-    """
+        of "100 mg"."""
 
-    def __init__(self, **kwargs):
-        self.name = kwargs.get("name")
-        self.entity = kwargs.get("entity")
+    def __init__(self, **kwargs: Any) -> None:
+        self.name = kwargs.get("name", None)
+        self.entity = kwargs.get("entity", None)
 
     @staticmethod
     def _get_entity(healthcare_role_result, entities):
         numbers = _get_indices(healthcare_role_result.ref)
         entity_index = numbers[
             1
         ]  # first number parsed from index is document #, second is entity index
@@ -647,96 +649,106 @@
     @classmethod
     def _from_generated(cls, healthcare_role_result, entities):
         return cls(
             name=healthcare_role_result.role,
             entity=HealthcareRelationRole._get_entity(healthcare_role_result, entities),
         )
 
-    def __repr__(self):
-        return "HealthcareRelationRole(name={}, entity={})".format(
-            self.name, repr(self.entity)
-        )
+    def __repr__(self) -> str:
+        return f"HealthcareRelationRole(name={self.name}, entity={repr(self.entity)})"[:1024]
 
 
 class DetectLanguageResult(DictMixin):
     """DetectLanguageResult is a result object which contains
     the detected language of a particular document.
+    """
 
-    :ivar id: Unique, non-empty document identifier that matches the
+    id: str  # pylint: disable=redefined-builtin
+    """Unique, non-empty document identifier that matches the
         document id that was passed in with the request. If not specified
-        in the request, an id is assigned for the document.
-    :vartype id: str
-    :ivar primary_language: The primary language detected in the document.
-    :vartype primary_language: ~azure.ai.textanalytics.DetectedLanguage
-    :ivar warnings: Warnings encountered while processing document. Results will still be returned
-        if there are warnings, but they may not be fully accurate.
-    :vartype warnings: list[~azure.ai.textanalytics.TextAnalyticsWarning]
-    :ivar statistics: If `show_stats=True` was specified in the request this
-        field will contain information about the document payload.
-    :vartype statistics:
-        Optional[~azure.ai.textanalytics.TextDocumentStatistics]
-    :ivar bool is_error: Boolean check for error item when iterating over list of
-        results. Always False for an instance of a DetectLanguageResult.
-    :ivar str kind: The text analysis kind - "LanguageDetection".
-    """
+        in the request, an id is assigned for the document."""
+    primary_language: DetectedLanguage
+    """The primary language detected in the document."""
+    warnings: List["TextAnalyticsWarning"]
+    """Warnings encountered while processing document. Results will still be returned
+        if there are warnings, but they may not be fully accurate."""
+    statistics: Optional["TextDocumentStatistics"] = None
+    """If `show_stats=True` was specified in the request this
+        field will contain information about the document payload."""
+    is_error: Literal[False] = False
+    """Boolean check for error item when iterating over list of
+        results. Always False for an instance of a DetectLanguageResult."""
+    kind: Literal["LanguageDetection"] = "LanguageDetection"
+    """The text analysis kind - "LanguageDetection"."""
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         self.id = kwargs.get("id", None)
         self.primary_language = kwargs.get("primary_language", None)
         self.warnings = kwargs.get("warnings", [])
         self.statistics = kwargs.get("statistics", None)
         self.is_error: Literal[False] = False
         self.kind: Literal["LanguageDetection"] = "LanguageDetection"
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         return (
-            "DetectLanguageResult(id={}, primary_language={}, warnings={}, statistics={}, "
-            "is_error={})".format(
-                self.id,
-                repr(self.primary_language),
-                repr(self.warnings),
-                repr(self.statistics),
-                self.is_error,
-            )[:1024]
+            f"DetectLanguageResult(id={self.id}, primary_language={repr(self.primary_language)}, "
+            f"warnings={repr(self.warnings)}, statistics={repr(self.statistics)}, is_error={self.is_error}, "
+            f"kind={self.kind})"[:1024]
         )
 
 
 class CategorizedEntity(DictMixin):
     """CategorizedEntity contains information about a particular
     entity found in text.
 
-    :ivar text: Entity text as appears in the request.
-    :vartype text: str
-    :ivar category: Entity category, such as Person/Location/Org/SSN etc
-    :vartype category: str
-    :ivar subcategory: Entity subcategory, such as Age/Year/TimeRange etc
-    :vartype subcategory: Optional[str]
-    :ivar int length: The entity text length.  This value depends on the value of the
-        `string_index_type` parameter set in the original request, which is UnicodeCodePoints
-        by default.
-    :ivar int offset: The entity text offset from the start of the document.
-        The value depends on the value of the `string_index_type` parameter
-        set in the original request, which is UnicodeCodePoints by default.
-    :ivar confidence_score: Confidence score between 0 and 1 of the extracted
-        entity.
-    :vartype confidence_score: float
-    :ivar resolutions: The collection of entity resolution objects. More information can be found here:
-        https://aka.ms/azsdk/language/ner-resolutions
-    :vartype resolutions: Optional[list[AgeResolution or AreaResolution or BooleanResolution or
-        CurrencyResolution or DateTimeResolution or InformationResolution or LengthResolution or
-        NumberResolution or NumericRangeResolution or OrdinalResolution or SpeedResolution or
-        TemperatureResolution or TemporalSpanResolution or VolumeResolution or WeightResolution]]
-
     .. versionadded:: v3.1
         The *offset* and *length* properties.
     .. versionadded:: 2022-10-01-preview
         The *resolutions* property.
     """
 
-    def __init__(self, **kwargs):
+    text: str
+    """Entity text as appears in the request."""
+    category: str
+    """Entity category, such as Person/Location/Org/SSN etc"""
+    length: int
+    """The entity text length.  This value depends on the value of the
+        `string_index_type` parameter set in the original request, which is UnicodeCodePoints
+        by default."""
+    offset: int
+    """The entity text offset from the start of the document.
+        The value depends on the value of the `string_index_type` parameter
+        set in the original request, which is UnicodeCodePoints by default."""
+    confidence_score: float
+    """Confidence score between 0 and 1 of the extracted
+        entity."""
+    subcategory: Optional[str] = None
+    """Entity subcategory, such as Age/Year/TimeRange etc"""
+    resolutions: List[
+        Union[
+            AgeResolution,
+            AreaResolution,
+            CurrencyResolution,
+            DateTimeResolution,
+            InformationResolution,
+            LengthResolution,
+            NumberResolution,
+            NumericRangeResolution,
+            OrdinalResolution,
+            SpeedResolution,
+            TemperatureResolution,
+            TemporalSpanResolution,
+            VolumeResolution,
+            WeightResolution,
+        ]
+    ]
+    """The collection of entity resolution objects. More information can be found here:
+        https://aka.ms/azsdk/language/ner-resolutions"""
+
+    def __init__(self, **kwargs: Any) -> None:
         self.text = kwargs.get("text", None)
         self.category = kwargs.get("category", None)
         self.subcategory = kwargs.get("subcategory", None)
         self.length = kwargs.get("length", None)
         self.offset = kwargs.get("offset", None)
         self.confidence_score = kwargs.get("confidence_score", None)
         self.resolutions = kwargs.get("resolutions", None)
@@ -746,59 +758,58 @@
         offset = entity.offset
         length = entity.length
         if isinstance(entity, _v3_0_models.Entity):
             # we do not return offset for v3.0 since
             # the correct encoding was not introduced for v3.0
             offset = None
             length = None
+        entity_resolutions = entity.resolutions if hasattr(entity, "resolutions") else None
         return cls(
             text=entity.text,
             category=entity.category,
             subcategory=entity.subcategory,
             length=length,
             offset=offset,
             confidence_score=entity.confidence_score,
-            resolutions=entity.resolutions if hasattr(entity, "resolutions") else None
+            resolutions=entity_resolutions or []
         )
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         return (
-            "CategorizedEntity(text={}, category={}, subcategory={}, "
-            "length={}, offset={}, confidence_score={}, resolutions={})".format(
-                self.text,
-                self.category,
-                self.subcategory,
-                self.length,
-                self.offset,
-                self.confidence_score,
-                repr(self.resolutions)
-            )[:1024]
+            f"CategorizedEntity(text={self.text}, category={self.category}, subcategory={self.subcategory}, "
+            f"length={self.length}, offset={self.offset}, confidence_score={self.confidence_score}, "
+            f"resolutions={repr(self.resolutions)})"[:1024]
         )
 
 
 class PiiEntity(DictMixin):
     """PiiEntity contains information about a Personally Identifiable
     Information (PII) entity found in text.
+    """
 
-    :ivar str text: Entity text as appears in the request.
-    :ivar str category: Entity category, such as Financial Account
-        Identification/Social Security Number/Phone Number, etc.
-    :ivar str subcategory: Entity subcategory, such as Credit Card/EU
-        Phone number/ABA Routing Numbers, etc.
-    :ivar int length: The PII entity text length.  This value depends on the value
+    text: str
+    """Entity text as appears in the request."""
+    category: str
+    """Entity category, such as Financial Account
+        Identification/Social Security Number/Phone Number, etc."""
+    length: int
+    """The PII entity text length.  This value depends on the value
         of the `string_index_type` parameter specified in the original request, which
-        is UnicodeCodePoints by default.
-    :ivar int offset: The PII entity text offset from the start of the document.
+        is UnicodeCodePoints by default."""
+    offset: int
+    """The PII entity text offset from the start of the document.
         This value depends on the value of the `string_index_type` parameter specified
-        in the original request, which is UnicodeCodePoints by default.
-    :ivar float confidence_score: Confidence score between 0 and 1 of the extracted
-        entity.
-    """
+        in the original request, which is UnicodeCodePoints by default."""
+    confidence_score: float
+    """Confidence score between 0 and 1 of the extracted entity."""
+    subcategory: Optional[str] = None
+    """Entity subcategory, such as Credit Card/EU
+        Phone number/ABA Routing Numbers, etc."""
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         self.text = kwargs.get("text", None)
         self.category = kwargs.get("category", None)
         self.subcategory = kwargs.get("subcategory", None)
         self.length = kwargs.get("length", None)
         self.offset = kwargs.get("offset", None)
         self.confidence_score = kwargs.get("confidence_score", None)
 
@@ -809,55 +820,54 @@
             category=entity.category,
             subcategory=entity.subcategory,
             length=entity.length,
             offset=entity.offset,
             confidence_score=entity.confidence_score,
         )
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         return (
-            "PiiEntity(text={}, category={}, subcategory={}, length={}, "
-            "offset={}, confidence_score={})".format(
-                self.text,
-                self.category,
-                self.subcategory,
-                self.length,
-                self.offset,
-                self.confidence_score,
-            )[:1024]
+            f"PiiEntity(text={self.text}, category={self.category}, subcategory={self.subcategory}, "
+            f"length={self.length}, offset={self.offset}, confidence_score={self.confidence_score})"[:1024]
         )
 
 
 class HealthcareEntity(DictMixin):
     """HealthcareEntity contains information about a Healthcare entity found in text.
+    """
 
-    :ivar str text: Entity text as appears in the document.
-    :ivar Optional[str] normalized_text: Normalized version of the raw `text` we extract
-        from the document. Not all `text` will have a normalized version.
-    :ivar str category: Entity category, see the :class:`~azure.ai.textanalytics.HealthcareEntityCategory`
-        type for possible healthcare entity categories.
-    :ivar Optional[str] subcategory: Entity subcategory.
-    :ivar assertion: Contains various assertions about this entity. For example, if
-        an entity is a diagnosis, is this diagnosis 'conditional' on a symptom?
-        Are the doctors 'certain' about this diagnosis? Is this diagnosis 'associated'
-        with another diagnosis?
-    :vartype assertion: Optional[~azure.ai.textanalytics.HealthcareEntityAssertion]
-    :ivar int length: The entity text length.  This value depends on the value
+    text: str
+    """Entity text as appears in the document."""
+    category: str
+    """Entity category, see the :class:`~azure.ai.textanalytics.HealthcareEntityCategory`
+        type for possible healthcare entity categories."""
+    length: int
+    """The entity text length.  This value depends on the value
         of the `string_index_type` parameter specified in the original request, which is
-        UnicodeCodePoints by default.
-    :ivar int offset: The entity text offset from the start of the document.
+        UnicodeCodePoints by default."""
+    offset: int
+    """The entity text offset from the start of the document.
         This value depends on the value of the `string_index_type` parameter specified
-        in the original request, which is UnicodeCodePoints by default.
-    :ivar float confidence_score: Confidence score between 0 and 1 of the extracted
-        entity.
-    :ivar data_sources: A collection of entity references in known data sources.
-    :vartype data_sources: Optional[list[~azure.ai.textanalytics.HealthcareEntityDataSource]]
-    """
+        in the original request, which is UnicodeCodePoints by default."""
+    confidence_score: float
+    """Confidence score between 0 and 1 of the extracted entity."""
+    subcategory: Optional[str] = None
+    """Entity subcategory."""
+    assertion: Optional["HealthcareEntityAssertion"] = None
+    """Contains various assertions about this entity. For example, if
+        an entity is a diagnosis, is this diagnosis 'conditional' on a symptom?
+        Are the doctors 'certain' about this diagnosis? Is this diagnosis 'associated'
+        with another diagnosis?"""
+    normalized_text: Optional[str] = None
+    """Normalized version of the raw `text` we extract
+        from the document. Not all `text` will have a normalized version."""
+    data_sources: Optional[List["HealthcareEntityDataSource"]]
+    """A collection of entity references in known data sources."""
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         self.text = kwargs.get("text", None)
         self.normalized_text = kwargs.get("normalized_text", None)
         self.category = kwargs.get("category", None)
         self.subcategory = kwargs.get("subcategory", None)
         self.assertion = kwargs.get("assertion", None)
         self.length = kwargs.get("length", None)
         self.offset = kwargs.get("offset", None)
@@ -888,395 +898,367 @@
                 HealthcareEntityDataSource(entity_id=l.id, name=l.data_source)
                 for l in healthcare_entity.links
             ]
             if healthcare_entity.links
             else None,
         )
 
-    def __hash__(self):
+    def __hash__(self) -> int:
         return hash(repr(self))
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         return (
-            "HealthcareEntity(text={}, normalized_text={}, category={}, subcategory={}, assertion={}, length={}, "
-            "offset={}, confidence_score={}, data_sources={})".format(
-                self.text,
-                self.normalized_text,
-                self.category,
-                self.subcategory,
-                repr(self.assertion),
-                self.length,
-                self.offset,
-                self.confidence_score,
-                repr(self.data_sources),
-            )[:1024]
+            f"HealthcareEntity(text={self.text}, normalized_text={self.normalized_text}, "
+            f"category={self.category}, subcategory={self.subcategory}, assertion={repr(self.assertion)}, "
+            f"length={self.length}, offset={self.offset}, confidence_score={self.confidence_score}, "
+            f"data_sources={repr(self.data_sources)})"[:1024]
         )
 
 
 class HealthcareEntityAssertion(DictMixin):
     """Contains various assertions about a `HealthcareEntity`.
 
     For example, if an entity is a diagnosis, is this diagnosis 'conditional' on a symptom?
     Are the doctors 'certain' about this diagnosis? Is this diagnosis 'associated'
     with another diagnosis?
+    """
 
-    :ivar Optional[str] conditionality: Describes whether the healthcare entity it's on is conditional
+    conditionality: Optional[str] = None
+    """Describes whether the healthcare entity it's on is conditional
         on another entity. For example, "If the patient has a fever, he has pneumonia", the diagnosis of pneumonia
         is 'conditional' on whether the patient has a fever. Possible values are "hypothetical" and
-        "conditional".
-    :ivar Optional[str] certainty: Describes how certain the healthcare entity it's on is. For example,
+        "conditional"."""
+    certainty: Optional[str] = None
+    """Describes how certain the healthcare entity it's on is. For example,
         in "The patient may have a fever", the fever entity is not 100% certain, but is instead
         "positivePossible". Possible values are "positive", "positivePossible", "neutralPossible",
-        "negativePossible", and "negative".
-    :ivar Optional[str] association: Describes whether the healthcare entity it's on is the subject of the document, or
+        "negativePossible", and "negative"."""
+    association: Optional[str] = None
+    """Describes whether the healthcare entity it's on is the subject of the document, or
         if this entity describes someone else in the document. For example, in "The subject's mother has
         a fever", the "fever" entity is not associated with the subject themselves, but with the subject's
-        mother. Possible values are "subject" and "other".
-    """
+        mother. Possible values are "subject" and "other"."""
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         self.conditionality = kwargs.get("conditionality", None)
         self.certainty = kwargs.get("certainty", None)
         self.association = kwargs.get("association", None)
 
     @classmethod
     def _from_generated(cls, healthcare_assertion):
         return cls(
             conditionality=healthcare_assertion.conditionality,
             certainty=healthcare_assertion.certainty,
             association=healthcare_assertion.association,
         )
 
-    def __repr__(self):
-        return "HealthcareEntityAssertion(conditionality={}, certainty={}, association={})".format(
-            self.conditionality, self.certainty, self.association
-        )
+    def __repr__(self) -> str:
+        return f"HealthcareEntityAssertion(conditionality={self.conditionality}, certainty={self.certainty}, " \
+               f"association={self.association})"[:1024]
 
 
 class HealthcareEntityDataSource(DictMixin):
     """
     HealthcareEntityDataSource contains information representing an entity reference in a known data source.
-
-    :ivar str entity_id: ID of the entity in the given source catalog.
-    :ivar str name: The name of the entity catalog from where the entity was identified, such as UMLS, CHV, MSH, etc.
     """
 
-    def __init__(self, **kwargs):
+    entity_id: str
+    """ID of the entity in the given source catalog."""
+    name: str
+    """The name of the entity catalog from where the entity was identified, such as UMLS, CHV, MSH, etc."""
+
+    def __init__(self, **kwargs: Any) -> None:
         self.entity_id = kwargs.get("entity_id", None)
         self.name = kwargs.get("name", None)
 
-    def __repr__(self):
-        return "HealthcareEntityDataSource(entity_id={}, name={})".format(
-            self.entity_id, self.name
-        )[:1024]
+    def __repr__(self) -> str:
+        return (
+            f"HealthcareEntityDataSource(entity_id={self.entity_id}, name={self.name})"[:1024]
+        )
 
 
 class TextAnalyticsError(DictMixin):
     """TextAnalyticsError contains the error code, message, and
     other details that explain why the batch or individual document
     failed to be processed by the service.
+    """
 
-    :ivar code: Error code. Possible values include:
+    code: str
+    """Error code. Possible values include
      'invalidRequest', 'invalidArgument', 'internalServerError',
      'serviceUnavailable', 'invalidParameterValue', 'invalidRequestBodyFormat',
      'emptyRequest', 'missingInputRecords', 'invalidDocument', 'modelVersionIncorrect',
-     'invalidDocumentBatch', 'unsupportedLanguageCode', 'invalidCountryHint'
-    :vartype code: str
-    :ivar message: Error message.
-    :vartype message: str
-    :ivar target: Error target.
-    :vartype target: Optional[str]
-    """
+     'invalidDocumentBatch', 'unsupportedLanguageCode', 'invalidCountryHint'"""
+    message: str
+    """Error message."""
+    target: Optional[str] = None
+    """Error target."""
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         self.code = kwargs.get("code", None)
         self.message = kwargs.get("message", None)
         self.target = kwargs.get("target", None)
 
     @classmethod
     def _from_generated(cls, err):
         if err.innererror:
             return cls(
                 code=err.innererror.code,
                 message=err.innererror.message,
                 target=err.innererror.target,
             )
         return cls(code=err.code, message=err.message, target=err.target)
 
-    def __repr__(self):
-        return "TextAnalyticsError(code={}, message={}, target={})".format(
-            self.code, self.message, self.target
-        )[:1024]
+    def __repr__(self) -> str:
+        return f"TextAnalyticsError(code={self.code}, message={self.message}, target={self.target})"[:1024]
 
 
 class TextAnalyticsWarning(DictMixin):
     """TextAnalyticsWarning contains the warning code and message that explains why
     the response has a warning.
-
-    :ivar code: Warning code. Possible values include: 'LongWordsInDocument',
-     'DocumentTruncated'.
-    :vartype code: str
-    :ivar message: Warning message.
-    :vartype message: str
     """
 
-    def __init__(self, **kwargs):
+    code: str
+    """Warning code. Possible values include 'LongWordsInDocument',
+     'DocumentTruncated'."""
+    message: str
+    """Warning message."""
+
+    def __init__(self, **kwargs: Any) -> None:
         self.code = kwargs.get("code", None)
         self.message = kwargs.get("message", None)
 
     @classmethod
     def _from_generated(cls, warning):
         return cls(
             code=warning.code,
             message=warning.message,
         )
 
-    def __repr__(self):
-        return "TextAnalyticsWarning(code={}, message={})".format(
-            self.code, self.message
-        )[:1024]
+    def __repr__(self) -> str:
+        return f"TextAnalyticsWarning(code={self.code}, message={self.message})"[:1024]
 
 
 class ExtractKeyPhrasesResult(DictMixin):
     """ExtractKeyPhrasesResult is a result object which contains
     the key phrases found in a particular document.
 
-    :ivar id: Unique, non-empty document identifier that matches the
-        document id that was passed in with the request. If not specified
-        in the request, an id is assigned for the document.
-    :vartype id: str
-    :ivar key_phrases: A list of representative words or phrases.
-        The number of key phrases returned is proportional to the number of words
-        in the input document.
-    :vartype key_phrases: list[str]
-    :ivar warnings: Warnings encountered while processing document. Results will still be returned
-        if there are warnings, but they may not be fully accurate.
-    :vartype warnings: list[~azure.ai.textanalytics.TextAnalyticsWarning]
-    :ivar statistics: If `show_stats=True` was specified in the request this
-        field will contain information about the document payload.
-    :vartype statistics:
-        Optional[~azure.ai.textanalytics.TextDocumentStatistics]
-    :ivar detected_language: If 'language' is set to 'auto' for the document in the request this
-        field will contain the DetectedLanguage for the document.
-    :vartype detected_language: Optional[~azure.ai.textanalytics.DetectedLanguage]
-    :ivar bool is_error: Boolean check for error item when iterating over list of
-        results. Always False for an instance of a ExtractKeyPhrasesResult.
-    :ivar str kind: The text analysis kind - "KeyPhraseExtraction".
-
     .. versionadded:: 2022-10-01-preview
         The *detected_language* property.
     """
 
-    def __init__(self, **kwargs):
+    id: str  # pylint: disable=redefined-builtin
+    """Unique, non-empty document identifier that matches the
+        document id that was passed in with the request. If not specified
+        in the request, an id is assigned for the document."""
+    key_phrases: List[str]
+    """A list of representative words or phrases.
+        The number of key phrases returned is proportional to the number of words
+        in the input document."""
+    warnings: List[TextAnalyticsWarning]
+    """Warnings encountered while processing document. Results will still be returned
+        if there are warnings, but they may not be fully accurate."""
+    statistics: Optional["TextDocumentStatistics"] = None
+    """If `show_stats=True` was specified in the request this
+        field will contain information about the document payload."""
+    detected_language: Optional[DetectedLanguage] = None
+    """If 'language' is set to 'auto' for the document in the request this
+        field will contain the DetectedLanguage for the document."""
+    is_error: Literal[False] = False
+    """Boolean check for error item when iterating over list of
+        results. Always False for an instance of a ExtractKeyPhrasesResult."""
+    kind: Literal["KeyPhraseExtraction"] = "KeyPhraseExtraction"
+    """The text analysis kind - "KeyPhraseExtraction"."""
+
+    def __init__(self, **kwargs: Any) -> None:
         self.id = kwargs.get("id", None)
         self.key_phrases = kwargs.get("key_phrases", None)
         self.warnings = kwargs.get("warnings", [])
         self.statistics = kwargs.get("statistics", None)
         self.detected_language = kwargs.get('detected_language', None)
         self.is_error: Literal[False] = False
         self.kind: Literal["KeyPhraseExtraction"] = "KeyPhraseExtraction"
 
-    def __repr__(self):
-        return "ExtractKeyPhrasesResult(id={}, key_phrases={}, warnings={}, statistics={}, " \
-               "detected_language={}, is_error={})".format(
-            self.id,
-            self.key_phrases,
-            repr(self.warnings),
-            repr(self.statistics),
-            repr(self.detected_language),
-            self.is_error,
-        )[
-            :1024
-        ]
+    def __repr__(self) -> str:
+        return (
+            f"ExtractKeyPhrasesResult(id={self.id}, key_phrases={self.key_phrases}, "
+            f"warnings={repr(self.warnings)}, statistics={repr(self.statistics)}, "
+            f"detected_language={repr(self.detected_language)}, is_error={self.is_error}, kind={self.kind})"[:1024]
+        )
 
 
 class RecognizeLinkedEntitiesResult(DictMixin):
     """RecognizeLinkedEntitiesResult is a result object which contains
     links to a well-known knowledge base, like for example, Wikipedia or Bing.
 
-    :ivar id: Unique, non-empty document identifier that matches the
-        document id that was passed in with the request. If not specified
-        in the request, an id is assigned for the document.
-    :vartype id: str
-    :ivar entities: Recognized well-known entities in the document.
-    :vartype entities:
-        list[~azure.ai.textanalytics.LinkedEntity]
-    :ivar warnings: Warnings encountered while processing document. Results will still be returned
-        if there are warnings, but they may not be fully accurate.
-    :vartype warnings: list[~azure.ai.textanalytics.TextAnalyticsWarning]
-    :ivar statistics: If `show_stats=True` was specified in the request this
-        field will contain information about the document payload.
-    :vartype statistics:
-        Optional[~azure.ai.textanalytics.TextDocumentStatistics]
-    :ivar detected_language: If 'language' is set to 'auto' for the document in the request this
-        field will contain the DetectedLanguage for the document.
-    :vartype detected_language: Optional[~azure.ai.textanalytics.DetectedLanguage]
-    :ivar bool is_error: Boolean check for error item when iterating over list of
-        results. Always False for an instance of a RecognizeLinkedEntitiesResult.
-    :ivar str kind: The text analysis kind - "EntityLinking".
-
     .. versionadded:: 2022-10-01-preview
         The *detected_language* property.
     """
 
-    def __init__(self, **kwargs):
+    id: str  # pylint: disable=redefined-builtin
+    """Unique, non-empty document identifier that matches the
+        document id that was passed in with the request. If not specified
+        in the request, an id is assigned for the document."""
+    entities: List["LinkedEntity"]
+    """Recognized well-known entities in the document."""
+    warnings: List[TextAnalyticsWarning]
+    """Warnings encountered while processing document. Results will still be returned
+        if there are warnings, but they may not be fully accurate."""
+    statistics: Optional["TextDocumentStatistics"] = None
+    """If `show_stats=True` was specified in the request this
+        field will contain information about the document payload."""
+    detected_language: Optional[DetectedLanguage] = None
+    """If 'language' is set to 'auto' for the document in the request this
+        field will contain the DetectedLanguage for the document."""
+    is_error: Literal[False] = False
+    """Boolean check for error item when iterating over list of
+        results. Always False for an instance of a RecognizeLinkedEntitiesResult."""
+    kind: Literal["EntityLinking"] = "EntityLinking"
+    """The text analysis kind - "EntityLinking"."""
+
+    def __init__(self, **kwargs: Any) -> None:
         self.id = kwargs.get("id", None)
         self.entities = kwargs.get("entities", None)
         self.warnings = kwargs.get("warnings", [])
         self.statistics = kwargs.get("statistics", None)
         self.detected_language = kwargs.get('detected_language', None)
         self.is_error: Literal[False] = False
         self.kind: Literal["EntityLinking"] = "EntityLinking"
 
-    def __repr__(self):
-        return "RecognizeLinkedEntitiesResult(id={}, entities={}, warnings={}, statistics={}, " \
-               "detected_language={}, is_error={})".format(
-            self.id,
-            repr(self.entities),
-            repr(self.warnings),
-            repr(self.statistics),
-            repr(self.detected_language),
-            self.is_error,
-        )[
-            :1024
-        ]
+    def __repr__(self) -> str:
+        return (
+            f"RecognizeLinkedEntitiesResult(id={self.id}, entities={repr(self.entities)}, "
+            f"warnings={repr(self.warnings)}, statistics={repr(self.statistics)}, "
+            f"detected_language={repr(self.detected_language)}, is_error={self.is_error}, kind={self.kind})"[:1024]
+        )
 
 
 class AnalyzeSentimentResult(DictMixin):
     """AnalyzeSentimentResult is a result object which contains
     the overall predicted sentiment and confidence scores for your document
     and a per-sentence sentiment prediction with scores.
 
-    :ivar id: Unique, non-empty document identifier that matches the
-        document id that was passed in with the request. If not specified
-        in the request, an id is assigned for the document.
-    :vartype id: str
-    :ivar sentiment: Predicted sentiment for document (Negative,
-        Neutral, Positive, or Mixed). Possible values include: 'positive',
-        'neutral', 'negative', 'mixed'
-    :vartype sentiment: str
-    :ivar warnings: Warnings encountered while processing document. Results will still be returned
-        if there are warnings, but they may not be fully accurate.
-    :vartype warnings: list[~azure.ai.textanalytics.TextAnalyticsWarning]
-    :ivar statistics: If `show_stats=True` was specified in the request this
-        field will contain information about the document payload.
-    :vartype statistics:
-        Optional[~azure.ai.textanalytics.TextDocumentStatistics]
-    :ivar confidence_scores: Document level sentiment confidence
-        scores between 0 and 1 for each sentiment label.
-    :vartype confidence_scores:
-        ~azure.ai.textanalytics.SentimentConfidenceScores
-    :ivar sentences: Sentence level sentiment analysis.
-    :vartype sentences:
-        list[~azure.ai.textanalytics.SentenceSentiment]
-    :ivar detected_language: If 'language' is set to 'auto' for the document in the request this
-        field will contain the DetectedLanguage for the document.
-    :vartype detected_language: Optional[~azure.ai.textanalytics.DetectedLanguage]
-    :ivar bool is_error: Boolean check for error item when iterating over list of
-        results. Always False for an instance of a AnalyzeSentimentResult.
-    :ivar str kind: The text analysis kind - "SentimentAnalysis".
-
     .. versionadded:: 2022-10-01-preview
         The *detected_language* property.
     """
 
-    def __init__(self, **kwargs):
+    id: str  # pylint: disable=redefined-builtin
+    """Unique, non-empty document identifier that matches the
+        document id that was passed in with the request. If not specified
+        in the request, an id is assigned for the document."""
+    sentiment: str
+    """Predicted sentiment for document (Negative,
+        Neutral, Positive, or Mixed). Possible values include 'positive',
+        'neutral', 'negative', 'mixed'"""
+    confidence_scores: "SentimentConfidenceScores"
+    """Document level sentiment confidence
+        scores between 0 and 1 for each sentiment label."""
+    sentences: List["SentenceSentiment"]
+    """Sentence level sentiment analysis."""
+    warnings: List[TextAnalyticsWarning]
+    """Warnings encountered while processing document. Results will still be returned
+        if there are warnings, but they may not be fully accurate."""
+    statistics: Optional["TextDocumentStatistics"] = None
+    """If `show_stats=True` was specified in the request this
+        field will contain information about the document payload."""
+    detected_language: Optional[DetectedLanguage] = None
+    """If 'language' is set to 'auto' for the document in the request this
+        field will contain the DetectedLanguage for the document."""
+    is_error: Literal[False] = False
+    """Boolean check for error item when iterating over list of
+        results. Always False for an instance of a AnalyzeSentimentResult."""
+    kind: Literal["SentimentAnalysis"] = "SentimentAnalysis"
+    """The text analysis kind - "SentimentAnalysis"."""
+
+    def __init__(self, **kwargs: Any) -> None:
         self.id = kwargs.get("id", None)
         self.sentiment = kwargs.get("sentiment", None)
         self.warnings = kwargs.get("warnings", [])
         self.statistics = kwargs.get("statistics", None)
         self.confidence_scores = kwargs.get("confidence_scores", None)
         self.sentences = kwargs.get("sentences", None)
-        self.detected_language = kwargs.get("detected_language", None)
+        self.detected_language = kwargs.get('detected_language', None)
         self.is_error: Literal[False] = False
         self.kind: Literal["SentimentAnalysis"] = "SentimentAnalysis"
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         return (
-            "AnalyzeSentimentResult(id={}, sentiment={}, warnings={}, statistics={}, confidence_scores={}, "
-            "sentences={}, detected_language={}, is_error={})".format(
-                self.id,
-                self.sentiment,
-                repr(self.warnings),
-                repr(self.statistics),
-                repr(self.confidence_scores),
-                repr(self.sentences),
-                repr(self.detected_language),
-                self.is_error,
-            )[:1024]
+            f"AnalyzeSentimentResult(id={self.id}, sentiment={self.sentiment}, warnings={repr(self.warnings)}, "
+            f"statistics={repr(self.statistics)}, confidence_scores={repr(self.confidence_scores)}, "
+            f"sentences={repr(self.sentences)}, detected_language={repr(self.detected_language)}, "
+            f"is_error={self.is_error}, kind={self.kind})"[:1024]
         )
 
 
 class TextDocumentStatistics(DictMixin):
     """TextDocumentStatistics contains information about
     the document payload.
-
-    :ivar character_count: Number of text elements recognized in
-        the document.
-    :vartype character_count: int
-    :ivar transaction_count: Number of transactions for the document.
-    :vartype transaction_count: int
     """
 
-    def __init__(self, **kwargs):
+    character_count: int
+    """Number of text elements recognized in
+        the document."""
+    transaction_count: int
+    """Number of transactions for the document."""
+
+    def __init__(self, **kwargs: Any) -> None:
         self.character_count = kwargs.get("character_count", None)
         self.transaction_count = kwargs.get("transaction_count", None)
 
     @classmethod
     def _from_generated(cls, stats):
         if stats is None:
             return None
         return cls(
             character_count=stats.characters_count,
             transaction_count=stats.transactions_count,
         )
 
-    def __repr__(self):
-        return (
-            "TextDocumentStatistics(character_count={}, transaction_count={})".format(
-                self.character_count, self.transaction_count
-            )[:1024]
-        )
+    def __repr__(self) -> str:
+        return f"TextDocumentStatistics(character_count={self.character_count}, " \
+               f"transaction_count={self.transaction_count})"[:1024]
 
 
 class DocumentError(DictMixin):
     """DocumentError is an error object which represents an error on
     the individual document.
+    """
 
-    :ivar id: Unique, non-empty document identifier that matches the
+    id: str  # pylint: disable=redefined-builtin
+    """Unique, non-empty document identifier that matches the
         document id that was passed in with the request. If not specified
-        in the request, an id is assigned for the document.
-    :vartype id: str
-    :ivar error: The document error.
-    :vartype error: ~azure.ai.textanalytics.TextAnalyticsError
-    :ivar bool is_error: Boolean check for error item when iterating over list of
-        results. Always True for an instance of a DocumentError.
-    :ivar str kind: Error kind - "DocumentError".
-    """
+        in the request, an id is assigned for the document."""
+    error: TextAnalyticsError
+    """The document error."""
+    is_error: Literal[True] = True
+    """Boolean check for error item when iterating over list of
+        results. Always True for an instance of a DocumentError."""
+    kind: Literal["DocumentError"] = "DocumentError"
+    """Error kind - "DocumentError"."""
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         self.id = kwargs.get("id", None)
         self.error = kwargs.get("error", None)
         self.is_error: Literal[True] = True
         self.kind: Literal["DocumentError"] = "DocumentError"
 
-    def __getattr__(self, attr):
+    def __getattr__(self, attr: str) -> Any:
         result_set = set()
         result_set.update(
-            RecognizeEntitiesResult().keys()
+            RecognizeEntitiesResult().keys()  # type: ignore[operator]
             + RecognizePiiEntitiesResult().keys()
             + DetectLanguageResult().keys()
             + RecognizeLinkedEntitiesResult().keys()
             + AnalyzeSentimentResult().keys()
             + ExtractKeyPhrasesResult().keys()
             + AnalyzeHealthcareEntitiesResult().keys()
             + RecognizeCustomEntitiesResult().keys()
             + ClassifyDocumentResult().keys()
             + ExtractSummaryResult().keys()
-            + AbstractSummaryResult().keys()
+            + AbstractiveSummaryResult().keys()
             + DynamicClassificationResult().keys()
         )
         result_attrs = result_set.difference(DocumentError().keys())
         if attr in result_attrs:
             raise AttributeError(
                 "'DocumentError' object has no attribute '{}'. The service was unable to process this document:\n"
                 "Document Id: {}\nError: {} - {}\n".format(
@@ -1290,84 +1272,88 @@
     @classmethod
     def _from_generated(cls, doc_err):
         return cls(
             id=doc_err.id,
             error=TextAnalyticsError._from_generated(  # pylint: disable=protected-access
                 doc_err.error
             ),
-            is_error=True,
         )
 
-    def __repr__(self):
-        return "DocumentError(id={}, error={}, is_error={})".format(
-            self.id, repr(self.error), self.is_error
-        )[:1024]
+    def __repr__(self) -> str:
+        return f"DocumentError(id={self.id}, error={repr(self.error)}, " \
+               f"is_error={self.is_error}, kind={self.kind})"[:1024]
 
 
 class DetectLanguageInput(LanguageInput):
     """The input document to be analyzed for detecting language.
 
     :keyword str id: Required. Unique, non-empty document identifier.
     :keyword str text: Required. The input text to process.
     :keyword Optional[str] country_hint: A country hint to help better detect
      the language of the text. Accepts two letter country codes
      specified by ISO 3166-1 alpha-2. Defaults to "US". Pass
      in the string "none" to not use a country_hint.
-    :ivar id: Required. Unique, non-empty document identifier.
-    :vartype id: str
-    :ivar text: Required. The input text to process.
-    :vartype text: str
-    :ivar country_hint: A country hint to help better detect
-     the language of the text. Accepts two letter country codes
-     specified by ISO 3166-1 alpha-2. Defaults to "US". Pass
-     in the string "none" to not use a country_hint.
-    :vartype country_hint: Optional[str]
     """
 
-    def __init__(self, **kwargs):
-        super().__init__(**kwargs)
-        self.id = kwargs.get("id", None)
-        self.text = kwargs.get("text", None)
-        self.country_hint = kwargs.get("country_hint", None)
+    id: str  # pylint: disable=redefined-builtin
+    """Required. Unique, non-empty document identifier."""
+    text: str
+    """Required. The input text to process."""
+    country_hint: Optional[str] = None
+    """A country hint to help better detect
+        the language of the text. Accepts two letter country codes
+        specified by ISO 3166-1 alpha-2. Defaults to "US". Pass
+        in the string "none" to not use a country_hint."""
 
-    def __repr__(self):
-        return "DetectLanguageInput(id={}, text={}, country_hint={})".format(
-            self.id, self.text, self.country_hint
-        )[:1024]
+    def __init__(
+        self,
+        *,
+        id: str,  # pylint: disable=redefined-builtin
+        text: str,
+        country_hint: Optional[str] = None,
+        **kwargs: Any  # pylint: disable=unused-argument
+    ) -> None:
+        super().__init__(id=id, text=text, country_hint=country_hint)
+        self.id = id
+        self.text = text
+        self.country_hint = country_hint
+
+    def __repr__(self) -> str:
+        return f"DetectLanguageInput(id={self.id}, text={self.text}, country_hint={self.country_hint})"[:1024]
 
 
 class LinkedEntity(DictMixin):
     """LinkedEntity contains a link to the well-known recognized
     entity in text. The link comes from a data source like Wikipedia
     or Bing. It additionally includes all of the matches of this
     entity found in the document.
 
-    :ivar name: Entity Linking formal name.
-    :vartype name: str
-    :ivar matches: List of instances this entity appears in the text.
-    :vartype matches:
-        list[~azure.ai.textanalytics.LinkedEntityMatch]
-    :ivar language: Language used in the data source.
-    :vartype language: str
-    :ivar data_source_entity_id: Unique identifier of the recognized entity from the data
-        source.
-    :vartype data_source_entity_id: Optional[str]
-    :ivar url: URL to the entity's page from the data source.
-    :vartype url: str
-    :ivar data_source: Data source used to extract entity linking,
-        such as Wiki/Bing etc.
-    :vartype data_source: str
-    :ivar Optional[str] bing_entity_search_api_id: Bing Entity Search unique identifier of the recognized entity.
-        Use in conjunction with the Bing Entity Search SDK to fetch additional relevant information.
-
     .. versionadded:: v3.1
         The *bing_entity_search_api_id* property.
     """
 
-    def __init__(self, **kwargs):
+    name: str
+    """Entity Linking formal name."""
+    matches: List["LinkedEntityMatch"]
+    """List of instances this entity appears in the text."""
+    language: str
+    """Language used in the data source."""
+    url: str
+    """URL to the entity's page from the data source."""
+    data_source: str
+    """Data source used to extract entity linking,
+        such as Wiki/Bing etc."""
+    data_source_entity_id: Optional[str] = None
+    """Unique identifier of the recognized entity from the data
+        source."""
+    bing_entity_search_api_id: Optional[str] = None
+    """Bing Entity Search unique identifier of the recognized entity.
+        Use in conjunction with the Bing Entity Search SDK to fetch additional relevant information."""
+
+    def __init__(self, **kwargs: Any) -> None:
         self.name = kwargs.get("name", None)
         self.matches = kwargs.get("matches", None)
         self.language = kwargs.get("language", None)
         self.data_source_entity_id = kwargs.get("data_source_entity_id", None)
         self.url = kwargs.get("url", None)
         self.data_source = kwargs.get("data_source", None)
         self.bing_entity_search_api_id = kwargs.get("bing_entity_search_api_id", None)
@@ -1386,51 +1372,46 @@
             language=entity.language,
             data_source_entity_id=entity.id,
             url=entity.url,
             data_source=entity.data_source,
             bing_entity_search_api_id=bing_entity_search_api_id,
         )
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         return (
-            "LinkedEntity(name={}, matches={}, language={}, data_source_entity_id={}, url={}, "
-            "data_source={}, bing_entity_search_api_id={})".format(
-                self.name,
-                repr(self.matches),
-                self.language,
-                self.data_source_entity_id,
-                self.url,
-                self.data_source,
-                self.bing_entity_search_api_id,
-            )[:1024]
+            f"LinkedEntity(name={self.name}, matches={repr(self.matches)}, language={self.language}, "
+            f"data_source_entity_id={self.data_source_entity_id}, url={self.url}, "
+            f"data_source={self.data_source}, bing_entity_search_api_id={self.bing_entity_search_api_id})"[:1024]
         )
 
 
 class LinkedEntityMatch(DictMixin):
     """A match for the linked entity found in text. Provides
     the confidence score of the prediction and where the entity
     was found in the text.
 
-    :ivar confidence_score: If a well-known item is recognized, a
-        decimal number denoting the confidence level between 0 and 1 will be
-        returned.
-    :vartype confidence_score: float
-    :ivar text: Entity text as appears in the request.
-    :ivar int length: The linked entity match text length.  This value depends on the value of the
-        `string_index_type` parameter set in the original request, which is UnicodeCodePoints by default.
-    :ivar int offset: The linked entity match text offset from the start of the document.
-        The value depends on the value of the `string_index_type` parameter
-        set in the original request, which is UnicodeCodePoints by default.
-    :vartype text: str
-
     .. versionadded:: v3.1
         The *offset* and *length* properties.
     """
 
-    def __init__(self, **kwargs):
+    confidence_score: float
+    """If a well-known item is recognized, a
+        decimal number denoting the confidence level between 0 and 1 will be
+        returned."""
+    text: str
+    """Entity text as appears in the request."""
+    length: int
+    """The linked entity match text length.  This value depends on the value of the
+        `string_index_type` parameter set in the original request, which is UnicodeCodePoints by default."""
+    offset: int
+    """The linked entity match text offset from the start of the document.
+        The value depends on the value of the `string_index_type` parameter
+        set in the original request, which is UnicodeCodePoints by default."""
+
+    def __init__(self, **kwargs: Any) -> None:
         self.confidence_score = kwargs.get("confidence_score", None)
         self.text = kwargs.get("text", None)
         self.length = kwargs.get("length", None)
         self.offset = kwargs.get("offset", None)
 
     @classmethod
     def _from_generated(cls, match):
@@ -1444,77 +1425,80 @@
         return cls(
             confidence_score=match.confidence_score,
             text=match.text,
             length=length,
             offset=offset,
         )
 
-    def __repr__(self):
-        return "LinkedEntityMatch(confidence_score={}, text={}, length={}, offset={})".format(
-            self.confidence_score, self.text, self.length, self.offset
-        )[
-            :1024
-        ]
+    def __repr__(self) -> str:
+        return f"LinkedEntityMatch(confidence_score={self.confidence_score}, text={self.text}, " \
+               f"length={self.length}, offset={self.offset})"[:1024]
 
 
 class TextDocumentInput(DictMixin, MultiLanguageInput):
     """The input document to be analyzed by the service.
 
     :keyword str id: Required. Unique, non-empty document identifier.
     :keyword str text: Required. The input text to process.
     :keyword str language: This is the 2 letter ISO 639-1 representation
      of a language. For example, use "en" for English; "es" for Spanish etc.
      For automatic language detection, use "auto" (Only supported by long-running
      operation APIs with API version 2022-10-01-preview or newer). If
      not set, uses "en" for English as default.
-    :ivar id: Required. Unique, non-empty document identifier.
-    :vartype id: str
-    :ivar text: Required. The input text to process.
-    :vartype text: str
-    :ivar language: This is the 2 letter ISO 639-1 representation
-     of a language. For example, use "en" for English; "es" for Spanish etc.
-     For automatic language detection, use "auto" (Only supported by long-running
-     operation APIs with API version 2022-10-01-preview or newer). If
-     not set, uses "en" for English as default.
-    :vartype language: Optional[str]
 
     .. versionadded:: 2022-10-01-preview
         The 'auto' option for language.
     """
 
-    def __init__(self, **kwargs):
-        super().__init__(**kwargs)
-        self.id = kwargs.get("id", None)
-        self.text = kwargs.get("text", None)
-        self.language = kwargs.get("language", None)
+    id: str  # pylint: disable=redefined-builtin
+    """Required. Unique, non-empty document identifier."""
+    text: str
+    """Required. The input text to process."""
+    language: Optional[str] = None
+    """This is the 2 letter ISO 639-1 representation
+     of a language. For example, use "en" for English; "es" for Spanish etc.
+     For automatic language detection, use "auto" (Only supported by long-running
+     operation APIs with API version 2022-10-01-preview or newer). If
+     not set, uses "en" for English as default."""
+
+    def __init__(
+        self,
+        *,
+        id: str,  # pylint: disable=redefined-builtin
+        text: str,
+        language: Optional[str] = None,
+        **kwargs: Any  # pylint: disable=unused-argument
+    ) -> None:
+        super().__init__(id=id, text=text, language=language)
+        self.id = id
+        self.text = text
+        self.language = language
 
-    def __repr__(self):
-        return "TextDocumentInput(id={}, text={}, language={})".format(
-            self.id, self.text, self.language
-        )[:1024]
+    def __repr__(self) -> str:
+        return f"TextDocumentInput(id={self.id}, text={self.text}, language={self.language})"[:1024]
 
 
 class TextDocumentBatchStatistics(DictMixin):
     """TextDocumentBatchStatistics contains information about the
     request payload. Note: This object is not returned
     in the response and needs to be retrieved by a response hook.
-
-    :ivar document_count: Number of documents submitted in the request.
-    :vartype document_count: int
-    :ivar valid_document_count: Number of valid documents. This
-        excludes empty, over-size limit or non-supported languages documents.
-    :vartype valid_document_count: int
-    :ivar erroneous_document_count: Number of invalid documents.
-        This includes empty, over-size limit or non-supported languages documents.
-    :vartype erroneous_document_count: int
-    :ivar transaction_count: Number of transactions for the request.
-    :vartype transaction_count: int
     """
 
-    def __init__(self, **kwargs):
+    document_count: int
+    """Number of documents submitted in the request"""
+    valid_document_count: int
+    """Number of valid documents. This
+        excludes empty, over-size limit or non-supported languages documents."""
+    erroneous_document_count: int
+    """Number of invalid documents.
+        This includes empty, over-size limit or non-supported languages documents."""
+    transaction_count: int
+    """Number of transactions for the request."""
+
+    def __init__(self, **kwargs: Any) -> None:
         self.document_count = kwargs.get("document_count", None)
         self.valid_document_count = kwargs.get("valid_document_count", None)
         self.erroneous_document_count = kwargs.get("erroneous_document_count", None)
         self.transaction_count = kwargs.get("transaction_count", None)
 
     @classmethod
     def _from_generated(cls, statistics):
@@ -1523,58 +1507,55 @@
         return cls(
             document_count=statistics["documentsCount"],
             valid_document_count=statistics["validDocumentsCount"],
             erroneous_document_count=statistics["erroneousDocumentsCount"],
             transaction_count=statistics["transactionsCount"],
         )
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         return (
-            "TextDocumentBatchStatistics(document_count={}, valid_document_count={}, erroneous_document_count={}, "
-            "transaction_count={})".format(
-                self.document_count,
-                self.valid_document_count,
-                self.erroneous_document_count,
-                self.transaction_count,
-            )[:1024]
+            f"TextDocumentBatchStatistics(document_count={self.document_count}, "
+            f"valid_document_count={self.valid_document_count}, "
+            f"erroneous_document_count={self.erroneous_document_count}, "
+            f"transaction_count={self.transaction_count})"[:1024]
         )
 
 
 class SentenceSentiment(DictMixin):
     """SentenceSentiment contains the predicted sentiment and
     confidence scores for each individual sentence in the document.
 
-    :ivar text: The sentence text.
-    :vartype text: str
-    :ivar sentiment: The predicted Sentiment for the sentence.
-        Possible values include: 'positive', 'neutral', 'negative'
-    :vartype sentiment: str
-    :ivar confidence_scores: The sentiment confidence score between 0
-        and 1 for the sentence for all labels.
-    :vartype confidence_scores:
-        ~azure.ai.textanalytics.SentimentConfidenceScores
-    :ivar int length: The sentence text length.  This value depends on the value of the
+    .. versionadded:: v3.1
+        The *offset*, *length*, and *mined_opinions* properties.
+    """
+
+    text: str
+    """The sentence text."""
+    sentiment: str
+    """The predicted Sentiment for the sentence.
+        Possible values include 'positive', 'neutral', 'negative'"""
+    confidence_scores: "SentimentConfidenceScores"
+    """The sentiment confidence score between 0
+        and 1 for the sentence for all labels."""
+    length: int
+    """The sentence text length.  This value depends on the value of the
         `string_index_type` parameter set in the original request, which is UnicodeCodePoints
-        by default.
-    :ivar int offset: The sentence text offset from the start of the document.
+        by default."""
+    offset: int
+    """The sentence text offset from the start of the document.
         The value depends on the value of the `string_index_type` parameter
-        set in the original request, which is UnicodeCodePoints by default.
-    :ivar mined_opinions: The list of opinions mined from this sentence.
+        set in the original request, which is UnicodeCodePoints by default."""
+    mined_opinions: Optional[List["MinedOpinion"]] = None
+    """The list of opinions mined from this sentence.
         For example in the sentence "The food is good, but the service is bad", we would
         mine the two opinions "food is good" and "service is bad". Only returned
         if `show_opinion_mining` is set to True in the call to `analyze_sentiment` and
-        api version is v3.1 and up.
-    :vartype mined_opinions:
-        Optional[list[~azure.ai.textanalytics.MinedOpinion]]
-
-    .. versionadded:: v3.1
-        The *offset*, *length*, and *mined_opinions* properties.
-    """
+        api version is v3.1 and up."""
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         self.text = kwargs.get("text", None)
         self.sentiment = kwargs.get("sentiment", None)
         self.confidence_scores = kwargs.get("confidence_scores", None)
         self.length = kwargs.get("length", None)
         self.offset = kwargs.get("offset", None)
         self.mined_opinions = kwargs.get("mined_opinions", None)
 
@@ -1607,40 +1588,34 @@
                 sentence.confidence_scores
             ),
             length=length,
             offset=offset,
             mined_opinions=mined_opinions,
         )
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         return (
-            "SentenceSentiment(text={}, sentiment={}, confidence_scores={}, "
-            "length={}, offset={}, mined_opinions={})".format(
-                self.text,
-                self.sentiment,
-                repr(self.confidence_scores),
-                self.length,
-                self.offset,
-                repr(self.mined_opinions),
-            )[:1024]
+            f"SentenceSentiment(text={self.text}, sentiment={self.sentiment}, "
+            f"confidence_scores={repr(self.confidence_scores)}, length={self.length}, "
+            f"offset={self.offset}, mined_opinions={repr(self.mined_opinions)})"[:1024]
         )
 
 
 class MinedOpinion(DictMixin):
     """A mined opinion object represents an opinion we've extracted from a sentence.
     It consists of both a target that these opinions are about, and the assessments
     representing the opinion.
-
-    :ivar target: The target of an opinion about a product/service.
-    :vartype target: Optional[~azure.ai.textanalytics.TargetSentiment]
-    :ivar assessments: The assessments representing the opinion of the target.
-    :vartype assessments: Optional[list[~azure.ai.textanalytics.AssessmentSentiment]]
     """
 
-    def __init__(self, **kwargs):
+    target: "TargetSentiment"
+    """The target of an opinion about a product/service."""
+    assessments: List["AssessmentSentiment"]
+    """The assessments representing the opinion of the target."""
+
+    def __init__(self, **kwargs: Any) -> None:
         self.target = kwargs.get("target", None)
         self.assessments = kwargs.get("assessments", None)
 
     @staticmethod
     def _get_assessments(
         relations, results, sentiment
     ):  # pylint: disable=unused-argument
@@ -1671,43 +1646,44 @@
                 )
                 for assessment in cls._get_assessments(
                     target.relations, results, sentiment
                 )
             ],
         )
 
-    def __repr__(self):
-        return "MinedOpinion(target={}, assessments={})".format(
-            repr(self.target), repr(self.assessments)
-        )[:1024]
+    def __repr__(self) -> str:
+        return f"MinedOpinion(target={repr(self.target)}, assessments={repr(self.assessments)})"[:1024]
 
 
 class TargetSentiment(DictMixin):
     """TargetSentiment contains the predicted sentiment,
     confidence scores and other information about a key component of a product/service.
     For example in "The food at Hotel Foo is good", "food" is an key component of
     "Hotel Foo".
+    """
 
-    :ivar str text: The text value of the target.
-    :ivar str sentiment: The predicted Sentiment for the target. Possible values
-        include 'positive', 'mixed', and 'negative'.
-    :ivar confidence_scores: The sentiment confidence score between 0
+    text: str
+    """The text value of the target."""
+    sentiment: str
+    """The predicted Sentiment for the target. Possible values
+        include 'positive', 'mixed', and 'negative'."""
+    confidence_scores: "SentimentConfidenceScores"
+    """The sentiment confidence score between 0
         and 1 for the target for 'positive' and 'negative' labels. It's score
-        for 'neutral' will always be 0
-    :vartype confidence_scores:
-        ~azure.ai.textanalytics.SentimentConfidenceScores
-    :ivar int length: The target text length.  This value depends on the value of the
+        for 'neutral' will always be 0"""
+    length: int
+    """The target text length.  This value depends on the value of the
         `string_index_type` parameter set in the original request, which is UnicodeCodePoints
-        by default.
-    :ivar int offset: The target text offset from the start of the document.
+        by default."""
+    offset: int
+    """The target text offset from the start of the document.
         The value depends on the value of the `string_index_type` parameter
-        set in the original request, which is UnicodeCodePoints by default.
-    """
+        set in the original request, which is UnicodeCodePoints by default."""
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         self.text = kwargs.get("text", None)
         self.sentiment = kwargs.get("sentiment", None)
         self.confidence_scores = kwargs.get("confidence_scores", None)
         self.length = kwargs.get("length", None)
         self.offset = kwargs.get("offset", None)
 
     @classmethod
@@ -1718,52 +1694,51 @@
             confidence_scores=SentimentConfidenceScores._from_generated(  # pylint: disable=protected-access
                 target.confidence_scores
             ),
             length=target.length,
             offset=target.offset,
         )
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         return (
-            "TargetSentiment(text={}, sentiment={}, confidence_scores={}, "
-            "length={}, offset={})".format(
-                self.text,
-                self.sentiment,
-                repr(self.confidence_scores),
-                self.length,
-                self.offset,
-            )[:1024]
+            f"TargetSentiment(text={self.text}, sentiment={self.sentiment}, "
+            f"confidence_scores={repr(self.confidence_scores)}, "
+            f"length={self.length}, offset={self.offset})"[:1024]
         )
 
 
 class AssessmentSentiment(DictMixin):
     """AssessmentSentiment contains the predicted sentiment,
     confidence scores and other information about an assessment given about
     a particular target.  For example, in the sentence "The food is good", the assessment
     of the target 'food' is 'good'.
+    """
 
-    :ivar str text: The assessment text.
-    :ivar str sentiment: The predicted Sentiment for the assessment. Possible values
-        include 'positive', 'mixed', and 'negative'.
-    :ivar confidence_scores: The sentiment confidence score between 0
+    text: str
+    """The assessment text."""
+    sentiment: str
+    """The predicted Sentiment for the assessment. Possible values
+        include 'positive', 'mixed', and 'negative'."""
+    confidence_scores: "SentimentConfidenceScores"
+    """The sentiment confidence score between 0
         and 1 for the assessment for 'positive' and 'negative' labels. It's score
-        for 'neutral' will always be 0
-    :vartype confidence_scores:
-        ~azure.ai.textanalytics.SentimentConfidenceScores
-    :ivar int length: The assessment text length.  This value depends on the value of the
+        for 'neutral' will always be 0"""
+    length: int
+    """The assessment text length.  This value depends on the value of the
         `string_index_type` parameter set in the original request, which is UnicodeCodePoints
-        by default.
-    :ivar int offset: The assessment text offset from the start of the document.
+        by default."""
+    offset: int
+    """The assessment text offset from the start of the document.
         The value depends on the value of the `string_index_type` parameter
-        set in the original request, which is UnicodeCodePoints by default.
-    :ivar bool is_negated: Whether the value of the assessment is negated. For example, in
-        "The food is not good", the assessment "good" is negated.
-    """
+        set in the original request, which is UnicodeCodePoints by default."""
+    is_negated: bool
+    """Whether the value of the assessment is negated. For example, in
+        "The food is not good", the assessment "good" is negated."""
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         self.text = kwargs.get("text", None)
         self.sentiment = kwargs.get("sentiment", None)
         self.confidence_scores = kwargs.get("confidence_scores", None)
         self.length = kwargs.get("length", None)
         self.offset = kwargs.get("offset", None)
         self.is_negated = kwargs.get("is_negated", None)
 
@@ -1776,57 +1751,50 @@
                 assessment.confidence_scores
             ),
             length=assessment.length,
             offset=assessment.offset,
             is_negated=assessment.is_negated,
         )
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         return (
-            "AssessmentSentiment(text={}, sentiment={}, confidence_scores={}, length={}, offset={}, "
-            "is_negated={})".format(
-                self.text,
-                self.sentiment,
-                repr(self.confidence_scores),
-                self.length,
-                self.offset,
-                self.is_negated,
-            )[:1024]
+            f"AssessmentSentiment(text={self.text}, sentiment={self.sentiment}, "
+            f"confidence_scores={repr(self.confidence_scores)}, length={self.length}, "
+            f"offset={self.offset}, is_negated={self.is_negated})"[:1024]
         )
 
 
 class SentimentConfidenceScores(DictMixin):
     """The confidence scores (Softmax scores) between 0 and 1.
     Higher values indicate higher confidence.
-
-    :ivar positive: Positive score.
-    :vartype positive: float
-    :ivar neutral: Neutral score.
-    :vartype neutral: float
-    :ivar negative: Negative score.
-    :vartype negative: float
     """
 
-    def __init__(self, **kwargs):
+    positive: float
+    """Positive score."""
+    neutral: float
+    """Neutral score."""
+    negative: float
+    """Negative score."""
+
+    def __init__(self, **kwargs: Any) -> None:
         self.positive = kwargs.get("positive", 0.0)
         self.neutral = kwargs.get("neutral", 0.0)
         self.negative = kwargs.get("negative", 0.0)
 
     @classmethod
     def _from_generated(cls, score):
         return cls(
             positive=score.positive,
             neutral=score.neutral if hasattr(score, "neutral") else 0.0,
             negative=score.negative,
         )
 
-    def __repr__(self):
-        return "SentimentConfidenceScores(positive={}, neutral={}, negative={})".format(
-            self.positive, self.neutral, self.negative
-        )[:1024]
+    def __repr__(self) -> str:
+        return f"SentimentConfidenceScores(positive={self.positive}, " \
+               f"neutral={self.neutral}, negative={self.negative})"[:1024]
 
 
 class _AnalyzeActionsType(str, Enum, metaclass=CaseInsensitiveEnumMeta):
     """The type of action that was applied to the documents"""
 
     RECOGNIZE_ENTITIES = "recognize_entities"  #: Entities Recognition action.
     RECOGNIZE_PII_ENTITIES = (
@@ -1870,40 +1838,49 @@
         logged on the service side for troubleshooting. By default, the Language service logs your
         input text for 48 hours, solely to allow for troubleshooting issues in providing you with
         the service's natural language processing functions. Setting this parameter to true,
         disables input logging and may limit our ability to remediate issues that occur. Please see
         Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
         additional details, and Microsoft Responsible AI principles at
         https://www.microsoft.com/ai/responsible-ai.
-    :ivar Optional[str] model_version: The model version to use for the analysis.
-    :ivar Optional[str] string_index_type: Specifies the method used to interpret string offsets.
+    """
+
+    model_version: Optional[str] = None
+    """The model version to use for the analysis."""
+    string_index_type: Optional[str] = None
+    """Specifies the method used to interpret string offsets.
         `UnicodeCodePoint`, the Python encoding, is the default. To override the Python default,
         you can also pass in `Utf16CodeUnit` or `TextElement_v8`. For additional information
-        see https://aka.ms/text-analytics-offsets
-    :ivar Optional[bool] disable_service_logs: If set to true, you opt-out of having your text input
+        see https://aka.ms/text-analytics-offsets"""
+    disable_service_logs: Optional[bool] = None
+    """If set to true, you opt-out of having your text input
         logged on the service side for troubleshooting. By default, the Language service logs your
         input text for 48 hours, solely to allow for troubleshooting issues in providing you with
         the service's natural language processing functions. Setting this parameter to true,
         disables input logging and may limit our ability to remediate issues that occur. Please see
         Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
         additional details, and Microsoft Responsible AI principles at
-        https://www.microsoft.com/ai/responsible-ai.
-    """
+        https://www.microsoft.com/ai/responsible-ai."""
 
-    def __init__(self, **kwargs):
-        self.model_version = kwargs.get("model_version", None)
-        self.string_index_type = kwargs.get("string_index_type", "UnicodeCodePoint")
-        self.disable_service_logs = kwargs.get("disable_service_logs", None)
-
-    def __repr__(self, **kwargs):
-        return "RecognizeEntitiesAction(model_version={}, string_index_type={}, disable_service_logs={})".format(
-            self.model_version, self.string_index_type, self.disable_service_logs
-        )[
-            :1024
-        ]
+    def __init__(
+        self,
+        *,
+        model_version: Optional[str] = None,
+        string_index_type: Optional[str] = None,
+        disable_service_logs: Optional[bool] = None,
+        **kwargs: Any
+    ) -> None:
+        self.model_version = model_version
+        self.string_index_type: str = string_index_type if string_index_type is not None else STRING_INDEX_TYPE_DEFAULT
+        self.disable_service_logs = disable_service_logs
+
+    def __repr__(self) -> str:
+        return f"RecognizeEntitiesAction(model_version={self.model_version}, " \
+               f"string_index_type={self.string_index_type}, " \
+               f"disable_service_logs={self.disable_service_logs})"[:1024]
 
     def _to_generated(self, api_version, task_id):
         if is_language_api(api_version):
             return _v2022_10_01_preview_models.EntitiesLROTask(
                 task_name=task_id,
                 parameters=_v2022_10_01_preview_models.EntitiesTaskParameters(
                     model_version=self.model_version,
@@ -1944,49 +1921,59 @@
         logged on the service side for troubleshooting. By default, the Language service logs your
         input text for 48 hours, solely to allow for troubleshooting issues in providing you with
         the service's natural language processing functions. Setting this parameter to true,
         disables input logging and may limit our ability to remediate issues that occur. Please see
         Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
         additional details, and Microsoft Responsible AI principles at
         https://www.microsoft.com/ai/responsible-ai.
-    :ivar Optional[str] model_version: The model version to use for the analysis.
-    :ivar Optional[bool] show_opinion_mining: Whether to mine the opinions of a sentence and conduct more
+    """
+
+    show_opinion_mining: Optional[bool] = None
+    """Whether to mine the opinions of a sentence and conduct more
         granular analysis around the aspects of a product or service (also known as
         aspect-based sentiment analysis). If set to true, the returned
         :class:`~azure.ai.textanalytics.SentenceSentiment` objects
-        will have property `mined_opinions` containing the result of this analysis.
-    :ivar Optional[str] string_index_type: Specifies the method used to interpret string offsets.
+        will have property `mined_opinions` containing the result of this analysis."""
+    model_version: Optional[str] = None
+    """The model version to use for the analysis."""
+    string_index_type: Optional[str] = None
+    """Specifies the method used to interpret string offsets.
         `UnicodeCodePoint`, the Python encoding, is the default. To override the Python default,
         you can also pass in `Utf16CodeUnit` or `TextElement_v8`. For additional information
-        see https://aka.ms/text-analytics-offsets
-    :ivar Optional[bool] disable_service_logs: If set to true, you opt-out of having your text input
+        see https://aka.ms/text-analytics-offsets"""
+    disable_service_logs: Optional[bool] = None
+    """If set to true, you opt-out of having your text input
         logged on the service side for troubleshooting. By default, the Language service logs your
         input text for 48 hours, solely to allow for troubleshooting issues in providing you with
         the service's natural language processing functions. Setting this parameter to true,
         disables input logging and may limit our ability to remediate issues that occur. Please see
         Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
         additional details, and Microsoft Responsible AI principles at
-        https://www.microsoft.com/ai/responsible-ai.
-    """
+        https://www.microsoft.com/ai/responsible-ai."""
 
-    def __init__(self, **kwargs):
-        self.model_version = kwargs.get("model_version", None)
-        self.show_opinion_mining = kwargs.get("show_opinion_mining", None)
-        self.string_index_type = kwargs.get("string_index_type", "UnicodeCodePoint")
-        self.disable_service_logs = kwargs.get("disable_service_logs", None)
-
-    def __repr__(self, **kwargs):
-        return (
-            "AnalyzeSentimentAction(model_version={}, show_opinion_mining={}, string_index_type={}, "
-            "disable_service_logs={}".format(
-                self.model_version,
-                self.show_opinion_mining,
-                self.string_index_type,
-                self.disable_service_logs,
-            )[:1024]
+    def __init__(
+        self,
+        *,
+        show_opinion_mining: Optional[bool] = None,
+        model_version: Optional[str] = None,
+        string_index_type: Optional[str] = None,
+        disable_service_logs: Optional[bool] = None,
+        **kwargs: Any
+    ) -> None:
+        self.model_version = model_version
+        self.show_opinion_mining = show_opinion_mining
+        self.string_index_type: str = string_index_type if string_index_type is not None else STRING_INDEX_TYPE_DEFAULT
+        self.disable_service_logs = disable_service_logs
+
+    def __repr__(self) -> str:
+        return (
+            f"AnalyzeSentimentAction(model_version={self.model_version}, "
+            f"show_opinion_mining={self.show_opinion_mining}, "
+            f"string_index_type={self.string_index_type}, "
+            f"disable_service_logs={self.disable_service_logs}"[:1024]
         )
 
     def _to_generated(self, api_version, task_id):
         if is_language_api(api_version):
             return _v2022_10_01_preview_models.SentimentAnalysisLROTask(
                 task_name=task_id,
                 parameters=_v2022_10_01_preview_models.SentimentAnalysisTaskParameters(
@@ -2005,15 +1992,15 @@
             ),
             task_name=task_id
         )
 
 
 class RecognizePiiEntitiesAction(DictMixin):
     """RecognizePiiEntitiesAction encapsulates the parameters for starting a long-running PII
-    Entities Recognition operation.
+    Entities Recognition operation. See more information in the service docs: https://aka.ms/azsdk/language/pii
 
     If you just want to recognize pii entities in a list of documents, and not perform multiple
     long running actions on the input of documents, call method `recognize_pii_entities` instead
     of interfacing with this model.
 
     :keyword Optional[str] model_version: The model version to use for the analysis.
     :keyword Optional[str] domain_filter: An optional string to set the PII domain to include only a
@@ -2030,52 +2017,62 @@
     :keyword Optional[bool] disable_service_logs: Defaults to true, meaning that the Language service will not log your
         input text on the service side for troubleshooting. If set to False, the Language service logs your
         input text for 48 hours, solely to allow for troubleshooting issues in providing you with
         the service's natural language processing functions. Please see
         Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
         additional details, and Microsoft Responsible AI principles at
         https://www.microsoft.com/ai/responsible-ai.
-    :ivar Optional[str] model_version: The model version to use for the analysis.
-    :ivar Optional[str] domain_filter: An optional string to set the PII domain to include only a
-        subset of the PII entity categories. Possible values include 'phi' or None.
-    :ivar categories_filter: Instead of filtering over all PII entity categories, you can pass in a list of
+    """
+
+    categories_filter: Optional[List[Union[str, PiiEntityCategory]]] = None
+    """Instead of filtering over all PII entity categories, you can pass in a list of
         the specific PII entity categories you want to filter out. For example, if you only want to filter out
         U.S. social security numbers in a document, you can pass in
-        `[PiiEntityCategory.US_SOCIAL_SECURITY_NUMBER]` for this kwarg.
-    :vartype categories_filter: Optional[list[str or ~azure.ai.textanalytics.PiiEntityCategory]]
-    :ivar Optional[str] string_index_type: Specifies the method used to interpret string offsets.
+        `[PiiEntityCategory.US_SOCIAL_SECURITY_NUMBER]` for this kwarg."""
+    domain_filter: Optional[str] = None
+    """An optional string to set the PII domain to include only a
+        subset of the PII entity categories. Possible values include 'phi' or None."""
+    model_version: Optional[str] = None
+    """The model version to use for the analysis."""
+    string_index_type: Optional[str] = None
+    """Specifies the method used to interpret string offsets.
         `UnicodeCodePoint`, the Python encoding, is the default. To override the Python default,
         you can also pass in `Utf16CodeUnit` or `TextElement_v8`. For additional information
-        see https://aka.ms/text-analytics-offsets
-    :ivar Optional[bool] disable_service_logs: Defaults to true, meaning that the Language service will not log your
+        see https://aka.ms/text-analytics-offsets"""
+    disable_service_logs: Optional[bool] = None
+    """Defaults to true, meaning that the Language service will not log your
         input text on the service side for troubleshooting. If set to False, the Language service logs your
         input text for 48 hours, solely to allow for troubleshooting issues in providing you with
         the service's natural language processing functions. Please see
         Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
         additional details, and Microsoft Responsible AI principles at
-        https://www.microsoft.com/ai/responsible-ai.
-    """
+        https://www.microsoft.com/ai/responsible-ai."""
+
+    def __init__(
+        self,
+        *,
+        categories_filter: Optional[List[Union[str, PiiEntityCategory]]] = None,
+        domain_filter: Optional[str] = None,
+        model_version: Optional[str] = None,
+        string_index_type: Optional[str] = None,
+        disable_service_logs: Optional[bool] = None,
+        **kwargs: Any
+    ) -> None:
+        self.model_version = model_version
+        self.domain_filter = domain_filter
+        self.categories_filter = categories_filter
+        self.string_index_type: str = string_index_type if string_index_type is not None else STRING_INDEX_TYPE_DEFAULT
+        self.disable_service_logs = disable_service_logs
 
-    def __init__(self, **kwargs):
-        self.model_version = kwargs.get("model_version", None)
-        self.domain_filter = kwargs.get("domain_filter", None)
-        self.categories_filter = kwargs.get("categories_filter", None)
-        self.string_index_type = kwargs.get("string_index_type", "UnicodeCodePoint")
-        self.disable_service_logs = kwargs.get("disable_service_logs", None)
-
-    def __repr__(self, **kwargs):
-        return (
-            "RecognizePiiEntitiesAction(model_version={}, domain_filter={}, categories_filter={}, "
-            "string_index_type={}, disable_service_logs={}".format(
-                self.model_version,
-                self.domain_filter,
-                self.categories_filter,
-                self.string_index_type,
-                self.disable_service_logs,
-            )[:1024]
+    def __repr__(self) -> str:
+        return (
+            f"RecognizePiiEntitiesAction(model_version={self.model_version}, "
+            f"domain_filter={self.domain_filter}, categories_filter={self.categories_filter}, "
+            f"string_index_type={self.string_index_type}, "
+            f"disable_service_logs={self.disable_service_logs}"[:1024]
         )
 
     def _to_generated(self, api_version, task_id):
         if is_language_api(api_version):
             return _v2022_10_01_preview_models.PiiLROTask(
                 task_name=task_id,
                 parameters=_v2022_10_01_preview_models.PiiTaskParameters(
@@ -2112,35 +2109,41 @@
         logged on the service side for troubleshooting. By default, the Language service logs your
         input text for 48 hours, solely to allow for troubleshooting issues in providing you with
         the service's natural language processing functions. Setting this parameter to true,
         disables input logging and may limit our ability to remediate issues that occur. Please see
         Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
         additional details, and Microsoft Responsible AI principles at
         https://www.microsoft.com/ai/responsible-ai.
-    :ivar Optional[str] model_version: The model version to use for the analysis.
-    :ivar Optional[bool] disable_service_logs: If set to true, you opt-out of having your text input
+    """
+
+    model_version: Optional[str] = None
+    """The model version to use for the analysis."""
+    disable_service_logs: Optional[bool] = None
+    """If set to true, you opt-out of having your text input
         logged on the service side for troubleshooting. By default, the Language service logs your
         input text for 48 hours, solely to allow for troubleshooting issues in providing you with
         the service's natural language processing functions. Setting this parameter to true,
         disables input logging and may limit our ability to remediate issues that occur. Please see
         Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
         additional details, and Microsoft Responsible AI principles at
-        https://www.microsoft.com/ai/responsible-ai.
-    """
+        https://www.microsoft.com/ai/responsible-ai."""
 
-    def __init__(self, **kwargs):
-        self.model_version = kwargs.get("model_version", None)
-        self.disable_service_logs = kwargs.get("disable_service_logs", None)
-
-    def __repr__(self, **kwargs):
-        return (
-            "ExtractKeyPhrasesAction(model_version={}, disable_service_logs={})".format(
-                self.model_version, self.disable_service_logs
-            )[:1024]
-        )
+    def __init__(
+        self,
+        *,
+        model_version: Optional[str] = None,
+        disable_service_logs: Optional[bool] = None,
+        **kwargs: Any
+    ) -> None:
+        self.model_version = model_version
+        self.disable_service_logs = disable_service_logs
+
+    def __repr__(self) -> str:
+        return f"ExtractKeyPhrasesAction(model_version={self.model_version}, " \
+               f"disable_service_logs={self.disable_service_logs})"[:1024]
 
     def _to_generated(self, api_version, task_id):
         if is_language_api(api_version):
             return _v2022_10_01_preview_models.KeyPhraseLROTask(
                 task_name=task_id,
                 parameters=_v2022_10_01_preview_models.KeyPhraseTaskParameters(
                     model_version=self.model_version,
@@ -2174,40 +2177,50 @@
         logged on the service side for troubleshooting. By default, the Language service logs your
         input text for 48 hours, solely to allow for troubleshooting issues in providing you with
         the service's natural language processing functions. Setting this parameter to true,
         disables input logging and may limit our ability to remediate issues that occur. Please see
         Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
         additional details, and Microsoft Responsible AI principles at
         https://www.microsoft.com/ai/responsible-ai.
-    :ivar Optional[str] model_version: The model version to use for the analysis.
-    :ivar Optional[str] string_index_type: Specifies the method used to interpret string offsets.
+    """
+
+    model_version: Optional[str] = None
+    """The model version to use for the analysis."""
+    string_index_type: Optional[str] = None
+    """Specifies the method used to interpret string offsets.
         `UnicodeCodePoint`, the Python encoding, is the default. To override the Python default,
         you can also pass in `Utf16CodeUnit` or `TextElement_v8`. For additional information
-        see https://aka.ms/text-analytics-offsets
-    :ivar Optional[bool] disable_service_logs: If set to true, you opt-out of having your text input
+        see https://aka.ms/text-analytics-offsets"""
+    disable_service_logs: Optional[bool] = None
+    """If set to true, you opt-out of having your text input
         logged on the service side for troubleshooting. By default, the Language service logs your
         input text for 48 hours, solely to allow for troubleshooting issues in providing you with
         the service's natural language processing functions. Setting this parameter to true,
         disables input logging and may limit our ability to remediate issues that occur. Please see
         Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
         additional details, and Microsoft Responsible AI principles at
-        https://www.microsoft.com/ai/responsible-ai.
-    """
+        https://www.microsoft.com/ai/responsible-ai."""
 
-    def __init__(self, **kwargs):
-        self.model_version = kwargs.get("model_version", None)
-        self.string_index_type = kwargs.get("string_index_type", "UnicodeCodePoint")
-        self.disable_service_logs = kwargs.get("disable_service_logs", None)
-
-    def __repr__(self, **kwargs):
-        return (
-            "RecognizeLinkedEntitiesAction(model_version={}, string_index_type={}), "
-            "disable_service_logs={}".format(
-                self.model_version, self.string_index_type, self.disable_service_logs
-            )[:1024]
+    def __init__(
+        self,
+        *,
+        model_version: Optional[str] = None,
+        string_index_type: Optional[str] = None,
+        disable_service_logs: Optional[bool] = None,
+        **kwargs: Any
+    ) -> None:
+        self.model_version = model_version
+        self.string_index_type: str = string_index_type if string_index_type is not None else STRING_INDEX_TYPE_DEFAULT
+        self.disable_service_logs = disable_service_logs
+
+    def __repr__(self) -> str:
+        return (
+            f"RecognizeLinkedEntitiesAction(model_version={self.model_version}, "
+            f"string_index_type={self.string_index_type}), "
+            f"disable_service_logs={self.disable_service_logs}"[:1024]
         )
 
     def _to_generated(self, api_version, task_id):
         if is_language_api(api_version):
             return _v2022_10_01_preview_models.EntityLinkingLROTask(
                 task_name=task_id,
                 parameters=_v2022_10_01_preview_models.EntityLinkingTaskParameters(
@@ -2242,52 +2255,58 @@
         logged on the service side for troubleshooting. By default, the Language service logs your
         input text for 48 hours, solely to allow for troubleshooting issues in providing you with
         the service's natural language processing functions. Setting this parameter to true,
         disables input logging and may limit our ability to remediate issues that occur. Please see
         Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
         additional details, and Microsoft Responsible AI principles at
         https://www.microsoft.com/ai/responsible-ai.
-    :ivar str project_name: This field indicates the project name for the model.
-    :ivar str deployment_name: This field indicates the deployment name for the model.
-    :ivar Optional[str] string_index_type: Specifies the method used to interpret string offsets.
+
+    .. versionadded:: 2022-05-01
+        The *RecognizeCustomEntitiesAction* model.
+    """
+
+    project_name: str
+    """This field indicates the project name for the model."""
+    deployment_name: str
+    """This field indicates the deployment name for the model."""
+    string_index_type: Optional[str] = None
+    """Specifies the method used to interpret string offsets.
         `UnicodeCodePoint`, the Python encoding, is the default. To override the Python default,
         you can also pass in `Utf16CodeUnit` or `TextElement_v8`. For additional information
-        see https://aka.ms/text-analytics-offsets
-    :ivar Optional[bool] disable_service_logs: If set to true, you opt-out of having your text input
+        see https://aka.ms/text-analytics-offsets"""
+    disable_service_logs: Optional[bool] = None
+    """If set to true, you opt-out of having your text input
         logged on the service side for troubleshooting. By default, the Language service logs your
         input text for 48 hours, solely to allow for troubleshooting issues in providing you with
         the service's natural language processing functions. Setting this parameter to true,
         disables input logging and may limit our ability to remediate issues that occur. Please see
         Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
         additional details, and Microsoft Responsible AI principles at
-        https://www.microsoft.com/ai/responsible-ai.
-
-    .. versionadded:: 2022-05-01
-        The *RecognizeCustomEntitiesAction* model.
-    """
+        https://www.microsoft.com/ai/responsible-ai."""
 
     def __init__(
         self,
         project_name: str,
         deployment_name: str,
-        **kwargs
+        *,
+        string_index_type: Optional[str] = None,
+        disable_service_logs: Optional[bool] = None,
+        **kwargs: Any
     ) -> None:
         self.project_name = project_name
         self.deployment_name = deployment_name
-        self.disable_service_logs = kwargs.get('disable_service_logs', None)
-        self.string_index_type = kwargs.get('string_index_type', "UnicodeCodePoint")
+        self.disable_service_logs = disable_service_logs
+        self.string_index_type: str = string_index_type if string_index_type is not None else STRING_INDEX_TYPE_DEFAULT
 
-    def __repr__(self):
-        return "RecognizeCustomEntitiesAction(project_name={}, deployment_name={}, disable_service_logs={}, " \
-               "string_index_type={})".format(
-            self.project_name,
-            self.deployment_name,
-            self.disable_service_logs,
-            self.string_index_type,
-        )[:1024]
+    def __repr__(self) -> str:
+        return (
+            f"RecognizeCustomEntitiesAction(project_name={self.project_name}, "
+            f"deployment_name={self.deployment_name}, disable_service_logs={self.disable_service_logs}, "
+            f"string_index_type={self.string_index_type})"[:1024]
+        )
 
     def _to_generated(self, api_version, task_id):  # pylint: disable=unused-argument
         return _v2022_10_01_preview_models.CustomEntitiesLROTask(
             task_name=task_id,
             parameters=_v2022_10_01_preview_models.CustomEntitiesTaskParameters(
                 project_name=self.project_name,
                 deployment_name=self.deployment_name,
@@ -2297,57 +2316,54 @@
         )
 
 
 class RecognizeCustomEntitiesResult(DictMixin):
     """RecognizeCustomEntitiesResult is a result object which contains
     the custom recognized entities from a particular document.
 
-    :ivar str id: Unique, non-empty document identifier that matches the
-        document id that was passed in with the request. If not specified
-        in the request, an id is assigned for the document.
-    :ivar entities: Recognized custom entities in the document.
-    :vartype entities:
-        list[~azure.ai.textanalytics.CategorizedEntity]
-    :ivar warnings: Warnings encountered while processing document.
-    :vartype warnings: list[~azure.ai.textanalytics.TextAnalyticsWarning]
-    :ivar statistics: If `show_stats=True` was specified in the request this
-        field will contain information about the document payload.
-    :vartype statistics: Optional[~azure.ai.textanalytics.TextDocumentStatistics]
-    :ivar detected_language: If 'language' is set to 'auto' for the document in the request this
-        field will contain the DetectedLanguage for the document.
-    :vartype detected_language: Optional[~azure.ai.textanalytics.DetectedLanguage]
-    :ivar bool is_error: Boolean check for error item when iterating over list of
-        results. Always False for an instance of a RecognizeCustomEntitiesResult.
-    :ivar str kind: The text analysis kind - "CustomEntityRecognition".
-
     .. versionadded:: 2022-10-01-preview
         The *detected_language* property.
     """
 
-    def __init__(self, **kwargs):
+    id: str  # pylint: disable=redefined-builtin
+    """Unique, non-empty document identifier that matches the
+        document id that was passed in with the request. If not specified
+        in the request, an id is assigned for the document."""
+    entities: List[CategorizedEntity]
+    """Recognized custom entities in the document."""
+    warnings: List[TextAnalyticsWarning]
+    """Warnings encountered while processing document."""
+    statistics: Optional[TextDocumentStatistics] = None
+    """If `show_stats=True` was specified in the request this
+        field will contain information about the document payload."""
+    detected_language: Optional[DetectedLanguage] = None
+    """If 'language' is set to 'auto' for the document in the request this
+        field will contain the DetectedLanguage for the document."""
+    is_error: Literal[False] = False
+    """Boolean check for error item when iterating over list of
+        results. Always False for an instance of a RecognizeCustomEntitiesResult."""
+    kind: Literal["CustomEntityRecognition"] = "CustomEntityRecognition"
+    """The text analysis kind - "CustomEntityRecognition"."""
+
+    def __init__(self, **kwargs: Any) -> None:
         self.id = kwargs.get("id", None)
         self.entities = kwargs.get("entities", None)
         self.warnings = kwargs.get("warnings", [])
         self.statistics = kwargs.get("statistics", None)
         self.detected_language = kwargs.get("detected_language", None)
         self.is_error: Literal[False] = False
         self.kind: Literal["CustomEntityRecognition"] = "CustomEntityRecognition"
 
-    def __repr__(self):
-        return "RecognizeCustomEntitiesResult(id={}, entities={}, warnings={}, statistics={}, " \
-               "detected_language={}, is_error={})".format(
-                self.id,
-                repr(self.entities),
-                repr(self.warnings),
-                repr(self.statistics),
-                repr(self.detected_language),
-                self.is_error,
-            )[
-                :1024
-            ]
+    def __repr__(self) -> str:
+        return (
+            f"RecognizeCustomEntitiesResult(id={self.id}, entities={repr(self.entities)}, "
+            f"warnings={repr(self.warnings)}, statistics={repr(self.statistics)}, "
+            f"detected_language={repr(self.detected_language)}, is_error={self.is_error},"
+            f" kind={self.kind})"[:1024]
+        )
 
     @classmethod
     def _from_generated(cls, result):
         return cls(
             id=result.id,
             entities=[
                 CategorizedEntity._from_generated(e)  # pylint: disable=protected-access
@@ -2379,46 +2395,50 @@
         logged on the service side for troubleshooting. By default, the Language service logs your
         input text for 48 hours, solely to allow for troubleshooting issues in providing you with
         the service's natural language processing functions. Setting this parameter to true,
         disables input logging and may limit our ability to remediate issues that occur. Please see
         Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
         additional details, and Microsoft Responsible AI principles at
         https://www.microsoft.com/ai/responsible-ai.
-    :ivar str project_name: This field indicates the project name for the model.
-    :ivar str deployment_name: This field indicates the deployment name for the model.
-    :ivar Optional[bool] disable_service_logs: If set to true, you opt-out of having your text input
+
+    .. versionadded:: 2022-05-01
+        The *MultiLabelClassifyAction* model.
+    """
+
+    project_name: str
+    """This field indicates the project name for the model."""
+    deployment_name: str
+    """This field indicates the deployment name for the model."""
+    disable_service_logs: Optional[bool] = None
+    """If set to true, you opt-out of having your text input
         logged on the service side for troubleshooting. By default, the Language service logs your
         input text for 48 hours, solely to allow for troubleshooting issues in providing you with
         the service's natural language processing functions. Setting this parameter to true,
         disables input logging and may limit our ability to remediate issues that occur. Please see
         Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
         additional details, and Microsoft Responsible AI principles at
-        https://www.microsoft.com/ai/responsible-ai.
-
-    .. versionadded:: 2022-05-01
-        The *MultiLabelClassifyAction* model.
-    """
+        https://www.microsoft.com/ai/responsible-ai."""
 
     def __init__(
         self,
         project_name: str,
         deployment_name: str,
-        **kwargs
+        *,
+        disable_service_logs: Optional[bool] = None,
+        **kwargs: Any
     ) -> None:
         self.project_name = project_name
         self.deployment_name = deployment_name
-        self.disable_service_logs = kwargs.get('disable_service_logs', None)
+        self.disable_service_logs = disable_service_logs
 
-    def __repr__(self):
-        return "MultiLabelClassifyAction(project_name={}, deployment_name={}, " \
-               "disable_service_logs={})".format(
-            self.project_name,
-            self.deployment_name,
-            self.disable_service_logs,
-        )[:1024]
+    def __repr__(self) -> str:
+        return (
+            f"MultiLabelClassifyAction(project_name={self.project_name}, deployment_name={self.deployment_name}, "
+            f"disable_service_logs={self.disable_service_logs})"[:1024]
+        )
 
     def _to_generated(self, api_version, task_id):  # pylint: disable=unused-argument
         return _v2022_10_01_preview_models.CustomMultiLabelClassificationLROTask(
             task_name=task_id,
             parameters=_v2022_10_01_preview_models.CustomMultiLabelClassificationTaskParameters(
                 project_name=self.project_name,
                 deployment_name=self.deployment_name,
@@ -2427,57 +2447,52 @@
         )
 
 
 class ClassifyDocumentResult(DictMixin):
     """ClassifyDocumentResult is a result object which contains
     the classifications for a particular document.
 
-    :ivar str id: Unique, non-empty document identifier.
-    :ivar classifications: Recognized classification results in the document.
-    :vartype classifications: list[~azure.ai.textanalytics.ClassificationCategory]
-    :ivar warnings: Warnings encountered while processing document.
-    :vartype warnings: list[~azure.ai.textanalytics.TextAnalyticsWarning]
-    :ivar statistics: If `show_stats=True` was specified in the request this
-        field will contain information about the document payload.
-    :vartype statistics: Optional[~azure.ai.textanalytics.TextDocumentStatistics]
-    :ivar detected_language: If 'language' is set to 'auto' for the document in the request this
-        field will contain the DetectedLanguage for the document.
-    :vartype detected_language: Optional[~azure.ai.textanalytics.DetectedLanguage]
-    :ivar bool is_error: Boolean check for error item when iterating over list of
-        results. Always False for an instance of a ClassifyDocumentResult.
-    :ivar str kind: The text analysis kind - "CustomDocumentClassification".
-
     .. versionadded:: 2022-10-01-preview
         The *detected_language* property.
     """
 
-    def __init__(
-        self,
-        **kwargs
-    ):
+    id: str  # pylint: disable=redefined-builtin
+    """Unique, non-empty document identifier."""
+    classifications: List["ClassificationCategory"]
+    """Recognized classification results in the document."""
+    warnings: List[TextAnalyticsWarning]
+    """Warnings encountered while processing document."""
+    statistics: Optional[TextDocumentStatistics] = None
+    """If `show_stats=True` was specified in the request this
+        field will contain information about the document payload."""
+    detected_language: Optional[DetectedLanguage] = None
+    """If 'language' is set to 'auto' for the document in the request this
+        field will contain the DetectedLanguage for the document."""
+    is_error: Literal[False] = False
+    """Boolean check for error item when iterating over list of
+        results. Always False for an instance of a ClassifyDocumentResult."""
+    kind: Literal["CustomDocumentClassification"] = "CustomDocumentClassification"
+    """The text analysis kind - "CustomDocumentClassification"."""
+
+    def __init__(self, **kwargs: Any) -> None:
         self.id = kwargs.get('id', None)
         self.classifications = kwargs.get('classifications', None)
         self.warnings = kwargs.get('warnings', [])
         self.statistics = kwargs.get('statistics', None)
         self.detected_language = kwargs.get('detected_language', None)
         self.is_error: Literal[False] = False
         self.kind: Literal["CustomDocumentClassification"] = "CustomDocumentClassification"
 
-    def __repr__(self):
-        return "ClassifyDocumentResult(id={}, classifications={}, warnings={}, statistics={}, detected_language={} " \
-               "is_error={})".format(
-                self.id,
-                repr(self.classifications),
-                repr(self.warnings),
-                repr(self.statistics),
-                repr(self.detected_language),
-                self.is_error,
-            )[
-                :1024
-            ]
+    def __repr__(self) -> str:
+        return (
+            f"ClassifyDocumentResult(id={self.id}, classifications={repr(self.classifications)}, "
+            f"warnings={repr(self.warnings)}, statistics={repr(self.statistics)}, "
+            f"detected_language={repr(self.detected_language)} "
+            f"is_error={self.is_error}, kind={self.kind})"[:1024]
+        )
 
     @classmethod
     def _from_generated(cls, result):
         return cls(
             id=result.id,
             classifications=[
                 ClassificationCategory._from_generated(e)  # pylint: disable=protected-access
@@ -2509,86 +2524,81 @@
         logged on the service side for troubleshooting. By default, the Language service logs your
         input text for 48 hours, solely to allow for troubleshooting issues in providing you with
         the service's natural language processing functions. Setting this parameter to true,
         disables input logging and may limit our ability to remediate issues that occur. Please see
         Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
         additional details, and Microsoft Responsible AI principles at
         https://www.microsoft.com/ai/responsible-ai.
-    :ivar str project_name: This field indicates the project name for the model.
-    :ivar str deployment_name: This field indicates the deployment name for the model.
-    :ivar Optional[bool] disable_service_logs: If set to true, you opt-out of having your text input
+
+    .. versionadded:: 2022-05-01
+        The *SingleLabelClassifyAction* model.
+    """
+
+    project_name: str
+    """This field indicates the project name for the model."""
+    deployment_name: str
+    """This field indicates the deployment name for the model."""
+    disable_service_logs: Optional[bool] = None
+    """If set to true, you opt-out of having your text input
         logged on the service side for troubleshooting. By default, the Language service logs your
         input text for 48 hours, solely to allow for troubleshooting issues in providing you with
         the service's natural language processing functions. Setting this parameter to true,
         disables input logging and may limit our ability to remediate issues that occur. Please see
         Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
         additional details, and Microsoft Responsible AI principles at
-        https://www.microsoft.com/ai/responsible-ai.
-
-    .. versionadded:: 2022-05-01
-        The *SingleLabelClassifyAction* model.
-    """
+        https://www.microsoft.com/ai/responsible-ai."""
 
     def __init__(
         self,
         project_name: str,
         deployment_name: str,
-        **kwargs
+        *,
+        disable_service_logs: Optional[bool] = None,
+        **kwargs: Any
     ) -> None:
         self.project_name = project_name
         self.deployment_name = deployment_name
-        self.disable_service_logs = kwargs.get('disable_service_logs', None)
+        self.disable_service_logs = disable_service_logs
 
-    def __repr__(self):
-        return "SingleLabelClassifyAction(project_name={}, deployment_name={}, " \
-               "disable_service_logs={})".format(
-            self.project_name,
-            self.deployment_name,
-            self.disable_service_logs,
-        )[:1024]
+    def __repr__(self) -> str:
+        return (
+            f"SingleLabelClassifyAction(project_name={self.project_name}, deployment_name={self.deployment_name}, "
+            f"disable_service_logs={self.disable_service_logs})"[:1024]
+        )
 
     def _to_generated(self, api_version, task_id):  # pylint: disable=unused-argument
         return _v2022_10_01_preview_models.CustomSingleLabelClassificationLROTask(
             task_name=task_id,
             parameters=_v2022_10_01_preview_models.CustomSingleLabelClassificationTaskParameters(
                 project_name=self.project_name,
                 deployment_name=self.deployment_name,
                 logging_opt_out=self.disable_service_logs,
             )
         )
 
 
 class ClassificationCategory(DictMixin):
     """ClassificationCategory represents a classification of the input document.
-
-    :ivar str category: Classification category for the document.
-    :ivar float confidence_score: Confidence score between 0 and 1 of the recognized classification.
     """
 
-    def __init__(
-        self,
-        **kwargs
-    ):
+    category: str
+    """Classification category for the document."""
+    confidence_score: float
+    """Confidence score between 0 and 1 of the recognized classification."""
+
+    def __init__(self, **kwargs: Any) -> None:
         self.category = kwargs.get('category', None)
         self.confidence_score = kwargs.get('confidence_score', None)
 
-    def __repr__(self):
-        return "ClassificationCategory(category={}, confidence_score={})".format(
-            self.category,
-            self.confidence_score,
-        )[:1024]
+    def __repr__(self) -> str:
+        return f"ClassificationCategory(category={self.category}, " \
+               f"confidence_score={self.confidence_score})"[:1024]
 
     @classmethod
     def _from_generated(cls, result):
-        # FIXME: https://github.com/Azure/azure-sdk-for-python/issues/27089
-        if isinstance(result, dict):
-            return cls(
-                category=result["category"],
-                confidence_score=result["confidenceScore"]
-            )
         return cls(
             category=result.category,
             confidence_score=result.confidence_score
         )
 
 
 class AnalyzeHealthcareEntitiesAction(DictMixin):
@@ -2616,59 +2626,68 @@
         on the result object. For additional information see https://www.hl7.org/fhir/overview.html.
         The only acceptable values to pass in are None and "4.0.1". The default value is None.
     :keyword document_type: Document type that can be provided as input for Fhir Documents. Expect to
         have fhir_version provided when used. Behavior of using None enum is the same as not using the
         document_type parameter. Known values are: "None", "ClinicalTrial", "DischargeSummary",
         "ProgressNote", "HistoryAndPhysical", "Consult", "Imaging", "Pathology", and "ProcedureNote".
     :paramtype document_type: Optional[str or ~azure.ai.textanalytics.HealthcareDocumentType]
-    :ivar Optional[str] model_version: The model version to use for the analysis.
-    :ivar Optional[str] string_index_type: Specifies the method used to interpret string offsets.
+
+    .. versionadded:: 2022-05-01
+        The *AnalyzeHealthcareEntitiesAction* model.
+    .. versionadded:: 2022-10-01-preview
+        The *fhir_version* and *document_type* keyword arguments.
+    """
+
+    model_version: Optional[str] = None
+    """The model version to use for the analysis."""
+    string_index_type: Optional[str] = None
+    """Specifies the method used to interpret string offsets.
         `UnicodeCodePoint`, the Python encoding, is the default. To override the Python default,
         you can also pass in `Utf16CodeUnit` or `TextElement_v8`. For additional information
-        see https://aka.ms/text-analytics-offsets
-    :ivar Optional[bool] disable_service_logs: If set to true, you opt-out of having your text input
+        see https://aka.ms/text-analytics-offsets"""
+    disable_service_logs: Optional[bool] = None
+    """If set to true, you opt-out of having your text input
         logged on the service side for troubleshooting. By default, the Language service logs your
         input text for 48 hours, solely to allow for troubleshooting issues in providing you with
         the service's natural language processing functions. Setting this parameter to true,
         disables input logging and may limit our ability to remediate issues that occur. Please see
         Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
         additional details, and Microsoft Responsible AI principles at
-        https://www.microsoft.com/ai/responsible-ai.
-    :ivar Optional[str] fhir_version: The FHIR Spec version that the result will use to format the fhir_bundle
+        https://www.microsoft.com/ai/responsible-ai."""
+    fhir_version: Optional[str] = None
+    """The FHIR Spec version that the result will use to format the fhir_bundle
         on the result object. For additional information see https://www.hl7.org/fhir/overview.html.
-        The only acceptable values to pass in are None and "4.0.1". The default value is None.
-    :ivar document_type: Document type that can be provided as input for Fhir Documents. Expect to
+        The only acceptable values to pass in are None and "4.0.1". The default value is None."""
+    document_type: Optional[Union[str, HealthcareDocumentType]] = None
+    """Document type that can be provided as input for Fhir Documents. Expect to
         have fhir_version provided when used. Behavior of using None enum is the same as not using the
-        document_type parameter. Known values are: "None", "ClinicalTrial", "DischargeSummary",
-        "ProgressNote", "HistoryAndPhysical", "Consult", "Imaging", "Pathology", and "ProcedureNote".
-    :vartype document_type: Optional[str or ~azure.ai.textanalytics.HealthcareDocumentType]
+        document_type parameter. Known values are "None", "ClinicalTrial", "DischargeSummary",
+        "ProgressNote", "HistoryAndPhysical", "Consult", "Imaging", "Pathology", and "ProcedureNote"."""
 
-    .. versionadded:: 2022-05-01
-        The *AnalyzeHealthcareEntitiesAction* model.
-    .. versionadded:: 2022-10-01-preview
-        The *fhir_version* and *document_type* keyword arguments.
-    """
+    def __init__(
+        self,
+        *,
+        model_version: Optional[str] = None,
+        string_index_type: Optional[str] = None,
+        disable_service_logs: Optional[bool] = None,
+        fhir_version: Optional[str] = None,
+        document_type: Optional[Union[str, HealthcareDocumentType]] = None,
+        **kwargs: Any
+    ) -> None:
+        self.model_version = model_version
+        self.string_index_type: str = string_index_type if string_index_type is not None else STRING_INDEX_TYPE_DEFAULT
+        self.disable_service_logs = disable_service_logs
+        self.fhir_version = fhir_version
+        self.document_type = document_type
 
-    def __init__(self, **kwargs):
-        self.model_version = kwargs.get("model_version", None)
-        self.string_index_type = kwargs.get("string_index_type", "UnicodeCodePoint")
-        self.disable_service_logs = kwargs.get("disable_service_logs", None)
-        self.fhir_version = kwargs.get("fhir_version", None)
-        self.document_type = kwargs.get("document_type", None)
-
-    def __repr__(self):
-        return (
-            "AnalyzeHealthcareEntitiesAction(model_version={}, string_index_type={}, disable_service_logs={}, "
-            "fhir_version={}, document_type={})".format(
-                self.model_version,
-                self.string_index_type,
-                self.disable_service_logs,
-                self.fhir_version,
-                self.document_type
-            )[:1024]
+    def __repr__(self) -> str:
+        return (
+            f"AnalyzeHealthcareEntitiesAction(model_version={self.model_version}, "
+            f"string_index_type={self.string_index_type}, disable_service_logs={self.disable_service_logs}, "
+            f"fhir_version={self.fhir_version}, document_type={self.document_type})"[:1024]
         )
 
     def _to_generated(self, api_version, task_id):  # pylint: disable=unused-argument
         return _v2022_10_01_preview_models.HealthcareLROTask(
             task_name=task_id,
             parameters=_v2022_10_01_preview_models.HealthcareTaskParameters(
                 model_version=self.model_version,
@@ -2696,51 +2715,61 @@
         the service's natural language processing functions. Setting this parameter to true,
         disables input logging and may limit our ability to remediate issues that occur. Please see
         Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
         additional details, and Microsoft Responsible AI principles at
         https://www.microsoft.com/ai/responsible-ai.
     :keyword Optional[int] max_sentence_count: Maximum number of sentences to return. Defaults to 3.
     :keyword Optional[str] order_by:  Possible values include: "Offset", "Rank". Default value: "Offset".
-    :ivar Optional[str] model_version: The model version to use for the analysis.
-    :ivar Optional[str] string_index_type: Specifies the method used to interpret string offsets.
+
+    .. versionadded:: 2022-10-01-preview
+        The *ExtractSummaryAction* model.
+    """
+
+    model_version: Optional[str] = None
+    """The model version to use for the analysis."""
+    string_index_type: Optional[str] = None
+    """Specifies the method used to interpret string offsets.
         `UnicodeCodePoint`, the Python encoding, is the default. To override the Python default,
         you can also pass in `Utf16CodeUnit` or `TextElement_v8`. For additional information
-        see https://aka.ms/text-analytics-offsets
-    :ivar Optional[bool] disable_service_logs: If set to true, you opt-out of having your text input
+        see https://aka.ms/text-analytics-offsets"""
+    disable_service_logs: Optional[bool] = None
+    """If set to true, you opt-out of having your text input
         logged on the service side for troubleshooting. By default, the Language service logs your
         input text for 48 hours, solely to allow for troubleshooting issues in providing you with
         the service's natural language processing functions. Setting this parameter to true,
         disables input logging and may limit our ability to remediate issues that occur. Please see
         Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
         additional details, and Microsoft Responsible AI principles at
-        https://www.microsoft.com/ai/responsible-ai.
-    :ivar Optional[int] max_sentence_count: Number of sentences to return. Defaults to 3.
-    :ivar Optional[str] order_by:  Possible values include: "Offset", "Rank". Default value: "Offset".
+        https://www.microsoft.com/ai/responsible-ai."""
+    max_sentence_count: Optional[int] = None
+    """Number of sentences to return. Defaults to 3."""
+    order_by: Optional[str] = None
+    """Possible values include "Offset", "Rank". Default value is "Offset"."""
 
-    .. versionadded:: 2022-10-01-preview
-        The *ExtractSummaryAction* model.
-    """
+    def __init__(
+        self,
+        *,
+        model_version: Optional[str] = None,
+        string_index_type: Optional[str] = None,
+        disable_service_logs: Optional[bool] = None,
+        max_sentence_count: Optional[int] = None,
+        order_by: Optional[str] = None,
+        **kwargs: Any
+    ) -> None:
+        self.model_version = model_version
+        self.string_index_type: str = string_index_type if string_index_type is not None else STRING_INDEX_TYPE_DEFAULT
+        self.disable_service_logs = disable_service_logs
+        self.max_sentence_count = max_sentence_count
+        self.order_by = order_by
 
-    def __init__(self, **kwargs):
-        self.model_version = kwargs.get("model_version", None)
-        self.string_index_type = kwargs.get("string_index_type", "UnicodeCodePoint")
-        self.disable_service_logs = kwargs.get("disable_service_logs", None)
-        self.max_sentence_count = kwargs.get("max_sentence_count", None)
-        self.order_by = kwargs.get("order_by", None)
-
-    def __repr__(self):
-        return (
-            "ExtractSummaryAction(model_version={}, string_index_type={}, disable_service_logs={}, "
-            "max_sentence_count={}, order_by={})".format(
-                self.model_version,
-                self.string_index_type,
-                self.disable_service_logs,
-                self.max_sentence_count,
-                self.order_by,
-            )[:1024]
+    def __repr__(self) -> str:
+        return (
+            f"ExtractSummaryAction(model_version={self.model_version}, "
+            f"string_index_type={self.string_index_type}, disable_service_logs={self.disable_service_logs}, "
+            f"max_sentence_count={self.max_sentence_count}, order_by={self.order_by})"[:1024]
         )
 
     def _to_generated(self, api_version, task_id):  # pylint: disable=unused-argument
         return _v2022_10_01_preview_models.ExtractiveSummarizationLROTask(  # pylint: disable=no-member
             task_name=task_id,
             parameters=_v2022_10_01_preview_models.ExtractiveSummarizationTaskParameters(  # pylint: disable=no-member
                 model_version=self.model_version,
@@ -2752,54 +2781,52 @@
         )
 
 
 class ExtractSummaryResult(DictMixin):
     """ExtractSummaryResult is a result object which contains
     the extractive text summarization from a particular document.
 
-    :ivar str id: Unique, non-empty document identifier.
-    :ivar sentences: A ranked list of sentences representing the extracted summary.
-    :vartype sentences: list[~azure.ai.textanalytics.SummarySentence]
-    :ivar warnings: Warnings encountered while processing document.
-    :vartype warnings: list[~azure.ai.textanalytics.TextAnalyticsWarning]
-    :ivar statistics: If `show_stats=True` was specified in the request this
-        field will contain information about the document payload.
-    :vartype statistics: Optional[~azure.ai.textanalytics.TextDocumentStatistics]
-    :ivar detected_language: If 'language' is set to 'auto' for the document in the request this
-        field will contain the DetectedLanguage for the document.
-    :vartype detected_language: Optional[~azure.ai.textanalytics.DetectedLanguage]
-    :ivar bool is_error: Boolean check for error item when iterating over list of
-        results. Always False for an instance of an ExtractSummaryResult.
-    :ivar str kind: The text analysis kind - "ExtractiveSummarization".
-
     .. versionadded:: 2022-10-01-preview
         The *ExtractSummaryResult* model.
     """
 
-    def __init__(self, **kwargs):
+    id: str  # pylint: disable=redefined-builtin
+    """Unique, non-empty document identifier."""
+    sentences: List["SummarySentence"]
+    """A ranked list of sentences representing the extracted summary."""
+    warnings: List[TextAnalyticsWarning]
+    """Warnings encountered while processing document."""
+    statistics: Optional[TextDocumentStatistics] = None
+    """If `show_stats=True` was specified in the request this
+        field will contain information about the document payload."""
+    detected_language: Optional[DetectedLanguage] = None
+    """If 'language' is set to 'auto' for the document in the request this
+        field will contain the DetectedLanguage for the document."""
+    is_error: Literal[False] = False
+    """Boolean check for error item when iterating over list of
+        results. Always False for an instance of an ExtractSummaryResult."""
+    kind: Literal["ExtractiveSummarization"] = "ExtractiveSummarization"
+    """The text analysis kind - "ExtractiveSummarization"."""
+
+    def __init__(self, **kwargs: Any) -> None:
         self.id = kwargs.get("id", None)
         self.sentences = kwargs.get("sentences", None)
         self.warnings = kwargs.get("warnings", None)
         self.statistics = kwargs.get("statistics", None)
         self.detected_language = kwargs.get("detected_language", None)
         self.is_error: Literal[False] = False
         self.kind: Literal["ExtractiveSummarization"] = "ExtractiveSummarization"
 
-    def __repr__(self):
-        return "ExtractSummaryResult(id={}, sentences={}, warnings={}, statistics={}, detected_language={}," \
-               " is_error={})".format(
-                self.id,
-                repr(self.sentences),
-                repr(self.warnings),
-                repr(self.statistics),
-                repr(self.detected_language),
-                self.is_error,
-            )[
-            :1024
-        ]
+    def __repr__(self) -> str:
+        return (
+            f"ExtractSummaryResult(id={self.id}, sentences={repr(self.sentences)}, "
+            f"warnings={repr(self.warnings)}, statistics={repr(self.statistics)}, "
+            f"detected_language={repr(self.detected_language)},"
+            f" is_error={self.is_error}, kind={self.kind})"[:1024]
+        )
 
     @classmethod
     def _from_generated(cls, summary):
         return cls(
             id=summary.id,
             sentences=[
                 SummarySentence._from_generated(  # pylint: disable=protected-access
@@ -2814,106 +2841,101 @@
                 for w in summary.warnings
             ],
             statistics=TextDocumentStatistics._from_generated(  # pylint: disable=protected-access
                 summary.statistics
             ),
             detected_language=DetectedLanguage._from_generated(  # pylint: disable=protected-access
                 summary.detected_language
-            )  if hasattr(summary, "detected_language") and summary.detected_language else None
+            ) if hasattr(summary, "detected_language") and summary.detected_language else None
         )
 
 
 class SummarySentence(DictMixin):
     """Represents a single sentence from the extractive text summarization.
 
-    :ivar str text: The extracted sentence text.
-    :ivar float rank_score: A float value representing the relevance of the sentence within
-        the summary. Higher values indicate higher importance.
-    :ivar int offset: The sentence offset from the start of the document.
-        The value depends on the value of the `string_index_type` parameter
-        set in the original request, which is UnicodeCodePoint by default.
-    :ivar int length: The length of the sentence. This value depends on the value of the
-        `string_index_type` parameter set in the original request, which is UnicodeCodePoint
-        by default.
-
     .. versionadded:: 2022-10-01-preview
         The *SummarySentence* model.
     """
 
-    def __init__(self, **kwargs):
+    text: str
+    """The extracted sentence text."""
+    rank_score: float
+    """A float value representing the relevance of the sentence within
+        the summary. Higher values indicate higher importance."""
+    offset: int
+    """The sentence offset from the start of the document.
+        The value depends on the value of the `string_index_type` parameter
+        set in the original request, which is UnicodeCodePoint by default."""
+    length: int
+    """The length of the sentence. This value depends on the value of the
+        `string_index_type` parameter set in the original request, which is UnicodeCodePoint
+        by default."""
+
+    def __init__(self, **kwargs: Any) -> None:
         self.text = kwargs.get("text", None)
         self.rank_score = kwargs.get("rank_score", None)
         self.offset = kwargs.get("offset", None)
         self.length = kwargs.get("length", None)
 
-    def __repr__(self):
-        return "SummarySentence(text={}, rank_score={}, offset={}, length={})".format(
-            self.text,
-            self.rank_score,
-            self.offset,
-            self.length,
-        )[:1024]
+    def __repr__(self) -> str:
+        return f"SummarySentence(text={self.text}, rank_score={self.rank_score}, " \
+               f"offset={self.offset}, length={self.length})"[:1024]
 
     @classmethod
     def _from_generated(cls, sentence):
         return cls(
             text=sentence.text,
             rank_score=sentence.rank_score,
             offset=sentence.offset,
             length=sentence.length,
         )
 
 
-class AbstractSummaryResult(DictMixin):
-    """AbstractSummaryResult is a result object which contains
+class AbstractiveSummaryResult(DictMixin):
+    """AbstractiveSummaryResult is a result object which contains
     the summary generated for a particular document.
 
-    :ivar id: Unique, non-empty document identifier. Required.
-    :vartype id: str
-    :ivar detected_language: If 'language' is set to 'auto' for the document in the request this
-        field will contain the DetectedLanguage for the document.
-    :vartype detected_language: Optional[~azure.ai.textanalytics.DetectedLanguage]
-    :ivar warnings: Warnings encountered while processing document. Results will still be returned
-        if there are warnings, but they may not be fully accurate.
-    :vartype warnings: list[~azure.ai.textanalytics.TextAnalyticsWarning]
-    :ivar statistics: If `show_stats=True` was specified in the request this
-        field will contain information about the document payload.
-    :vartype statistics:
-        Optional[~azure.ai.textanalytics.TextDocumentStatistics]
-    :ivar summaries: A list of abstractive summaries. Required.
-    :vartype summaries: list[~azure.ai.textanalytics.AbstractiveSummary]
-    :ivar bool is_error: Boolean check for error item when iterating over list of
-        results. Always False for an instance of a AbstractSummaryResult.
-    :ivar str kind: The text analysis kind - "AbstractiveSummarization".
-
     .. versionadded:: 2022-10-01-preview
-        The *AbstractSummaryResult* model.
+        The *AbstractiveSummaryResult* model.
     """
 
-    def __init__(self, **kwargs):
+    id: str  # pylint: disable=redefined-builtin
+    """Unique, non-empty document identifier. Required."""
+    summaries: List["AbstractiveSummary"]
+    """A list of abstractive summaries. Required."""
+    warnings: List[TextAnalyticsWarning]
+    """Warnings encountered while processing document. Results will still be returned
+        if there are warnings, but they may not be fully accurate."""
+    detected_language: Optional[DetectedLanguage] = None
+    """If 'language' is set to 'auto' for the document in the request this
+        field will contain the DetectedLanguage for the document."""
+    statistics: Optional[TextDocumentStatistics] = None
+    """If `show_stats=True` was specified in the request this
+        field will contain information about the document payload."""
+    is_error: Literal[False] = False
+    """Boolean check for error item when iterating over list of
+        results. Always False for an instance of a AbstractiveSummaryResult."""
+    kind: Literal["AbstractiveSummarization"] = "AbstractiveSummarization"
+    """The text analysis kind - "AbstractiveSummarization"."""
+
+    def __init__(self, **kwargs: Any) -> None:
         self.id = kwargs.get("id", None)
         self.detected_language = kwargs.get("detected_language", None)
         self.warnings = kwargs.get("warnings", None)
         self.statistics = kwargs.get("statistics", None)
         self.summaries = kwargs.get("summaries", None)
         self.is_error: Literal[False] = False
         self.kind: Literal["AbstractiveSummarization"] = "AbstractiveSummarization"
 
-    def __repr__(self):
-        return "AbstractSummaryResult(id={}, detected_language={}, warnings={}, statistics={}, " \
-               "summaries={}, is_error={})".format(
-                self.id,
-                repr(self.detected_language),
-                repr(self.warnings),
-                repr(self.statistics),
-                repr(self.summaries),
-                self.is_error,
-            )[
-                :1024
-            ]
+    def __repr__(self) -> str:
+        return (
+            f"AbstractiveSummaryResult(id={self.id}, detected_language={repr(self.detected_language)}, "
+            f"warnings={repr(self.warnings)}, statistics={repr(self.statistics)}, "
+            f"summaries={repr(self.summaries)}, is_error={self.is_error}, kind={self.kind})"[:1024]
+        )
 
     @classmethod
     def _from_generated(cls, result):
         return cls(
             id=result.id,
             detected_language=DetectedLanguage._from_generated(  # pylint: disable=protected-access
                 result.detected_language
@@ -2933,82 +2955,72 @@
             ],
         )
 
 
 class AbstractiveSummary(DictMixin):
     """An object representing a single summary with context for given document.
 
-    :ivar text: The text of the summary. Required.
-    :vartype text: str
-    :ivar contexts: The context list of the summary.
-    :vartype contexts: Optional[list[~azure.ai.textanalytics.SummaryContext]]
-
     .. versionadded:: 2022-10-01-preview
         The *AbstractiveSummary* model.
     """
 
-    def __init__(self, **kwargs):
+    text: str
+    """The text of the summary. Required."""
+    contexts: List["SummaryContext"]
+    """The context list of the summary."""
+
+    def __init__(self, **kwargs: Any) -> None:
         self.text = kwargs.get("text", None)
         self.contexts = kwargs.get("contexts", None)
 
-    def __repr__(self):
-        return "AbstractiveSummary(text={}, contexts={})".format(
-                self.text,
-                repr(self.contexts),
-            )[
-                :1024
-            ]
+    def __repr__(self) -> str:
+        return f"AbstractiveSummary(text={self.text}, contexts={repr(self.contexts)})"[:1024]
 
     @classmethod
     def _from_generated(cls, result):
         return cls(
             text=result.text,
             contexts=[
                 SummaryContext._from_generated(context)  # pylint: disable=protected-access
                 for context in result.contexts
-            ] if result.contexts else None
+            ] if result.contexts else []
         )
 
 
 class SummaryContext(DictMixin):
     """The context of the summary.
 
-    :ivar offset: Start position for the context. Use of different 'stringIndexType' values can
-     affect the offset returned. Required.
-    :vartype offset: int
-    :ivar length: The length of the context. Use of different 'stringIndexType' values can affect
-     the length returned. Required.
-    :vartype length: int
-
     .. versionadded:: 2022-10-01-preview
         The *SummaryContext* model.
     """
 
-    def __init__(self, **kwargs):
+    offset: int
+    """Start position for the context. Use of different 'string_index_type' values can
+     affect the offset returned. Required."""
+    length: int
+    """The length of the context. Use of different 'string_index_type' values can affect
+     the length returned. Required."""
+
+    def __init__(self, **kwargs: Any) -> None:
         self.offset = kwargs.get("offset", None)
         self.length = kwargs.get("length", None)
 
-    def __repr__(self):
-        return "SummaryContext(offset={}, length={})".format(
-                self.offset,
-                self.length,
-            )[
-                :1024
-            ]
+    def __repr__(self) -> str:
+        return f"SummaryContext(offset={self.offset}, length={self.length})"[:1024]
 
     @classmethod
     def _from_generated(cls, summary):
         return cls(
             offset=summary.offset,
             length=summary.length
         )
 
 
-class AbstractSummaryAction(DictMixin):
-    """AbstractSummaryAction encapsulates the parameters for starting a long-running
+class AbstractiveSummaryAction(DictMixin):
+    """AbstractiveSummaryAction encapsulates the parameters for starting a long-running
     abstractive summarization operation. For a conceptual discussion of extractive summarization,
     see the service documentation:
     https://learn.microsoft.com/azure/cognitive-services/language-service/summarization/overview
 
     Abstractive summarization generates a summary for the input documents. Abstractive summarization
     is different from extractive summarization in that extractive summarization is the strategy of
     concatenating extracted sentences from the input document into a summary, while abstractive
@@ -3027,49 +3039,58 @@
         logged on the service side for troubleshooting. By default, the Language service logs your
         input text for 48 hours, solely to allow for troubleshooting issues in providing you with
         the service's natural language processing functions. Setting this parameter to true,
         disables input logging and may limit our ability to remediate issues that occur. Please see
         Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
         additional details, and Microsoft Responsible AI principles at
         https://www.microsoft.com/ai/responsible-ai.
-    :ivar Optional[int] sentence_count: It controls the approximate number of sentences in the output summaries.
-    :ivar Optional[str] model_version: The model version to use for the analysis.
-    :ivar Optional[str] string_index_type: Specifies the method used to interpret string offsets.
+
+    .. versionadded:: 2022-10-01-preview
+        The *AbstractiveSummaryAction* model.
+    """
+
+    sentence_count: Optional[int] = None
+    """It controls the approximate number of sentences in the output summaries."""
+    model_version: Optional[str] = None
+    """The model version to use for the analysis."""
+    string_index_type: Optional[str] = None
+    """Specifies the method used to interpret string offsets.
         `UnicodeCodePoint`, the Python encoding, is the default. To override the Python default,
         you can also pass in `Utf16CodeUnit` or `TextElement_v8`. For additional information
-        see https://aka.ms/text-analytics-offsets
-    :ivar Optional[bool] disable_service_logs: If set to true, you opt-out of having your text input
+        see https://aka.ms/text-analytics-offsets"""
+    disable_service_logs: Optional[bool] = None
+    """If set to true, you opt-out of having your text input
         logged on the service side for troubleshooting. By default, the Language service logs your
         input text for 48 hours, solely to allow for troubleshooting issues in providing you with
         the service's natural language processing functions. Setting this parameter to true,
         disables input logging and may limit our ability to remediate issues that occur. Please see
         Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
         additional details, and Microsoft Responsible AI principles at
-        https://www.microsoft.com/ai/responsible-ai.
+        https://www.microsoft.com/ai/responsible-ai."""
 
-    .. versionadded:: 2022-10-01-preview
-        The *AbstractSummaryAction* model.
-    """
+    def __init__(
+        self,
+        *,
+        sentence_count: Optional[int] = None,
+        model_version: Optional[str] = None,
+        string_index_type: Optional[str] = None,
+        disable_service_logs: Optional[bool] = None,
+        **kwargs: Any
+    ) -> None:
+        self.sentence_count = sentence_count
+        self.model_version = model_version
+        self.string_index_type: str = string_index_type if string_index_type is not None else STRING_INDEX_TYPE_DEFAULT
+        self.disable_service_logs = disable_service_logs
 
-    def __init__(self, **kwargs):
-        self.sentence_count = kwargs.get("sentence_count", None)
-        self.model_version = kwargs.get("model_version", None)
-        self.string_index_type = kwargs.get("string_index_type", "UnicodeCodePoint")
-        self.disable_service_logs = kwargs.get("disable_service_logs", None)
-
-    def __repr__(self):
-        return "AbstractSummaryAction(model_version={}, string_index_type={}, disable_service_logs={}, " \
-               "sentence_count={})".format(
-            self.model_version,
-            self.string_index_type,
-            self.disable_service_logs,
-            self.sentence_count,
-        )[
-            :1024
-        ]
+    def __repr__(self) -> str:
+        return (
+            f"AbstractiveSummaryAction(model_version={self.model_version}, "
+            f"string_index_type={self.string_index_type}, disable_service_logs={self.disable_service_logs}, "
+            f"sentence_count={self.sentence_count})"[:1024]
+        )
 
     def _to_generated(self, api_version, task_id):  # pylint: disable=unused-argument
         return _v2022_10_01_preview_models.AbstractiveSummarizationLROTask(
             task_name=task_id,
             parameters=_v2022_10_01_preview_models.AbstractiveSummarizationTaskParameters(
                 model_version=self.model_version,
                 string_index_type=string_index_type_compatibility(self.string_index_type),
@@ -3079,52 +3100,47 @@
         )
 
 
 class DynamicClassificationResult(DictMixin):
     """DynamicClassificationResult is a result object which contains
     the classifications for a particular document.
 
-    :ivar str id: Unique, non-empty document identifier.
-    :ivar classifications: Recognized classification results in the document.
-    :vartype classifications: list[~azure.ai.textanalytics.ClassificationCategory]
-    :ivar warnings: Warnings encountered while processing document.
-    :vartype warnings: list[~azure.ai.textanalytics.TextAnalyticsWarning]
-    :ivar statistics: If `show_stats=True` was specified in the request this
-        field will contain information about the document payload.
-    :vartype statistics: Optional[~azure.ai.textanalytics.TextDocumentStatistics]
-    :ivar bool is_error: Boolean check for error item when iterating over list of
-        results. Always False for an instance of a DynamicClassificationResult.
-    :ivar str kind: The text analysis kind - "DynamicClassification".
-
     .. versionadded:: 2022-10-01-preview
         The *DynamicClassificationResult* model.
     """
 
-    def __init__(
-        self,
-        **kwargs
-    ):
+    id: str  # pylint: disable=redefined-builtin
+    """Unique, non-empty document identifier."""
+    classifications: List[ClassificationCategory]
+    """Recognized classification results in the document."""
+    warnings: List[TextAnalyticsWarning]
+    """Warnings encountered while processing document."""
+    statistics: Optional[TextDocumentStatistics] = None
+    """If `show_stats=True` was specified in the request this
+        field will contain information about the document payload."""
+    is_error: Literal[False] = False
+    """Boolean check for error item when iterating over list of
+        results. Always False for an instance of a DynamicClassificationResult."""
+    kind: Literal["DynamicClassification"] = "DynamicClassification"
+    """The text analysis kind - "DynamicClassification"."""
+
+    def __init__(self, **kwargs: Any) -> None:
         self.id = kwargs.get('id', None)
         self.classifications = kwargs.get('classifications', None)
         self.warnings = kwargs.get('warnings', [])
         self.statistics = kwargs.get('statistics', None)
         self.is_error: Literal[False] = False
         self.kind: Literal["DynamicClassification"] = "DynamicClassification"
 
-    def __repr__(self):
-        return "DynamicClassificationResult(id={}, classifications={}, warnings={}, statistics={}, " \
-               "is_error={})".format(
-                self.id,
-                repr(self.classifications),
-                repr(self.warnings),
-                repr(self.statistics),
-                self.is_error,
-            )[
-                :1024
-            ]
+    def __repr__(self) -> str:
+        return (
+            f"DynamicClassificationResult(id={self.id}, classifications={repr(self.classifications)}, "
+            f"warnings={repr(self.warnings)}, statistics={repr(self.statistics)}, "
+            f"is_error={self.is_error}, kind={self.kind})"[:1024]
+        )
 
     @classmethod
     def _from_generated(cls, result):
         return cls(
             id=result.id,
             classifications=[
                 ClassificationCategory._from_generated(c)  # pylint: disable=protected-access
```

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_request_handlers.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_request_handlers.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/_serialization.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/_serialization.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/__init__.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/_configuration.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/_configuration.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/_operations_mixin.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/_operations_mixin.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/_text_analytics_client.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/_text_analytics_client.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/_vendor.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/_vendor.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/__init__.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/_configuration.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/_configuration.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/_text_analytics_client.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/_text_analytics_client.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/_patch.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/_vendor.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/aio/_vendor.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/__init__.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/aio/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/_configuration.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/_configuration.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/_text_analytics_client.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/_text_analytics_client.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/_patch.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/aio/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/operations/_text_analytics_client_operations.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/operations/_text_analytics_client_operations.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/operations/__init__.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/aio/operations/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/operations/_patch.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/operations/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/operations/_text_analytics_client_operations.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/operations/_text_analytics_client_operations.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/operations/__init__.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/operations/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/operations/_patch.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/operations/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/models/__init__.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/models/__init__.py`

 * *Files 9% similar despite different names*

```diff
@@ -11,35 +11,34 @@
 from ._models_py3 import AbstractiveSummarizationResult
 from ._models_py3 import AbstractiveSummarizationResultBase
 from ._models_py3 import AbstractiveSummarizationTaskParameters
 from ._models_py3 import AbstractiveSummarizationTaskParametersBase
 from ._models_py3 import AbstractiveSummary
 from ._models_py3 import AbstractiveSummaryDocumentResult
 from ._models_py3 import AbstractiveSummaryDocumentResultWithDetectedLanguage
-from ._models_py3 import AgeResolution
+from ._patch import AgeResolution
 from ._models_py3 import AnalyzeTextDynamicClassificationInput
 from ._models_py3 import AnalyzeTextEntityLinkingInput
 from ._models_py3 import AnalyzeTextEntityRecognitionInput
 from ._models_py3 import AnalyzeTextJobState
 from ._models_py3 import AnalyzeTextJobStatistics
 from ._models_py3 import AnalyzeTextJobsInput
 from ._models_py3 import AnalyzeTextKeyPhraseExtractionInput
 from ._models_py3 import AnalyzeTextLROResult
 from ._models_py3 import AnalyzeTextLROTask
 from ._models_py3 import AnalyzeTextLanguageDetectionInput
 from ._models_py3 import AnalyzeTextPiiEntitiesRecognitionInput
 from ._models_py3 import AnalyzeTextSentimentAnalysisInput
 from ._models_py3 import AnalyzeTextTask
 from ._models_py3 import AnalyzeTextTaskResult
-from ._models_py3 import AreaResolution
+from ._patch import AreaResolution
 from ._models_py3 import BaseResolution
-from ._models_py3 import BooleanResolution
 from ._models_py3 import ClassificationDocumentResult
 from ._models_py3 import ClassificationResult
-from ._models_py3 import CurrencyResolution
+from ._patch import CurrencyResolution
 from ._models_py3 import CustomEntitiesLROTask
 from ._models_py3 import CustomEntitiesResult
 from ._models_py3 import CustomEntitiesResultDocumentsItem
 from ._models_py3 import CustomEntitiesTaskParameters
 from ._models_py3 import CustomEntityRecognitionLROResult
 from ._models_py3 import CustomLabelClassificationResult
 from ._models_py3 import CustomLabelClassificationResultDocumentsItem
@@ -47,15 +46,15 @@
 from ._models_py3 import CustomMultiLabelClassificationLROTask
 from ._models_py3 import CustomMultiLabelClassificationTaskParameters
 from ._models_py3 import CustomResult
 from ._models_py3 import CustomSingleLabelClassificationLROResult
 from ._models_py3 import CustomSingleLabelClassificationLROTask
 from ._models_py3 import CustomSingleLabelClassificationTaskParameters
 from ._models_py3 import CustomTaskParameters
-from ._models_py3 import DateTimeResolution
+from ._patch import DateTimeResolution
 from ._models_py3 import DetectedLanguage
 from ._models_py3 import DocumentDetectedLanguage
 from ._models_py3 import DocumentDetectedLanguageString
 from ._models_py3 import DocumentError
 from ._models_py3 import DocumentRequestStatistics
 from ._models_py3 import DocumentResult
 from ._models_py3 import DocumentStatistics
@@ -96,15 +95,15 @@
 from ._models_py3 import HealthcareEntityLink
 from ._models_py3 import HealthcareLROResult
 from ._models_py3 import HealthcareLROTask
 from ._models_py3 import HealthcareRelation
 from ._models_py3 import HealthcareRelationEntity
 from ._models_py3 import HealthcareResult
 from ._models_py3 import HealthcareTaskParameters
-from ._models_py3 import InformationResolution
+from ._patch import InformationResolution
 from ._models_py3 import InnerErrorModel
 from ._models_py3 import InputError
 from ._models_py3 import JobState
 from ._models_py3 import KeyPhraseExtractionLROResult
 from ._models_py3 import KeyPhraseLROTask
 from ._models_py3 import KeyPhraseResult
 from ._models_py3 import KeyPhraseResultDocumentsItem
@@ -113,23 +112,23 @@
 from ._models_py3 import KeyPhrasesDocumentResult
 from ._models_py3 import LanguageDetectionAnalysisInput
 from ._models_py3 import LanguageDetectionDocumentResult
 from ._models_py3 import LanguageDetectionResult
 from ._models_py3 import LanguageDetectionTaskParameters
 from ._models_py3 import LanguageDetectionTaskResult
 from ._models_py3 import LanguageInput
-from ._models_py3 import LengthResolution
+from ._patch import LengthResolution
 from ._models_py3 import LinkedEntitiesDocumentResult
 from ._models_py3 import LinkedEntity
 from ._models_py3 import Match
 from ._models_py3 import MultiLanguageAnalysisInput
 from ._models_py3 import MultiLanguageInput
-from ._models_py3 import NumberResolution
-from ._models_py3 import NumericRangeResolution
-from ._models_py3 import OrdinalResolution
+from ._patch import NumberResolution
+from ._patch import NumericRangeResolution
+from ._patch import OrdinalResolution
 from ._models_py3 import PIIResultWithDetectedLanguage
 from ._models_py3 import Pagination
 from ._models_py3 import PiiEntitiesDocumentResult
 from ._models_py3 import PiiEntityRecognitionLROResult
 from ._models_py3 import PiiLROTask
 from ._models_py3 import PiiResult
 from ._models_py3 import PiiTaskParameters
@@ -145,27 +144,27 @@
 from ._models_py3 import SentimentAnalysisTaskParameters
 from ._models_py3 import SentimentConfidenceScorePerLabel
 from ._models_py3 import SentimentDocumentResult
 from ._models_py3 import SentimentLROResult
 from ._models_py3 import SentimentResponse
 from ._models_py3 import SentimentResponseDocumentsItem
 from ._models_py3 import SentimentTaskResult
-from ._models_py3 import SpeedResolution
+from ._patch import SpeedResolution
 from ._models_py3 import SummaryContext
 from ._models_py3 import TargetConfidenceScoreLabel
 from ._models_py3 import TargetRelation
 from ._models_py3 import TaskIdentifier
 from ._models_py3 import TaskParameters
 from ._models_py3 import TaskState
 from ._models_py3 import TasksState
 from ._models_py3 import TasksStateTasks
-from ._models_py3 import TemperatureResolution
-from ._models_py3 import TemporalSpanResolution
-from ._models_py3 import VolumeResolution
-from ._models_py3 import WeightResolution
+from ._patch import TemperatureResolution
+from ._patch import TemporalSpanResolution
+from ._patch import VolumeResolution
+from ._patch import WeightResolution
 
 
 from ._text_analytics_client_enums import AgeUnit
 from ._text_analytics_client_enums import AnalyzeTextLROResultsKind
 from ._text_analytics_client_enums import AnalyzeTextLROTaskKind
 from ._text_analytics_client_enums import AnalyzeTextTaskKind
 from ._text_analytics_client_enums import AnalyzeTextTaskResultsKind
@@ -229,15 +228,14 @@
     'AnalyzeTextLanguageDetectionInput',
     'AnalyzeTextPiiEntitiesRecognitionInput',
     'AnalyzeTextSentimentAnalysisInput',
     'AnalyzeTextTask',
     'AnalyzeTextTaskResult',
     'AreaResolution',
     'BaseResolution',
-    'BooleanResolution',
     'ClassificationDocumentResult',
     'ClassificationResult',
     'CurrencyResolution',
     'CustomEntitiesLROTask',
     'CustomEntitiesResult',
     'CustomEntitiesResultDocumentsItem',
     'CustomEntitiesTaskParameters',
```

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/models/_models_py3.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/models/_models_py3.py`

 * *Files 1% similar despite different names*

```diff
@@ -886,40 +886,39 @@
         self.value = value
 
 
 class BaseResolution(_serialization.Model):
     """The abstract base class for entity resolutions.
 
     You probably want to use the sub-classes and not this class directly. Known sub-classes are:
-    AgeResolution, AreaResolution, BooleanResolution, CurrencyResolution, DateTimeResolution,
-    InformationResolution, LengthResolution, NumberResolution, NumericRangeResolution,
-    OrdinalResolution, SpeedResolution, TemperatureResolution, TemporalSpanResolution,
-    VolumeResolution, WeightResolution
+    AgeResolution, AreaResolution, CurrencyResolution, DateTimeResolution, InformationResolution,
+    LengthResolution, NumberResolution, NumericRangeResolution, OrdinalResolution, SpeedResolution,
+    TemperatureResolution, TemporalSpanResolution, VolumeResolution, WeightResolution
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar resolution_kind: The entity resolution object kind. Required. Known values are:
-     "BooleanResolution", "DateTimeResolution", "NumberResolution", "OrdinalResolution",
-     "SpeedResolution", "WeightResolution", "LengthResolution", "VolumeResolution",
-     "AreaResolution", "AgeResolution", "InformationResolution", "TemperatureResolution",
-     "CurrencyResolution", "NumericRangeResolution", and "TemporalSpanResolution".
+     "DateTimeResolution", "NumberResolution", "OrdinalResolution", "SpeedResolution",
+     "WeightResolution", "LengthResolution", "VolumeResolution", "AreaResolution", "AgeResolution",
+     "InformationResolution", "TemperatureResolution", "CurrencyResolution",
+     "NumericRangeResolution", and "TemporalSpanResolution".
     :vartype resolution_kind: str or
      ~azure.ai.textanalytics.v2022_10_01_preview.models.ResolutionKind
     """
 
     _validation = {
         'resolution_kind': {'required': True},
     }
 
     _attribute_map = {
         "resolution_kind": {"key": "resolutionKind", "type": "str"},
     }
 
     _subtype_map = {
-        'resolution_kind': {'AgeResolution': 'AgeResolution', 'AreaResolution': 'AreaResolution', 'BooleanResolution': 'BooleanResolution', 'CurrencyResolution': 'CurrencyResolution', 'DateTimeResolution': 'DateTimeResolution', 'InformationResolution': 'InformationResolution', 'LengthResolution': 'LengthResolution', 'NumberResolution': 'NumberResolution', 'NumericRangeResolution': 'NumericRangeResolution', 'OrdinalResolution': 'OrdinalResolution', 'SpeedResolution': 'SpeedResolution', 'TemperatureResolution': 'TemperatureResolution', 'TemporalSpanResolution': 'TemporalSpanResolution', 'VolumeResolution': 'VolumeResolution', 'WeightResolution': 'WeightResolution'}
+        'resolution_kind': {'AgeResolution': 'AgeResolution', 'AreaResolution': 'AreaResolution', 'CurrencyResolution': 'CurrencyResolution', 'DateTimeResolution': 'DateTimeResolution', 'InformationResolution': 'InformationResolution', 'LengthResolution': 'LengthResolution', 'NumberResolution': 'NumberResolution', 'NumericRangeResolution': 'NumericRangeResolution', 'OrdinalResolution': 'OrdinalResolution', 'SpeedResolution': 'SpeedResolution', 'TemperatureResolution': 'TemperatureResolution', 'TemporalSpanResolution': 'TemporalSpanResolution', 'VolumeResolution': 'VolumeResolution', 'WeightResolution': 'WeightResolution'}
     }
 
     def __init__(
         self,
         **kwargs
     ):
         """
@@ -932,18 +931,18 @@
     """Represents the Age entity resolution model.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar value: The numeric value that the extracted text denotes. Required.
     :vartype value: float
     :ivar resolution_kind: The entity resolution object kind. Required. Known values are:
-     "BooleanResolution", "DateTimeResolution", "NumberResolution", "OrdinalResolution",
-     "SpeedResolution", "WeightResolution", "LengthResolution", "VolumeResolution",
-     "AreaResolution", "AgeResolution", "InformationResolution", "TemperatureResolution",
-     "CurrencyResolution", "NumericRangeResolution", and "TemporalSpanResolution".
+     "DateTimeResolution", "NumberResolution", "OrdinalResolution", "SpeedResolution",
+     "WeightResolution", "LengthResolution", "VolumeResolution", "AreaResolution", "AgeResolution",
+     "InformationResolution", "TemperatureResolution", "CurrencyResolution",
+     "NumericRangeResolution", and "TemporalSpanResolution".
     :vartype resolution_kind: str or
      ~azure.ai.textanalytics.v2022_10_01_preview.models.ResolutionKind
     :ivar unit: The Age Unit of measurement. Required. Known values are: "Unspecified", "Year",
      "Month", "Week", and "Day".
     :vartype unit: str or ~azure.ai.textanalytics.v2022_10_01_preview.models.AgeUnit
     """
 
@@ -1699,18 +1698,18 @@
     """Represents the area entity resolution model.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar value: The numeric value that the extracted text denotes. Required.
     :vartype value: float
     :ivar resolution_kind: The entity resolution object kind. Required. Known values are:
-     "BooleanResolution", "DateTimeResolution", "NumberResolution", "OrdinalResolution",
-     "SpeedResolution", "WeightResolution", "LengthResolution", "VolumeResolution",
-     "AreaResolution", "AgeResolution", "InformationResolution", "TemperatureResolution",
-     "CurrencyResolution", "NumericRangeResolution", and "TemporalSpanResolution".
+     "DateTimeResolution", "NumberResolution", "OrdinalResolution", "SpeedResolution",
+     "WeightResolution", "LengthResolution", "VolumeResolution", "AreaResolution", "AgeResolution",
+     "InformationResolution", "TemperatureResolution", "CurrencyResolution",
+     "NumericRangeResolution", and "TemporalSpanResolution".
     :vartype resolution_kind: str or
      ~azure.ai.textanalytics.v2022_10_01_preview.models.ResolutionKind
     :ivar unit: The area Unit of measurement. Required. Known values are: "Unspecified",
      "SquareKilometer", "SquareHectometer", "SquareDecameter", "SquareDecimeter", "SquareMeter",
      "SquareCentimeter", "SquareMillimeter", "SquareInch", "SquareFoot", "SquareMile", "SquareYard",
      and "Acre".
     :vartype unit: str or ~azure.ai.textanalytics.v2022_10_01_preview.models.AreaUnit
@@ -1746,55 +1745,14 @@
         """
         super().__init__(value=value, **kwargs)
         self.value = value
         self.resolution_kind = 'AreaResolution'  # type: str
         self.unit = unit
 
 
-class BooleanResolution(BaseResolution):
-    """A resolution for boolean expressions.
-
-    All required parameters must be populated in order to send to Azure.
-
-    :ivar resolution_kind: The entity resolution object kind. Required. Known values are:
-     "BooleanResolution", "DateTimeResolution", "NumberResolution", "OrdinalResolution",
-     "SpeedResolution", "WeightResolution", "LengthResolution", "VolumeResolution",
-     "AreaResolution", "AgeResolution", "InformationResolution", "TemperatureResolution",
-     "CurrencyResolution", "NumericRangeResolution", and "TemporalSpanResolution".
-    :vartype resolution_kind: str or
-     ~azure.ai.textanalytics.v2022_10_01_preview.models.ResolutionKind
-    :ivar value: Required.
-    :vartype value: bool
-    """
-
-    _validation = {
-        'resolution_kind': {'required': True},
-        'value': {'required': True},
-    }
-
-    _attribute_map = {
-        "resolution_kind": {"key": "resolutionKind", "type": "str"},
-        "value": {"key": "value", "type": "bool"},
-    }
-
-    def __init__(
-        self,
-        *,
-        value: bool,
-        **kwargs
-    ):
-        """
-        :keyword value: Required.
-        :paramtype value: bool
-        """
-        super().__init__(**kwargs)
-        self.resolution_kind = 'BooleanResolution'  # type: str
-        self.value = value
-
-
 class ClassificationDocumentResult(DocumentResult):
     """ClassificationDocumentResult.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar id: Unique, non-empty document identifier. Required.
     :vartype id: str
@@ -1889,18 +1847,18 @@
     """Represents the currency entity resolution model.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar value: The numeric value that the extracted text denotes. Required.
     :vartype value: float
     :ivar resolution_kind: The entity resolution object kind. Required. Known values are:
-     "BooleanResolution", "DateTimeResolution", "NumberResolution", "OrdinalResolution",
-     "SpeedResolution", "WeightResolution", "LengthResolution", "VolumeResolution",
-     "AreaResolution", "AgeResolution", "InformationResolution", "TemperatureResolution",
-     "CurrencyResolution", "NumericRangeResolution", and "TemporalSpanResolution".
+     "DateTimeResolution", "NumberResolution", "OrdinalResolution", "SpeedResolution",
+     "WeightResolution", "LengthResolution", "VolumeResolution", "AreaResolution", "AgeResolution",
+     "InformationResolution", "TemperatureResolution", "CurrencyResolution",
+     "NumericRangeResolution", and "TemporalSpanResolution".
     :vartype resolution_kind: str or
      ~azure.ai.textanalytics.v2022_10_01_preview.models.ResolutionKind
     :ivar iso4217: The alphabetic code based on another ISO standard, ISO 3166, which lists the
      codes for country names. The first two letters of the ISO 4217 three-letter code are the same
      as the code for the country name, and, where possible, the third letter corresponds to the
      first letter of the currency name.
     :vartype iso4217: str
@@ -2856,18 +2814,18 @@
 
 class DateTimeResolution(BaseResolution):
     """A resolution for datetime entity instances.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar resolution_kind: The entity resolution object kind. Required. Known values are:
-     "BooleanResolution", "DateTimeResolution", "NumberResolution", "OrdinalResolution",
-     "SpeedResolution", "WeightResolution", "LengthResolution", "VolumeResolution",
-     "AreaResolution", "AgeResolution", "InformationResolution", "TemperatureResolution",
-     "CurrencyResolution", "NumericRangeResolution", and "TemporalSpanResolution".
+     "DateTimeResolution", "NumberResolution", "OrdinalResolution", "SpeedResolution",
+     "WeightResolution", "LengthResolution", "VolumeResolution", "AreaResolution", "AgeResolution",
+     "InformationResolution", "TemperatureResolution", "CurrencyResolution",
+     "NumericRangeResolution", and "TemporalSpanResolution".
     :vartype resolution_kind: str or
      ~azure.ai.textanalytics.v2022_10_01_preview.models.ResolutionKind
     :ivar timex: An extended ISO 8601 date/time representation as described in
      (https://github.com/Microsoft/Recognizers-Text/blob/master/Patterns/English/English-DateTime.yaml).
      Required.
     :vartype timex: str
     :ivar date_time_sub_kind: The DateTime SubKind. Required. Known values are: "Time", "Date",
@@ -5610,18 +5568,18 @@
     """Represents the information (data) entity resolution model.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar value: The numeric value that the extracted text denotes. Required.
     :vartype value: float
     :ivar resolution_kind: The entity resolution object kind. Required. Known values are:
-     "BooleanResolution", "DateTimeResolution", "NumberResolution", "OrdinalResolution",
-     "SpeedResolution", "WeightResolution", "LengthResolution", "VolumeResolution",
-     "AreaResolution", "AgeResolution", "InformationResolution", "TemperatureResolution",
-     "CurrencyResolution", "NumericRangeResolution", and "TemporalSpanResolution".
+     "DateTimeResolution", "NumberResolution", "OrdinalResolution", "SpeedResolution",
+     "WeightResolution", "LengthResolution", "VolumeResolution", "AreaResolution", "AgeResolution",
+     "InformationResolution", "TemperatureResolution", "CurrencyResolution",
+     "NumericRangeResolution", and "TemporalSpanResolution".
     :vartype resolution_kind: str or
      ~azure.ai.textanalytics.v2022_10_01_preview.models.ResolutionKind
     :ivar unit: The information (data) Unit of measurement. Required. Known values are:
      "Unspecified", "Bit", "Kilobit", "Megabit", "Gigabit", "Terabit", "Petabit", "Byte",
      "Kilobyte", "Megabyte", "Gigabyte", "Terabyte", and "Petabyte".
     :vartype unit: str or ~azure.ai.textanalytics.v2022_10_01_preview.models.InformationUnit
     """
@@ -6386,18 +6344,18 @@
     """Represents the length entity resolution model.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar value: The numeric value that the extracted text denotes. Required.
     :vartype value: float
     :ivar resolution_kind: The entity resolution object kind. Required. Known values are:
-     "BooleanResolution", "DateTimeResolution", "NumberResolution", "OrdinalResolution",
-     "SpeedResolution", "WeightResolution", "LengthResolution", "VolumeResolution",
-     "AreaResolution", "AgeResolution", "InformationResolution", "TemperatureResolution",
-     "CurrencyResolution", "NumericRangeResolution", and "TemporalSpanResolution".
+     "DateTimeResolution", "NumberResolution", "OrdinalResolution", "SpeedResolution",
+     "WeightResolution", "LengthResolution", "VolumeResolution", "AreaResolution", "AgeResolution",
+     "InformationResolution", "TemperatureResolution", "CurrencyResolution",
+     "NumericRangeResolution", and "TemporalSpanResolution".
     :vartype resolution_kind: str or
      ~azure.ai.textanalytics.v2022_10_01_preview.models.ResolutionKind
     :ivar unit: The length Unit of measurement. Required. Known values are: "Unspecified",
      "Kilometer", "Hectometer", "Decameter", "Meter", "Decimeter", "Centimeter", "Millimeter",
      "Micrometer", "Nanometer", "Picometer", "Mile", "Yard", "Inch", "Foot", "LightYear", and "Pt".
     :vartype unit: str or ~azure.ai.textanalytics.v2022_10_01_preview.models.LengthUnit
     """
@@ -6648,18 +6606,18 @@
 
 class NumberResolution(BaseResolution):
     """A resolution for numeric entity instances.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar resolution_kind: The entity resolution object kind. Required. Known values are:
-     "BooleanResolution", "DateTimeResolution", "NumberResolution", "OrdinalResolution",
-     "SpeedResolution", "WeightResolution", "LengthResolution", "VolumeResolution",
-     "AreaResolution", "AgeResolution", "InformationResolution", "TemperatureResolution",
-     "CurrencyResolution", "NumericRangeResolution", and "TemporalSpanResolution".
+     "DateTimeResolution", "NumberResolution", "OrdinalResolution", "SpeedResolution",
+     "WeightResolution", "LengthResolution", "VolumeResolution", "AreaResolution", "AgeResolution",
+     "InformationResolution", "TemperatureResolution", "CurrencyResolution",
+     "NumericRangeResolution", and "TemporalSpanResolution".
     :vartype resolution_kind: str or
      ~azure.ai.textanalytics.v2022_10_01_preview.models.ResolutionKind
     :ivar number_kind: The type of the extracted number entity. Required. Known values are:
      "Integer", "Decimal", "Power", "Fraction", "Percent", and "Unspecified".
     :vartype number_kind: str or ~azure.ai.textanalytics.v2022_10_01_preview.models.NumberKind
     :ivar value: A numeric representation of what the extracted text denotes. Required.
     :vartype value: float
@@ -6699,18 +6657,18 @@
 
 class NumericRangeResolution(BaseResolution):
     """represents the resolution of numeric intervals.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar resolution_kind: The entity resolution object kind. Required. Known values are:
-     "BooleanResolution", "DateTimeResolution", "NumberResolution", "OrdinalResolution",
-     "SpeedResolution", "WeightResolution", "LengthResolution", "VolumeResolution",
-     "AreaResolution", "AgeResolution", "InformationResolution", "TemperatureResolution",
-     "CurrencyResolution", "NumericRangeResolution", and "TemporalSpanResolution".
+     "DateTimeResolution", "NumberResolution", "OrdinalResolution", "SpeedResolution",
+     "WeightResolution", "LengthResolution", "VolumeResolution", "AreaResolution", "AgeResolution",
+     "InformationResolution", "TemperatureResolution", "CurrencyResolution",
+     "NumericRangeResolution", and "TemporalSpanResolution".
     :vartype resolution_kind: str or
      ~azure.ai.textanalytics.v2022_10_01_preview.models.ResolutionKind
     :ivar range_kind: The kind of range that the resolution object represents. Required. Known
      values are: "Number", "Speed", "Weight", "Length", "Volume", "Area", "Age", "Information",
      "Temperature", and "Currency".
     :vartype range_kind: str or ~azure.ai.textanalytics.v2022_10_01_preview.models.RangeKind
     :ivar minimum: The beginning value of  the interval. Required.
@@ -6760,18 +6718,18 @@
 
 class OrdinalResolution(BaseResolution):
     """A resolution for ordinal numbers entity instances.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar resolution_kind: The entity resolution object kind. Required. Known values are:
-     "BooleanResolution", "DateTimeResolution", "NumberResolution", "OrdinalResolution",
-     "SpeedResolution", "WeightResolution", "LengthResolution", "VolumeResolution",
-     "AreaResolution", "AgeResolution", "InformationResolution", "TemperatureResolution",
-     "CurrencyResolution", "NumericRangeResolution", and "TemporalSpanResolution".
+     "DateTimeResolution", "NumberResolution", "OrdinalResolution", "SpeedResolution",
+     "WeightResolution", "LengthResolution", "VolumeResolution", "AreaResolution", "AgeResolution",
+     "InformationResolution", "TemperatureResolution", "CurrencyResolution",
+     "NumericRangeResolution", and "TemporalSpanResolution".
     :vartype resolution_kind: str or
      ~azure.ai.textanalytics.v2022_10_01_preview.models.ResolutionKind
     :ivar offset: The offset With respect to the reference (e.g., offset = -1 in "show me the
      second to last". Required.
     :vartype offset: str
     :ivar relative_to: The reference point that the ordinal number denotes. Required. Known values
      are: "Current", "End", and "Start".
@@ -7981,18 +7939,18 @@
     """Represents the speed entity resolution model.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar value: The numeric value that the extracted text denotes. Required.
     :vartype value: float
     :ivar resolution_kind: The entity resolution object kind. Required. Known values are:
-     "BooleanResolution", "DateTimeResolution", "NumberResolution", "OrdinalResolution",
-     "SpeedResolution", "WeightResolution", "LengthResolution", "VolumeResolution",
-     "AreaResolution", "AgeResolution", "InformationResolution", "TemperatureResolution",
-     "CurrencyResolution", "NumericRangeResolution", and "TemporalSpanResolution".
+     "DateTimeResolution", "NumberResolution", "OrdinalResolution", "SpeedResolution",
+     "WeightResolution", "LengthResolution", "VolumeResolution", "AreaResolution", "AgeResolution",
+     "InformationResolution", "TemperatureResolution", "CurrencyResolution",
+     "NumericRangeResolution", and "TemporalSpanResolution".
     :vartype resolution_kind: str or
      ~azure.ai.textanalytics.v2022_10_01_preview.models.ResolutionKind
     :ivar unit: The speed Unit of measurement. Required. Known values are: "Unspecified",
      "MeterPerSecond", "KilometerPerHour", "KilometerPerMinute", "KilometerPerSecond",
      "MilePerHour", "Knot", "FootPerSecond", "FootPerMinute", "YardPerMinute", "YardPerSecond",
      "MeterPerMillisecond", "CentimeterPerMillisecond", and "KilometerPerMillisecond".
     :vartype unit: str or ~azure.ai.textanalytics.v2022_10_01_preview.models.SpeedUnit
@@ -8223,18 +8181,18 @@
     """Represents the temperature entity resolution model.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar value: The numeric value that the extracted text denotes. Required.
     :vartype value: float
     :ivar resolution_kind: The entity resolution object kind. Required. Known values are:
-     "BooleanResolution", "DateTimeResolution", "NumberResolution", "OrdinalResolution",
-     "SpeedResolution", "WeightResolution", "LengthResolution", "VolumeResolution",
-     "AreaResolution", "AgeResolution", "InformationResolution", "TemperatureResolution",
-     "CurrencyResolution", "NumericRangeResolution", and "TemporalSpanResolution".
+     "DateTimeResolution", "NumberResolution", "OrdinalResolution", "SpeedResolution",
+     "WeightResolution", "LengthResolution", "VolumeResolution", "AreaResolution", "AgeResolution",
+     "InformationResolution", "TemperatureResolution", "CurrencyResolution",
+     "NumericRangeResolution", and "TemporalSpanResolution".
     :vartype resolution_kind: str or
      ~azure.ai.textanalytics.v2022_10_01_preview.models.ResolutionKind
     :ivar unit: The temperature Unit of measurement. Required. Known values are: "Unspecified",
      "Fahrenheit", "Kelvin", "Rankine", and "Celsius".
     :vartype unit: str or ~azure.ai.textanalytics.v2022_10_01_preview.models.TemperatureUnit
     """
 
@@ -8272,18 +8230,18 @@
 
 class TemporalSpanResolution(BaseResolution):
     """represents the resolution of a date and/or time span.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar resolution_kind: The entity resolution object kind. Required. Known values are:
-     "BooleanResolution", "DateTimeResolution", "NumberResolution", "OrdinalResolution",
-     "SpeedResolution", "WeightResolution", "LengthResolution", "VolumeResolution",
-     "AreaResolution", "AgeResolution", "InformationResolution", "TemperatureResolution",
-     "CurrencyResolution", "NumericRangeResolution", and "TemporalSpanResolution".
+     "DateTimeResolution", "NumberResolution", "OrdinalResolution", "SpeedResolution",
+     "WeightResolution", "LengthResolution", "VolumeResolution", "AreaResolution", "AgeResolution",
+     "InformationResolution", "TemperatureResolution", "CurrencyResolution",
+     "NumericRangeResolution", and "TemporalSpanResolution".
     :vartype resolution_kind: str or
      ~azure.ai.textanalytics.v2022_10_01_preview.models.ResolutionKind
     :ivar begin: An extended ISO 8601 date/time representation as described in
      (https://github.com/Microsoft/Recognizers-Text/blob/master/Patterns/English/English-DateTime.yaml).
     :vartype begin: str
     :ivar end: An extended ISO 8601 date/time representation as described in
      (https://github.com/Microsoft/Recognizers-Text/blob/master/Patterns/English/English-DateTime.yaml).
@@ -8356,18 +8314,18 @@
     """Represents the volume entity resolution model.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar value: The numeric value that the extracted text denotes. Required.
     :vartype value: float
     :ivar resolution_kind: The entity resolution object kind. Required. Known values are:
-     "BooleanResolution", "DateTimeResolution", "NumberResolution", "OrdinalResolution",
-     "SpeedResolution", "WeightResolution", "LengthResolution", "VolumeResolution",
-     "AreaResolution", "AgeResolution", "InformationResolution", "TemperatureResolution",
-     "CurrencyResolution", "NumericRangeResolution", and "TemporalSpanResolution".
+     "DateTimeResolution", "NumberResolution", "OrdinalResolution", "SpeedResolution",
+     "WeightResolution", "LengthResolution", "VolumeResolution", "AreaResolution", "AgeResolution",
+     "InformationResolution", "TemperatureResolution", "CurrencyResolution",
+     "NumericRangeResolution", and "TemporalSpanResolution".
     :vartype resolution_kind: str or
      ~azure.ai.textanalytics.v2022_10_01_preview.models.ResolutionKind
     :ivar unit: The Volume Unit of measurement. Required. Known values are: "Unspecified",
      "CubicMeter", "CubicCentimeter", "CubicMillimeter", "Hectoliter", "Decaliter", "Liter",
      "Centiliter", "Milliliter", "CubicYard", "CubicInch", "CubicFoot", "CubicMile", "FluidOunce",
      "Teaspoon", "Tablespoon", "Pint", "Quart", "Cup", "Gill", "Pinch", "FluidDram", "Barrel",
      "Minim", "Cord", "Peck", "Bushel", and "Hogshead".
@@ -8413,18 +8371,18 @@
     """Represents the weight entity resolution model.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar value: The numeric value that the extracted text denotes. Required.
     :vartype value: float
     :ivar resolution_kind: The entity resolution object kind. Required. Known values are:
-     "BooleanResolution", "DateTimeResolution", "NumberResolution", "OrdinalResolution",
-     "SpeedResolution", "WeightResolution", "LengthResolution", "VolumeResolution",
-     "AreaResolution", "AgeResolution", "InformationResolution", "TemperatureResolution",
-     "CurrencyResolution", "NumericRangeResolution", and "TemporalSpanResolution".
+     "DateTimeResolution", "NumberResolution", "OrdinalResolution", "SpeedResolution",
+     "WeightResolution", "LengthResolution", "VolumeResolution", "AreaResolution", "AgeResolution",
+     "InformationResolution", "TemperatureResolution", "CurrencyResolution",
+     "NumericRangeResolution", and "TemporalSpanResolution".
     :vartype resolution_kind: str or
      ~azure.ai.textanalytics.v2022_10_01_preview.models.ResolutionKind
     :ivar unit: The weight Unit of measurement. Required. Known values are: "Unspecified",
      "Kilogram", "Gram", "Milligram", "Gallon", "MetricTon", "Ton", "Pound", "Ounce", "Grain",
      "PennyWeight", "LongTonBritish", "ShortTonUS", "ShortHundredWeightUS", "Stone", and "Dram".
     :vartype unit: str or ~azure.ai.textanalytics.v2022_10_01_preview.models.WeightUnit
     """
```

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_10_01_preview/models/_text_analytics_client_enums.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/models/_text_analytics_client_enums.py`

 * *Files 1% similar despite different names*

```diff
@@ -538,15 +538,14 @@
     END = "End"
     START = "Start"
 
 class ResolutionKind(str, Enum, metaclass=CaseInsensitiveEnumMeta):
     """The entity resolution object kind.
     """
 
-    BOOLEAN_RESOLUTION = "BooleanResolution"
     DATE_TIME_RESOLUTION = "DateTimeResolution"
     NUMBER_RESOLUTION = "NumberResolution"
     ORDINAL_RESOLUTION = "OrdinalResolution"
     SPEED_RESOLUTION = "SpeedResolution"
     WEIGHT_RESOLUTION = "WeightResolution"
     LENGTH_RESOLUTION = "LengthResolution"
     VOLUME_RESOLUTION = "VolumeResolution"
```

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/aio/__init__.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/aio/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/aio/_configuration.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/aio/_configuration.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/aio/_operations_mixin.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/aio/_operations_mixin.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/aio/_text_analytics_client.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/aio/_text_analytics_client.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/_vendor.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/_vendor.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/__init__.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/_configuration.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/_configuration.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/_text_analytics_client.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/_text_analytics_client.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/_patch.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/aio/_vendor.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/aio/_vendor.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/aio/__init__.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/aio/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/aio/_configuration.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/aio/_configuration.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/aio/_text_analytics_client.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/aio/_text_analytics_client.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/aio/_patch.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/aio/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/aio/operations/_text_analytics_client_operations.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/aio/operations/_text_analytics_client_operations.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/aio/operations/__init__.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/aio/operations/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/aio/operations/_patch.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/models/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/operations/_text_analytics_client_operations.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/operations/_text_analytics_client_operations.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/operations/__init__.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/operations/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/operations/_patch.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/aio/operations/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/models/__init__.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/models/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/models/_models_py3.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/models/_models_py3.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/models/_patch.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/models/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_1/models/_text_analytics_client_enums.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/models/_text_analytics_client_enums.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/_vendor.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/_vendor.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/__init__.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/_configuration.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/_configuration.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/_text_analytics_client.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/_text_analytics_client.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/_patch.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/aio/_vendor.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/aio/_vendor.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/aio/__init__.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/aio/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/aio/_configuration.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/aio/_configuration.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/aio/_text_analytics_client.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/aio/_text_analytics_client.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/aio/_patch.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/aio/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/aio/operations/_text_analytics_client_operations.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/aio/operations/_text_analytics_client_operations.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/aio/operations/__init__.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/aio/operations/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/aio/operations/_patch.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_1/operations/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/operations/_text_analytics_client_operations.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/operations/_text_analytics_client_operations.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/operations/__init__.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/operations/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/operations/_patch.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/aio/operations/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/models/__init__.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/models/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/models/_models_py3.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/models/_models_py3.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/models/_patch.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/models/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v3_0/models/_text_analytics_client_enums.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/models/_text_analytics_client_enums.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/_vendor.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/_vendor.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/__init__.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/_configuration.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/_configuration.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/_text_analytics_client.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/_text_analytics_client.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/_patch.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v3_0/operations/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/aio/_vendor.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/_vendor.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/aio/__init__.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/aio/_configuration.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/aio/_configuration.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/aio/_text_analytics_client.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/aio/_text_analytics_client.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/aio/_patch.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/aio/operations/_text_analytics_client_operations.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/aio/operations/_text_analytics_client_operations.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/aio/operations/__init__.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/operations/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/aio/operations/_patch.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/aio/operations/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/operations/_text_analytics_client_operations.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/operations/_text_analytics_client_operations.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/operations/__init__.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/operations/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/operations/_patch.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/operations/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/models/__init__.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/models/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/models/_models_py3.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/models/_models_py3.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/models/_patch.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_10_01_preview/aio/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/_generated/v2022_05_01/models/_text_analytics_client_enums.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_generated/v2022_05_01/models/_text_analytics_client_enums.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/aio/_base_client_async.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/aio/_base_client_async.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,15 +1,16 @@
 # ------------------------------------
 # Copyright (c) Microsoft Corporation.
 # Licensed under the MIT License.
 # ------------------------------------
-from typing import Any, Union
+from typing import Any, Union, Optional
 from azure.core.credentials import AzureKeyCredential
 from azure.core.credentials_async import AsyncTokenCredential
 from azure.core.pipeline.policies import AzureKeyCredentialPolicy, HttpLoggingPolicy
+from .._base_client import TextAnalyticsApiVersion
 from .._generated.aio import TextAnalyticsClient as _TextAnalyticsClient
 from .._policies import TextAnalyticsResponseHookPolicy, QuotaExceededPolicy
 from .._user_agent import USER_AGENT
 from .._version import DEFAULT_API_VERSION
 
 
 def _authentication_policy(credential):
@@ -29,14 +30,16 @@
 
 
 class AsyncTextAnalyticsClientBase:
     def __init__(
         self,
         endpoint: str,
         credential: Union[AzureKeyCredential, AsyncTokenCredential],
+        *,
+        api_version: Optional[Union[str, TextAnalyticsApiVersion]] = None,
         **kwargs: Any
     ) -> None:
         http_logging_policy = HttpLoggingPolicy(**kwargs)
         http_logging_policy.allowed_header_names.update(
             {
                 "Operation-Location",
                 "apim-request-id",
@@ -62,17 +65,17 @@
                 "api-version"
             }
         )
         try:
             endpoint = endpoint.rstrip("/")
         except AttributeError:
             raise ValueError("Parameter 'endpoint' must be a string.")
-        self._api_version = kwargs.pop("api_version", DEFAULT_API_VERSION)
+        self._api_version = api_version if api_version is not None else DEFAULT_API_VERSION
         if hasattr(self._api_version, "value"):
-            self._api_version = self._api_version.value
+            self._api_version = self._api_version.value  # type: ignore
         self._client = _TextAnalyticsClient(
             endpoint=endpoint,
             credential=credential,  # type: ignore
             api_version=self._api_version,
             sdk_moniker=USER_AGENT,
             authentication_policy=kwargs.pop("authentication_policy", _authentication_policy(credential)),
             custom_hook_policy=kwargs.pop("custom_hook_policy", TextAnalyticsResponseHookPolicy(**kwargs)),
```

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/aio/_lro_async.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/aio/_lro_async.py`

 * *Files 4% similar despite different names*

```diff
@@ -2,15 +2,16 @@
 # Copyright (c) Microsoft Corporation.
 # Licensed under the MIT License.
 # ------------------------------------
 
 import base64
 import functools
 import json
-from typing import Mapping, Any, TypeVar, Generator, Awaitable, cast
+import datetime
+from typing import Mapping, Any, TypeVar, Generator, Awaitable, cast, Optional
 from typing_extensions import Protocol, runtime_checkable
 from azure.core.exceptions import HttpResponseError
 from azure.core.polling import AsyncLROPoller
 from azure.core.polling.base_polling import OperationFailed, BadStatus
 from azure.core.polling.async_base_polling import AsyncLROBasePolling
 from azure.core.tracing.decorator_async import distributed_trace_async
 from .._lro import TextAnalyticsOperationResourcePolling
@@ -86,28 +87,28 @@
         :rtype: None
         :raises ~azure.core.exceptions.HttpResponseError: When the operation has already reached a terminal state.
         """
         ...
 
 
 class TextAnalyticsAsyncLROPollingMethod(AsyncLROBasePolling):
-    def finished(self):
+    def finished(self) -> bool:
         """Is this polling finished?
         :rtype: bool
         """
         return TextAnalyticsAsyncLROPollingMethod._finished(self.status())
 
     @staticmethod
-    def _finished(status):
+    def _finished(status) -> bool:
         if hasattr(status, "value"):
             status = status.value
         return str(status).lower() in _FINISHED
 
     @staticmethod
-    def _failed(status):
+    def _failed(status) -> bool:
         if hasattr(status, "value"):
             status = status.value
         return str(status).lower() in _FAILED
 
     @staticmethod
     def _raise_if_bad_http_status_and_method(response):
         """Check response status code is valid.
@@ -131,14 +132,17 @@
 
         :param callable update_cmd: The function to call to retrieve the
          latest status of the long running operation.
         :raises: OperationFailed if operation status 'Failed' or 'Canceled'.
         :raises: BadStatus if response status invalid.
         :raises: BadResponse if response invalid.
         """
+
+        if not self.finished():
+            await self.update_status()
         while not self.finished():
             await self._delay()
             await self.update_status()
 
         if TextAnalyticsAsyncLROPollingMethod._failed(self.status()):
             try:
                 job = json.loads(self._pipeline_response.http_response.text())
@@ -156,64 +160,63 @@
                 self._pipeline_response.http_response
             )
 
 
 class AsyncAnalyzeHealthcareEntitiesLROPollingMethod(
     TextAnalyticsAsyncLROPollingMethod
 ):
-    def __init__(self, *args, **kwargs):
+    def __init__(self, *args: Any, **kwargs: Any) -> None:
         self._text_analytics_client = kwargs.pop("text_analytics_client")
         self._doc_id_order = kwargs.pop("doc_id_order", None)
         self._show_stats = kwargs.pop("show_stats", None)
         super().__init__(
             *args, **kwargs
         )
 
     @property
     def _current_body(self):
         from .._generated.models import JobState
         return JobState.deserialize(self._pipeline_response)
 
     @property
-    def created_on(self):
+    def created_on(self) -> Optional[datetime.datetime]:
         if not self._current_body:
             return None
         return self._current_body.created_date_time
 
     @property
-    def expires_on(self):
+    def expires_on(self) -> Optional[datetime.datetime]:
         if not self._current_body:
             return None
         return self._current_body.expiration_date_time
 
     @property
-    def last_modified_on(self):
+    def last_modified_on(self) -> Optional[datetime.datetime]:
         if not self._current_body:
             return None
         return self._current_body.last_update_date_time
 
     @property
-    def id(self):
+    def id(self) -> str:
         if self._current_body and self._current_body.job_id is not None:
             return self._current_body.job_id
         return self._get_id_from_headers()
 
     def _get_id_from_headers(self) -> str:
         return self._initial_response.http_response.headers[
             "Operation-Location"
         ].split("/jobs/")[1].split("?")[0]
 
     @property
-    def display_name(self):
+    def display_name(self) -> Optional[str]:
         if not self._current_body:
             return None
         return self._current_body.display_name
 
-    def get_continuation_token(self):
-        # type() -> str
+    def get_continuation_token(self) -> str:
         import pickle
         self._initial_response.context.options["doc_id_order"] = self._doc_id_order
         self._initial_response.context.options["show_stats"] = self._show_stats
         return base64.b64encode(pickle.dumps(self._initial_response)).decode('ascii')
 
 
 class AsyncAnalyzeHealthcareEntitiesLROPoller(AsyncLROPoller[PollingReturnType]):
@@ -236,15 +239,15 @@
             "id": self.polling_method().id,
             "created_on": self.polling_method().created_on,
             "expires_on": self.polling_method().expires_on,
             "display_name": self.polling_method().display_name,
             "last_modified_on": self.polling_method().last_modified_on,
         }
 
-    def __getattr__(self, item):
+    def __getattr__(self, item: str) -> Any:
         attrs = [
             "created_on",
             "expires_on",
             "display_name",
             "last_modified_on",
             "id"
         ]
@@ -281,15 +284,15 @@
             client,
             initial_response,
             functools.partial(deserialization_callback, initial_response),
             polling_method  # type: ignore
         )
 
     @distributed_trace_async
-    async def cancel(self, **kwargs) -> "AsyncLROPoller[None]":  # type: ignore
+    async def cancel(self, **kwargs: Any) -> "AsyncLROPoller[None]":  # type: ignore
         """Cancel the operation currently being polled.
 
         :keyword int polling_interval: The polling interval to use to poll the cancellation status.
             The default value is 5 seconds.
         :return: Returns an instance of an AsyncLROPoller that returns None.
         :rtype: ~azure.core.polling.AsyncLROPoller[None]
         :raises ~azure.core.exceptions.HttpResponseError: When the operation has already reached a terminal state.
@@ -320,87 +323,86 @@
         except HttpResponseError as error:
             from .._response_handlers import process_http_response_error
 
             process_http_response_error(error)
 
 
 class AsyncAnalyzeActionsLROPollingMethod(TextAnalyticsAsyncLROPollingMethod):
-    def __init__(self, *args, **kwargs):
+    def __init__(self, *args: Any, **kwargs: Any) -> None:
         self._doc_id_order = kwargs.pop("doc_id_order", None)
         self._task_id_order = kwargs.pop("task_id_order", None)
         self._show_stats = kwargs.pop("show_stats", None)
-        self._text_analytics_client = kwargs.pop("text_analytics_client", None)
+        self._text_analytics_client = kwargs.pop("text_analytics_client")
         super().__init__(*args, **kwargs)
 
     @property
     def _current_body(self):
         from .._generated.models import JobState
         return JobState.deserialize(self._pipeline_response)
 
     @property
-    def created_on(self):
+    def created_on(self) -> Optional[datetime.datetime]:
         if not self._current_body:
             return None
         return self._current_body.created_date_time
 
     @property
-    def display_name(self):
+    def display_name(self) -> Optional[str]:
         if not self._current_body:
             return None
         return self._current_body.display_name
 
     @property
-    def expires_on(self):
+    def expires_on(self) -> Optional[datetime.datetime]:
         if not self._current_body:
             return None
         return self._current_body.expiration_date_time
 
     @property
-    def actions_failed_count(self):
+    def actions_failed_count(self) -> Optional[int]:
         if not self._current_body:
             return None
         return self._current_body.additional_properties.get("tasks", {}).get("failed", None)
 
     @property
-    def actions_in_progress_count(self):
+    def actions_in_progress_count(self) -> Optional[int]:
         if not self._current_body:
             return None
         return self._current_body.additional_properties.get("tasks", {}).get("inProgress", None)
 
     @property
-    def actions_succeeded_count(self):
+    def actions_succeeded_count(self) -> Optional[int]:
         if not self._current_body:
             return None
         return self._current_body.additional_properties.get("tasks", {}).get("completed", None)
 
     @property
-    def last_modified_on(self):
+    def last_modified_on(self) -> Optional[datetime.datetime]:
         if not self._current_body:
             return None
         return self._current_body.last_update_date_time
 
     @property
-    def total_actions_count(self):
+    def total_actions_count(self) -> Optional[int]:
         if not self._current_body:
             return None
         return self._current_body.additional_properties.get("tasks", {}).get("total", None)
 
     @property
-    def id(self):
+    def id(self) -> str:
         if self._current_body and self._current_body.job_id is not None:
             return self._current_body.job_id
         return self._get_id_from_headers()
 
     def _get_id_from_headers(self) -> str:
         return self._initial_response.http_response.headers[
             "Operation-Location"
         ].split("/jobs/")[1].split("?")[0]
 
-    def get_continuation_token(self):
-        # type: () -> str
+    def get_continuation_token(self) -> str:
         import pickle
         self._initial_response.context.options["doc_id_order"] = self._doc_id_order
         self._initial_response.context.options["task_id_order"] = self._task_id_order
         self._initial_response.context.options["show_stats"] = self._show_stats
         return base64.b64encode(pickle.dumps(self._initial_response)).decode('ascii')
 
 
@@ -428,15 +430,15 @@
             "last_modified_on": self.polling_method().last_modified_on,
             "actions_failed_count": self.polling_method().actions_failed_count,
             "actions_in_progress_count": self.polling_method().actions_in_progress_count,
             "actions_succeeded_count": self.polling_method().actions_succeeded_count,
             "total_actions_count": self.polling_method().total_actions_count,
         }
 
-    def __getattr__(self, item):
+    def __getattr__(self, item: str) -> Any:
         attrs = [
             "created_on",
             "expires_on",
             "display_name",
             "actions_failed_count",
             "actions_in_progress_count",
             "actions_succeeded_count",
```

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/aio/__init__.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/aio/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/aio/_response_handlers_async.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/aio/_response_handlers_async.py`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/azure/ai/textanalytics/aio/_text_analytics_client_async.py` & `azure-ai-textanalytics-5.3.0b2/azure/ai/textanalytics/_text_analytics_client.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,162 +1,180 @@
 # ------------------------------------
 # Copyright (c) Microsoft Corporation.
 # Licensed under the MIT License.
 # ------------------------------------
 # pylint: disable=too-many-lines
 
-from typing import Union, Any, List, Dict, cast
-from azure.core.async_paging import AsyncItemPaged
-from azure.core.tracing.decorator_async import distributed_trace_async
+from typing import (
+    Union,
+    Any,
+    List,
+    Dict,
+    cast,
+    Optional,
+)
+from azure.core.paging import ItemPaged
+from azure.core.tracing.decorator import distributed_trace
 from azure.core.exceptions import HttpResponseError
 from azure.core.credentials import AzureKeyCredential
-from azure.core.credentials_async import AsyncTokenCredential
-from ._base_client_async import AsyncTextAnalyticsClientBase
-from .._request_handlers import (
+from azure.core.credentials import TokenCredential
+from ._base_client import TextAnalyticsClientBase, TextAnalyticsApiVersion
+from ._lro import AnalyzeActionsLROPoller, AnalyzeHealthcareEntitiesLROPoller, TextAnalysisLROPoller
+from ._request_handlers import (
     _validate_input,
     _determine_action_type,
 )
-from .._validate import validate_multiapi_args, check_for_unsupported_actions_types
-from .._response_handlers import (
+from ._validate import validate_multiapi_args, check_for_unsupported_actions_types
+from ._response_handlers import (
     process_http_response_error,
     entities_result,
     linked_entities_result,
     key_phrases_result,
     sentiment_result,
     language_result,
     pii_entities_result,
+    healthcare_paged_result,
+    analyze_paged_result,
     _get_result_from_continuation_token,
     dynamic_classification_result,
 )
-from ._response_handlers_async import healthcare_paged_result, analyze_paged_result
-from .._models import (
+
+from ._lro import (
+    TextAnalyticsOperationResourcePolling,
+    AnalyzeActionsLROPollingMethod,
+    AnalyzeHealthcareEntitiesLROPollingMethod,
+)
+from ._generated.models import HealthcareDocumentType, ClassificationType
+from ._models import (
     DetectLanguageInput,
     TextDocumentInput,
     DetectLanguageResult,
     RecognizeEntitiesResult,
     RecognizeLinkedEntitiesResult,
     ExtractKeyPhrasesResult,
     AnalyzeSentimentResult,
     DocumentError,
     RecognizePiiEntitiesResult,
     RecognizeEntitiesAction,
     RecognizePiiEntitiesAction,
-    ExtractKeyPhrasesAction,
-    _AnalyzeActionsType,
     RecognizeLinkedEntitiesAction,
+    ExtractKeyPhrasesAction,
     AnalyzeSentimentAction,
     AnalyzeHealthcareEntitiesResult,
     RecognizeCustomEntitiesAction,
     RecognizeCustomEntitiesResult,
     SingleLabelClassifyAction,
     MultiLabelClassifyAction,
     ClassifyDocumentResult,
     AnalyzeHealthcareEntitiesAction,
+    _AnalyzeActionsType,
     ExtractSummaryAction,
     ExtractSummaryResult,
-    AbstractSummaryAction,
-    AbstractSummaryResult,
+    AbstractiveSummaryAction,
+    AbstractiveSummaryResult,
     DynamicClassificationResult,
+    PiiEntityDomain,
+    PiiEntityCategory,
 )
-from .._check import is_language_api, string_index_type_compatibility
-from .._lro import TextAnalyticsOperationResourcePolling
-from ._lro_async import (
-    AsyncAnalyzeHealthcareEntitiesLROPollingMethod,
-    AsyncAnalyzeActionsLROPollingMethod,
-    AsyncAnalyzeHealthcareEntitiesLROPoller,
-    AsyncAnalyzeActionsLROPoller,
-    AsyncTextAnalysisLROPoller,
-)
+from ._check import is_language_api, string_index_type_compatibility
 
 
-AsyncAnalyzeActionsResponse = AsyncTextAnalysisLROPoller[
-    AsyncItemPaged[
+AnalyzeActionsResponse = TextAnalysisLROPoller[
+    ItemPaged[
         List[
             Union[
                 RecognizeEntitiesResult,
                 RecognizeLinkedEntitiesResult,
                 RecognizePiiEntitiesResult,
                 ExtractKeyPhrasesResult,
                 AnalyzeSentimentResult,
                 RecognizeCustomEntitiesResult,
                 ClassifyDocumentResult,
                 AnalyzeHealthcareEntitiesResult,
                 ExtractSummaryResult,
-                AbstractSummaryResult,
+                AbstractiveSummaryResult,
                 DocumentError,
             ]
         ]
     ]
 ]
 
 
-class TextAnalyticsClient(AsyncTextAnalyticsClientBase):
+class TextAnalyticsClient(TextAnalyticsClientBase):
     """The Language service API is a suite of natural language processing (NLP) skills built with the best-in-class
     Microsoft machine learning algorithms. The API can be used to analyze unstructured text for
     tasks such as sentiment analysis, key phrase extraction, entities recognition,
     and language detection, and more.
 
     Further documentation can be found in
     https://docs.microsoft.com/azure/cognitive-services/language-service/overview
 
     :param str endpoint: Supported Cognitive Services or Language resource
         endpoints (protocol and hostname, for example: 'https://<resource-name>.cognitiveservices.azure.com').
     :param credential: Credentials needed for the client to connect to Azure.
         This can be the an instance of AzureKeyCredential if using a Cognitive Services/Language API key
         or a token credential from :mod:`azure.identity`.
-    :type credential: ~azure.core.credentials.AzureKeyCredential or ~azure.core.credentials_async.AsyncTokenCredential
+    :type credential: ~azure.core.credentials.AzureKeyCredential or ~azure.core.credentials.TokenCredential
     :keyword str default_country_hint: Sets the default country_hint to use for all operations.
         Defaults to "US". If you don't want to use a country hint, pass the string "none".
     :keyword str default_language: Sets the default language to use for all operations.
         Defaults to "en".
     :keyword api_version: The API version of the service to use for requests. It defaults to the
         latest service version. Setting to an older version may result in reduced feature compatibility.
     :paramtype api_version: str or ~azure.ai.textanalytics.TextAnalyticsApiVersion
 
     .. admonition:: Example:
 
-        .. literalinclude:: ../samples/async_samples/sample_authentication_async.py
-            :start-after: [START create_ta_client_with_key_async]
-            :end-before: [END create_ta_client_with_key_async]
+        .. literalinclude:: ../samples/sample_authentication.py
+            :start-after: [START create_ta_client_with_key]
+            :end-before: [END create_ta_client_with_key]
             :language: python
             :dedent: 4
             :caption: Creating the TextAnalyticsClient with endpoint and API key.
 
-        .. literalinclude:: ../samples/async_samples/sample_authentication_async.py
-            :start-after: [START create_ta_client_with_aad_async]
-            :end-before: [END create_ta_client_with_aad_async]
+        .. literalinclude:: ../samples/sample_authentication.py
+            :start-after: [START create_ta_client_with_aad]
+            :end-before: [END create_ta_client_with_aad]
             :language: python
             :dedent: 4
             :caption: Creating the TextAnalyticsClient with endpoint and token credential from Azure Active Directory.
     """
 
     def __init__(
         self,
         endpoint: str,
-        credential: Union[AzureKeyCredential, AsyncTokenCredential],
-        **kwargs: Any
+        credential: Union[AzureKeyCredential, TokenCredential],
+        *,
+        default_language: Optional[str] = None,
+        default_country_hint: Optional[str] = None,
+        api_version: Optional[Union[str, TextAnalyticsApiVersion]] = None,
+        **kwargs: Any,
     ) -> None:
         super().__init__(
-            endpoint=endpoint, credential=credential, **kwargs
+            endpoint=endpoint, credential=credential, api_version=api_version, **kwargs
         )
-
-        self._default_language = kwargs.pop("default_language", "en")
-        self._default_country_hint = kwargs.pop("default_country_hint", "US")
-        self._string_code_unit = (
-            None if kwargs.get("api_version") == "v3.0" else "UnicodeCodePoint"
+        self._default_language = default_language if default_language is not None else "en"
+        self._default_country_hint = default_country_hint if default_country_hint is not None else "US"
+        self._string_index_type_default = (
+            None if api_version == "v3.0" else "UnicodeCodePoint"
         )
 
-    @distributed_trace_async
+    @distributed_trace
     @validate_multiapi_args(
         version_method_added="v3.0",
         args_mapping={"v3.1": ["disable_service_logs"]}
     )
-    async def detect_language(
+    def detect_language(
         self,
         documents: Union[List[str], List[DetectLanguageInput], List[Dict[str, str]]],
+        *,
+        country_hint: Optional[str] = None,
+        disable_service_logs: Optional[bool] = None,
+        model_version: Optional[str] = None,
+        show_stats: Optional[bool] = None,
         **kwargs: Any,
     ) -> List[Union[DetectLanguageResult, DocumentError]]:
         """Detect language for a batch of documents.
 
         Returns the detected language and a numeric score between zero and
         one. Scores close to one indicate 100% certainty that the identified
         language is true. See https://aka.ms/talangs for the list of enabled languages.
@@ -170,63 +188,60 @@
             `{"id": "1", "country_hint": "us", "text": "hello world"}`.
         :type documents:
             list[str] or list[~azure.ai.textanalytics.DetectLanguageInput] or list[dict[str, str]]
         :keyword str country_hint: Country of origin hint for the entire batch. Accepts two
             letter country codes specified by ISO 3166-1 alpha-2. Per-document
             country hints will take precedence over whole batch hints. Defaults to
             "US". If you don't want to use a country hint, pass the string "none".
-        :keyword str model_version: This value indicates which model will
-            be used for scoring, e.g. "latest", "2019-10-01". If a model-version
+        :keyword str model_version: Version of the model used on the service side for scoring,
+            e.g. "latest", "2019-10-01". If a model version
             is not specified, the API will default to the latest, non-preview version.
             See here for more info: https://aka.ms/text-analytics-model-versioning
         :keyword bool show_stats: If set to true, response will contain document
             level statistics in the `statistics` field of the document-level response.
         :keyword bool disable_service_logs: If set to true, you opt-out of having your text input
             logged on the service side for troubleshooting. By default, the Language service logs your
             input text for 48 hours, solely to allow for troubleshooting issues in providing you with
             the service's natural language processing functions. Setting this parameter to true,
             disables input logging and may limit our ability to remediate issues that occur. Please see
             Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
             additional details, and Microsoft Responsible AI principles at
             https://www.microsoft.com/ai/responsible-ai.
-        :return: The combined list of :class:`~azure.ai.textanalytics.DetectLanguageResult`
-            and :class:`~azure.ai.textanalytics.DocumentError` in the order the original documents
-            were passed in.
+        :return: The combined list of :class:`~azure.ai.textanalytics.DetectLanguageResult` and
+            :class:`~azure.ai.textanalytics.DocumentError` in the order the original documents were
+            passed in.
         :rtype: list[~azure.ai.textanalytics.DetectLanguageResult or ~azure.ai.textanalytics.DocumentError]
         :raises ~azure.core.exceptions.HttpResponseError or TypeError or ValueError:
 
         .. versionadded:: v3.1
             The *disable_service_logs* keyword argument.
 
         .. admonition:: Example:
 
-            .. literalinclude:: ../samples/async_samples/sample_detect_language_async.py
-                :start-after: [START detect_language_async]
-                :end-before: [END detect_language_async]
+            .. literalinclude:: ../samples/sample_detect_language.py
+                :start-after: [START detect_language]
+                :end-before: [END detect_language]
                 :language: python
                 :dedent: 4
                 :caption: Detecting language in a batch of documents.
         """
-        country_hint_arg = kwargs.pop("country_hint", None)
-        country_hint = (
-            country_hint_arg
-            if country_hint_arg is not None
+
+        country_hint_arg = (
+            country_hint
+            if country_hint is not None
             else self._default_country_hint
         )
-        docs = _validate_input(documents, "country_hint", country_hint)
-        model_version = kwargs.pop("model_version", None)
-        show_stats = kwargs.pop("show_stats", None)
-        disable_service_logs = kwargs.pop("disable_service_logs", None)
+        docs = _validate_input(documents, "country_hint", country_hint_arg)
 
         try:
             if is_language_api(self._api_version):
                 models = self._client.models(api_version=self._api_version)
                 return cast(
                     List[Union[DetectLanguageResult, DocumentError]],
-                    await self._client.analyze_text(
+                    self._client.analyze_text(
                         body=models.AnalyzeTextLanguageDetectionInput(
                             analysis_input={"documents": docs},
                             parameters=models.LanguageDetectionTaskParameters(
                                 logging_opt_out=disable_service_logs,
                                 model_version=model_version
                             )
                         ),
@@ -235,49 +250,55 @@
                         **kwargs
                     )
                 )
 
             # api_versions 3.0, 3.1
             return cast(
                 List[Union[DetectLanguageResult, DocumentError]],
-                await self._client.languages(
+                self._client.languages(
                     documents=docs,
                     model_version=model_version,
                     show_stats=show_stats,
                     logging_opt_out=disable_service_logs,
                     cls=kwargs.pop("cls", language_result),
                     **kwargs
                 )
             )
         except HttpResponseError as error:
             return process_http_response_error(error)
 
-    @distributed_trace_async
+    @distributed_trace
     @validate_multiapi_args(
         version_method_added="v3.0",
         args_mapping={"v3.1": ["string_index_type", "disable_service_logs"]}
     )
-    async def recognize_entities(
+    def recognize_entities(
         self,
         documents: Union[List[str], List[TextDocumentInput], List[Dict[str, str]]],
+        *,
+        disable_service_logs: Optional[bool] = None,
+        language: Optional[str] = None,
+        model_version: Optional[str] = None,
+        show_stats: Optional[bool] = None,
+        string_index_type: Optional[str] = None,
         **kwargs: Any,
     ) -> List[Union[RecognizeEntitiesResult, DocumentError]]:
         """Recognize entities for a batch of documents.
 
         Identifies and categorizes entities in your text as people, places,
         organizations, date/time, quantities, percentages, currencies, and more.
         For the list of supported entity types, check: https://aka.ms/taner
 
         See https://aka.ms/azsdk/textanalytics/data-limits for service data limits.
 
         :param documents: The set of documents to process as part of this batch.
             If you wish to specify the ID and language on a per-item basis you must
-            use as input a list[:class:`~azure.ai.textanalytics.TextDocumentInput`] or a list of
-            dict representations of :class:`~azure.ai.textanalytics.TextDocumentInput`, like
-            `{"id": "1", "language": "en", "text": "hello world"}`.
+            use as input a list[:class:`~azure.ai.textanalytics.TextDocumentInput`] or a list
+            of dict representations of :class:`~azure.ai.textanalytics.TextDocumentInput`,
+            like `{"id": "1", "language": "en", "text": "hello world"}`.
         :type documents:
             list[str] or list[~azure.ai.textanalytics.TextDocumentInput] or list[dict[str, str]]
         :keyword str language: The 2 letter ISO 639-1 representation of language for the
             entire batch. For example, use "en" for English; "es" for Spanish etc.
             If not set, uses "en" for English as default. Per-document language will
             take precedence over whole batch language. See https://aka.ms/talangs for
             supported languages in Language API.
@@ -296,89 +317,94 @@
             input text for 48 hours, solely to allow for troubleshooting issues in providing you with
             the service's natural language processing functions. Setting this parameter to true,
             disables input logging and may limit our ability to remediate issues that occur. Please see
             Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
             additional details, and Microsoft Responsible AI principles at
             https://www.microsoft.com/ai/responsible-ai.
         :return: The combined list of :class:`~azure.ai.textanalytics.RecognizeEntitiesResult` and
-            :class:`~azure.ai.textanalytics.DocumentError` in the order the original documents were
-            passed in.
+            :class:`~azure.ai.textanalytics.DocumentError` in the order the original documents
+            were passed in.
         :rtype: list[~azure.ai.textanalytics.RecognizeEntitiesResult or ~azure.ai.textanalytics.DocumentError]
         :raises ~azure.core.exceptions.HttpResponseError or TypeError or ValueError:
 
         .. versionadded:: v3.1
             The *disable_service_logs* and *string_index_type* keyword arguments.
 
         .. admonition:: Example:
 
-            .. literalinclude:: ../samples/async_samples/sample_recognize_entities_async.py
-                :start-after: [START recognize_entities_async]
-                :end-before: [END recognize_entities_async]
+            .. literalinclude:: ../samples/sample_recognize_entities.py
+                :start-after: [START recognize_entities]
+                :end-before: [END recognize_entities]
                 :language: python
                 :dedent: 4
                 :caption: Recognize entities in a batch of documents.
         """
-        language_arg = kwargs.pop("language", None)
-        language = language_arg if language_arg is not None else self._default_language
-        docs = _validate_input(documents, "language", language)
-        model_version = kwargs.pop("model_version", None)
-        show_stats = kwargs.pop("show_stats", None)
-        disable_service_logs = kwargs.pop("disable_service_logs", None)
-        string_index_type = kwargs.pop("string_index_type", self._string_code_unit)
+
+        language_arg = language if language is not None else self._default_language
+        docs = _validate_input(documents, "language", language_arg)
+        string_index_type_arg = string_index_type if string_index_type is not None else self._string_index_type_default
 
         try:
             if is_language_api(self._api_version):
                 models = self._client.models(api_version=self._api_version)
                 return cast(
                     List[Union[RecognizeEntitiesResult, DocumentError]],
-                    await self._client.analyze_text(
+                    self._client.analyze_text(
                         body=models.AnalyzeTextEntityRecognitionInput(
                             analysis_input={"documents": docs},
                             parameters=models.EntitiesTaskParameters(
                                 logging_opt_out=disable_service_logs,
                                 model_version=model_version,
-                                string_index_type=string_index_type_compatibility(string_index_type)
+                                string_index_type=string_index_type_compatibility(string_index_type_arg)
                             )
                         ),
                         show_stats=show_stats,
                         cls=kwargs.pop("cls", entities_result),
                         **kwargs
                     )
                 )
 
             # api_versions 3.0, 3.1
             return cast(
                 List[Union[RecognizeEntitiesResult, DocumentError]],
-                await self._client.entities_recognition_general(
+                self._client.entities_recognition_general(
                     documents=docs,
                     model_version=model_version,
                     show_stats=show_stats,
-                    string_index_type=string_index_type,
+                    string_index_type=string_index_type_arg,
                     logging_opt_out=disable_service_logs,
                     cls=kwargs.pop("cls", entities_result),
                     **kwargs,
                 )
             )
         except HttpResponseError as error:
             return process_http_response_error(error)
 
-    @distributed_trace_async
+    @distributed_trace
     @validate_multiapi_args(
         version_method_added="v3.1"
     )
-    async def recognize_pii_entities(
+    def recognize_pii_entities(
         self,
         documents: Union[List[str], List[TextDocumentInput], List[Dict[str, str]]],
+        *,
+        categories_filter: Optional[List[Union[str, PiiEntityCategory]]] = None,
+        disable_service_logs: Optional[bool] = None,
+        domain_filter: Optional[Union[str, PiiEntityDomain]] = None,
+        language: Optional[str] = None,
+        model_version: Optional[str] = None,
+        show_stats: Optional[bool] = None,
+        string_index_type: Optional[str] = None,
         **kwargs: Any,
     ) -> List[Union[RecognizePiiEntitiesResult, DocumentError]]:
         """Recognize entities containing personal information for a batch of documents.
 
         Returns a list of personal information entities ("SSN",
         "Bank Account", etc) in the document.  For the list of supported entity types,
-        check https://aka.ms/tanerpii
+        check https://aka.ms/azsdk/language/pii
 
         See https://aka.ms/azsdk/textanalytics/data-limits for service data limits.
 
         :param documents: The set of documents to process as part of this batch.
             If you wish to specify the ID and language on a per-item basis you must
             use as input a list[:class:`~azure.ai.textanalytics.TextDocumentInput`] or a list of
             dict representations of :class:`~azure.ai.textanalytics.TextDocumentInput`, like
@@ -394,15 +420,15 @@
             be used for scoring, e.g. "latest", "2019-10-01". If a model-version
             is not specified, the API will default to the latest, non-preview version.
             See here for more info: https://aka.ms/text-analytics-model-versioning
         :keyword bool show_stats: If set to true, response will contain document
             level statistics in the `statistics` field of the document-level response.
         :keyword domain_filter: Filters the response entities to ones only included in the specified domain.
             I.e., if set to 'phi', will only return entities in the Protected Healthcare Information domain.
-            See https://aka.ms/tanerpii for more information.
+            See https://aka.ms/azsdk/language/pii for more information.
         :paramtype domain_filter: str or ~azure.ai.textanalytics.PiiEntityDomain
         :keyword categories_filter: Instead of filtering over all PII entity categories, you can pass in a list of
             the specific PII entity categories you want to filter out. For example, if you only want to filter out
             U.S. social security numbers in a document, you can pass in
             `[PiiEntityCategory.US_SOCIAL_SECURITY_NUMBER]` for this kwarg.
         :paramtype categories_filter: list[str or ~azure.ai.textanalytics.PiiEntityCategory]
         :keyword str string_index_type: Specifies the method used to interpret string offsets.
@@ -430,72 +456,72 @@
             .. literalinclude:: ../samples/sample_recognize_pii_entities.py
                 :start-after: [START recognize_pii_entities]
                 :end-before: [END recognize_pii_entities]
                 :language: python
                 :dedent: 4
                 :caption: Recognize personally identifiable information entities in a batch of documents.
         """
-        language_arg = kwargs.pop("language", None)
-        language = language_arg if language_arg is not None else self._default_language
-        docs = _validate_input(documents, "language", language)
-        model_version = kwargs.pop("model_version", None)
-        show_stats = kwargs.pop("show_stats", None)
-        domain_filter = kwargs.pop("domain_filter", None)
-        categories_filter = kwargs.pop("categories_filter", None)
-        string_index_type = kwargs.pop("string_index_type", self._string_code_unit)
-        disable_service_logs = kwargs.pop("disable_service_logs", None)
+        language_arg = language if language is not None else self._default_language
+        docs = _validate_input(documents, "language", language_arg)
+        string_index_type_arg = string_index_type if string_index_type is not None else self._string_index_type_default
 
         try:
             if is_language_api(self._api_version):
                 models = self._client.models(api_version=self._api_version)
                 return cast(
                     List[Union[RecognizePiiEntitiesResult, DocumentError]],
-                    await self._client.analyze_text(
+                    self._client.analyze_text(
                         body=models.AnalyzeTextPiiEntitiesRecognitionInput(
                             analysis_input={"documents": docs},
                             parameters=models.PiiTaskParameters(
                                 logging_opt_out=disable_service_logs,
                                 model_version=model_version,
                                 domain=domain_filter,
                                 pii_categories=categories_filter,
-                                string_index_type=string_index_type_compatibility(string_index_type)
+                                string_index_type=string_index_type_compatibility(string_index_type_arg)
                             )
                         ),
                         show_stats=show_stats,
                         cls=kwargs.pop("cls", pii_entities_result),
                         **kwargs
                     )
                 )
 
             # api_versions 3.0, 3.1
             return cast(
                 List[Union[RecognizePiiEntitiesResult, DocumentError]],
-                await self._client.entities_recognition_pii(
+                self._client.entities_recognition_pii(
                     documents=docs,
                     model_version=model_version,
                     show_stats=show_stats,
                     domain=domain_filter,
                     pii_categories=categories_filter,
                     logging_opt_out=disable_service_logs,
-                    string_index_type=string_index_type,
+                    string_index_type=string_index_type_arg,
                     cls=kwargs.pop("cls", pii_entities_result),
                     **kwargs
                 )
             )
         except HttpResponseError as error:
             return process_http_response_error(error)
 
-    @distributed_trace_async
+    @distributed_trace
     @validate_multiapi_args(
         version_method_added="v3.0",
         args_mapping={"v3.1": ["string_index_type", "disable_service_logs"]}
     )
-    async def recognize_linked_entities(
+    def recognize_linked_entities(
         self,
         documents: Union[List[str], List[TextDocumentInput], List[Dict[str, str]]],
+        *,
+        disable_service_logs: Optional[bool] = None,
+        language: Optional[str] = None,
+        model_version: Optional[str] = None,
+        show_stats: Optional[bool] = None,
+        string_index_type: Optional[str] = None,
         **kwargs: Any,
     ) -> List[Union[RecognizeLinkedEntitiesResult, DocumentError]]:
         """Recognize linked entities from a well-known knowledge base for a batch of documents.
 
         Identifies and disambiguates the identity of each entity found in text (for example,
         determining whether an occurrence of the word Mars refers to the planet, or to the
         Roman god of war). Recognized entities are associated with URLs to a well-known
@@ -540,74 +566,302 @@
         :raises ~azure.core.exceptions.HttpResponseError or TypeError or ValueError:
 
         .. versionadded:: v3.1
             The *disable_service_logs* and *string_index_type* keyword arguments.
 
         .. admonition:: Example:
 
-            .. literalinclude:: ../samples/async_samples/sample_recognize_linked_entities_async.py
-                :start-after: [START recognize_linked_entities_async]
-                :end-before: [END recognize_linked_entities_async]
+            .. literalinclude:: ../samples/sample_recognize_linked_entities.py
+                :start-after: [START recognize_linked_entities]
+                :end-before: [END recognize_linked_entities]
                 :language: python
                 :dedent: 4
                 :caption: Recognize linked entities in a batch of documents.
         """
-        language_arg = kwargs.pop("language", None)
-        language = language_arg if language_arg is not None else self._default_language
-        docs = _validate_input(documents, "language", language)
-        model_version = kwargs.pop("model_version", None)
-        show_stats = kwargs.pop("show_stats", None)
-        disable_service_logs = kwargs.pop("disable_service_logs", None)
-        string_index_type = kwargs.pop("string_index_type", self._string_code_unit)
+
+        language_arg = language if language is not None else self._default_language
+        docs = _validate_input(documents, "language", language_arg)
+        string_index_type_arg = string_index_type if string_index_type is not None else self._string_index_type_default
 
         try:
             if is_language_api(self._api_version):
                 models = self._client.models(api_version=self._api_version)
                 return cast(
                     List[Union[RecognizeLinkedEntitiesResult, DocumentError]],
-                    await self._client.analyze_text(
+                    self._client.analyze_text(
                         body=models.AnalyzeTextEntityLinkingInput(
                             analysis_input={"documents": docs},
                             parameters=models.EntityLinkingTaskParameters(
                                 logging_opt_out=disable_service_logs,
                                 model_version=model_version,
-                                string_index_type=string_index_type_compatibility(string_index_type)
+                                string_index_type=string_index_type_compatibility(string_index_type_arg)
                             )
                         ),
                         show_stats=show_stats,
                         cls=kwargs.pop("cls", linked_entities_result),
                         **kwargs
                     )
                 )
 
             # api_versions 3.0, 3.1
             return cast(
                 List[Union[RecognizeLinkedEntitiesResult, DocumentError]],
-                await self._client.entities_linking(
+                self._client.entities_linking(
                     documents=docs,
                     logging_opt_out=disable_service_logs,
                     model_version=model_version,
-                    string_index_type=string_index_type,
+                    string_index_type=string_index_type_arg,
                     show_stats=show_stats,
                     cls=kwargs.pop("cls", linked_entities_result),
                     **kwargs
                 )
             )
         except HttpResponseError as error:
             return process_http_response_error(error)
 
-    @distributed_trace_async
+    # pylint: disable=unused-argument
+    def _healthcare_result_callback(
+        self, raw_response, deserialized, doc_id_order, task_id_order=None, show_stats=False, bespoke=False
+    ):
+        if deserialized is None:
+            models = self._client.models(api_version=self._api_version)
+            response_cls = \
+                models.AnalyzeTextJobState if is_language_api(self._api_version) else models.HealthcareJobState
+            deserialized = response_cls.deserialize(raw_response)
+        return healthcare_paged_result(
+            doc_id_order,
+            self._client.analyze_text_job_status if is_language_api(self._api_version) else self._client.health_status,
+            raw_response,
+            deserialized,
+            show_stats=show_stats,
+        )
+
+    @distributed_trace
+    @validate_multiapi_args(
+        version_method_added="v3.1",
+        args_mapping={
+            "2022-10-01-preview": ["fhir_version", "document_type"],
+            "2022-05-01": ["display_name"]
+        }
+    )
+    def begin_analyze_healthcare_entities(
+        self,
+        documents: Union[List[str], List[TextDocumentInput], List[Dict[str, str]]],
+        *,
+        continuation_token: Optional[str] = None,
+        disable_service_logs: Optional[bool] = None,
+        display_name: Optional[str] = None,
+        document_type: Optional[Union[str, HealthcareDocumentType]] = None,
+        fhir_version: Optional[str] = None,
+        language: Optional[str] = None,
+        model_version: Optional[str] = None,
+        polling_interval: Optional[int] = None,
+        show_stats: Optional[bool] = None,
+        string_index_type: Optional[str] = None,
+        **kwargs: Any,
+    ) -> AnalyzeHealthcareEntitiesLROPoller[ItemPaged[Union[AnalyzeHealthcareEntitiesResult, DocumentError]]]:
+        """Analyze healthcare entities and identify relationships between these entities in a batch of documents.
+
+        Entities are associated with references that can be found in existing knowledge bases,
+        such as UMLS, CHV, MSH, etc.
+
+        We also extract the relations found between entities, for example in "The subject took 100 mg of ibuprofen",
+        we would extract the relationship between the "100 mg" dosage and the "ibuprofen" medication.
+
+        :param documents: The set of documents to process as part of this batch.
+            If you wish to specify the ID and language on a per-item basis you must
+            use as input a list[:class:`~azure.ai.textanalytics.TextDocumentInput`] or a list of
+            dict representations of :class:`~azure.ai.textanalytics.TextDocumentInput`, like
+            `{"id": "1", "language": "en", "text": "hello world"}`.
+        :type documents:
+            list[str] or list[~azure.ai.textanalytics.TextDocumentInput] or list[dict[str, str]]
+        :keyword str model_version: This value indicates which model will
+            be used for scoring, e.g. "latest", "2019-10-01". If a model-version
+            is not specified, the API will default to the latest, non-preview version.
+            See here for more info: https://aka.ms/text-analytics-model-versioning
+        :keyword bool show_stats: If set to true, response will contain document level statistics.
+        :keyword str language: The 2 letter ISO 639-1 representation of language for the
+            entire batch. For example, use "en" for English; "es" for Spanish etc.
+            For automatic language detection, use "auto" (Only supported by API version
+            2022-10-01-preview and newer). If not set, uses "en" for English as default.
+            Per-document language will take precedence over whole batch language.
+            See https://aka.ms/talangs for supported languages in Language API.
+        :keyword str display_name: An optional display name to set for the requested analysis.
+        :keyword str string_index_type: Specifies the method used to interpret string offsets.
+            `UnicodeCodePoint`, the Python encoding, is the default. To override the Python default,
+            you can also pass in `Utf16CodeUnit` or `TextElement_v8`. For additional information
+            see https://aka.ms/text-analytics-offsets
+        :keyword int polling_interval: Waiting time between two polls for LRO operations
+            if no Retry-After header is present. Defaults to 5 seconds.
+        :keyword str continuation_token:
+            Call `continuation_token()` on the poller object to save the long-running operation (LRO)
+            state into an opaque token. Pass the value as the `continuation_token` keyword argument
+            to restart the LRO from a saved state.
+        :keyword bool disable_service_logs: Defaults to true, meaning that the Language service will not log your
+            input text on the service side for troubleshooting. If set to False, the Language service logs your
+            input text for 48 hours, solely to allow for troubleshooting issues in providing you with
+            the service's natural language processing functions. Please see
+            Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
+            additional details, and Microsoft Responsible AI principles at
+            https://www.microsoft.com/ai/responsible-ai.
+        :keyword str fhir_version: The FHIR Spec version that the result will use to format the fhir_bundle
+            on the result object. For additional information see https://www.hl7.org/fhir/overview.html.
+            The only acceptable values to pass in are None and "4.0.1". The default value is None.
+        :keyword document_type: Document type that can be provided as input for Fhir Documents. Expect to
+            have fhir_version provided when used. Behavior of using None enum is the same as not using the
+            document_type parameter. Known values are: "None", "ClinicalTrial", "DischargeSummary",
+            "ProgressNote", "HistoryAndPhysical", "Consult", "Imaging", "Pathology", and "ProcedureNote".
+        :paramtype document_type: str or ~azure.ai.textanalytics.HealthcareDocumentType
+        :return: An instance of an AnalyzeHealthcareEntitiesLROPoller. Call `result()` on the this
+            object to return a heterogeneous pageable of
+            :class:`~azure.ai.textanalytics.AnalyzeHealthcareEntitiesResult` and
+            :class:`~azure.ai.textanalytics.DocumentError`.
+        :rtype:
+            ~azure.ai.textanalytics.AnalyzeHealthcareEntitiesLROPoller[~azure.core.paging.ItemPaged[
+            ~azure.ai.textanalytics.AnalyzeHealthcareEntitiesResult or ~azure.ai.textanalytics.DocumentError]]
+        :raises ~azure.core.exceptions.HttpResponseError or TypeError or ValueError:
+
+        .. versionadded:: v3.1
+            The *begin_analyze_healthcare_entities* client method.
+        .. versionadded:: 2022-05-01
+            The *display_name* keyword argument.
+        .. versionadded:: 2022-10-01-preview
+            The *fhir_version* and *document_type* keyword arguments.
+
+        .. admonition:: Example:
+
+            .. literalinclude:: ../samples/sample_analyze_healthcare_entities.py
+                :start-after: [START analyze_healthcare_entities]
+                :end-before: [END analyze_healthcare_entities]
+                :language: python
+                :dedent: 4
+                :caption: Recognize healthcare entities in a batch of documents.
+        """
+
+        language_arg = language if language is not None else self._default_language
+        polling_interval_arg = polling_interval if polling_interval is not None else 5
+        string_index_type_arg = string_index_type if string_index_type is not None else self._string_index_type_default
+
+        if continuation_token:
+            return cast(
+                AnalyzeHealthcareEntitiesLROPoller[
+                    ItemPaged[Union[AnalyzeHealthcareEntitiesResult, DocumentError]]
+                ],
+                _get_result_from_continuation_token(
+                    self._client._client,  # pylint: disable=protected-access
+                    continuation_token,
+                    AnalyzeHealthcareEntitiesLROPoller,
+                    AnalyzeHealthcareEntitiesLROPollingMethod(
+                        text_analytics_client=self._client,
+                        timeout=polling_interval_arg,
+                        **kwargs
+                    ),
+                    self._healthcare_result_callback
+                )
+            )
+
+        docs = _validate_input(documents, "language", language_arg)
+        doc_id_order = [doc.get("id") for doc in docs]
+        my_cls = kwargs.pop(
+            "cls",
+            lambda pipeline_response, deserialized, _: self._healthcare_result_callback(
+                pipeline_response, deserialized, doc_id_order, show_stats=show_stats
+            ),
+        )
+        models = self._client.models(api_version=self._api_version)
+
+        try:
+            if is_language_api(self._api_version):
+                input_docs = models.MultiLanguageAnalysisInput(
+                    documents=docs
+                )
+                return cast(
+                    AnalyzeHealthcareEntitiesLROPoller[
+                        ItemPaged[Union[AnalyzeHealthcareEntitiesResult, DocumentError]]
+                    ],
+                    self._client.begin_analyze_text_submit_job(  # type: ignore
+                        body=models.AnalyzeTextJobsInput(
+                            analysis_input=input_docs,
+                            display_name=display_name,
+                            tasks=[
+                                models.HealthcareLROTask(
+                                    task_name="0",
+                                    parameters=models.HealthcareTaskParameters(
+                                        model_version=model_version,
+                                        logging_opt_out=disable_service_logs,
+                                        string_index_type=string_index_type_compatibility(string_index_type_arg),
+                                        fhir_version=fhir_version,
+                                        document_type=document_type,
+                                    )
+                                )
+                            ]
+                        ),
+                        cls=my_cls,
+                        polling=AnalyzeHealthcareEntitiesLROPollingMethod(
+                            text_analytics_client=self._client,
+                            timeout=polling_interval_arg,
+                            show_stats=show_stats,
+                            doc_id_order=doc_id_order,
+                            lro_algorithms=[
+                                TextAnalyticsOperationResourcePolling(
+                                    show_stats=show_stats,
+                                )
+                            ],
+                            **kwargs
+                        ),
+                        continuation_token=continuation_token,
+                        poller_cls=AnalyzeHealthcareEntitiesLROPoller,
+                        **kwargs
+                    )
+                )
+
+            # v3.1
+            return cast(
+                AnalyzeHealthcareEntitiesLROPoller[
+                    ItemPaged[Union[AnalyzeHealthcareEntitiesResult, DocumentError]]
+                ],
+                self._client.begin_health(
+                    docs,
+                    model_version=model_version,
+                    string_index_type=string_index_type_arg,
+                    logging_opt_out=disable_service_logs,
+                    cls=my_cls,
+                    polling=AnalyzeHealthcareEntitiesLROPollingMethod(
+                        text_analytics_client=self._client,
+                        timeout=polling_interval_arg,
+                        doc_id_order=doc_id_order,
+                        show_stats=show_stats,
+                        lro_algorithms=[
+                            TextAnalyticsOperationResourcePolling(
+                                show_stats=show_stats,
+                            )
+                        ],
+                        **kwargs
+                    ),
+                    continuation_token=continuation_token,
+                    **kwargs
+                )
+            )
+        except HttpResponseError as error:
+            return process_http_response_error(error)
+
+    @distributed_trace
     @validate_multiapi_args(
         version_method_added="v3.0",
         args_mapping={"v3.1": ["disable_service_logs"]}
     )
-    async def extract_key_phrases(
+    def extract_key_phrases(
         self,
         documents: Union[List[str], List[TextDocumentInput], List[Dict[str, str]]],
-        **kwargs: Any,
+        *,
+        disable_service_logs: Optional[bool] = None,
+        language: Optional[str] = None,
+        model_version: Optional[str] = None,
+        show_stats: Optional[bool] = None,
+        **kwargs: Any
     ) -> List[Union[ExtractKeyPhrasesResult, DocumentError]]:
         """Extract key phrases from a batch of documents.
 
         Returns a list of strings denoting the key phrases in the input
         text. For example, for the input text "The food was delicious and there
         were wonderful staff", the API returns the main talking points: "food"
         and "wonderful staff"
@@ -647,34 +901,31 @@
         :raises ~azure.core.exceptions.HttpResponseError or TypeError or ValueError:
 
         .. versionadded:: v3.1
             The *disable_service_logs* keyword argument.
 
         .. admonition:: Example:
 
-            .. literalinclude:: ../samples/async_samples/sample_extract_key_phrases_async.py
-                :start-after: [START extract_key_phrases_async]
-                :end-before: [END extract_key_phrases_async]
+            .. literalinclude:: ../samples/sample_extract_key_phrases.py
+                :start-after: [START extract_key_phrases]
+                :end-before: [END extract_key_phrases]
                 :language: python
                 :dedent: 4
                 :caption: Extract the key phrases in a batch of documents.
         """
-        language_arg = kwargs.pop("language", None)
-        language = language_arg if language_arg is not None else self._default_language
-        docs = _validate_input(documents, "language", language)
-        model_version = kwargs.pop("model_version", None)
-        show_stats = kwargs.pop("show_stats", None)
-        disable_service_logs = kwargs.pop("disable_service_logs", None)
+
+        language_arg = language if language is not None else self._default_language
+        docs = _validate_input(documents, "language", language_arg)
 
         try:
             if is_language_api(self._api_version):
                 models = self._client.models(api_version=self._api_version)
                 return cast(
                     List[Union[ExtractKeyPhrasesResult, DocumentError]],
-                    await self._client.analyze_text(
+                    self._client.analyze_text(
                         body=models.AnalyzeTextKeyPhraseExtractionInput(
                             analysis_input={"documents": docs},
                             parameters=models.KeyPhraseTaskParameters(
                                 logging_opt_out=disable_service_logs,
                                 model_version=model_version,
                             )
                         ),
@@ -683,34 +934,41 @@
                         **kwargs
                     )
                 )
 
             # api_versions 3.0, 3.1
             return cast(
                 List[Union[ExtractKeyPhrasesResult, DocumentError]],
-                await self._client.key_phrases(
+                self._client.key_phrases(
                     documents=docs,
                     model_version=model_version,
                     show_stats=show_stats,
                     logging_opt_out=disable_service_logs,
                     cls=kwargs.pop("cls", key_phrases_result),
                     **kwargs
                 )
             )
         except HttpResponseError as error:
             return process_http_response_error(error)
 
-    @distributed_trace_async
+    @distributed_trace
     @validate_multiapi_args(
         version_method_added="v3.0",
         args_mapping={"v3.1": ["show_opinion_mining", "disable_service_logs", "string_index_type"]}
     )
-    async def analyze_sentiment(
+    def analyze_sentiment(
         self,
         documents: Union[List[str], List[TextDocumentInput], List[Dict[str, str]]],
+        *,
+        disable_service_logs: Optional[bool] = None,
+        language: Optional[str] = None,
+        model_version: Optional[str] = None,
+        show_opinion_mining: Optional[bool] = None,
+        show_stats: Optional[bool] = None,
+        string_index_type: Optional[str] = None,
         **kwargs: Any,
     ) -> List[Union[AnalyzeSentimentResult, DocumentError]]:
         """Analyze sentiment for a batch of documents. Turn on opinion mining with `show_opinion_mining`.
 
         Returns a sentiment prediction, as well as sentiment scores for
         each sentiment class (Positive, Negative, and Neutral) for the document
         and each sentence within it.
@@ -760,295 +1018,64 @@
         :raises ~azure.core.exceptions.HttpResponseError or TypeError or ValueError:
 
         .. versionadded:: v3.1
             The *show_opinion_mining*, *disable_service_logs*, and *string_index_type* keyword arguments.
 
         .. admonition:: Example:
 
-            .. literalinclude:: ../samples/async_samples/sample_analyze_sentiment_async.py
-                :start-after: [START analyze_sentiment_async]
-                :end-before: [END analyze_sentiment_async]
+            .. literalinclude:: ../samples/sample_analyze_sentiment.py
+                :start-after: [START analyze_sentiment]
+                :end-before: [END analyze_sentiment]
                 :language: python
                 :dedent: 4
                 :caption: Analyze sentiment in a batch of documents.
         """
-        language_arg = kwargs.pop("language", None)
-        language = language_arg if language_arg is not None else self._default_language
-        docs = _validate_input(documents, "language", language)
-        model_version = kwargs.pop("model_version", None)
-        show_stats = kwargs.pop("show_stats", None)
-        show_opinion_mining = kwargs.pop("show_opinion_mining", None)
-        disable_service_logs = kwargs.pop("disable_service_logs", None)
-        string_index_type = kwargs.pop("string_index_type", self._string_code_unit)
+
+        language_arg = language if language is not None else self._default_language
+        docs = _validate_input(documents, "language", language_arg)
+        string_index_type_arg = string_index_type if string_index_type is not None else self._string_index_type_default
 
         try:
             if is_language_api(self._api_version):
                 models = self._client.models(api_version=self._api_version)
                 return cast(
                     List[Union[AnalyzeSentimentResult, DocumentError]],
-                    await self._client.analyze_text(
+                    self._client.analyze_text(
                         body=models.AnalyzeTextSentimentAnalysisInput(
                             analysis_input={"documents": docs},
                             parameters=models.SentimentAnalysisTaskParameters(
                                 logging_opt_out=disable_service_logs,
                                 model_version=model_version,
-                                string_index_type=string_index_type_compatibility(string_index_type),
+                                string_index_type=string_index_type_compatibility(string_index_type_arg),
                                 opinion_mining=show_opinion_mining,
                             )
                         ),
                         show_stats=show_stats,
                         cls=kwargs.pop("cls", sentiment_result),
                         **kwargs
                     )
                 )
 
             # api_versions 3.0, 3.1
             return cast(
                 List[Union[AnalyzeSentimentResult, DocumentError]],
-                await self._client.sentiment(
+                self._client.sentiment(
                     documents=docs,
                     logging_opt_out=disable_service_logs,
                     model_version=model_version,
-                    string_index_type=string_index_type,
+                    string_index_type=string_index_type_arg,
                     opinion_mining=show_opinion_mining,
                     show_stats=show_stats,
                     cls=kwargs.pop("cls", sentiment_result),
                     **kwargs
                 )
             )
         except HttpResponseError as error:
             return process_http_response_error(error)
 
-    # pylint: disable=unused-argument
-    def _healthcare_result_callback(
-        self, raw_response, deserialized, doc_id_order, task_id_order=None, show_stats=False, bespoke=False
-    ):
-        if deserialized is None:
-            models = self._client.models(api_version=self._api_version)
-            response_cls = \
-                models.AnalyzeTextJobState if is_language_api(self._api_version) else models.HealthcareJobState
-            deserialized = response_cls.deserialize(raw_response)
-        return healthcare_paged_result(
-            doc_id_order,
-            self._client.analyze_text_job_status if is_language_api(self._api_version) else self._client.health_status,
-            raw_response,
-            deserialized,
-            show_stats=show_stats,
-        )
-
-    @distributed_trace_async
-    @validate_multiapi_args(
-        version_method_added="v3.1",
-        args_mapping={
-            "2022-10-01-preview": ["fhir_version", "document_type", "autodetect_default_language"],
-            "2022-05-01": ["display_name"]
-        }
-    )
-    async def begin_analyze_healthcare_entities(
-        self,
-        documents: Union[List[str], List[TextDocumentInput], List[Dict[str, str]]],
-        **kwargs: Any,
-    ) -> AsyncAnalyzeHealthcareEntitiesLROPoller[
-        AsyncItemPaged[Union[AnalyzeHealthcareEntitiesResult, DocumentError]]
-    ]:
-        """Analyze healthcare entities and identify relationships between these entities in a batch of documents.
-
-        Entities are associated with references that can be found in existing knowledge bases,
-        such as UMLS, CHV, MSH, etc.
-
-        We also extract the relations found between entities, for example in "The subject took 100 mg of ibuprofen",
-        we would extract the relationship between the "100 mg" dosage and the "ibuprofen" medication.
-
-        :param documents: The set of documents to process as part of this batch.
-            If you wish to specify the ID and language on a per-item basis you must
-            use as input a list[:class:`~azure.ai.textanalytics.TextDocumentInput`] or a list of
-            dict representations of :class:`~azure.ai.textanalytics.TextDocumentInput`, like
-            `{"id": "1", "language": "en", "text": "hello world"}`.
-        :type documents:
-            list[str] or list[~azure.ai.textanalytics.TextDocumentInput] or list[dict[str, str]]
-        :keyword str model_version: This value indicates which model will
-            be used for scoring, e.g. "latest", "2019-10-01". If a model-version
-            is not specified, the API will default to the latest, non-preview version.
-            See here for more info: https://aka.ms/text-analytics-model-versioning
-        :keyword bool show_stats: If set to true, response will contain document level statistics.
-        :keyword str language: The 2 letter ISO 639-1 representation of language for the
-            entire batch. For example, use "en" for English; "es" for Spanish etc.
-            For automatic language detection, use "auto" (Only supported by API version
-            2022-10-01-preview and newer). If not set, uses "en" for English as default.
-            Per-document language will take precedence over whole batch language.
-            See https://aka.ms/talangs for supported languages in Language API.
-        :keyword str autodetect_default_language: Default/fallback language to use for documents requesting
-            automatic language detection.
-        :keyword str display_name: An optional display name to set for the requested analysis.
-        :keyword str string_index_type: Specifies the method used to interpret string offsets.
-            Can be one of 'UnicodeCodePoint' (default), 'Utf16CodeUnit', or 'TextElement_v8'.
-            For additional information see https://aka.ms/text-analytics-offsets
-        :keyword int polling_interval: Waiting time between two polls for LRO operations
-            if no Retry-After header is present. Defaults to 5 seconds.
-        :keyword str continuation_token:
-            Call `continuation_token()` on the poller object to save the long-running operation (LRO)
-            state into an opaque token. Pass the value as the `continuation_token` keyword argument
-            to restart the LRO from a saved state.
-        :keyword bool disable_service_logs: Defaults to true, meaning that the Language service will not log your
-            input text on the service side for troubleshooting. If set to False, the Language service logs your
-            input text for 48 hours, solely to allow for troubleshooting issues in providing you with
-            the Text Analytics natural language processing functions. Please see
-            Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
-            additional details, and Microsoft Responsible AI principles at
-            https://www.microsoft.com/ai/responsible-ai.
-        :keyword str fhir_version: The FHIR Spec version that the result will use to format the fhir_bundle
-            on the result object. For additional information see https://www.hl7.org/fhir/overview.html.
-            The only acceptable values to pass in are None and "4.0.1". The default value is None.
-        :keyword document_type: Document type that can be provided as input for Fhir Documents. Expect to
-            have fhir_version provided when used. Behavior of using None enum is the same as not using the
-            document_type parameter. Known values are: "None", "ClinicalTrial", "DischargeSummary",
-            "ProgressNote", "HistoryAndPhysical", "Consult", "Imaging", "Pathology", and "ProcedureNote".
-        :paramtype document_type: str or ~azure.ai.textanalytics.HealthcareDocumentType
-        :return: An instance of an AsyncAnalyzeHealthcareEntitiesLROPoller. Call `result()` on the poller
-            object to return a heterogeneous pageable of
-            :class:`~azure.ai.textanalytics.AnalyzeHealthcareEntitiesResult` and
-            :class:`~azure.ai.textanalytics.DocumentError`.
-        :rtype:
-            ~azure.ai.textanalytics.aio.AsyncAnalyzeHealthcareEntitiesLROPoller[~azure.core.async_paging.AsyncItemPaged[
-            ~azure.ai.textanalytics.AnalyzeHealthcareEntitiesResult or ~azure.ai.textanalytics.DocumentError]]
-        :raises ~azure.core.exceptions.HttpResponseError or TypeError or ValueError:
-
-        .. versionadded:: v3.1
-            The *begin_analyze_healthcare_entities* client method.
-        .. versionadded:: 2022-05-01
-            The *display_name* keyword argument.
-        .. versionadded:: 2022-10-01-preview
-            The *fhir_version*, *document_type*, and *autodetect_default_language* keyword arguments.
-
-        .. admonition:: Example:
-
-            .. literalinclude:: ../samples/async_samples/sample_analyze_healthcare_entities_async.py
-                :start-after: [START analyze_healthcare_entities_async]
-                :end-before: [END analyze_healthcare_entities_async]
-                :language: python
-                :dedent: 4
-                :caption: Analyze healthcare entities in a batch of documents.
-        """
-        language_arg = kwargs.pop("language", None)
-        language = language_arg if language_arg is not None else self._default_language
-        model_version = kwargs.pop("model_version", None)
-        show_stats = kwargs.pop("show_stats", None)
-        polling_interval = kwargs.pop("polling_interval", 5)
-        continuation_token = kwargs.pop("continuation_token", None)
-        string_index_type = kwargs.pop("string_index_type", self._string_code_unit)
-        disable_service_logs = kwargs.pop("disable_service_logs", None)
-        display_name = kwargs.pop("display_name", None)
-        fhir_version = kwargs.pop("fhir_version", None)
-        document_type = kwargs.pop("document_type", None)
-        autodetect_default_language = kwargs.pop("autodetect_default_language", None)
-
-        if continuation_token:
-            return cast(
-                AsyncAnalyzeHealthcareEntitiesLROPoller[
-                    AsyncItemPaged[Union[AnalyzeHealthcareEntitiesResult, DocumentError]]
-                ],
-                _get_result_from_continuation_token(
-                    self._client._client,  # pylint: disable=protected-access
-                    continuation_token,
-                    AsyncAnalyzeHealthcareEntitiesLROPoller,
-                    AsyncAnalyzeHealthcareEntitiesLROPollingMethod(
-                        text_analytics_client=self._client,
-                        timeout=polling_interval,
-                        **kwargs
-                    ),
-                    self._healthcare_result_callback
-                )
-            )
-
-        docs = _validate_input(documents, "language", language)
-        doc_id_order = [doc.get("id") for doc in docs]
-        my_cls = kwargs.pop(
-            "cls",
-            lambda pipeline_response, deserialized, _: self._healthcare_result_callback(
-                pipeline_response, deserialized, doc_id_order, show_stats=show_stats
-            ),
-        )
-        models = self._client.models(api_version=self._api_version)
-
-        try:
-            if is_language_api(self._api_version):
-                docs = models.MultiLanguageAnalysisInput(
-                    documents=_validate_input(documents, "language", language)
-                )
-                return cast(
-                    AsyncAnalyzeHealthcareEntitiesLROPoller[
-                        AsyncItemPaged[Union[AnalyzeHealthcareEntitiesResult, DocumentError]]
-                    ],
-                    await self._client.begin_analyze_text_submit_job(  # type: ignore
-                        body=models.AnalyzeTextJobsInput(
-                            analysis_input=docs,
-                            display_name=display_name,
-                            default_language=autodetect_default_language,
-                            tasks=[
-                                models.HealthcareLROTask(
-                                    task_name="0",
-                                    parameters=models.HealthcareTaskParameters(
-                                        model_version=model_version,
-                                        logging_opt_out=disable_service_logs,
-                                        string_index_type=string_index_type_compatibility(string_index_type),
-                                        fhir_version=fhir_version,
-                                        document_type=document_type,
-                                    )
-                                )
-                            ]
-                        ),
-                        cls=my_cls,
-                        polling=AsyncAnalyzeHealthcareEntitiesLROPollingMethod(
-                            text_analytics_client=self._client,
-                            timeout=polling_interval,
-                            show_stats=show_stats,
-                            doc_id_order=doc_id_order,
-                            lro_algorithms=[
-                                TextAnalyticsOperationResourcePolling(
-                                    show_stats=show_stats,
-                                )
-                            ],
-                            **kwargs
-                        ),
-                        continuation_token=continuation_token,
-                        poller_cls=AsyncAnalyzeHealthcareEntitiesLROPoller,
-                        **kwargs
-                    )
-                )
-
-            # v3.1
-            return cast(
-                AsyncAnalyzeHealthcareEntitiesLROPoller[
-                    AsyncItemPaged[Union[AnalyzeHealthcareEntitiesResult, DocumentError]]
-                ],
-                await self._client.begin_health(
-                    docs,
-                    model_version=model_version,
-                    string_index_type=string_index_type,
-                    logging_opt_out=disable_service_logs,
-                    cls=my_cls,
-                    polling=AsyncAnalyzeHealthcareEntitiesLROPollingMethod(
-                        text_analytics_client=self._client,
-                        doc_id_order=doc_id_order,
-                        show_stats=show_stats,
-                        timeout=polling_interval,
-                        lro_algorithms=[
-                            TextAnalyticsOperationResourcePolling(
-                                show_stats=show_stats,
-                            )
-                        ],
-                        **kwargs,
-                    ),
-                    continuation_token=continuation_token,
-                    **kwargs,
-                )
-            )
-        except HttpResponseError as error:
-            return process_http_response_error(error)
-
     def _analyze_result_callback(
         self, raw_response, deserialized, doc_id_order, task_id_order=None, show_stats=False, bespoke=False
     ):
 
         if deserialized is None:
             models = self._client.models(api_version=self._api_version)
             response_cls = models.AnalyzeTextJobState if is_language_api(self._api_version) else models.AnalyzeJobState
@@ -1059,55 +1086,58 @@
             self._client.analyze_text_job_status if is_language_api(self._api_version) else self._client.analyze_status,
             raw_response,
             deserialized,
             show_stats=show_stats,
             bespoke=bespoke
         )
 
-    @distributed_trace_async
+    @distributed_trace
     @validate_multiapi_args(
         version_method_added="v3.1",
         custom_wrapper=check_for_unsupported_actions_types,
-        args_mapping={
-            "2022-10-01-preview": ["autodetect_default_language"],
-        }
     )
-    async def begin_analyze_actions(
+    def begin_analyze_actions(
         self,
         documents: Union[List[str], List[TextDocumentInput], List[Dict[str, str]]],
         actions: List[
             Union[
                 RecognizeEntitiesAction,
                 RecognizeLinkedEntitiesAction,
                 RecognizePiiEntitiesAction,
                 ExtractKeyPhrasesAction,
                 AnalyzeSentimentAction,
                 RecognizeCustomEntitiesAction,
                 SingleLabelClassifyAction,
                 MultiLabelClassifyAction,
                 AnalyzeHealthcareEntitiesAction,
                 ExtractSummaryAction,
-                AbstractSummaryAction,
+                AbstractiveSummaryAction,
             ]
         ],
+        *,
+        continuation_token: Optional[str] = None,
+        display_name: Optional[str] = None,
+        language: Optional[str] = None,
+        polling_interval: Optional[int] = None,
+        show_stats: Optional[bool] = None,
         **kwargs: Any,
-    ) -> AsyncTextAnalysisLROPoller[
-        AsyncItemPaged[
+    ) -> TextAnalysisLROPoller[
+        ItemPaged[
             List[
                 Union[
                     RecognizeEntitiesResult,
                     RecognizeLinkedEntitiesResult,
                     RecognizePiiEntitiesResult,
                     ExtractKeyPhrasesResult,
                     AnalyzeSentimentResult,
                     RecognizeCustomEntitiesResult,
                     ClassifyDocumentResult,
                     AnalyzeHealthcareEntitiesResult,
                     ExtractSummaryResult,
-                    AbstractSummaryResult,
+                    AbstractiveSummaryResult,
                     DocumentError,
                 ]
             ]
         ]
     ]:
         """Start a long-running operation to perform a variety of text analysis actions over a batch of documents.
 
@@ -1131,118 +1161,108 @@
         :param actions: A heterogeneous list of actions to perform on the input documents.
             Each action object encapsulates the parameters used for the particular action type.
             The action results will be in the same order of the input actions.
         :type actions:
             list[RecognizeEntitiesAction or RecognizePiiEntitiesAction or ExtractKeyPhrasesAction or
             RecognizeLinkedEntitiesAction or AnalyzeSentimentAction or
             RecognizeCustomEntitiesAction or SingleLabelClassifyAction or
-            MultiLabelClassifyAction or AnalyzeHealthcareEntitiesAction or
-            AbstractSummaryAction or ExtractSummaryAction]
+            MultiLabelClassifyAction or AnalyzeHealthcareEntitiesAction or ExtractSummaryAction
+            or AbstractiveSummaryAction]
         :keyword str display_name: An optional display name to set for the requested analysis.
         :keyword str language: The 2 letter ISO 639-1 representation of language for the
             entire batch. For example, use "en" for English; "es" for Spanish etc.
             For automatic language detection, use "auto" (Only supported by API version
             2022-10-01-preview and newer). If not set, uses "en" for English as default.
             Per-document language will take precedence over whole batch language.
             See https://aka.ms/talangs for supported languages in Language API.
-        :keyword str autodetect_default_language: Default/fallback language to use for documents requesting
-            automatic language detection.
         :keyword bool show_stats: If set to true, response will contain document level statistics.
         :keyword int polling_interval: Waiting time between two polls for LRO operations
             if no Retry-After header is present. Defaults to 5 seconds.
         :keyword str continuation_token:
             Call `continuation_token()` on the poller object to save the long-running operation (LRO)
             state into an opaque token. Pass the value as the `continuation_token` keyword argument
             to restart the LRO from a saved state.
-        :return: An instance of an AsyncTextAnalysisLROPoller. Call `result()` on the poller
+        :return: An instance of an TextAnalysisLROPoller. Call `result()` on the poller
             object to return a pageable heterogeneous list of lists. This list of lists is first ordered
             by the documents you input, then ordered by the actions you input. For example,
             if you have documents input ["Hello", "world"], and actions
             :class:`~azure.ai.textanalytics.RecognizeEntitiesAction` and
             :class:`~azure.ai.textanalytics.AnalyzeSentimentAction`, when iterating over the list of lists,
             you will first iterate over the action results for the "Hello" document, getting the
             :class:`~azure.ai.textanalytics.RecognizeEntitiesResult` of "Hello",
             then the :class:`~azure.ai.textanalytics.AnalyzeSentimentResult` of "Hello".
             Then, you will get the :class:`~azure.ai.textanalytics.RecognizeEntitiesResult` and
             :class:`~azure.ai.textanalytics.AnalyzeSentimentResult` of "world".
         :rtype:
-            ~azure.ai.textanalytics.aio.AsyncTextAnalysisLROPoller[~azure.core.async_paging.AsyncItemPaged[
+            ~azure.ai.textanalytics.TextAnalysisLROPoller[~azure.core.paging.ItemPaged[
             list[RecognizeEntitiesResult or RecognizeLinkedEntitiesResult or RecognizePiiEntitiesResult or
             ExtractKeyPhrasesResult or AnalyzeSentimentResult or RecognizeCustomEntitiesResult
             or ClassifyDocumentResult or AnalyzeHealthcareEntitiesResult or ExtractSummaryResult
-            or AbstractSummaryResult or DocumentError]]]
+            or AbstractiveSummaryResult or DocumentError]]]
         :raises ~azure.core.exceptions.HttpResponseError or TypeError or ValueError:
 
         .. versionadded:: v3.1
             The *begin_analyze_actions* client method.
         .. versionadded:: 2022-05-01
             The *RecognizeCustomEntitiesAction*, *SingleLabelClassifyAction*,
             *MultiLabelClassifyAction*, and *AnalyzeHealthcareEntitiesAction* input options and the
             corresponding *RecognizeCustomEntitiesResult*, *ClassifyDocumentResult*,
             and *AnalyzeHealthcareEntitiesResult* result objects
         .. versionadded:: 2022-10-01-preview
             The *ExtractSummaryAction* and *AbstractSummaryAction* input options and the corresponding
             *ExtractSummaryResult* and *AbstractSummaryResult* result objects.
-            The *autodetect_default_language* keyword argument.
 
         .. admonition:: Example:
 
-            .. literalinclude:: ../samples/async_samples/sample_analyze_actions_async.py
-                :start-after: [START analyze_async]
-                :end-before: [END analyze_async]
+            .. literalinclude:: ../samples/sample_analyze_actions.py
+                :start-after: [START analyze]
+                :end-before: [END analyze]
                 :language: python
                 :dedent: 4
-                :caption: Start a long-running operation to perform a variety of text analysis actions over
-                    a batch of documents.
+                :caption: Start a long-running operation to perform a variety of text analysis
+                    actions over a batch of documents.
         """
 
-        display_name = kwargs.pop("display_name", None)
-        language_arg = kwargs.pop("language", None)
-        language = language_arg if language_arg is not None else self._default_language
-
-        show_stats = kwargs.pop("show_stats", None)
-        polling_interval = kwargs.pop("polling_interval", 5)
-        continuation_token = kwargs.pop("continuation_token", None)
+        language_arg = language if language is not None else self._default_language
+        polling_interval_arg = polling_interval if polling_interval is not None else 5
         bespoke = kwargs.pop("bespoke", False)
-        autodetect_default_language = kwargs.pop("autodetect_default_language", None)
 
         if continuation_token:
             return cast(
-                AsyncAnalyzeActionsResponse,
+                AnalyzeActionsResponse,
                 _get_result_from_continuation_token(
                     self._client._client,  # pylint: disable=protected-access
                     continuation_token,
-                    AsyncAnalyzeActionsLROPoller,
-                    AsyncAnalyzeActionsLROPollingMethod(
+                    AnalyzeActionsLROPoller,
+                    AnalyzeActionsLROPollingMethod(
                         text_analytics_client=self._client,
-                        timeout=polling_interval,
+                        timeout=polling_interval_arg,
                         **kwargs
                     ),
                     self._analyze_result_callback,
                     bespoke
                 )
             )
 
         models = self._client.models(api_version=self._api_version)
 
         input_model_cls = \
             models.MultiLanguageAnalysisInput if is_language_api(self._api_version) else models.MultiLanguageBatchInput
         docs = input_model_cls(
-            documents=_validate_input(documents, "language", language)
+            documents=_validate_input(documents, "language", language_arg)
         )
         doc_id_order = [doc.get("id") for doc in docs.documents]
         try:
             generated_tasks = [
                 action._to_generated(self._api_version, str(idx))  # pylint: disable=protected-access
                 for idx, action in enumerate(actions)
             ]
         except AttributeError as e:
             raise TypeError("Unsupported action type in list.") from e
         task_order = [(_determine_action_type(a), a.task_name) for a in generated_tasks]
-
         response_cls = kwargs.pop(
             "cls",
             lambda pipeline_response, deserialized, _:
                 self._analyze_result_callback(
                     pipeline_response,
                     deserialized,
                     doc_id_order,
@@ -1251,26 +1271,25 @@
                     bespoke=bespoke
                 ),
         )
 
         try:
             if is_language_api(self._api_version):
                 return cast(
-                    AsyncAnalyzeActionsResponse,
-                    await self._client.begin_analyze_text_submit_job(
+                    AnalyzeActionsResponse,
+                    self._client.begin_analyze_text_submit_job(
                         body=models.AnalyzeTextJobsInput(
                             analysis_input=docs,
                             display_name=display_name,
-                            default_language=autodetect_default_language,
                             tasks=generated_tasks
                         ),
                         cls=response_cls,
-                        polling=AsyncAnalyzeActionsLROPollingMethod(
+                        polling=AnalyzeActionsLROPollingMethod(
                             text_analytics_client=self._client,
-                            timeout=polling_interval,
+                            timeout=polling_interval_arg,
                             show_stats=show_stats,
                             doc_id_order=doc_id_order,
                             task_id_order=task_order,
                             lro_algorithms=[
                                 TextAnalyticsOperationResourcePolling(
                                     show_stats=show_stats,
                                 )
@@ -1305,52 +1324,57 @@
                     if _determine_action_type(a) == _AnalyzeActionsType.ANALYZE_SENTIMENT
                 ],
             )
             analyze_body = models.AnalyzeBatchInput(
                 display_name=display_name, tasks=analyze_tasks, analysis_input=docs
             )
             return cast(
-                AsyncAnalyzeActionsResponse,
-                await self._client.begin_analyze(
+                AnalyzeActionsResponse,
+                self._client.begin_analyze(
                     body=analyze_body,
                     cls=response_cls,
-                    polling=AsyncAnalyzeActionsLROPollingMethod(
+                    polling=AnalyzeActionsLROPollingMethod(
                         text_analytics_client=self._client,
-                        timeout=polling_interval,
+                        timeout=polling_interval_arg,
                         show_stats=show_stats,
                         doc_id_order=doc_id_order,
                         task_id_order=task_order,
                         lro_algorithms=[
                             TextAnalyticsOperationResourcePolling(
                                 show_stats=show_stats,
                             )
                         ],
-                        **kwargs,
+                        **kwargs
                     ),
                     continuation_token=continuation_token,
-                    **kwargs,
+                    **kwargs
                 )
             )
         except HttpResponseError as error:
             return process_http_response_error(error)
 
-    @distributed_trace_async
+    @distributed_trace
     @validate_multiapi_args(
         version_method_added="2022-05-01",
-        args_mapping={
-            "2022-10-01-preview": ["autodetect_default_language"],
-        }
     )
-    async def begin_recognize_custom_entities(
+    def begin_recognize_custom_entities(
         self,
         documents: Union[List[str], List[TextDocumentInput], List[Dict[str, str]]],
         project_name: str,
         deployment_name: str,
+        *,
+        continuation_token: Optional[str] = None,
+        disable_service_logs: Optional[bool] = None,
+        display_name: Optional[str] = None,
+        language: Optional[str] = None,
+        polling_interval: Optional[int] = None,
+        show_stats: Optional[bool] = None,
+        string_index_type: Optional[str] = None,
         **kwargs: Any,
-    ) -> AsyncTextAnalysisLROPoller[AsyncItemPaged[Union[RecognizeCustomEntitiesResult, DocumentError]]]:
+    ) -> TextAnalysisLROPoller[ItemPaged[Union[RecognizeCustomEntitiesResult, DocumentError]]]:
         """Start a long-running custom named entity recognition operation.
 
         For information on regional support of custom features and how to train a model to
         recognize custom entities, see https://aka.ms/azsdk/textanalytics/customentityrecognition
 
         :param documents: The set of documents to process as part of this batch.
             If you wish to specify the ID and language on a per-item basis you must
@@ -1363,16 +1387,14 @@
         :param str deployment_name: This field indicates the deployment name for the model.
         :keyword str language: The 2 letter ISO 639-1 representation of language for the
             entire batch. For example, use "en" for English; "es" for Spanish etc.
             For automatic language detection, use "auto" (Only supported by API version
             2022-10-01-preview and newer). If not set, uses "en" for English as default.
             Per-document language will take precedence over whole batch language.
             See https://aka.ms/talangs for supported languages in Language API.
-        :keyword str autodetect_default_language: Default/fallback language to use for documents requesting
-            automatic language detection.
         :keyword bool show_stats: If set to true, response will contain document level statistics.
         :keyword bool disable_service_logs: If set to true, you opt-out of having your text input
             logged on the service side for troubleshooting. By default, the Language service logs your
             input text for 48 hours, solely to allow for troubleshooting issues in providing you with
             the service's natural language processing functions. Setting this parameter to true,
             disables input logging and may limit our ability to remediate issues that occur. Please see
             Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
@@ -1385,98 +1407,101 @@
         :keyword int polling_interval: Waiting time between two polls for LRO operations
             if no Retry-After header is present. Defaults to 5 seconds.
         :keyword str continuation_token:
             Call `continuation_token()` on the poller object to save the long-running operation (LRO)
             state into an opaque token. Pass the value as the `continuation_token` keyword argument
             to restart the LRO from a saved state.
         :keyword str display_name: An optional display name to set for the requested analysis.
-        :return: An instance of an AsyncTextAnalysisLROPoller. Call `result()` on the this
+        :return: An instance of an TextAnalysisLROPoller. Call `result()` on the this
             object to return a heterogeneous pageable of
             :class:`~azure.ai.textanalytics.RecognizeCustomEntitiesResult` and
             :class:`~azure.ai.textanalytics.DocumentError`.
         :rtype:
-            ~azure.ai.textanalytics.aio.AsyncTextAnalysisLROPoller[~azure.core.async_paging.AsyncItemPaged[
+            ~azure.ai.textanalytics.TextAnalysisLROPoller[~azure.core.paging.ItemPaged[
             ~azure.ai.textanalytics.RecognizeCustomEntitiesResult or ~azure.ai.textanalytics.DocumentError]]
         :raises ~azure.core.exceptions.HttpResponseError:
 
         .. versionadded:: 2022-05-01
             The *begin_recognize_custom_entities* client method.
-        .. versionadded:: 2022-10-01-preview
-            The *autodetect_default_language* keyword argument.
 
         .. admonition:: Example:
 
-            .. literalinclude:: ../samples/async_samples/sample_recognize_custom_entities_async.py
-                :start-after: [START recognize_custom_entities_async]
-                :end-before: [END recognize_custom_entities_async]
+            .. literalinclude:: ../samples/sample_recognize_custom_entities.py
+                :start-after: [START recognize_custom_entities]
+                :end-before: [END recognize_custom_entities]
                 :language: python
                 :dedent: 4
                 :caption: Recognize custom entities in a batch of documents.
         """
 
-        continuation_token = kwargs.pop("continuation_token", None)
-        string_index_type = kwargs.pop("string_index_type", self._string_code_unit)
-        disable_service_logs = kwargs.pop("disable_service_logs", None)
-        polling_interval = kwargs.pop("polling_interval", 5)
+        polling_interval_arg = polling_interval if polling_interval is not None else 5
+        string_index_type_arg = string_index_type if string_index_type is not None else self._string_index_type_default
 
         if continuation_token:
             return cast(
-                AsyncTextAnalysisLROPoller[AsyncItemPaged[Union[RecognizeCustomEntitiesResult, DocumentError]]],
+                TextAnalysisLROPoller[ItemPaged[Union[RecognizeCustomEntitiesResult, DocumentError]]],
                 _get_result_from_continuation_token(
                     self._client._client,  # pylint: disable=protected-access
                     continuation_token,
-                    AsyncAnalyzeActionsLROPoller,
-                    AsyncAnalyzeActionsLROPollingMethod(
+                    AnalyzeActionsLROPoller,
+                    AnalyzeActionsLROPollingMethod(
                         text_analytics_client=self._client,
-                        timeout=polling_interval,
+                        timeout=polling_interval_arg,
                         **kwargs
                     ),
                     self._analyze_result_callback,
                     bespoke=True
                 )
             )
 
         try:
             return cast(
-                AsyncTextAnalysisLROPoller[
-                    AsyncItemPaged[Union[RecognizeCustomEntitiesResult, DocumentError]]
+                TextAnalysisLROPoller[
+                    ItemPaged[Union[RecognizeCustomEntitiesResult, DocumentError]]
                 ],
-                await self.begin_analyze_actions(
+                self.begin_analyze_actions(
                     documents,
                     actions=[
                         RecognizeCustomEntitiesAction(
                             project_name=project_name,
                             deployment_name=deployment_name,
-                            string_index_type=string_index_type,
+                            string_index_type=string_index_type_arg,
                             disable_service_logs=disable_service_logs
                         )
                     ],
-                    polling_interval=polling_interval,
+                    display_name=display_name,
+                    show_stats=show_stats,
+                    language=language,
+                    polling_interval=polling_interval_arg,
                     bespoke=True,
                     **kwargs
                 )
             )
 
         except HttpResponseError as error:
             return process_http_response_error(error)
 
-    @distributed_trace_async
+    @distributed_trace
     @validate_multiapi_args(
         version_method_added="2022-05-01",
-        args_mapping={
-            "2022-10-01-preview": ["autodetect_default_language"],
-        }
     )
-    async def begin_single_label_classify(
+    def begin_single_label_classify(
         self,
         documents: Union[List[str], List[TextDocumentInput], List[Dict[str, str]]],
         project_name: str,
         deployment_name: str,
+        *,
+        continuation_token: Optional[str] = None,
+        disable_service_logs: Optional[bool] = None,
+        display_name: Optional[str] = None,
+        language: Optional[str] = None,
+        polling_interval: Optional[int] = None,
+        show_stats: Optional[bool] = None,
         **kwargs: Any,
-    ) -> AsyncTextAnalysisLROPoller[AsyncItemPaged[Union[ClassifyDocumentResult, DocumentError]]]:
+    ) -> TextAnalysisLROPoller[ItemPaged[Union[ClassifyDocumentResult, DocumentError]]]:
         """Start a long-running custom single label classification operation.
 
         For information on regional support of custom features and how to train a model to
         classify your documents, see https://aka.ms/azsdk/textanalytics/customfunctionalities
 
         :param documents: The set of documents to process as part of this batch.
             If you wish to specify the ID and language on a per-item basis you must
@@ -1489,16 +1514,14 @@
         :param str deployment_name: This field indicates the deployment name for the model.
         :keyword str language: The 2 letter ISO 639-1 representation of language for the
             entire batch. For example, use "en" for English; "es" for Spanish etc.
             For automatic language detection, use "auto" (Only supported by API version
             2022-10-01-preview and newer). If not set, uses "en" for English as default.
             Per-document language will take precedence over whole batch language.
             See https://aka.ms/talangs for supported languages in Language API.
-        :keyword str autodetect_default_language: Default/fallback language to use for documents requesting
-            automatic language detection.
         :keyword bool show_stats: If set to true, response will contain document level statistics.
         :keyword bool disable_service_logs: If set to true, you opt-out of having your text input
             logged on the service side for troubleshooting. By default, the Language service logs your
             input text for 48 hours, solely to allow for troubleshooting issues in providing you with
             the service's natural language processing functions. Setting this parameter to true,
             disables input logging and may limit our ability to remediate issues that occur. Please see
             Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
@@ -1507,96 +1530,99 @@
         :keyword int polling_interval: Waiting time between two polls for LRO operations
             if no Retry-After header is present. Defaults to 5 seconds.
         :keyword str continuation_token:
             Call `continuation_token()` on the poller object to save the long-running operation (LRO)
             state into an opaque token. Pass the value as the `continuation_token` keyword argument
             to restart the LRO from a saved state.
         :keyword str display_name: An optional display name to set for the requested analysis.
-        :return: An instance of an AsyncTextAnalysisLROPoller. Call `result()` on the this
+        :return: An instance of an TextAnalysisLROPoller. Call `result()` on the this
             object to return a heterogeneous pageable of
             :class:`~azure.ai.textanalytics.ClassifyDocumentResult` and
             :class:`~azure.ai.textanalytics.DocumentError`.
         :rtype:
-            ~azure.ai.textanalytics.aio.AsyncTextAnalysisLROPoller[~azure.core.async_paging.AsyncItemPaged[
+            ~azure.ai.textanalytics.TextAnalysisLROPoller[~azure.core.paging.ItemPaged[
             ~azure.ai.textanalytics.ClassifyDocumentResult or ~azure.ai.textanalytics.DocumentError]]
         :raises ~azure.core.exceptions.HttpResponseError:
 
         .. versionadded:: 2022-05-01
             The *begin_single_label_classify* client method.
-        .. versionadded:: 2022-10-01-preview
-            The *autodetect_default_language* keyword argument.
 
         .. admonition:: Example:
 
-            .. literalinclude:: ../samples/async_samples/sample_single_label_classify_async.py
-                :start-after: [START single_label_classify_async]
-                :end-before: [END single_label_classify_async]
+            .. literalinclude:: ../samples/sample_single_label_classify.py
+                :start-after: [START single_label_classify]
+                :end-before: [END single_label_classify]
                 :language: python
                 :dedent: 4
                 :caption: Perform single label classification on a batch of documents.
         """
 
-        continuation_token = kwargs.pop("continuation_token", None)
-        disable_service_logs = kwargs.pop("disable_service_logs", None)
-        polling_interval = kwargs.pop("polling_interval", 5)
+        polling_interval_arg = polling_interval if polling_interval is not None else 5
 
         if continuation_token:
             return cast(
-                AsyncTextAnalysisLROPoller[AsyncItemPaged[Union[ClassifyDocumentResult, DocumentError]]],
+                TextAnalysisLROPoller[ItemPaged[Union[ClassifyDocumentResult, DocumentError]]],
                 _get_result_from_continuation_token(
                     self._client._client,  # pylint: disable=protected-access
                     continuation_token,
-                    AsyncAnalyzeActionsLROPoller,
-                    AsyncAnalyzeActionsLROPollingMethod(
+                    AnalyzeActionsLROPoller,
+                    AnalyzeActionsLROPollingMethod(
                         text_analytics_client=self._client,
-                        timeout=polling_interval,
+                        timeout=polling_interval_arg,
                         **kwargs
                     ),
                     self._analyze_result_callback,
                     bespoke=True
                 )
             )
 
         try:
             return cast(
-                AsyncTextAnalysisLROPoller[
-                    AsyncItemPaged[Union[ClassifyDocumentResult, DocumentError]]
+                TextAnalysisLROPoller[
+                    ItemPaged[Union[ClassifyDocumentResult, DocumentError]]
                 ],
-                await self.begin_analyze_actions(
+                self.begin_analyze_actions(
                     documents,
                     actions=[
                         SingleLabelClassifyAction(
                             project_name=project_name,
                             deployment_name=deployment_name,
                             disable_service_logs=disable_service_logs
                         )
                     ],
-                    polling_interval=polling_interval,
+                    polling_interval=polling_interval_arg,
+                    display_name=display_name,
+                    show_stats=show_stats,
+                    language=language,
                     bespoke=True,
                     **kwargs
                 )
             )
 
         except HttpResponseError as error:
             return process_http_response_error(error)
 
-    @distributed_trace_async
+    @distributed_trace
     @validate_multiapi_args(
         version_method_added="2022-05-01",
-        args_mapping={
-            "2022-10-01-preview": ["autodetect_default_language"],
-        }
     )
-    async def begin_multi_label_classify(
+    def begin_multi_label_classify(
         self,
         documents: Union[List[str], List[TextDocumentInput], List[Dict[str, str]]],
         project_name: str,
         deployment_name: str,
+        *,
+        continuation_token: Optional[str] = None,
+        disable_service_logs: Optional[bool] = None,
+        display_name: Optional[str] = None,
+        language: Optional[str] = None,
+        polling_interval: Optional[int] = None,
+        show_stats: Optional[bool] = None,
         **kwargs: Any,
-    ) -> AsyncTextAnalysisLROPoller[AsyncItemPaged[Union[ClassifyDocumentResult, DocumentError]]]:
+    ) -> TextAnalysisLROPoller[ItemPaged[Union[ClassifyDocumentResult, DocumentError]]]:
         """Start a long-running custom multi label classification operation.
 
         For information on regional support of custom features and how to train a model to
         classify your documents, see https://aka.ms/azsdk/textanalytics/customfunctionalities
 
         :param documents: The set of documents to process as part of this batch.
             If you wish to specify the ID and language on a per-item basis you must
@@ -1609,16 +1635,14 @@
         :param str deployment_name: This field indicates the deployment name for the model.
         :keyword str language: The 2 letter ISO 639-1 representation of language for the
             entire batch. For example, use "en" for English; "es" for Spanish etc.
             For automatic language detection, use "auto" (Only supported by API version
             2022-10-01-preview and newer). If not set, uses "en" for English as default.
             Per-document language will take precedence over whole batch language.
             See https://aka.ms/talangs for supported languages in Language API.
-        :keyword str autodetect_default_language: Default/fallback language to use for documents requesting
-            automatic language detection.
         :keyword bool show_stats: If set to true, response will contain document level statistics.
         :keyword bool disable_service_logs: If set to true, you opt-out of having your text input
             logged on the service side for troubleshooting. By default, the Language service logs your
             input text for 48 hours, solely to allow for troubleshooting issues in providing you with
             the service's natural language processing functions. Setting this parameter to true,
             disables input logging and may limit our ability to remediate issues that occur. Please see
             Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
@@ -1627,90 +1651,95 @@
         :keyword int polling_interval: Waiting time between two polls for LRO operations
             if no Retry-After header is present. Defaults to 5 seconds.
         :keyword str continuation_token:
             Call `continuation_token()` on the poller object to save the long-running operation (LRO)
             state into an opaque token. Pass the value as the `continuation_token` keyword argument
             to restart the LRO from a saved state.
         :keyword str display_name: An optional display name to set for the requested analysis.
-        :return: An instance of an AsyncTextAnalysisLROPoller. Call `result()` on the this
+        :return: An instance of an TextAnalysisLROPoller. Call `result()` on the this
             object to return a heterogeneous pageable of
             :class:`~azure.ai.textanalytics.ClassifyDocumentResult` and
             :class:`~azure.ai.textanalytics.DocumentError`.
         :rtype:
-            ~azure.ai.textanalytics.aio.AsyncTextAnalysisLROPoller[~azure.core.async_paging.AsyncItemPaged[
+            ~azure.ai.textanalytics.TextAnalysisLROPoller[~azure.core.paging.ItemPaged[
             ~azure.ai.textanalytics.ClassifyDocumentResult or ~azure.ai.textanalytics.DocumentError]]
         :raises ~azure.core.exceptions.HttpResponseError:
 
         .. versionadded:: 2022-05-01
             The *begin_multi_label_classify* client method.
-        .. versionadded:: 2022-10-01-preview
-            The *autodetect_default_language* keyword argument.
 
         .. admonition:: Example:
 
-            .. literalinclude:: ../samples/async_samples/sample_multi_label_classify_async.py
-                :start-after: [START multi_label_classify_async]
-                :end-before: [END multi_label_classify_async]
+            .. literalinclude:: ../samples/sample_multi_label_classify.py
+                :start-after: [START multi_label_classify]
+                :end-before: [END multi_label_classify]
                 :language: python
                 :dedent: 4
                 :caption: Perform multi label classification on a batch of documents.
         """
 
-        continuation_token = kwargs.pop("continuation_token", None)
-        disable_service_logs = kwargs.pop("disable_service_logs", None)
-        polling_interval = kwargs.pop("polling_interval", 5)
+        polling_interval_arg = polling_interval if polling_interval is not None else 5
 
         if continuation_token:
             return cast(
-                AsyncTextAnalysisLROPoller[AsyncItemPaged[Union[ClassifyDocumentResult, DocumentError]]],
+                TextAnalysisLROPoller[ItemPaged[Union[ClassifyDocumentResult, DocumentError]]],
                 _get_result_from_continuation_token(
                     self._client._client,  # pylint: disable=protected-access
                     continuation_token,
-                    AsyncAnalyzeActionsLROPoller,
-                    AsyncAnalyzeActionsLROPollingMethod(
+                    AnalyzeActionsLROPoller,
+                    AnalyzeActionsLROPollingMethod(
                         text_analytics_client=self._client,
-                        timeout=polling_interval,
+                        timeout=polling_interval_arg,
                         **kwargs
                     ),
                     self._analyze_result_callback,
                     bespoke=True
                 )
             )
 
         try:
             return cast(
-                AsyncTextAnalysisLROPoller[
-                    AsyncItemPaged[Union[ClassifyDocumentResult, DocumentError]]
+                TextAnalysisLROPoller[
+                    ItemPaged[Union[ClassifyDocumentResult, DocumentError]]
                 ],
-                await self.begin_analyze_actions(
+                self.begin_analyze_actions(
                     documents,
                     actions=[
                         MultiLabelClassifyAction(
                             project_name=project_name,
                             deployment_name=deployment_name,
                             disable_service_logs=disable_service_logs
                         )
                     ],
-                    polling_interval=polling_interval,
+                    polling_interval=polling_interval_arg,
+                    display_name=display_name,
+                    show_stats=show_stats,
+                    language=language,
                     bespoke=True,
                     **kwargs
                 )
             )
 
         except HttpResponseError as error:
             return process_http_response_error(error)
 
-    @distributed_trace_async
+    @distributed_trace
     @validate_multiapi_args(
         version_method_added="2022-10-01-preview",
     )
-    async def dynamic_classification(
+    def dynamic_classification(
         self,
         documents: Union[List[str], List[TextDocumentInput], List[Dict[str, str]]],
         categories: List[str],
+        *,
+        classification_type: Optional[Union[str, ClassificationType]] = None,
+        disable_service_logs: Optional[bool] = None,
+        language: Optional[str] = None,
+        model_version: Optional[str] = None,
+        show_stats: Optional[bool] = None,
         **kwargs: Any,
     ) -> List[Union[DynamicClassificationResult, DocumentError]]:
         """Perform dynamic classification on a batch of documents.
 
         On the fly classification of the input documents into one or multiple categories.
         Assigns either one or multiple categories per document. This type of classification
         doesn't require model training.
@@ -1758,34 +1787,30 @@
         :raises ~azure.core.exceptions.HttpResponseError:
 
         .. versionadded:: 2022-10-01-preview
             The *dynamic_classification* client method.
 
         .. admonition:: Example:
 
-            .. literalinclude:: ../samples/async_samples/sample_dynamic_classification_async.py
-                :start-after: [START dynamic_classification_async]
-                :end-before: [END dynamic_classification_async]
+            .. literalinclude:: ../samples/sample_dynamic_classification.py
+                :start-after: [START dynamic_classification]
+                :end-before: [END dynamic_classification]
                 :language: python
                 :dedent: 4
                 :caption: Perform dynamic classification on a batch of documents.
         """
-        language_arg = kwargs.pop("language", None)
-        language = language_arg if language_arg is not None else self._default_language
-        docs = _validate_input(documents, "language", language)
-        model_version = kwargs.pop("model_version", None)
-        show_stats = kwargs.pop("show_stats", None)
-        disable_service_logs = kwargs.pop("disable_service_logs", None)
-        classification_type = kwargs.pop("classification_type", None)
+
+        language_arg = language if language is not None else self._default_language
+        docs = _validate_input(documents, "language", language_arg)
 
         try:
             models = self._client.models(api_version=self._api_version)
             return cast(
                 List[Union[DynamicClassificationResult, DocumentError]],
-                await self._client.analyze_text(
+                self._client.analyze_text(
                     body=models.AnalyzeTextDynamicClassificationInput(
                         analysis_input={"documents": docs},
                         parameters=models.DynamicClassificationTaskParameters(
                             categories=categories,
                             logging_opt_out=disable_service_logs,
                             model_version=model_version,
                             classification_type=classification_type,
@@ -1794,7 +1819,263 @@
                     show_stats=show_stats,
                     cls=kwargs.pop("cls", dynamic_classification_result),
                     **kwargs
                 )
             )
         except HttpResponseError as error:
             return process_http_response_error(error)
+
+    @distributed_trace
+    @validate_multiapi_args(
+        version_method_added="2022-10-01-preview"
+    )
+    def begin_extract_summary(
+        self,
+        documents: Union[List[str], List[TextDocumentInput], List[Dict[str, str]]],
+        *,
+        continuation_token: Optional[str] = None,
+        disable_service_logs: Optional[bool] = None,
+        display_name: Optional[str] = None,
+        language: Optional[str] = None,
+        polling_interval: Optional[int] = None,
+        show_stats: Optional[bool] = None,
+        model_version: Optional[str] = None,
+        string_index_type: Optional[str] = None,
+        max_sentence_count: Optional[int] = None,
+        order_by: Optional[str] = None,
+        **kwargs: Any,
+    ) -> TextAnalysisLROPoller[ItemPaged[Union[ExtractSummaryResult, DocumentError]]]:
+        """Start a long-running extractive summarization operation.
+
+        For a conceptual discussion of extractive summarization, see the service documentation:
+        https://learn.microsoft.com/azure/cognitive-services/language-service/summarization/overview
+
+        :param documents: The set of documents to process as part of this batch.
+            If you wish to specify the ID and language on a per-item basis you must
+            use as input a list[:class:`~azure.ai.textanalytics.TextDocumentInput`] or a list of
+            dict representations of :class:`~azure.ai.textanalytics.TextDocumentInput`, like
+            `{"id": "1", "language": "en", "text": "hello world"}`.
+        :type documents:
+            list[str] or list[~azure.ai.textanalytics.TextDocumentInput] or list[dict[str, str]]
+        :keyword str language: The 2 letter ISO 639-1 representation of language for the
+            entire batch. For example, use "en" for English; "es" for Spanish etc.
+            For automatic language detection, use "auto" (Only supported by API version
+            2022-10-01-preview and newer). If not set, uses "en" for English as default.
+            Per-document language will take precedence over whole batch language.
+            See https://aka.ms/talangs for supported languages in Language API.
+        :keyword bool show_stats: If set to true, response will contain document level statistics.
+        :keyword Optional[int] max_sentence_count: Maximum number of sentences to return. Defaults to 3.
+        :keyword Optional[str] order_by:  Possible values include: "Offset", "Rank". Default value: "Offset".
+        :keyword Optional[str] model_version: The model version to use for the analysis.
+        :keyword Optional[str] string_index_type: Specifies the method used to interpret string offsets.
+        :keyword bool disable_service_logs: If set to true, you opt-out of having your text input
+            logged on the service side for troubleshooting. By default, the Language service logs your
+            input text for 48 hours, solely to allow for troubleshooting issues in providing you with
+            the service's natural language processing functions. Setting this parameter to true,
+            disables input logging and may limit our ability to remediate issues that occur. Please see
+            Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
+            additional details, and Microsoft Responsible AI principles at
+            https://www.microsoft.com/ai/responsible-ai.
+        :keyword int polling_interval: Waiting time between two polls for LRO operations
+            if no Retry-After header is present. Defaults to 5 seconds.
+        :keyword str continuation_token:
+            Call `continuation_token()` on the poller object to save the long-running operation (LRO)
+            state into an opaque token. Pass the value as the `continuation_token` keyword argument
+            to restart the LRO from a saved state.
+        :keyword str display_name: An optional display name to set for the requested analysis.
+        :return: An instance of an TextAnalysisLROPoller. Call `result()` on the this
+            object to return a heterogeneous pageable of
+            :class:`~azure.ai.textanalytics.ExtractSummaryResult` and
+            :class:`~azure.ai.textanalytics.DocumentError`.
+        :rtype:
+            ~azure.ai.textanalytics.TextAnalysisLROPoller[~azure.core.paging.ItemPaged[
+            ~azure.ai.textanalytics.ExtractSummaryResult or ~azure.ai.textanalytics.DocumentError]]
+        :raises ~azure.core.exceptions.HttpResponseError:
+
+        .. versionadded:: 2022-10-01-preview
+            The *begin_extract_summary* client method.
+
+        .. admonition:: Example:
+
+            .. literalinclude:: ../samples/sample_extract_summary.py
+                :start-after: [START extract_summary]
+                :end-before: [END extract_summary]
+                :language: python
+                :dedent: 4
+                :caption: Perform extractive summarization on a batch of documents.
+        """
+
+        polling_interval_arg = polling_interval if polling_interval is not None else 5
+        string_index_type_arg = string_index_type if string_index_type is not None else self._string_index_type_default
+
+        if continuation_token:
+            return cast(
+                TextAnalysisLROPoller[ItemPaged[Union[ExtractSummaryResult, DocumentError]]],
+                _get_result_from_continuation_token(
+                    self._client._client,  # pylint: disable=protected-access
+                    continuation_token,
+                    AnalyzeActionsLROPoller,
+                    AnalyzeActionsLROPollingMethod(
+                        text_analytics_client=self._client,
+                        timeout=polling_interval_arg,
+                        **kwargs
+                    ),
+                    self._analyze_result_callback,
+                    bespoke=True
+                )
+            )
+
+        try:
+            return cast(
+                TextAnalysisLROPoller[
+                    ItemPaged[Union[ExtractSummaryResult, DocumentError]]
+                ],
+                self.begin_analyze_actions(
+                    documents,
+                    actions=[
+                        ExtractSummaryAction(
+                            model_version=model_version,
+                            string_index_type=string_index_type_arg,
+                            max_sentence_count=max_sentence_count,
+                            order_by=order_by,
+                            disable_service_logs=disable_service_logs,
+                        )
+                    ],
+                    polling_interval=polling_interval_arg,
+                    display_name=display_name,
+                    show_stats=show_stats,
+                    language=language,
+                    bespoke=True,
+                    **kwargs
+                )
+            )
+
+        except HttpResponseError as error:
+            return process_http_response_error(error)
+
+    @distributed_trace
+    @validate_multiapi_args(
+        version_method_added="2022-10-01-preview"
+    )
+    def begin_abstractive_summary(
+        self,
+        documents: Union[List[str], List[TextDocumentInput], List[Dict[str, str]]],
+        *,
+        continuation_token: Optional[str] = None,
+        disable_service_logs: Optional[bool] = None,
+        display_name: Optional[str] = None,
+        language: Optional[str] = None,
+        polling_interval: Optional[int] = None,
+        show_stats: Optional[bool] = None,
+        model_version: Optional[str] = None,
+        string_index_type: Optional[str] = None,
+        sentence_count: Optional[int] = None,
+        **kwargs: Any,
+    ) -> TextAnalysisLROPoller[ItemPaged[Union[AbstractiveSummaryResult, DocumentError]]]:
+        """Start a long-running abstractive summarization operation.
+
+        For a conceptual discussion of abstractive summarization, see the service documentation:
+        https://learn.microsoft.com/azure/cognitive-services/language-service/summarization/overview
+
+        .. note:: The abstractive summarization feature is part of a gated preview. Request access here:
+            https://aka.ms/applyforgatedsummarizationfeatures
+
+        :param documents: The set of documents to process as part of this batch.
+            If you wish to specify the ID and language on a per-item basis you must
+            use as input a list[:class:`~azure.ai.textanalytics.TextDocumentInput`] or a list of
+            dict representations of :class:`~azure.ai.textanalytics.TextDocumentInput`, like
+            `{"id": "1", "language": "en", "text": "hello world"}`.
+        :type documents:
+            list[str] or list[~azure.ai.textanalytics.TextDocumentInput] or list[dict[str, str]]
+        :keyword str language: The 2 letter ISO 639-1 representation of language for the
+            entire batch. For example, use "en" for English; "es" for Spanish etc.
+            For automatic language detection, use "auto" (Only supported by API version
+            2022-10-01-preview and newer). If not set, uses "en" for English as default.
+            Per-document language will take precedence over whole batch language.
+            See https://aka.ms/talangs for supported languages in Language API.
+        :keyword bool show_stats: If set to true, response will contain document level statistics.
+        :keyword Optional[int] sentence_count: It controls the approximate number of sentences in the output summaries.
+        :keyword Optional[str] model_version: The model version to use for the analysis.
+        :keyword Optional[str] string_index_type: Specifies the method used to interpret string offsets.
+        :keyword bool disable_service_logs: If set to true, you opt-out of having your text input
+            logged on the service side for troubleshooting. By default, the Language service logs your
+            input text for 48 hours, solely to allow for troubleshooting issues in providing you with
+            the service's natural language processing functions. Setting this parameter to true,
+            disables input logging and may limit our ability to remediate issues that occur. Please see
+            Cognitive Services Compliance and Privacy notes at https://aka.ms/cs-compliance for
+            additional details, and Microsoft Responsible AI principles at
+            https://www.microsoft.com/ai/responsible-ai.
+        :keyword int polling_interval: Waiting time between two polls for LRO operations
+            if no Retry-After header is present. Defaults to 5 seconds.
+        :keyword str continuation_token:
+            Call `continuation_token()` on the poller object to save the long-running operation (LRO)
+            state into an opaque token. Pass the value as the `continuation_token` keyword argument
+            to restart the LRO from a saved state.
+        :keyword str display_name: An optional display name to set for the requested analysis.
+        :return: An instance of an TextAnalysisLROPoller. Call `result()` on the this
+            object to return a heterogeneous pageable of
+            :class:`~azure.ai.textanalytics.AbstractiveSummaryResult` and
+            :class:`~azure.ai.textanalytics.DocumentError`.
+        :rtype:
+            ~azure.ai.textanalytics.TextAnalysisLROPoller[~azure.core.paging.ItemPaged[
+            ~azure.ai.textanalytics.AbstractiveSummaryResult or ~azure.ai.textanalytics.DocumentError]]
+        :raises ~azure.core.exceptions.HttpResponseError:
+
+        .. versionadded:: 2022-10-01-preview
+            The *begin_abstractive_summary* client method.
+
+        .. admonition:: Example:
+
+            .. literalinclude:: ../samples/sample_abstractive_summary.py
+                :start-after: [START abstractive_summary]
+                :end-before: [END abstractive_summary]
+                :language: python
+                :dedent: 4
+                :caption: Perform abstractive summarization on a batch of documents.
+        """
+
+        polling_interval_arg = polling_interval if polling_interval is not None else 5
+        string_index_type_arg = string_index_type if string_index_type is not None else self._string_index_type_default
+
+        if continuation_token:
+            return cast(
+                TextAnalysisLROPoller[ItemPaged[Union[AbstractiveSummaryResult, DocumentError]]],
+                _get_result_from_continuation_token(
+                    self._client._client,  # pylint: disable=protected-access
+                    continuation_token,
+                    AnalyzeActionsLROPoller,
+                    AnalyzeActionsLROPollingMethod(
+                        text_analytics_client=self._client,
+                        timeout=polling_interval_arg,
+                        **kwargs
+                    ),
+                    self._analyze_result_callback,
+                    bespoke=True
+                )
+            )
+
+        try:
+            return cast(
+                TextAnalysisLROPoller[
+                    ItemPaged[Union[AbstractiveSummaryResult, DocumentError]]
+                ],
+                self.begin_analyze_actions(
+                    documents,
+                    actions=[
+                        AbstractiveSummaryAction(
+                            model_version=model_version,
+                            string_index_type=string_index_type_arg,
+                            sentence_count=sentence_count,
+                            disable_service_logs=disable_service_logs,
+                        )
+                    ],
+                    polling_interval=polling_interval_arg,
+                    display_name=display_name,
+                    show_stats=show_stats,
+                    language=language,
+                    bespoke=True,
+                    **kwargs
+                )
+            )
+
+        except HttpResponseError as error:
+            return process_http_response_error(error)
```

## Comparing `azure-ai-textanalytics-5.3.0b1/azure_ai_textanalytics.egg-info/PKG-INFO` & `azure-ai-textanalytics-5.3.0b2/azure_ai_textanalytics.egg-info/PKG-INFO`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: azure-ai-textanalytics
-Version: 5.3.0b1
+Version: 5.3.0b2
 Summary: Microsoft Azure Text Analytics Client Library for Python
 Home-page: https://github.com/Azure/azure-sdk-for-python
 Author: Microsoft Corporation
 Author-email: azpysdkhelp@microsoft.com
 License: MIT License
 Keywords: azure,azure sdk,text analytics,cognitive services,natural language processing
 Classifier: Development Status :: 4 - Beta
@@ -49,38 +49,15 @@
 - You must have an [Azure subscription][azure_subscription] and a
   [Cognitive Services or Language service resource][ta_or_cs_resource] to use this package.
 
 #### Create a Cognitive Services or Language service resource
 
 The Language service supports both [multi-service and single-service access][multi_and_single_service].
 Create a Cognitive Services resource if you plan to access multiple cognitive services under a single endpoint/key. For Language service access only, create a Language service resource.
-
-You can create the resource using
-
-**Option 1:** [Azure Portal][azure_portal_create_ta_resource]
-
-**Option 2:** [Azure CLI][azure_cli_create_ta_resource].
-Below is an example of how you can create a Language service resource using the CLI:
-
-```bash
-# Create a new resource group to hold the Language service resource -
-# if using an existing resource group, skip this step
-az group create --name my-resource-group --location westus2
-```
-
-```bash
-# Create text analytics
-az cognitiveservices account create \
-    --name text-analytics-resource \
-    --resource-group my-resource-group \
-    --kind TextAnalytics \
-    --sku F0 \
-    --location westus2 \
-    --yes
-```
+You can create the resource using the [Azure Portal][azure_portal_create_ta_resource] or [Azure CLI][azure_cli] following the steps in [this document][azure_cli_create_ta_resource].
 
 Interaction with the service using the client library begins with a [client](#textanalyticsclient "TextAnalyticsClient").
 To create a client object, you will need the Cognitive Services or Language service `endpoint` to
 your resource and a `credential` that allows you access:
 
 ```python
 from azure.core.credentials import AzureKeyCredential
@@ -97,22 +74,36 @@
 
 Install the Azure Text Analytics client library for Python with [pip][pip]:
 
 ```bash
 pip install azure-ai-textanalytics --pre
 ```
 
+<!-- SNIPPET:sample_authentication.create_ta_client_with_key -->
+
+```python
+import os
+from azure.core.credentials import AzureKeyCredential
+from azure.ai.textanalytics import TextAnalyticsClient
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
+key = os.environ["AZURE_LANGUAGE_KEY"]
+
+text_analytics_client = TextAnalyticsClient(endpoint, AzureKeyCredential(key))
+```
+
+<!-- END SNIPPET -->
+
 > Note that `5.2.X` and newer targets the Azure Cognitive Service for Language APIs. These APIs include the text analysis and natural language processing features found in the previous versions of the Text Analytics client library.
 In addition, the service API has changed from semantic to date-based versioning. This version of the client library defaults to the latest supported API version, which currently is `2022-10-01-preview`.
 
 This table shows the relationship between SDK versions and supported API versions of the service
 
 | SDK version  | Supported API version of service  |
 | ------------ | --------------------------------- |
-| 5.3.0b1 - Latest beta release | 3.0, 3.1, 2022-05-01, 2022-10-01-preview (default) |
+| 5.3.0b2 - Latest beta release | 3.0, 3.1, 2022-05-01, 2022-10-01-preview (default) |
 | 5.2.X - Latest stable release | 3.0, 3.1, 2022-05-01 (default) |
 | 5.1.0  | 3.0, 3.1 (default) |
 | 5.0.0  | 3.0 |
 
 API version can be selected by passing the [api_version][text_analytics_client] keyword argument into the client.
 For the latest Language service features, consider selecting the most recent beta API version. For production scenarios, the latest stable version is recommended. Setting to an older version may result in reduced feature compatibility.
 
@@ -137,22 +128,28 @@
 `az cognitiveservices account keys list --name "resource-name" --resource-group "resource-group-name"`
 
 #### Create a TextAnalyticsClient with an API Key Credential
 
 Once you have the value for the API key, you can pass it as a string into an instance of [AzureKeyCredential][azure-key-credential]. Use the key as the credential parameter
 to authenticate the client:
 
+<!-- SNIPPET:sample_authentication.create_ta_client_with_key -->
+
 ```python
+import os
 from azure.core.credentials import AzureKeyCredential
 from azure.ai.textanalytics import TextAnalyticsClient
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
+key = os.environ["AZURE_LANGUAGE_KEY"]
 
-credential = AzureKeyCredential("<api_key>")
-text_analytics_client = TextAnalyticsClient(endpoint="https://<resource-name>.cognitiveservices.azure.com/", credential=credential)
+text_analytics_client = TextAnalyticsClient(endpoint, AzureKeyCredential(key))
 ```
 
+<!-- END SNIPPET -->
+
 #### Create a TextAnalyticsClient with an Azure Active Directory Credential
 
 To use an [Azure Active Directory (AAD) token credential][cognitive_authentication_aad],
 provide an instance of the desired credential type obtained from the
 [azure-identity][azure_identity_credentials] library.
 Note that regional endpoints do not support AAD authentication. Create a [custom subdomain][custom_subdomain]
 name for your resource in order to use this type of authentication.
@@ -168,22 +165,29 @@
 can be used to authenticate the client:
 
 Set the values of the client ID, tenant ID, and client secret of the AAD application as environment variables:
 AZURE_CLIENT_ID, AZURE_TENANT_ID, AZURE_CLIENT_SECRET
 
 Use the returned token credential to authenticate the client:
 
+<!-- SNIPPET:sample_authentication.create_ta_client_with_aad -->
+
 ```python
+import os
 from azure.ai.textanalytics import TextAnalyticsClient
 from azure.identity import DefaultAzureCredential
 
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
 credential = DefaultAzureCredential()
-text_analytics_client = TextAnalyticsClient(endpoint="https://<resource-name>.cognitiveservices.azure.com/", credential=credential)
+
+text_analytics_client = TextAnalyticsClient(endpoint, credential=credential)
 ```
 
+<!-- END SNIPPET -->
+
 ## Key concepts
 
 ### TextAnalyticsClient
 
 The Text Analytics client library provides a [TextAnalyticsClient][text_analytics_client] to do analysis on [batches of documents](#examples "Examples").
 It provides both synchronous and asynchronous operations to access a specific use of text analysis, such as language detection or key phrase extraction.
 
@@ -246,15 +250,14 @@
         print(f"Sentiment is {result.sentiment}")
     elif result.kind == "KeyPhraseExtraction":
         print(f"Key phrases: {result.key_phrases}")
     elif result.is_error is True:
         print(f"Document error: {result.code}, {result.message}")
 ```
 
-
 ### Long-Running Operations
 
 Long-running operations are operations which consist of an initial request sent to the service to start an operation,
 followed by polling the service at intervals to determine whether the operation has completed or failed, and if it has
 succeeded, to get the result.
 
 Methods that support healthcare analysis, custom text analysis, or multiple analyses are modeled as long-running operations.
@@ -274,257 +277,337 @@
 - [Detect Language](#detect-language "Detect language")
 - [Healthcare Entities Analysis](#healthcare-entities-analysis "Healthcare Entities Analysis")
 - [Multiple Analysis](#multiple-analysis "Multiple analysis")
 - [Custom Entity Recognition][recognize_custom_entities_sample]
 - [Custom Single Label Classification][single_label_classify_sample]
 - [Custom Multi Label Classification][multi_label_classify_sample]
 - [Extractive Summarization][extract_summary_sample]
-- [Abstractive Summarization][abstract_summary_sample]
+- [Abstractive Summarization][abstractive_summary_sample]
 - [Dynamic Classification][dynamic_classification_sample]
 
-### Analyze sentiment
+### Analyze Sentiment
 
 [analyze_sentiment][analyze_sentiment] looks at its input text and determines whether its sentiment is positive, negative, neutral or mixed. It's response includes per-sentence sentiment analysis and confidence scores.
 
+<!-- SNIPPET:sample_analyze_sentiment.analyze_sentiment -->
+
 ```python
+import os
 from azure.core.credentials import AzureKeyCredential
 from azure.ai.textanalytics import TextAnalyticsClient
 
-credential = AzureKeyCredential("<api_key>")
-endpoint="https://<resource-name>.cognitiveservices.azure.com/"
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
+key = os.environ["AZURE_LANGUAGE_KEY"]
 
-text_analytics_client = TextAnalyticsClient(endpoint, credential)
+text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))
 
 documents = [
-    "I did not like the restaurant. The food was somehow both too spicy and underseasoned. Additionally, I thought the location was too far away from the playhouse.",
-    "The restaurant was decorated beautifully. The atmosphere was unlike any other restaurant I've been to.",
-    "The food was yummy. :)"
+    """I had the best day of my life. I decided to go sky-diving and it made me appreciate my whole life so much more.
+    I developed a deep-connection with my instructor as well, and I feel as if I've made a life-long friend in her.""",
+    """This was a waste of my time. All of the views on this drop are extremely boring, all I saw was grass. 0/10 would
+    not recommend to any divers, even first timers.""",
+    """This was pretty good! The sights were ok, and I had fun with my instructors! Can't complain too much about my experience""",
+    """I only have one word for my experience: WOW!!! I can't believe I have had such a wonderful skydiving company right
+    in my backyard this whole time! I will definitely be a repeat customer, and I want to take my grandmother skydiving too,
+    I know she'll love it!"""
 ]
 
-response = text_analytics_client.analyze_sentiment(documents, language="en")
-result = [doc for doc in response if not doc.is_error]
 
-for doc in result:
+result = text_analytics_client.analyze_sentiment(documents, show_opinion_mining=True)
+docs = [doc for doc in result if not doc.is_error]
+
+print("Let's visualize the sentiment of each of these documents")
+for idx, doc in enumerate(docs):
+    print(f"Document text: {documents[idx]}")
     print(f"Overall sentiment: {doc.sentiment}")
-    print(
-        f"Scores: positive={doc.confidence_scores.positive}; "
-        f"neutral={doc.confidence_scores.neutral}; "
-        f"negative={doc.confidence_scores.negative}\n"
-    )
 ```
 
+<!-- END SNIPPET -->
+
 The returned response is a heterogeneous list of result and error objects: list[[AnalyzeSentimentResult][analyze_sentiment_result], [DocumentError][document_error]]
 
 Please refer to the service documentation for a conceptual discussion of [sentiment analysis][sentiment_analysis]. To see how to conduct more granular analysis into the opinions related to individual aspects (such as attributes of a product or service) in a text, see [here][opinion_mining_sample].
 
-### Recognize entities
+### Recognize Entities
 
 [recognize_entities][recognize_entities] recognizes and categories entities in its input text as people, places, organizations, date/time, quantities, percentages, currencies, and more.
 
+<!-- SNIPPET:sample_recognize_entities.recognize_entities -->
+
 ```python
+import os
+import typing
 from azure.core.credentials import AzureKeyCredential
 from azure.ai.textanalytics import TextAnalyticsClient
 
-credential = AzureKeyCredential("<api_key>")
-endpoint="https://<resource-name>.cognitiveservices.azure.com/"
-
-text_analytics_client = TextAnalyticsClient(endpoint, credential)
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
+key = os.environ["AZURE_LANGUAGE_KEY"]
 
-documents = [
-    """
-    Microsoft was founded by Bill Gates and Paul Allen. Its headquarters are located in Redmond. Redmond is a
-    city in King County, Washington, United States, located 15 miles east of Seattle.
-    """,
-    "Jeff bought three dozen eggs because there was a 50% discount."
+text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))
+reviews = [
+    """I work for Foo Company, and we hired Contoso for our annual founding ceremony. The food
+    was amazing and we all can't say enough good words about the quality and the level of service.""",
+    """We at the Foo Company re-hired Contoso after all of our past successes with the company.
+    Though the food was still great, I feel there has been a quality drop since their last time
+    catering for us. Is anyone else running into the same problem?""",
+    """Bar Company is over the moon about the service we received from Contoso, the best sliders ever!!!!"""
 ]
 
-response = text_analytics_client.recognize_entities(documents, language="en")
-result = [doc for doc in response if not doc.is_error]
+result = text_analytics_client.recognize_entities(reviews)
+result = [review for review in result if not review.is_error]
+organization_to_reviews: typing.Dict[str, typing.List[str]] = {}
+
+for idx, review in enumerate(result):
+    for entity in review.entities:
+        print(f"Entity '{entity.text}' has category '{entity.category}'")
+        if entity.category == 'Organization':
+            organization_to_reviews.setdefault(entity.text, [])
+            organization_to_reviews[entity.text].append(reviews[idx])
 
-for doc in result:
-    for entity in doc.entities:
-        print(f"Entity: {entity.text}")
-        print(f"...Category: {entity.category}")
-        print(f"...Confidence Score: {entity.confidence_score}")
-        print(f"...Offset: {entity.offset}")
+for organization, reviews in organization_to_reviews.items():
+    print(
+        "\n\nOrganization '{}' has left us the following review(s): {}".format(
+            organization, "\n\n".join(reviews)
+        )
+    )
 ```
 
+<!-- END SNIPPET -->
+
 The returned response is a heterogeneous list of result and error objects: list[[RecognizeEntitiesResult][recognize_entities_result], [DocumentError][document_error]]
 
 Please refer to the service documentation for a conceptual discussion of [named entity recognition][named_entity_recognition]
 and [supported types][named_entity_categories].
 
-### Recognize linked entities
+### Recognize Linked Entities
 
 [recognize_linked_entities][recognize_linked_entities] recognizes and disambiguates the identity of each entity found in its input text (for example,
 determining whether an occurrence of the word Mars refers to the planet, or to the
 Roman god of war). Recognized entities are associated with URLs to a well-known knowledge base, like Wikipedia.
 
+<!-- SNIPPET:sample_recognize_linked_entities.recognize_linked_entities -->
+
 ```python
+import os
 from azure.core.credentials import AzureKeyCredential
 from azure.ai.textanalytics import TextAnalyticsClient
 
-credential = AzureKeyCredential("<api_key>")
-endpoint="https://<resource-name>.cognitiveservices.azure.com/"
-
-text_analytics_client = TextAnalyticsClient(endpoint, credential)
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
+key = os.environ["AZURE_LANGUAGE_KEY"]
 
+text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))
 documents = [
-    "Microsoft was founded by Bill Gates and Paul Allen. Its headquarters are located in Redmond.",
-    "Easter Island, a Chilean territory, is a remote volcanic island in Polynesia."
+    """
+    Microsoft was founded by Bill Gates with some friends he met at Harvard. One of his friends,
+    Steve Ballmer, eventually became CEO after Bill Gates as well. Steve Ballmer eventually stepped
+    down as CEO of Microsoft, and was succeeded by Satya Nadella.
+    Microsoft originally moved its headquarters to Bellevue, Washington in January 1979, but is now
+    headquartered in Redmond.
+    """
 ]
 
-response = text_analytics_client.recognize_linked_entities(documents, language="en")
-result = [doc for doc in response if not doc.is_error]
+result = text_analytics_client.recognize_linked_entities(documents)
+docs = [doc for doc in result if not doc.is_error]
 
-for doc in result:
+print(
+    "Let's map each entity to it's Wikipedia article. I also want to see how many times each "
+    "entity is mentioned in a document\n\n"
+)
+entity_to_url = {}
+for doc in docs:
     for entity in doc.entities:
-        print(f"Entity: {entity.name}")
-        print(f"...URL: {entity.url}")
-        print(f"...Data Source: {entity.data_source}")
-        print("...Entity matches:")
-        for match in entity.matches:
-            print(f"......Entity match text: {match.text}")
-            print(f"......Confidence Score: {match.confidence_score}")
-            print(f"......Offset: {match.offset}")
+        print("Entity '{}' has been mentioned '{}' time(s)".format(
+            entity.name, len(entity.matches)
+        ))
+        if entity.data_source == "Wikipedia":
+            entity_to_url[entity.name] = entity.url
 ```
 
+<!-- END SNIPPET -->
+
 The returned response is a heterogeneous list of result and error objects: list[[RecognizeLinkedEntitiesResult][recognize_linked_entities_result], [DocumentError][document_error]]
 
 Please refer to the service documentation for a conceptual discussion of [entity linking][linked_entity_recognition]
 and [supported types][linked_entities_categories].
 
-### Recognize PII entities
+### Recognize PII Entities
 
 [recognize_pii_entities][recognize_pii_entities] recognizes and categorizes Personally Identifiable Information (PII) entities in its input text, such as
 Social Security Numbers, bank account information, credit card numbers, and more.
 
+<!-- SNIPPET:sample_recognize_pii_entities.recognize_pii_entities -->
+
 ```python
+import os
 from azure.core.credentials import AzureKeyCredential
 from azure.ai.textanalytics import TextAnalyticsClient
 
-credential = AzureKeyCredential("<api_key>")
-endpoint="https://<resource-name>.cognitiveservices.azure.com/"
-
-text_analytics_client = TextAnalyticsClient(endpoint, credential)
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
+key = os.environ["AZURE_LANGUAGE_KEY"]
 
+text_analytics_client = TextAnalyticsClient(
+    endpoint=endpoint, credential=AzureKeyCredential(key)
+)
 documents = [
-    """
-    We have an employee called Parker who cleans up after customers. The employee's
-    SSN is 859-98-0987, and their phone number is 555-555-5555.
-    """
+    """Parker Doe has repaid all of their loans as of 2020-04-25.
+    Their SSN is 859-98-0987. To contact them, use their phone number
+    555-555-5555. They are originally from Brazil and have Brazilian CPF number 998.214.865-68"""
 ]
-response = text_analytics_client.recognize_pii_entities(documents, language="en")
-result = [doc for doc in response if not doc.is_error]
-for idx, doc in enumerate(result):
+
+result = text_analytics_client.recognize_pii_entities(documents)
+docs = [doc for doc in result if not doc.is_error]
+
+print(
+    "Let's compare the original document with the documents after redaction. "
+    "I also want to comb through all of the entities that got redacted"
+)
+for idx, doc in enumerate(docs):
     print(f"Document text: {documents[idx]}")
     print(f"Redacted document text: {doc.redacted_text}")
     for entity in doc.entities:
-        print(f"...Entity: {entity.text}")
-        print(f"......Category: {entity.category}")
-        print(f"......Confidence Score: {entity.confidence_score}")
-        print(f"......Offset: {entity.offset}")
+        print("...Entity '{}' with category '{}' got redacted".format(
+            entity.text, entity.category
+        ))
 ```
 
+<!-- END SNIPPET -->
+
 The returned response is a heterogeneous list of result and error objects: list[[RecognizePiiEntitiesResult][recognize_pii_entities_result], [DocumentError][document_error]]
 
 Please refer to the service documentation for [supported PII entity types][pii_entity_categories].
 
 Note: The Recognize PII Entities service is available in API version v3.1 and newer.
 
-### Extract key phrases
+### Extract Key Phrases
 
 [extract_key_phrases][extract_key_phrases] determines the main talking points in its input text. For example, for the input text "The food was delicious and there were wonderful staff", the API returns: "food" and "wonderful staff".
 
+<!-- SNIPPET:sample_extract_key_phrases.extract_key_phrases -->
+
 ```python
+import os
 from azure.core.credentials import AzureKeyCredential
 from azure.ai.textanalytics import TextAnalyticsClient
 
-credential = AzureKeyCredential("<api_key>")
-endpoint="https://<resource-name>.cognitiveservices.azure.com/"
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
+key = os.environ["AZURE_LANGUAGE_KEY"]
 
-text_analytics_client = TextAnalyticsClient(endpoint, credential)
-
-documents = [
-    "Redmond is a city in King County, Washington, United States, located 15 miles east of Seattle.",
+text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))
+articles = [
+    """
+    Washington, D.C. Autumn in DC is a uniquely beautiful season. The leaves fall from the trees
+    in a city chock-full of forests, leaving yellow leaves on the ground and a clearer view of the
+    blue sky above...
+    """,
     """
-    I need to take my cat to the veterinarian. He has been sick recently, and I need to take him
-    before I travel to South America for the summer.
+    Redmond, WA. In the past few days, Microsoft has decided to further postpone the start date of
+    its United States workers, due to the pandemic that rages with no end in sight...
     """,
+    """
+    Redmond, WA. Employees at Microsoft can be excited about the new coffee shop that will open on campus
+    once workers no longer have to work remotely...
+    """
 ]
 
-response = text_analytics_client.extract_key_phrases(documents, language="en")
-result = [doc for doc in response if not doc.is_error]
-
-for doc in result:
-    print(doc.key_phrases)
+result = text_analytics_client.extract_key_phrases(articles)
+for idx, doc in enumerate(result):
+    if not doc.is_error:
+        print("Key phrases in article #{}: {}".format(
+            idx + 1,
+            ", ".join(doc.key_phrases)
+        ))
 ```
 
+<!-- END SNIPPET -->
+
 The returned response is a heterogeneous list of result and error objects: list[[ExtractKeyPhrasesResult][extract_key_phrases_result], [DocumentError][document_error]]
 
 Please refer to the service documentation for a conceptual discussion of [key phrase extraction][key_phrase_extraction].
 
-### Detect language
+### Detect Language
 
 [detect_language][detect_language] determines the language of its input text, including the confidence score of the predicted language.
 
+<!-- SNIPPET:sample_detect_language.detect_language -->
+
 ```python
+import os
 from azure.core.credentials import AzureKeyCredential
 from azure.ai.textanalytics import TextAnalyticsClient
 
-credential = AzureKeyCredential("<api_key>")
-endpoint="https://<resource-name>.cognitiveservices.azure.com/"
-
-text_analytics_client = TextAnalyticsClient(endpoint, credential)
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
+key = os.environ["AZURE_LANGUAGE_KEY"]
 
+text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))
 documents = [
     """
-    This whole document is written in English. In order for the whole document to be written
-    in English, every sentence also has to be written in English, which it is.
+    The concierge Paulette was extremely helpful. Sadly when we arrived the elevator was broken, but with Paulette's help we barely noticed this inconvenience.
+    She arranged for our baggage to be brought up to our room with no extra charge and gave us a free meal to refurbish all of the calories we lost from
+    walking up the stairs :). Can't say enough good things about my experience!
     """,
-    "Il documento scritto in italiano.",
-    "Dies ist in deutsche Sprache verfasst."
+    """
+    最近由于工作压力太大，我们决定去富酒店度假。那儿的温泉实在太舒服了，我跟我丈夫都完全恢复了工作前的青春精神！加油！
+    """
 ]
 
-response = text_analytics_client.detect_language(documents)
-result = [doc for doc in response if not doc.is_error]
+result = text_analytics_client.detect_language(documents)
+reviewed_docs = [doc for doc in result if not doc.is_error]
+
+print("Let's see what language each review is in!")
 
-for doc in result:
-    print(f"Language detected: {doc.primary_language.name}")
-    print(f"ISO6391 name: {doc.primary_language.iso6391_name}")
-    print(f"Confidence score: {doc.primary_language.confidence_score}\n")
+for idx, doc in enumerate(reviewed_docs):
+    print("Review #{} is in '{}', which has ISO639-1 name '{}'\n".format(
+        idx, doc.primary_language.name, doc.primary_language.iso6391_name
+    ))
 ```
 
+<!-- END SNIPPET -->
+
 The returned response is a heterogeneous list of result and error objects: list[[DetectLanguageResult][detect_language_result], [DocumentError][document_error]]
 
 Please refer to the service documentation for a conceptual discussion of [language detection][language_detection]
 and [language and regional support][language_and_regional_support].
 
 ### Healthcare Entities Analysis
 
 [Long-running operation](#long-running-operations) [begin_analyze_healthcare_entities][analyze_healthcare_entities] extracts entities recognized within the healthcare domain, and identifies relationships between entities within the input document and links to known sources of information in various well known databases, such as UMLS, CHV, MSH, etc.
 
+<!-- SNIPPET:sample_analyze_healthcare_entities.analyze_healthcare_entities -->
+
 ```python
+import os
+import typing
 from azure.core.credentials import AzureKeyCredential
-from azure.ai.textanalytics import TextAnalyticsClient
+from azure.ai.textanalytics import TextAnalyticsClient, HealthcareEntityRelation
 
-credential = AzureKeyCredential("<api_key>")
-endpoint="https://<resource-name>.cognitiveservices.azure.com/"
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
+key = os.environ["AZURE_LANGUAGE_KEY"]
 
-text_analytics_client = TextAnalyticsClient(endpoint, credential)
+text_analytics_client = TextAnalyticsClient(
+    endpoint=endpoint,
+    credential=AzureKeyCredential(key),
+)
 
-documents = ["Subject is taking 100mg of ibuprofen twice daily"]
+documents = [
+    """
+    Patient needs to take 100 mg of ibuprofen, and 3 mg of potassium. Also needs to take
+    10 mg of Zocor.
+    """,
+    """
+    Patient needs to take 50 mg of ibuprofen, and 2 mg of Coumadin.
+    """
+]
 
 poller = text_analytics_client.begin_analyze_healthcare_entities(documents)
 result = poller.result()
 
 docs = [doc for doc in result if not doc.is_error]
 
-print("Results of Healthcare Entities Analysis:")
-for idx, doc in enumerate(docs):
+print("Let's first visualize the outputted healthcare result:")
+for doc in docs:
     for entity in doc.entities:
         print(f"Entity: {entity.text}")
         print(f"...Normalized Text: {entity.normalized_text}")
         print(f"...Category: {entity.category}")
         print(f"...Subcategory: {entity.subcategory}")
         print(f"...Offset: {entity.offset}")
         print(f"...Confidence score: {entity.confidence_score}")
@@ -539,16 +622,25 @@
             print(f"......Certainty: {entity.assertion.certainty}")
             print(f"......Association: {entity.assertion.association}")
     for relation in doc.entity_relations:
         print(f"Relation of type: {relation.relation_type} has the following roles")
         for role in relation.roles:
             print(f"...Role '{role.name}' with entity '{role.entity.text}'")
     print("------------------------------------------")
+
+print("Now, let's get all of medication dosage relations from the documents")
+dosage_of_medication_relations = [
+    entity_relation
+    for doc in docs
+    for entity_relation in doc.entity_relations if entity_relation.relation_type == HealthcareEntityRelation.DOSAGE_OF_MEDICATION
+]
 ```
 
+<!-- END SNIPPET -->
+
 Note: Healthcare Entities Analysis is only available with API version v3.1 and newer.
 
 ### Multiple Analysis
 
 [Long-running operation](#long-running-operations) [begin_analyze_actions][analyze_actions] performs multiple analyses over one set of documents in a single request. Currently it is supported using any combination of the following Language APIs in a single request:
 
 - Entities Recognition
@@ -559,65 +651,119 @@
 - Custom Entity Recognition (API version 2022-05-01 and newer)
 - Custom Single Label Classification (API version 2022-05-01 and newer)
 - Custom Multi Label Classification (API version 2022-05-01 and newer)
 - Healthcare Entities Analysis (API version 2022-05-01 and newer)
 - Extractive Summarization (API version 2022-10-01-preview and newer)
 - Abstractive Summarization (API version 2022-10-01-preview and newer)
 
+<!-- SNIPPET:sample_analyze_actions.analyze -->
+
 ```python
+import os
 from azure.core.credentials import AzureKeyCredential
 from azure.ai.textanalytics import (
     TextAnalyticsClient,
     RecognizeEntitiesAction,
+    RecognizeLinkedEntitiesAction,
+    RecognizePiiEntitiesAction,
+    ExtractKeyPhrasesAction,
     AnalyzeSentimentAction,
 )
 
-credential = AzureKeyCredential("<api_key>")
-endpoint="https://<resource-name>.cognitiveservices.azure.com/"
+endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
+key = os.environ["AZURE_LANGUAGE_KEY"]
 
-text_analytics_client = TextAnalyticsClient(endpoint, credential)
+text_analytics_client = TextAnalyticsClient(
+    endpoint=endpoint,
+    credential=AzureKeyCredential(key),
+)
 
-documents = ["Microsoft was founded by Bill Gates and Paul Allen."]
+documents = [
+    'We went to Contoso Steakhouse located at midtown NYC last week for a dinner party, and we adore the spot! '
+    'They provide marvelous food and they have a great menu. The chief cook happens to be the owner (I think his name is John Doe) '
+    'and he is super nice, coming out of the kitchen and greeted us all.'
+    ,
+
+    'We enjoyed very much dining in the place! '
+    'The Sirloin steak I ordered was tender and juicy, and the place was impeccably clean. You can even pre-order from their '
+    'online menu at www.contososteakhouse.com, call 312-555-0176 or send email to order@contososteakhouse.com! '
+    'The only complaint I have is the food didn\'t come fast enough. Overall I highly recommend it!'
+]
 
 poller = text_analytics_client.begin_analyze_actions(
     documents,
     display_name="Sample Text Analysis",
     actions=[
         RecognizeEntitiesAction(),
-        AnalyzeSentimentAction()
-    ]
+        RecognizePiiEntitiesAction(),
+        ExtractKeyPhrasesAction(),
+        RecognizeLinkedEntitiesAction(),
+        AnalyzeSentimentAction(),
+    ],
 )
 
-# returns multiple actions results in the same order as the inputted actions
 document_results = poller.result()
 for doc, action_results in zip(documents, document_results):
     print(f"\nDocument text: {doc}")
     for result in action_results:
         if result.kind == "EntityRecognition":
             print("...Results of Recognize Entities Action:")
             for entity in result.entities:
                 print(f"......Entity: {entity.text}")
                 print(f".........Category: {entity.category}")
                 print(f".........Confidence Score: {entity.confidence_score}")
                 print(f".........Offset: {entity.offset}")
 
+        elif result.kind == "PiiEntityRecognition":
+            print("...Results of Recognize PII Entities action:")
+            for pii_entity in result.entities:
+                print(f"......Entity: {pii_entity.text}")
+                print(f".........Category: {pii_entity.category}")
+                print(f".........Confidence Score: {pii_entity.confidence_score}")
+
+        elif result.kind == "KeyPhraseExtraction":
+            print("...Results of Extract Key Phrases action:")
+            print(f"......Key Phrases: {result.key_phrases}")
+
+        elif result.kind == "EntityLinking":
+            print("...Results of Recognize Linked Entities action:")
+            for linked_entity in result.entities:
+                print(f"......Entity name: {linked_entity.name}")
+                print(f".........Data source: {linked_entity.data_source}")
+                print(f".........Data source language: {linked_entity.language}")
+                print(
+                    f".........Data source entity ID: {linked_entity.data_source_entity_id}"
+                )
+                print(f".........Data source URL: {linked_entity.url}")
+                print(".........Document matches:")
+                for match in linked_entity.matches:
+                    print(f"............Match text: {match.text}")
+                    print(f"............Confidence Score: {match.confidence_score}")
+                    print(f"............Offset: {match.offset}")
+                    print(f"............Length: {match.length}")
+
         elif result.kind == "SentimentAnalysis":
             print("...Results of Analyze Sentiment action:")
             print(f"......Overall sentiment: {result.sentiment}")
-            print(f"......Scores: positive={result.confidence_scores.positive}; "
-                  f"neutral={result.confidence_scores.neutral}; "
-                  f"negative={result.confidence_scores.negative}\n")
+            print(
+                f"......Scores: positive={result.confidence_scores.positive}; \
+                neutral={result.confidence_scores.neutral}; \
+                negative={result.confidence_scores.negative} \n"
+            )
 
         elif result.is_error is True:
-            print(f"......Is an error with code '{result.code}' "
-                  f"and message '{result.message}'")
+            print(
+                f"...Is an error with code '{result.error.code}' and message '{result.error.message}'"
+            )
 
     print("------------------------------------------")
 ```
 
+<!-- END SNIPPET -->
+
 The returned response is an object encapsulating multiple iterables, each representing results of individual analyses.
 
 Note: Multiple analysis is available in API version v3.1 and newer.
 
 ## Optional Configuration
 
 Optional keyword arguments can be passed in at the client and per-operation level.
@@ -689,20 +835,21 @@
 - Detect language: [sample_detect_language.py][detect_language_sample] ([async version][detect_language_sample_async])
 - Healthcare Entities Analysis: [sample_analyze_healthcare_entities.py][analyze_healthcare_entities_sample] ([async version][analyze_healthcare_entities_sample_async])
 - Multiple Analysis: [sample_analyze_actions.py][analyze_sample] ([async version][analyze_sample_async])
 - Custom Entity Recognition: [sample_recognize_custom_entities.py][recognize_custom_entities_sample] ([async_version][recognize_custom_entities_sample_async])
 - Custom Single Label Classification: [sample_single_label_classify.py][single_label_classify_sample] ([async_version][single_label_classify_sample_async])
 - Custom Multi Label Classification: [sample_multi_label_classify.py][multi_label_classify_sample] ([async_version][multi_label_classify_sample_async])
 - Extractive text summarization: [sample_extract_summary.py][extract_summary_sample] ([async version][extract_summary_sample_async])
-- Abstractive text summarization: [sample_abstract_summary.py][abstract_summary_sample] ([async version][abstract_summary_sample_async])
+- Abstractive text summarization: [sample_abstractive_summary.py][abstractive_summary_sample] ([async version][abstractive_summary_sample_async])
 - Dynamic Classification: [sample_dynamic_classification.py][dynamic_classification_sample] ([async_version][dynamic_classification_sample_async])
 
 Advanced scenarios
 
 - Opinion Mining: [sample_analyze_sentiment_with_opinion_mining.py][opinion_mining_sample] ([async_version][opinion_mining_sample_async])
+- NER resolutions: [sample_recognize_entity_resolutions.py][recognize_entity_resolutions_sample] ([async_version][recognize_entity_resolutions_sample_async])
 
 ### Additional documentation
 
 For more extensive documentation on Azure Cognitive Service for Language, see the [Language Service documentation][language_product_documentation] on docs.microsoft.com.
 
 ## Contributing
 
@@ -719,15 +866,16 @@
 [ta_ref_docs]: https://aka.ms/azsdk-python-textanalytics-ref-docs
 [ta_samples]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples
 [language_product_documentation]: https://docs.microsoft.com/azure/cognitive-services/language-service
 [azure_subscription]: https://azure.microsoft.com/free/
 [ta_or_cs_resource]: https://docs.microsoft.com/azure/cognitive-services/cognitive-services-apis-create-account?tabs=multiservice%2Cwindows
 [pip]: https://pypi.org/project/pip/
 [azure_portal_create_ta_resource]: https://ms.portal.azure.com/#create/Microsoft.CognitiveServicesTextAnalytics
-[azure_cli_create_ta_resource]: https://docs.microsoft.com/azure/cognitive-services/cognitive-services-apis-create-account-cli?tabs=windows
+[azure_cli]: https://docs.microsoft.com/cli/azure
+[azure_cli_create_ta_resource]: https://learn.microsoft.com/azure/cognitive-services/cognitive-services-apis-create-account-cli
 [multi_and_single_service]: https://docs.microsoft.com/azure/cognitive-services/cognitive-services-apis-create-account?tabs=multiservice%2Cwindows
 [azure_cli_endpoint_lookup]: https://docs.microsoft.com/cli/azure/cognitiveservices/account?view=azure-cli-latest#az-cognitiveservices-account-show
 [azure_portal_get_endpoint]: https://docs.microsoft.com/azure/cognitive-services/cognitive-services-apis-create-account?tabs=multiservice%2Cwindows#get-the-keys-for-your-resource
 [cognitive_authentication]: https://docs.microsoft.com/azure/cognitive-services/authentication
 [cognitive_authentication_api_key]: https://docs.microsoft.com/azure/cognitive-services/cognitive-services-apis-create-account?tabs=multiservice%2Cwindows#get-the-keys-for-your-resource
 [install_azure_identity]: https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/identity/azure-identity#install-the-package
 [register_aad_app]: https://docs.microsoft.com/azure/cognitive-services/authentication#assign-a-role-to-a-service-principal
@@ -759,15 +907,15 @@
 [detect_language]: https://aka.ms/azsdk-python-textanalytics-detectlanguage
 [language_detection]: https://docs.microsoft.com/azure/cognitive-services/language-service/language-detection/overview
 [language_and_regional_support]: https://docs.microsoft.com/azure/cognitive-services/language-service/language-detection/language-support
 [sentiment_analysis]: https://docs.microsoft.com/azure/cognitive-services/language-service/sentiment-opinion-mining/overview
 [key_phrase_extraction]: https://docs.microsoft.com/azure/cognitive-services/language-service/key-phrase-extraction/overview
 [linked_entities_categories]: https://aka.ms/taner
 [linked_entity_recognition]: https://docs.microsoft.com/azure/cognitive-services/language-service/entity-linking/overview
-[pii_entity_categories]: https://aka.ms/tanerpii
+[pii_entity_categories]: https://aka.ms/azsdk/language/pii
 [named_entity_recognition]: https://docs.microsoft.com/azure/cognitive-services/language-service/named-entity-recognition/overview
 [named_entity_categories]: https://aka.ms/taner
 [azure_core_ref_docs]: https://aka.ms/azsdk-python-core-policies
 [azure_core]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/core/azure-core/README.md
 [azure_identity]: https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/identity/azure-identity
 [python_logging]: https://docs.python.org/3/library/logging.html
 [sample_authentication]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_authentication.py
@@ -795,26 +943,49 @@
 [single_label_classify_sample]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_single_label_classify.py
 [single_label_classify_sample_async]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/async_samples/sample_single_label_classify_async.py
 [multi_label_classify_sample]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_multi_label_classify.py
 [multi_label_classify_sample_async]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/async_samples/sample_multi_label_classify_async.py
 [healthcare_action_sample]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_analyze_healthcare_action.py
 [extract_summary_sample]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_extract_summary.py
 [extract_summary_sample_async]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/async_samples/sample_extract_summary_async.py
-[abstract_summary_sample]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_abstract_summary.py
-[abstract_summary_sample_async]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/async_samples/sample_abstract_summary_async.py
+[abstractive_summary_sample]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_abstractive_summary.py
+[abstractive_summary_sample_async]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/async_samples/sample_abstractive_summary_async.py
 [dynamic_classification_sample]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_dynamic_classification.py
 [dynamic_classification_sample_async]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/async_samples/sample_dynamic_classification_async.py
+[recognize_entity_resolutions_sample]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_recognize_entity_resolutions.py
+[recognize_entity_resolutions_sample_async]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/async_samples/sample_recognize_entity_resolutions_async.py
 [cla]: https://cla.microsoft.com
 [code_of_conduct]: https://opensource.microsoft.com/codeofconduct/
 [coc_faq]: https://opensource.microsoft.com/codeofconduct/faq/
 [coc_contact]: mailto:opencode@microsoft.com
 
 
 # Release History
 
+## 5.3.0b2 (2023-03-07)
+
+This version of the client library defaults to the service API version `2022-10-01-preview`.
+
+### Features Added
+
+- Added `begin_extract_summary` client method to perform extractive summarization on documents.
+- Added `begin_abstractive_summary` client method to perform abstractive summarization on documents.
+
+### Breaking Changes
+
+- Removed models `BaseResolution` and `BooleanResolution`.
+- Removed enum value `BooleanResolution` from `ResolutionKind`.
+- Renamed model `AbstractSummaryAction` to `AbstractiveSummaryAction`.
+- Renamed model `AbstractSummaryResult` to `AbstractiveSummaryResult`.
+- Removed keyword argument `autodetect_default_language` from long-running operation APIs.
+
+### Other Changes
+
+ - Improved static typing in the client library. 
+
 ## 5.3.0b1 (2022-11-17)
 
 This version of the client library defaults to the service API version `2022-10-01-preview`.
 
 ### Features Added
 - Added the Extractive Summarization feature and related models: `ExtractSummaryAction`, `ExtractSummaryResult`, and `SummarySentence`.
   Access the feature through the `begin_analyze_actions` API.
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/sample_get_detailed_diagnostics_information.py` & `azure-ai-textanalytics-5.3.0b2/samples/sample_get_detailed_diagnostics_information.py`

 * *Files 1% similar despite different names*

```diff
@@ -15,24 +15,22 @@
     python sample_get_detailed_diagnostics_information.py
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
 """
 
-import os
-import logging
-import json
-
-_LOGGER = logging.getLogger(__name__)
-
 
 def sample_get_detailed_diagnostics_information() -> None:
+    import os
+    import logging
+    import json
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics import TextAnalyticsClient
+    _LOGGER = logging.getLogger(__name__)
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
 
     # This client will log detailed information about its HTTP sessions, at DEBUG level
     text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key), logging_enable=True)
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/sample_recognize_pii_entities.py` & `azure-ai-textanalytics-5.3.0b2/samples/sample_recognize_pii_entities.py`

 * *Files 3% similar despite different names*

```diff
@@ -10,33 +10,34 @@
 DESCRIPTION:
     This sample demonstrates how to recognize personally identifiable information in a batch of documents.
     The endpoint recognize_pii_entities is only available for API version v3.1 and up.
 
     In this sample, we will be working for a company that handles loan payments. To follow privacy guidelines,
     we need to redact all of our information before we make it public.
 
+    See more information in the service docs: https://aka.ms/azsdk/language/pii
+
 USAGE:
     python sample_recognize_pii_entities.py
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
 """
 
-import os
-
 
 def sample_recognize_pii_entities() -> None:
     print(
         "In this sample we will be going through our customer's loan payment information and redacting "
         "all PII (personally identifiable information) before storing this information on our public website. "
         "I'm also looking to explicitly extract the SSN information, so I can update my database with SSNs for "
         "our customers"
     )
     # [START recognize_pii_entities]
+    import os
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics import TextAnalyticsClient
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
 
     text_analytics_client = TextAnalyticsClient(
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/sample_recognize_entities.py` & `azure-ai-textanalytics-5.3.0b2/samples/sample_recognize_entities.py`

 * *Files 6% similar despite different names*

```diff
@@ -16,25 +16,24 @@
     python sample_recognize_entities.py
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
 """
 
-from __future__ import annotations
-import os
-
 
 def sample_recognize_entities() -> None:
     print(
         "In this sample, we are a catering business, and we're looking to sort the reviews "
         "for our organization based off of the organization that hired us for catering"
     )
-    organization_to_reviews: dict[str, list[str]] = {}
+
     # [START recognize_entities]
+    import os
+    import typing
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics import TextAnalyticsClient
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
 
     text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))
@@ -45,26 +44,27 @@
         Though the food was still great, I feel there has been a quality drop since their last time
         catering for us. Is anyone else running into the same problem?""",
         """Bar Company is over the moon about the service we received from Contoso, the best sliders ever!!!!"""
     ]
 
     result = text_analytics_client.recognize_entities(reviews)
     result = [review for review in result if not review.is_error]
+    organization_to_reviews: typing.Dict[str, typing.List[str]] = {}
 
     for idx, review in enumerate(result):
         for entity in review.entities:
             print(f"Entity '{entity.text}' has category '{entity.category}'")
-    # [END recognize_entities]
             if entity.category == 'Organization':
                 organization_to_reviews.setdefault(entity.text, [])
                 organization_to_reviews[entity.text].append(reviews[idx])
 
     for organization, reviews in organization_to_reviews.items():
         print(
             "\n\nOrganization '{}' has left us the following review(s): {}".format(
                 organization, "\n\n".join(reviews)
             )
         )
+    # [END recognize_entities]
 
 
 if __name__ == '__main__':
     sample_recognize_entities()
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/sample_single_label_classify.py` & `azure-ai-textanalytics-5.3.0b2/samples/sample_single_label_classify.py`

 * *Files 0% similar despite different names*

```diff
@@ -23,19 +23,17 @@
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
     3) SINGLE_LABEL_CLASSIFY_PROJECT_NAME - your Language Studio project name
     4) SINGLE_LABEL_CLASSIFY_DEPLOYMENT_NAME - your Language Studio deployment name
 """
 
 
-import os
-
-
 def sample_classify_document_single_label() -> None:
     # [START single_label_classify]
+    import os
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics import TextAnalyticsClient
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
     project_name = os.environ["SINGLE_LABEL_CLASSIFY_PROJECT_NAME"]
     deployment_name = os.environ["SINGLE_LABEL_CLASSIFY_DEPLOYMENT_NAME"]
@@ -66,14 +64,14 @@
         if classification_result.kind == "CustomDocumentClassification":
             classification = classification_result.classifications[0]
             print("The document text '{}' was classified as '{}' with confidence score {}.".format(
                 doc, classification.category, classification.confidence_score)
             )
         elif classification_result.is_error is True:
             print("Document text '{}' has an error with code '{}' and message '{}'".format(
-                doc, classification_result.code, classification_result.message
+                doc, classification_result.error.code, classification_result.error.message
             ))
     # [END single_label_classify]
 
 
 if __name__ == "__main__":
     sample_classify_document_single_label()
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/sample_authentication.py` & `azure-ai-textanalytics-5.3.0b2/samples/sample_authentication.py`

 * *Files 1% similar despite different names*

```diff
@@ -24,20 +24,19 @@
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language API key
     3) AZURE_CLIENT_ID - the client ID of your active directory application.
     4) AZURE_TENANT_ID - the tenant ID of your active directory application.
     5) AZURE_CLIENT_SECRET - the secret of your active directory application.
 """
 
-import os
-
 
 def sample_authentication_with_api_key_credential() -> None:
     print("\n.. authentication_with_api_key_credential")
     # [START create_ta_client_with_key]
+    import os
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics import TextAnalyticsClient
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
 
     text_analytics_client = TextAnalyticsClient(endpoint, AzureKeyCredential(key))
     # [END create_ta_client_with_key]
@@ -55,14 +54,15 @@
 
 def sample_authentication_with_azure_active_directory() -> None:
     """DefaultAzureCredential will use the values from these environment
     variables: AZURE_CLIENT_ID, AZURE_TENANT_ID, AZURE_CLIENT_SECRET
     """
     print("\n.. authentication_with_azure_active_directory")
     # [START create_ta_client_with_aad]
+    import os
     from azure.ai.textanalytics import TextAnalyticsClient
     from azure.identity import DefaultAzureCredential
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     credential = DefaultAzureCredential()
 
     text_analytics_client = TextAnalyticsClient(endpoint, credential=credential)
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/sample_analyze_healthcare_action.py` & `azure-ai-textanalytics-5.3.0b2/samples/sample_analyze_healthcare_action.py`

 * *Files 2% similar despite different names*

```diff
@@ -18,18 +18,16 @@
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
 """
 
 
-import os
-
-
 def sample_analyze_healthcare_action() -> None:
+    import os
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics import (
         TextAnalyticsClient,
         AnalyzeHealthcareEntitiesAction,
         RecognizePiiEntitiesAction,
     )
 
@@ -87,20 +85,20 @@
                     print(f"Relation of type: {relation.relation_type} has the following roles")
                     for role in relation.roles:
                         print(f"...Role '{role.name}' with entity '{role.entity.text}'")
                 print(f"......FHIR object: {result.fhir_bundle}")
 
             elif result.kind == "PiiEntityRecognition":
                 print("Results of Recognize PII Entities action:")
-                for entity in result.entities:
-                    print(f"......Entity: {entity.text}")
-                    print(f".........Category: {entity.category}")
-                    print(f".........Confidence Score: {entity.confidence_score}")
+                for pii_entity in result.entities:
+                    print(f"......Entity: {pii_entity.text}")
+                    print(f".........Category: {pii_entity.category}")
+                    print(f".........Confidence Score: {pii_entity.confidence_score}")
 
             elif result.is_error is True:
-                print(f"...Is an error with code '{result.code}' and message '{result.message}'")
+                print(f"...Is an error with code '{result.error.code}' and message '{result.error.message}'")
 
             print("------------------------------------------")
 
 
 if __name__ == "__main__":
     sample_analyze_healthcare_action()
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/sample_recognize_custom_entities.py` & `azure-ai-textanalytics-5.3.0b2/samples/sample_recognize_custom_entities.py`

 * *Files 2% similar despite different names*

```diff
@@ -21,19 +21,17 @@
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
     3) CUSTOM_ENTITIES_PROJECT_NAME - your Language Studio project name
     4) CUSTOM_ENTITIES_DEPLOYMENT_NAME - your Language Studio deployment name
 """
 
 
-import os
-
-
 def sample_recognize_custom_entities() -> None:
     # [START recognize_custom_entities]
+    import os
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics import TextAnalyticsClient
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
     project_name = os.environ["CUSTOM_ENTITIES_PROJECT_NAME"]
     deployment_name = os.environ["CUSTOM_ENTITIES_DEPLOYMENT_NAME"]
@@ -66,15 +64,15 @@
                 print(
                     "Entity '{}' has category '{}' with confidence score of '{}'".format(
                         entity.text, entity.category, entity.confidence_score
                     )
                 )
         elif custom_entities_result.is_error is True:
             print("...Is an error with code '{}' and message '{}'".format(
-                custom_entities_result.code, custom_entities_result.message
+                custom_entities_result.error.code, custom_entities_result.error.message
                 )
             )
     # [END recognize_custom_entities]
 
 
 if __name__ == "__main__":
     sample_recognize_custom_entities()
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/sample_extract_key_phrases.py` & `azure-ai-textanalytics-5.3.0b2/samples/sample_extract_key_phrases.py`

 * *Files 2% similar despite different names*

```diff
@@ -17,23 +17,22 @@
     python sample_extract_key_phrases.py
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
 """
 
-import os
-
 
 def sample_extract_key_phrases() -> None:
     print(
         "In this sample, we want to find the articles that mention Microsoft to read."
     )
     articles_that_mention_microsoft = []
     # [START extract_key_phrases]
+    import os
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics import TextAnalyticsClient
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
 
     text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/sample_multi_label_classify.py` & `azure-ai-textanalytics-5.3.0b2/samples/sample_multi_label_classify.py`

 * *Files 2% similar despite different names*

```diff
@@ -23,19 +23,17 @@
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
     3) MULTI_LABEL_CLASSIFY_PROJECT_NAME - your Language Studio project name
     4) MULTI_LABEL_CLASSIFY_DEPLOYMENT_NAME - your Language Studio deployment name
 """
 
 
-import os
-
-
 def sample_classify_document_multi_label() -> None:
     # [START multi_label_classify]
+    import os
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics import TextAnalyticsClient
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
     project_name = os.environ["MULTI_LABEL_CLASSIFY_PROJECT_NAME"]
     deployment_name = os.environ["MULTI_LABEL_CLASSIFY_DEPLOYMENT_NAME"]
@@ -68,14 +66,14 @@
             print(f"\nThe movie plot '{doc}' was classified as the following genres:\n")
             for classification in classifications:
                 print("'{}' with confidence score {}.".format(
                     classification.category, classification.confidence_score
                 ))
         elif classification_result.is_error is True:
             print("Movie plot '{}' has an error with code '{}' and message '{}'".format(
-                doc, classification_result.code, classification_result.message
+                doc, classification_result.error.code, classification_result.error.message
             ))
     # [END multi_label_classify]
 
 
 if __name__ == "__main__":
     sample_classify_document_multi_label()
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/sample_analyze_healthcare_entities.py` & `azure-ai-textanalytics-5.3.0b2/samples/sample_analyze_healthcare_entities.py`

 * *Files 2% similar despite different names*

```diff
@@ -18,29 +18,28 @@
     python sample_analyze_healthcare_entities.py
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
 """
 
-from __future__ import annotations
-
 
 def sample_analyze_healthcare_entities() -> None:
 
     print(
         "In this sample we will be combing through the prescriptions our pharmacy has fulfilled "
         "so we can catalog how much inventory we have"
     )
     print(
         "We start out with a list of prescription documents."
     )
 
     # [START analyze_healthcare_entities]
     import os
+    import typing
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics import TextAnalyticsClient, HealthcareEntityRelation
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
 
     text_analytics_client = TextAnalyticsClient(
@@ -100,15 +99,15 @@
         "Now, I will create a dictionary of medication to total dosage. "
         "I will use a regex to extract the dosage amount. For simplicity sake, I will assume "
         "all dosages are represented with numbers and have mg unit."
     )
     import re
     from collections import defaultdict
 
-    medication_to_dosage: dict[str, int] = defaultdict(int)
+    medication_to_dosage: typing.Dict[str, int] = defaultdict(int)
 
     for relation in dosage_of_medication_relations:
         # The DosageOfMedication relation should only contain the dosage and medication roles
 
         dosage_role = next(iter(filter(lambda x: x.name == "Dosage", relation.roles)))
         medication_role = next(iter(filter(lambda x: x.name == "Medication", relation.roles)))
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/sample_analyze_sentiment_with_opinion_mining.py` & `azure-ai-textanalytics-5.3.0b2/samples/sample_analyze_sentiment_with_opinion_mining.py`

 * *Files 1% similar despite different names*

```diff
@@ -38,20 +38,18 @@
     Users have made 1 complaints about 'service', specifically saying that it's 'unacceptable'
     Users have made 3 complaints about 'toilet', specifically saying that it's 'smelly', 'broken', 'dirty'
 
 
     Looking at the breakdown, I can see what aspects of my hotel need improvement, and based off of both the number and content of the complaints users have made about my toilets, I need to get that fixed ASAP.
 """
 
-from __future__ import annotations
-import os
-import typing
-
 
 def sample_analyze_sentiment_with_opinion_mining() -> None:
+    import os
+    import typing
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics import TextAnalyticsClient
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
 
     text_analytics_client = TextAnalyticsClient(
@@ -98,30 +96,31 @@
         "let's find the complaints users have about individual aspects of this hotel"
     )
 
     print(
         "\nIn order to do that, I'm going to extract targets of a negative sentiment. "
         "I'm going to map each of these targets to the mined opinion object we get back to aggregate the reviews by target. "
     )
-    target_to_complaints: dict[str, typing.Any] = {}
+    target_to_complaints: typing.Dict[str, typing.Any] = {}
 
     for document in doc_result:
         for sentence in document.sentences:
-            for mined_opinion in sentence.mined_opinions:
-                target = mined_opinion.target
-                if target.sentiment == 'negative':
-                    target_to_complaints.setdefault(target.text, [])
-                    target_to_complaints[target.text].append(mined_opinion)
+            if sentence.mined_opinions:
+                for mined_opinion in sentence.mined_opinions:
+                    target = mined_opinion.target
+                    if target.sentiment == 'negative':
+                        target_to_complaints.setdefault(target.text, [])
+                        target_to_complaints[target.text].append(mined_opinion)
 
     print("\nLet's now go through the aspects of our hotel people have complained about and see what users have specifically said")
 
-    for target, complaints in target_to_complaints.items():
+    for target_name, complaints in target_to_complaints.items():
         print("Users have made {} complaint(s) about '{}', specifically saying that it's '{}'".format(
             len(complaints),
-            target,
+            target_name,
             "', '".join(
                 [assessment.text for complaint in complaints for assessment in complaint.assessments]
             )
         ))
 
 
     print(
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/sample_analyze_actions.py` & `azure-ai-textanalytics-5.3.0b2/samples/sample_analyze_actions.py`

 * *Files 3% similar despite different names*

```diff
@@ -18,19 +18,17 @@
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
 """
 
 
-import os
-
-
 def sample_analyze_actions() -> None:
     # [START analyze]
+    import os
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics import (
         TextAnalyticsClient,
         RecognizeEntitiesAction,
         RecognizeLinkedEntitiesAction,
         RecognizePiiEntitiesAction,
         ExtractKeyPhrasesAction,
@@ -79,18 +77,18 @@
                     print(f"......Entity: {entity.text}")
                     print(f".........Category: {entity.category}")
                     print(f".........Confidence Score: {entity.confidence_score}")
                     print(f".........Offset: {entity.offset}")
 
             elif result.kind == "PiiEntityRecognition":
                 print("...Results of Recognize PII Entities action:")
-                for entity in result.entities:
-                    print(f"......Entity: {entity.text}")
-                    print(f".........Category: {entity.category}")
-                    print(f".........Confidence Score: {entity.confidence_score}")
+                for pii_entity in result.entities:
+                    print(f"......Entity: {pii_entity.text}")
+                    print(f".........Category: {pii_entity.category}")
+                    print(f".........Confidence Score: {pii_entity.confidence_score}")
 
             elif result.kind == "KeyPhraseExtraction":
                 print("...Results of Extract Key Phrases action:")
                 print(f"......Key Phrases: {result.key_phrases}")
 
             elif result.kind == "EntityLinking":
                 print("...Results of Recognize Linked Entities action:")
@@ -116,15 +114,15 @@
                     f"......Scores: positive={result.confidence_scores.positive}; \
                     neutral={result.confidence_scores.neutral}; \
                     negative={result.confidence_scores.negative} \n"
                 )
 
             elif result.is_error is True:
                 print(
-                    f"...Is an error with code '{result.code}' and message '{result.message}'"
+                    f"...Is an error with code '{result.error.code}' and message '{result.error.message}'"
                 )
 
         print("------------------------------------------")
 
     # [END analyze]
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/sample_analyze_healthcare_entities_with_cancellation.py` & `azure-ai-textanalytics-5.3.0b2/samples/sample_analyze_healthcare_entities_with_cancellation.py`

 * *Files 2% similar despite different names*

```diff
@@ -15,20 +15,18 @@
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
 """
 
 
-import os
-from azure.core.exceptions import HttpResponseError
-
-
 def sample_analyze_healthcare_entities_with_cancellation() -> None:
     # [START analyze_healthcare_entities_with_cancellation]
+    import os
+    from azure.core.exceptions import HttpResponseError
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics import TextAnalyticsClient
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
 
     text_analytics_client = TextAnalyticsClient(
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/README.md` & `azure-ai-textanalytics-5.3.0b2/samples/README.md`

 * *Files 3% similar despite different names*

```diff
@@ -31,15 +31,15 @@
 |[sample_analyze_actions.py][analyze_sample] and [sample_analyze_actions_async.py][analyze_sample_async]|Run multiple analyses together in a single request|
 |[sample_recognize_custom_entities.py][recognize_custom_entities_sample] and [sample_recognize_custom_entities_async.py][recognize_custom_entities_sample_async]|Use a custom model to recognize custom entities in documents|
 |[sample_single_label_classify.py][single_label_classify_sample] and [sample_single_label_classify_async.py][single_label_classify_sample_async]|Use a custom model to classify documents into a single category|
 |[sample_multi_label_classify.py][multi_label_classify_sample] and [sample_multi_label_classify_async.py][multi_label_classify_sample_async]|Use a custom model to classify documents into multiple categories|
 |[sample_model_version.py][sample_model_version] and [sample_model_version_async.py][sample_model_version_async]|Set the model version for pre-built Text Analytics models|
 |[sample_analyze_healthcare_action.py][sample_analyze_healthcare_action] and [sample_analyze_healthcare_action_async.py][sample_analyze_healthcare_action_async]|Run a healthcare and PII analysis together|
 |[sample_extract_summary.py][extract_summary_sample] and [sample_extract_summary_async.py][extract_summary_sample_async]|As part of the analyze API, run extractive text summarization on documents|
-|[sample_abstract_summary.py][abstract_summary_sample] and [sample_abstract_summary_async.py][abstract_summary_sample_async]|As part of the analyze API, run abstractive text summarization on documents|
+|[sample_abstractive_summary.py][abstractive_summary_sample] and [sample_abstractive_summary_async.py][abstractive_summary_sample_async]|As part of the analyze API, run abstractive text summarization on documents|
 |[sample_dynamic_classification.py][dynamic_classification_sample] and [sample_dynamic_classification_async.py][dynamic_classification_sample_async]|Dynamically classify documents without needing to train a model.|
 
 ## Prerequisites
 * Python 3.7 or later is required to use this package
 * You must have an [Azure subscription][azure_subscription] and an
 [Azure Language account][azure_language_account] to run these samples.
 
@@ -72,14 +72,16 @@
 what you can do with the Azure Text Analytics client library.
 
 |**Advanced Sample File Name**|**Description**|
 |----------------|-------------|
 |[sample_analyze_sentiment_with_opinion_mining.py][sample_analyze_sentiment_with_opinion_mining] and [sample_analyze_sentiment_with_opinion_mining_async.py][sample_analyze_sentiment_with_opinion_mining_async]|Analyze sentiment in documents with granular analysis into individual opinions present in a sentence. Only available with API version v3.1 and up.|
 |[sample_get_detailed_diagnostics_information.py][get_detailed_diagnostics_information] and [sample_get_detailed_diagnostics_information_async.py][get_detailed_diagnostics_information_async]|Get the request batch statistics, model version, and raw response in JSON format through a callback|
 |[sample_analyze_healthcare_entities_with_cancellation.py][sample_analyze_healthcare_entities_with_cancellation] and [sample_analyze_healthcare_entities_with_cancellation_async.py][sample_analyze_healthcare_entities_with_cancellation_async]|Cancel an analyze healthcare entities operation after it's started.|
+|[sample_recognize_entity_resolutions.py][recognize_entity_resolutions_sample] and [sample_recognize_entity_resolutions_async.py][recognize_entity_resolutions_sample_async]|Use NER with resolutions to normalize entities to standard formats.|
+
 
 [versioning_story_readme]: https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/textanalytics/azure-ai-textanalytics#install-the-package
 [azure_identity]: https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/identity/azure-identity
 [sample_authentication]: https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_authentication.py
 [sample_authentication_async]: https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/textanalytics/azure-ai-textanalytics/samples/async_samples/sample_authentication_async.py
 [detect_language]: https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_detect_language.py
 [detect_language_async]: https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/textanalytics/azure-ai-textanalytics/samples/async_samples/sample_detect_language_async.py
@@ -113,16 +115,18 @@
 [multi_label_classify_sample_async]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/async_samples/sample_multi_label_classify_async.py
 [sample_model_version]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_model_version.py
 [sample_model_version_async]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/async_samples/sample_model_version_async.py
 [sample_analyze_healthcare_action]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_analyze_healthcare_action.py
 [sample_analyze_healthcare_action_async]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/async_samples/sample_analyze_healthcare_action_async.py
 [extract_summary_sample]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_extract_summary.py
 [extract_summary_sample_async]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/async_samples/sample_extract_summary_async.py
-[abstract_summary_sample]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_abstract_summary.py
-[abstract_summary_sample_async]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/async_samples/sample_abstract_summary_async.py
+[abstractive_summary_sample]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_abstractive_summary.py
+[abstractive_summary_sample_async]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/async_samples/sample_abstractive_summary_async.py
 [dynamic_classification_sample]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_dynamic_classification.py
 [dynamic_classification_sample_async]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/async_samples/sample_dynamic_classification_async.py
+[recognize_entity_resolutions_sample]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_recognize_entity_resolutions.py
+[recognize_entity_resolutions_sample_async]: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/async_samples/sample_recognize_entity_resolutions_async.py
 [pip]: https://pypi.org/project/pip/
 [azure_subscription]: https://azure.microsoft.com/free/
 [azure_language_account]: https://docs.microsoft.com/azure/cognitive-services/cognitive-services-apis-create-account?tabs=singleservice%2Cwindows
 [azure_identity_pip]: https://pypi.org/project/azure-identity/
 [api_reference_documentation]: https://aka.ms/azsdk-python-textanalytics-ref-docs
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/sample_recognize_linked_entities.py` & `azure-ai-textanalytics-5.3.0b2/samples/sample_recognize_linked_entities.py`

 * *Files 2% similar despite different names*

```diff
@@ -20,25 +20,23 @@
     python sample_recognize_linked_entities.py
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
 """
 
-import os
-
-
 
 def sample_recognize_linked_entities() -> None:
     print(
         "In this sample, we are students conducting research for a class project. We will extract "
         "links to Wikipedia articles for all entities listed in our research documents, so we have "
         "all of the necessary information for research purposes."
     )
     # [START recognize_linked_entities]
+    import os
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics import TextAnalyticsClient
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
 
     text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))
@@ -66,15 +64,15 @@
                 entity.name, len(entity.matches)
             ))
             if entity.data_source == "Wikipedia":
                 entity_to_url[entity.name] = entity.url
     # [END recognize_linked_entities]
 
     print("\nNow let's see all of the Wikipedia articles we've extracted from our research documents")
-    for entity, url in entity_to_url.items():
+    for entity_name, url in entity_to_url.items():
         print("Link to Wikipedia article for '{}': {}".format(
-                entity, url
+                entity_name, url
         ))
 
 
 if __name__ == '__main__':
     sample_recognize_linked_entities()
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/sample_abstract_summary.py` & `azure-ai-textanalytics-5.3.0b2/samples/sample_abstractive_summary.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,44 +1,40 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 
 """
-FILE: sample_abstract_summary.py
+FILE: sample_abstractive_summary.py
 
 DESCRIPTION:
     This sample demonstrates how to submit text documents for abstractive text summarization.
     Abstractive summarization is available as an action type through the begin_analyze_actions API.
 
     Abstractive summarization generates a summary that may not use the same words as those in
     the document, but captures the main idea.
 
     The abstractive summarization feature is part of a gated preview. Request access here:
     https://aka.ms/applyforgatedsummarizationfeatures
 
 USAGE:
-    python sample_abstract_summary.py
+    python sample_abstractive_summary.py
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
 """
 
 
-import os
-
-
 def sample_abstractive_summarization() -> None:
+    # [START abstractive_summary]
+    import os
     from azure.core.credentials import AzureKeyCredential
-    from azure.ai.textanalytics import (
-        TextAnalyticsClient,
-        AbstractSummaryAction
-    )
+    from azure.ai.textanalytics import TextAnalyticsClient
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
 
     text_analytics_client = TextAnalyticsClient(
         endpoint=endpoint,
         credential=AzureKeyCredential(key),
@@ -59,28 +55,22 @@
         "recognition, machine translation, conversational question answering, machine reading comprehension, "
         "and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious "
         "aspiration to produce a leap in AI capabilities, achieving multisensory and multilingual learning that "
         "is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational "
         "component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks."
     ]
 
-    poller = text_analytics_client.begin_analyze_actions(
-        document,
-        actions=[
-            AbstractSummaryAction(),
-        ],
-    )
-
-    document_results = poller.result()
-    for abstract_summary_results in document_results:
-        for result in abstract_summary_results:
-            if result.kind == "AbstractiveSummarization":
-                print("Summaries abstracted:")
-                [print(f"{summary.text}\n") for summary in result.summaries]
-            elif result.is_error is True:
-                print("...Is an error with code '{}' and message '{}'".format(
-                    result.code, result.message
-                ))
+    poller = text_analytics_client.begin_abstractive_summary(document)
+    abstractive_summary_results = poller.result()
+    for result in abstractive_summary_results:
+        if result.kind == "AbstractiveSummarization":
+            print("Summaries abstracted:")
+            [print(f"{summary.text}\n") for summary in result.summaries]
+        elif result.is_error is True:
+            print("...Is an error with code '{}' and message '{}'".format(
+                result.error.code, result.error.message
+            ))
+    # [END abstractive_summary]
 
 
 if __name__ == "__main__":
     sample_abstractive_summarization()
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/sample_alternative_document_input.py` & `azure-ai-textanalytics-5.3.0b2/samples/sample_alternative_document_input.py`

 * *Files 2% similar despite different names*

```diff
@@ -15,18 +15,17 @@
     python sample_alternative_document_input.py
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
 """
 
-import os
-
 
 def sample_alternative_document_input() -> None:
+    import os
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics import TextAnalyticsClient
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
 
     text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/sample_dynamic_classification.py` & `azure-ai-textanalytics-5.3.0b2/samples/sample_dynamic_classification.py`

 * *Files 2% similar despite different names*

```diff
@@ -19,19 +19,17 @@
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
 """
 
 
-import os
-
-
 def sample_dynamic_classification() -> None:
     # [START dynamic_classification]
+    import os
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics import TextAnalyticsClient
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
 
     text_analytics_client = TextAnalyticsClient(
@@ -54,14 +52,14 @@
             print(f"\n'{doc}' classifications:\n")
             for classification in classifications:
                 print("Category '{}' with confidence score {}.".format(
                     classification.category, classification.confidence_score
                 ))
         elif classification_result.is_error is True:
             print("Document '{}' has an error with code '{}' and message '{}'".format(
-                doc, classification_result.code, classification_result.message
+                doc, classification_result.error.code, classification_result.error.message
             ))
     # [END dynamic_classification]
 
 
 if __name__ == "__main__":
     sample_dynamic_classification()
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/sample_analyze_sentiment.py` & `azure-ai-textanalytics-5.3.0b2/samples/sample_analyze_sentiment.py`

 * *Files 1% similar despite different names*

```diff
@@ -19,28 +19,27 @@
     python sample_analyze_sentiment.py
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
 """
 
-import os
-
 
 def sample_analyze_sentiment() -> None:
     print(
         "In this sample we will be combing through reviews customers have left about their"
         "experience using our skydiving company, Contoso."
     )
     print(
         "We start out with a list of reviews. Let us extract the reviews we are sure are "
         "positive, so we can display them on our website and get even more customers!"
     )
 
     # [START analyze_sentiment]
+    import os
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics import TextAnalyticsClient
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
 
     text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/sample_extract_summary.py` & `azure-ai-textanalytics-5.3.0b2/samples/sample_extract_summary.py`

 * *Files 9% similar despite different names*

```diff
@@ -16,23 +16,19 @@
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
 """
 
 
-import os
-
-
 def sample_extractive_summarization():
+    # [START extract_summary]
+    import os
     from azure.core.credentials import AzureKeyCredential
-    from azure.ai.textanalytics import (
-        TextAnalyticsClient,
-        ExtractSummaryAction
-    )
+    from azure.ai.textanalytics import TextAnalyticsClient
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
 
     text_analytics_client = TextAnalyticsClient(
         endpoint=endpoint,
         credential=AzureKeyCredential(key),
@@ -53,29 +49,23 @@
         "recognition, machine translation, conversational question answering, machine reading comprehension, "
         "and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious "
         "aspiration to produce a leap in AI capabilities, achieving multisensory and multilingual learning that "
         "is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational "
         "component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks."
     ]
 
-    poller = text_analytics_client.begin_analyze_actions(
-        document,
-        actions=[
-            ExtractSummaryAction(),
-        ],
-    )
-
-    document_results = poller.result()
-    for extract_summary_results in document_results:
-        for result in extract_summary_results:
-            if result.kind == "ExtractiveSummarization":
-                print("Summary extracted: \n{}".format(
-                    " ".join([sentence.text for sentence in result.sentences]))
-                )
-            elif result.is_error is True:
-                print("...Is an error with code '{}' and message '{}'".format(
-                    result.code, result.message
-                ))
+    poller = text_analytics_client.begin_extract_summary(document)
+    extract_summary_results = poller.result()
+    for result in extract_summary_results:
+        if result.kind == "ExtractiveSummarization":
+            print("Summary extracted: \n{}".format(
+                " ".join([sentence.text for sentence in result.sentences]))
+            )
+        elif result.is_error is True:
+            print("...Is an error with code '{}' and message '{}'".format(
+                result.error.code, result.error.message
+            ))
+    # [END extract_summary]
 
 
 if __name__ == "__main__":
     sample_extractive_summarization()
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/sample_model_version.py` & `azure-ai-textanalytics-5.3.0b2/samples/sample_model_version.py`

 * *Files 2% similar despite different names*

```diff
@@ -20,19 +20,18 @@
     python sample_model_version.py
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
 """
 
-import os
-
 
 def sample_model_version() -> None:
     print("--------------Choosing model_version sample--------------")
+    import os
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics import TextAnalyticsClient, RecognizeEntitiesAction
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
 
     text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))
@@ -63,13 +62,13 @@
     for action_results in document_results:
         action_result = action_results[0]
         if action_result.kind == "EntityRecognition":
             for entity in action_result.entities:
                 print(f"......Entity '{entity.text}' has category '{entity.category}'")
         elif action_result.is_error is True:
             print("......Is an error with code '{}' and message '{}'".format(
-                action_result.code, action_result.message
+                action_result.error.code, action_result.error.message
             ))
 
 
 if __name__ == '__main__':
     sample_model_version()
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/sample_detect_language.py` & `azure-ai-textanalytics-5.3.0b2/samples/sample_detect_language.py`

 * *Files 0% similar despite different names*

```diff
@@ -19,24 +19,23 @@
     python sample_detect_language.py
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
 """
 
-import os
-
 
 def sample_detect_language() -> None:
     print(
         "In this sample we own a hotel with customers from all around the globe. We want to eventually "
         "translate these reviews into English so our manager can read them. However, we first need to know which language "
         "they are in for more accurate translation. This is the step we will be covering in this sample\n"
     )
     # [START detect_language]
+    import os
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics import TextAnalyticsClient
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
 
     text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/text_samples/custom_entities_sample.txt` & `azure-ai-textanalytics-5.3.0b2/samples/text_samples/custom_entities_sample.txt`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/text_samples/custom_classify_sample.txt` & `azure-ai-textanalytics-5.3.0b2/samples/text_samples/custom_classify_sample.txt`

 * *Files identical despite different names*

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_extract_key_phrases_async.py` & `azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_extract_key_phrases_async.py`

 * *Files 0% similar despite different names*

```diff
@@ -17,24 +17,24 @@
     python sample_extract_key_phrases_async.py
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
 """
 
-import os
 import asyncio
 
 
 async def sample_extract_key_phrases_async() -> None:
     print(
         "In this sample, we want to find the articles that mention Microsoft to read."
     )
-    articles_that_mention_microsoft = []
+
     # [START extract_key_phrases_async]
+    import os
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics.aio import TextAnalyticsClient
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
 
     text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))
@@ -53,14 +53,15 @@
         once workers no longer have to work remotely...
         """
     ]
 
     async with text_analytics_client:
         result = await text_analytics_client.extract_key_phrases(articles)
 
+    articles_that_mention_microsoft = []
     for idx, doc in enumerate(result):
         if not doc.is_error:
             print("Key phrases in article #{}: {}".format(
                 idx + 1,
                 ", ".join(doc.key_phrases)
             ))
     # [END extract_key_phrases_async]
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_get_detailed_diagnostics_information_async.py` & `azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_get_detailed_diagnostics_information_async.py`

 * *Files 0% similar despite different names*

```diff
@@ -15,25 +15,24 @@
     python sample_get_detailed_diagnostics_information_async.py
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
 """
 
-import os
 import asyncio
-import logging
-import json
-
-_LOGGER = logging.getLogger(__name__)
 
 
 async def sample_get_detailed_diagnostics_information_async() -> None:
+    import os
+    import logging
+    import json
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics.aio import TextAnalyticsClient
+    _LOGGER = logging.getLogger(__name__)
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
 
     # This client will log detailed information about its HTTP sessions, at DEBUG level
     text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key), logging_enable=True)
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_analyze_sentiment_with_opinion_mining_async.py` & `azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_analyze_sentiment_with_opinion_mining_async.py`

 * *Files 3% similar despite different names*

```diff
@@ -38,21 +38,20 @@
     Users have made 1 complaints about 'service', specifically saying that it's 'unacceptable'
     Users have made 3 complaints about 'toilet', specifically saying that it's 'smelly', 'broken', 'dirty'
 
 
     Looking at the breakdown, I can see what aspects of my hotel need improvement, and based off of both the number and content of the complaints users have made about my toilets, I need to get that fixed ASAP.
 """
 
-from __future__ import annotations
-import os
 import asyncio
-import typing
 
 
 async def sample_analyze_sentiment_with_opinion_mining() -> None:
+    import os
+    import typing
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics.aio import TextAnalyticsClient
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
 
     text_analytics_client = TextAnalyticsClient(
@@ -100,30 +99,31 @@
         "let's find the complaints users have about individual aspects of this hotel"
     )
 
     print(
         "\nIn order to do that, I'm going to extract the targets of a negative sentiment. "
         "I'm going to map each of these targets to the mined opinion object we get back to aggregate the reviews by target. "
     )
-    target_to_complaints: dict[str, typing.Any] = {}
+    target_to_complaints: typing.Dict[str, typing.Any] = {}
 
     for document in doc_result:
         for sentence in document.sentences:
-            for mined_opinion in sentence.mined_opinions:
-                target = mined_opinion.target
-                if target.sentiment == 'negative':
-                    target_to_complaints.setdefault(target.text, [])
-                    target_to_complaints[target.text].append(mined_opinion)
+            if sentence.mined_opinions:
+                for mined_opinion in sentence.mined_opinions:
+                    target = mined_opinion.target
+                    if target.sentiment == 'negative':
+                        target_to_complaints.setdefault(target.text, [])
+                        target_to_complaints[target.text].append(mined_opinion)
 
     print("\nLet's now go through the aspects of our hotel people have complained about and see what users have specifically said")
 
-    for target, complaints in target_to_complaints.items():
+    for target_name, complaints in target_to_complaints.items():
         print("Users have made {} complaint(s) about '{}', specifically saying that it's '{}'".format(
             len(complaints),
-            target,
+            target_name,
             "', '".join(
                 [assessment.text for complaint in complaints for assessment in complaint.assessments]
             )
         ))
 
 
     print(
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_single_label_classify_async.py` & `azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_single_label_classify_async.py`

 * *Files 8% similar despite different names*

```diff
@@ -22,21 +22,20 @@
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
     3) SINGLE_LABEL_CLASSIFY_PROJECT_NAME - your Language Studio project name
     4) SINGLE_LABEL_CLASSIFY_DEPLOYMENT_NAME - your Language Studio deployment name
 """
 
-
-import os
 import asyncio
 
 
 async def sample_classify_document_single_label_async() -> None:
     # [START single_label_classify_async]
+    import os
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics.aio import TextAnalyticsClient
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
     project_name = os.environ["SINGLE_LABEL_CLASSIFY_PROJECT_NAME"]
     deployment_name = os.environ["SINGLE_LABEL_CLASSIFY_DEPLOYMENT_NAME"]
@@ -74,15 +73,15 @@
         if classification_result.kind == "CustomDocumentClassification":
             classification = classification_result.classifications[0]
             print("The document text '{}' was classified as '{}' with confidence score {}.".format(
                 doc, classification.category, classification.confidence_score)
             )
         elif classification_result.is_error is True:
             print("Document text '{}' has an error with code '{}' and message '{}'".format(
-                doc, classification_result.code, classification_result.message
+                doc, classification_result.error.code, classification_result.error.message
             ))
     # [END single_label_classify_async]
 
 
 async def main():
     await sample_classify_document_single_label_async()
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_multi_label_classify_async.py` & `azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_multi_label_classify_async.py`

 * *Files 2% similar despite different names*

```diff
@@ -22,21 +22,20 @@
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
     3) MULTI_LABEL_CLASSIFY_PROJECT_NAME - your Language Studio project name
     4) MULTI_LABEL_CLASSIFY_DEPLOYMENT_NAME - your Language Studio deployment name
 """
 
-
-import os
 import asyncio
 
 
 async def sample_classify_document_multi_label_async() -> None:
     # [START multi_label_classify_async]
+    import os
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics.aio import TextAnalyticsClient
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
     project_name = os.environ["MULTI_LABEL_CLASSIFY_PROJECT_NAME"]
     deployment_name = os.environ["MULTI_LABEL_CLASSIFY_DEPLOYMENT_NAME"]
@@ -75,15 +74,15 @@
             print(f"\nThe movie plot '{doc}' was classified as the following genres:\n")
             for classification in classifications:
                 print("'{}' with confidence score {}.".format(
                     classification.category, classification.confidence_score
                 ))
         elif classification_result.is_error is True:
             print("Movie plot '{}' has an error with code '{}' and message '{}'".format(
-                doc, classification_result.code, classification_result.message
+                doc, classification_result.error.code, classification_result.error.message
             ))
     # [END multi_label_classify_async]
 
 
 async def main():
     await sample_classify_document_multi_label_async()
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_extract_summary_async.py` & `azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_extract_summary_async.py`

 * *Files 13% similar despite different names*

```diff
@@ -15,23 +15,22 @@
     python sample_extract_summary_async.py
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
 """
 
-
-import os
 import asyncio
 
 
 async def sample_extractive_summarization_async():
+    # [START extract_summary_async]
+    import os
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics.aio import TextAnalyticsClient
-    from azure.ai.textanalytics import ExtractSummaryAction
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
 
     text_analytics_client = TextAnalyticsClient(
         endpoint=endpoint,
         credential=AzureKeyCredential(key),
@@ -53,32 +52,26 @@
         "and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious "
         "aspiration to produce a leap in AI capabilities, achieving multisensory and multilingual learning that "
         "is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational "
         "component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks."
     ]
 
     async with text_analytics_client:
-        poller = await text_analytics_client.begin_analyze_actions(
-            document,
-            actions=[
-                ExtractSummaryAction(),
-            ],
-        )
-
-        document_results = await poller.result()
-        async for extract_summary_results in document_results:
-            for result in extract_summary_results:
-                if result.kind == "ExtractiveSummarization":
-                    print("Summary extracted: \n{}".format(
-                        " ".join([sentence.text for sentence in result.sentences]))
-                    )
-                elif result.is_error is True:
-                    print("...Is an error with code '{}' and message '{}'".format(
-                        result.code, result.message
-                    ))
+        poller = await text_analytics_client.begin_extract_summary(document)
+        extract_summary_results = await poller.result()
+        async for result in extract_summary_results:
+            if result.kind == "ExtractiveSummarization":
+                print("Summary extracted: \n{}".format(
+                    " ".join([sentence.text for sentence in result.sentences]))
+                )
+            elif result.is_error is True:
+                print("...Is an error with code '{}' and message '{}'".format(
+                    result.error.code, result.error.message
+                ))
+    # [END extract_summary_async]
 
 
 async def main():
     await sample_extractive_summarization_async()
 
 
 if __name__ == '__main__':
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_analyze_sentiment_async.py` & `azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_analyze_sentiment_async.py`

 * *Files 1% similar despite different names*

```diff
@@ -19,28 +19,28 @@
     python sample_analyze_sentiment_async.py
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
 """
 
-import os
 import asyncio
 
 
 async def sample_analyze_sentiment_async() -> None:
     print(
         "In this sample we will be combing through reviews customers have left about their"
         "experience using our skydiving company, Contoso."
     )
     print(
         "We start out with a list of reviews. Let us extract the reviews we are sure are "
         "positive, so we can display them on our website and get even more customers!"
     )
     # [START analyze_sentiment_async]
+    import os
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics.aio import TextAnalyticsClient
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
 
     text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_analyze_healthcare_action_async.py` & `azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_analyze_healthcare_action_async.py`

 * *Files 4% similar despite different names*

```diff
@@ -17,20 +17,19 @@
     python sample_analyze_healthcare_action_async.py
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
 """
 
-
-import os
 import asyncio
 
 
 async def sample_analyze_healthcare_action() -> None:
+    import os
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics.aio import TextAnalyticsClient
     from azure.ai.textanalytics import (
         AnalyzeHealthcareEntitiesAction,
         RecognizePiiEntitiesAction,
     )
 
@@ -98,21 +97,21 @@
                         print(f"Relation of type: {relation.relation_type} has the following roles")
                         for role in relation.roles:
                             print(f"...Role '{role.name}' with entity '{role.entity.text}'")
                     print(f"......FHIR object: {result.fhir_bundle}")
 
                 elif result.kind == "PiiEntityRecognition":
                     print("Results of Recognize PII Entities action:")
-                    for entity in result.entities:
-                        print(f"......Entity: {entity.text}")
-                        print(f".........Category: {entity.category}")
-                        print(f".........Confidence Score: {entity.confidence_score}")
+                    for pii_entity in result.entities:
+                        print(f"......Entity: {pii_entity.text}")
+                        print(f".........Category: {pii_entity.category}")
+                        print(f".........Confidence Score: {pii_entity.confidence_score}")
 
                 elif result.is_error is True:
-                    print(f"...Is an error with code '{result.code}' and message '{result.message}'")
+                    print(f"...Is an error with code '{result.error.code}' and message '{result.error.message}'")
 
                 print("------------------------------------------")
 
 
 async def main():
     await sample_analyze_healthcare_action()
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_authentication_async.py` & `azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_authentication_async.py`

 * *Files 1% similar despite different names*

```diff
@@ -24,21 +24,21 @@
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language API key
     3) AZURE_CLIENT_ID - the client ID of your active directory application.
     4) AZURE_TENANT_ID - the tenant ID of your active directory application.
     5) AZURE_CLIENT_SECRET - the secret of your active directory application.
 """
 
-import os
 import asyncio
 
 
 async def sample_authentication_with_api_key_credential_async() -> None:
     print("\n.. authentication_with_api_key_credential_async")
     # [START create_ta_client_with_key_async]
+    import os
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics.aio import TextAnalyticsClient
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
 
     text_analytics_client = TextAnalyticsClient(endpoint, AzureKeyCredential(key))
     # [END create_ta_client_with_key_async]
@@ -58,14 +58,15 @@
 
 async def sample_authentication_with_azure_active_directory_async() -> None:
     """DefaultAzureCredential will use the values from these environment
     variables: AZURE_CLIENT_ID, AZURE_TENANT_ID, AZURE_CLIENT_SECRET
     """
     print("\n.. authentication_with_azure_active_directory_async")
     # [START create_ta_client_with_aad_async]
+    import os
     from azure.ai.textanalytics.aio import TextAnalyticsClient
     from azure.identity.aio import DefaultAzureCredential
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     credential = DefaultAzureCredential()
 
     text_analytics_client = TextAnalyticsClient(endpoint, credential=credential)
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_recognize_custom_entities_async.py` & `azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_recognize_custom_entities_async.py`

 * *Files 4% similar despite different names*

```diff
@@ -20,21 +20,20 @@
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
     3) CUSTOM_ENTITIES_PROJECT_NAME - your Language Language Studio project name
     4) CUSTOM_ENTITIES_DEPLOYMENT_NAME - your Language deployed model name
 """
 
-
-import os
 import asyncio
 
 
 async def sample_recognize_custom_entities_async() -> None:
     # [START recognize_custom_entities_async]
+    import os
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics.aio import TextAnalyticsClient
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
     project_name = os.environ["CUSTOM_ENTITIES_PROJECT_NAME"]
     deployment_name = os.environ["CUSTOM_ENTITIES_DEPLOYMENT_NAME"]
@@ -70,15 +69,15 @@
                     print(
                         "Entity '{}' has category '{}' with confidence score of '{}'".format(
                             entity.text, entity.category, entity.confidence_score
                         )
                     )
             elif custom_entities_result.is_error is True:
                 print("...Is an error with code '{}' and message '{}'".format(
-                    custom_entities_result.code, custom_entities_result.message
+                    custom_entities_result.error.code, custom_entities_result.error.message
                     )
                 )
     # [END recognize_custom_entities_async]
 
 
 async def main():
     await sample_recognize_custom_entities_async()
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_recognize_linked_entities_async.py` & `azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_recognize_linked_entities_async.py`

 * *Files 2% similar despite different names*

```diff
@@ -20,25 +20,25 @@
     python sample_recognize_linked_entities_async.py
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
 """
 
-import os
 import asyncio
 
 
 async def sample_recognize_linked_entities_async() -> None:
     print(
         "In this sample, we are students conducting research for a class project. We will extract "
         "links to Wikipedia articles for all entities listed in our research documents, so we have "
         "all of the necessary information for research purposes."
     )
     # [START recognize_linked_entities_async]
+    import os
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics.aio import TextAnalyticsClient
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
 
     text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))
@@ -68,17 +68,17 @@
                 entity.name, len(entity.matches)
             ))
             if entity.data_source == "Wikipedia":
                 entity_to_url[entity.name] = entity.url
     # [END recognize_linked_entities_async]
 
     print("\nNow let's see all of the Wikipedia articles we've extracted from our research documents")
-    for entity, url in entity_to_url.items():
+    for entity_name, url in entity_to_url.items():
         print("Link to Wikipedia article for '{}': {}".format(
-                entity, url
+                entity_name, url
         ))
 
 
 async def main():
     await sample_recognize_linked_entities_async()
 
 if __name__ == '__main__':
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_analyze_actions_async.py` & `azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_analyze_actions_async.py`

 * *Files 1% similar despite different names*

```diff
@@ -17,21 +17,20 @@
     python sample_analyze_actions_async.py
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
 """
 
-
-import os
 import asyncio
 
 
 async def sample_analyze_async() -> None:
     # [START analyze_async]
+    import os
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics.aio import TextAnalyticsClient
     from azure.ai.textanalytics import (
         RecognizeEntitiesAction,
         RecognizeLinkedEntitiesAction,
         RecognizePiiEntitiesAction,
         ExtractKeyPhrasesAction,
@@ -90,18 +89,18 @@
                     print(f"......Entity: {entity.text}")
                     print(f".........Category: {entity.category}")
                     print(f".........Confidence Score: {entity.confidence_score}")
                     print(f".........Offset: {entity.offset}")
 
             elif result.kind == "PiiEntityRecognition":
                 print("...Results of Recognize PII Entities action:")
-                for entity in result.entities:
-                    print(f"......Entity: {entity.text}")
-                    print(f".........Category: {entity.category}")
-                    print(f".........Confidence Score: {entity.confidence_score}")
+                for pii_entity in result.entities:
+                    print(f"......Entity: {pii_entity.text}")
+                    print(f".........Category: {pii_entity.category}")
+                    print(f".........Confidence Score: {pii_entity.confidence_score}")
 
             elif result.kind == "KeyPhraseExtraction":
                 print("...Results of Extract Key Phrases action:")
                 print(f"......Key Phrases: {result.key_phrases}")
 
             elif result.kind == "EntityLinking":
                 print("...Results of Recognize Linked Entities action:")
@@ -127,15 +126,15 @@
                     f"......Scores: positive={result.confidence_scores.positive}; \
                     neutral={result.confidence_scores.neutral}; \
                     negative={result.confidence_scores.negative} \n"
                 )
 
             elif result.is_error is True:
                 print(
-                    f"...Is an error with code '{result.code}' and message '{result.message}'"
+                    f"...Is an error with code '{result.error.code}' and message '{result.error.message}'"
                 )
 
         print("------------------------------------------")
 
     # [END analyze_async]
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_recognize_entities_async.py` & `azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_recognize_entities_async.py`

 * *Files 12% similar despite different names*

```diff
@@ -16,26 +16,26 @@
     python sample_recognize_entities_async.py
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
 """
 
-from __future__ import annotations
-import os
 import asyncio
 
 
 async def sample_recognize_entities_async() -> None:
     print(
         "In this sample, we are a catering business, and we're looking to sort the reviews "
         "for our organization based off of the organization that hired us for catering"
     )
-    organization_to_reviews: dict[str, list[str]] = {}
+
     # [START recognize_entities_async]
+    import os
+    import typing
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics.aio import TextAnalyticsClient
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
 
     text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))
@@ -48,29 +48,31 @@
         """Bar Company is over the moon about the service we received from Contoso, the best sliders ever!!!!"""
     ]
 
     async with text_analytics_client:
         result = await text_analytics_client.recognize_entities(reviews)
 
     result = [review for review in result if not review.is_error]
+    organization_to_reviews: typing.Dict[str, typing.List[str]] = {}
 
     for idx, review in enumerate(result):
         for entity in review.entities:
             print(f"Entity '{entity.text}' has category '{entity.category}'")
-    # [END recognize_entities_async]
+
             if entity.category == 'Organization':
                 organization_to_reviews.setdefault(entity.text, [])
                 organization_to_reviews[entity.text].append(reviews[idx])
 
     for organization, reviews in organization_to_reviews.items():
         print(
             "\n\nOrganization '{}' has left us the following review(s): {}".format(
                 organization, "\n\n".join(reviews)
             )
         )
+    # [END recognize_entities_async]
 
 
 async def main():
     await sample_recognize_entities_async()
 
 
 if __name__ == '__main__':
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_detect_language_async.py` & `azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_detect_language_async.py`

 * *Files 1% similar despite different names*

```diff
@@ -19,25 +19,25 @@
     python sample_detect_language_async.py
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
 """
 
-import os
 import asyncio
 
 
 async def sample_detect_language_async() -> None:
     print(
         "In this sample we own a hotel with customers from all around the globe. We want to eventually "
         "translate these reviews into English so our manager can read them. However, we first need to know which language "
         "they are in for more accurate translation. This is the step we will be covering in this sample\n"
     )
     # [START detect_language_async]
+    import os
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics.aio import TextAnalyticsClient
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
 
     text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_analyze_healthcare_entities_async.py` & `azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_analyze_healthcare_entities_async.py`

 * *Files 1% similar despite different names*

```diff
@@ -18,15 +18,14 @@
     python sample_analyze_healthcare_entities_async.py
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
 """
 
-from __future__ import annotations
 import asyncio
 
 
 async def sample_analyze_healthcare_entities_async() -> None:
 
     print(
         "In this sample we will be combing through the prescriptions our pharmacy has fulfilled "
@@ -34,14 +33,15 @@
     )
     print(
         "We start out with a list of prescription documents."
     )
 
     # [START analyze_healthcare_entities_async]
     import os
+    import typing
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics import HealthcareEntityRelation
     from azure.ai.textanalytics.aio import TextAnalyticsClient
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
 
@@ -102,15 +102,15 @@
         "Now, I will create a dictionary of medication to total dosage. "
         "I will use a regex to extract the dosage amount. For simplicity sake, I will assume "
         "all dosages are represented with numbers and have mg unit."
     )
     import re
     from collections import defaultdict
 
-    medication_to_dosage: dict[str, int] = defaultdict(int)
+    medication_to_dosage: typing.Dict[str, int] = defaultdict(int)
 
     for relation in dosage_of_medication_relations:
         # The DosageOfMedication relation should only contain the dosage and medication roles
 
         dosage_role = next(filter(lambda x: x.name == "Dosage", relation.roles))
         medication_role = next(filter(lambda x: x.name == "Medication", relation.roles))
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_recognize_pii_entities_async.py` & `azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_recognize_pii_entities_async.py`

 * *Files 11% similar despite different names*

```diff
@@ -10,34 +10,36 @@
 DESCRIPTION:
     This sample demonstrates how to recognize personally identifiable information in a batch of documents.
     The endpoint recognize_pii_entities is only available for API version v3.1 and up.
 
     In this sample, we will be working for a company that handles loan payments. To follow privacy guidelines,
     we need to redact all of our information before we make it public.
 
+    See more information in the service docs: https://aka.ms/azsdk/language/pii
+
 USAGE:
     python sample_recognize_pii_entities_async.py
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
 """
 
-import os
 import asyncio
 
 
 async def sample_recognize_pii_entities_async() -> None:
     print(
         "In this sample we will be going through our customer's loan payment information and redacting "
         "all PII (personally identifiable information) before storing this information on our public website. "
         "I'm also looking to explicitly extract the SSN information, so I can update my database with SSNs for "
         "our customers"
     )
     # [START recognize_pii_entities_async]
+    import os
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics.aio import TextAnalyticsClient
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
 
     text_analytics_client = TextAnalyticsClient(
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_dynamic_classification_async.py` & `azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_dynamic_classification_async.py`

 * *Files 2% similar despite different names*

```diff
@@ -18,22 +18,20 @@
     python sample_dynamic_classification_async.py
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
 """
 
-
-from cgitb import text
-import os
 import asyncio
 
 
 async def sample_dynamic_classification_async() -> None:
     # [START dynamic_classification_async]
+    import os
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics.aio import TextAnalyticsClient
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
 
     text_analytics_client = TextAnalyticsClient(
@@ -58,14 +56,14 @@
             print(f"\n'{doc}' classifications:\n")
             for classification in classifications:
                 print("Category '{}' with confidence score {}.".format(
                     classification.category, classification.confidence_score
                 ))
         elif classification_result.is_error is True:
             print("Document '{}' has an error with code '{}' and message '{}'".format(
-                doc, classification_result.code, classification_result.message
+                doc, classification_result.error.code, classification_result.error.message
             ))
     # [END dynamic_classification_async]
 
 
 if __name__ == "__main__":
     asyncio.run(sample_dynamic_classification_async())
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_abstract_summary_async.py` & `azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_abstractive_summary_async.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,43 +1,42 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 
 """
-FILE: sample_abstract_summary_async.py
+FILE: sample_abstractive_summary_async.py
 
 DESCRIPTION:
     This sample demonstrates how to submit text documents for abstractive text summarization.
     Abstractive summarization is available as an action type through the begin_analyze_actions API.
 
     Abstractive summarization generates a summary that may not use the same words as those in
     the document, but captures the main idea.
 
     The abstractive summarization feature is part of a gated preview. Request access here:
     https://aka.ms/applyforgatedsummarizationfeatures
 
 USAGE:
-    python sample_abstract_summary_async.py
+    python sample_abstractive_summary_async.py
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
 """
 
-
-import os
 import asyncio
 
 
 async def sample_abstractive_summarization_async() -> None:
+    # [START abstractive_summary_async]
+    import os
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics.aio import TextAnalyticsClient
-    from azure.ai.textanalytics import AbstractSummaryAction
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
 
     text_analytics_client = TextAnalyticsClient(
         endpoint=endpoint,
         credential=AzureKeyCredential(key),
@@ -58,31 +57,25 @@
         "recognition, machine translation, conversational question answering, machine reading comprehension, "
         "and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious "
         "aspiration to produce a leap in AI capabilities, achieving multisensory and multilingual learning that "
         "is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational "
         "component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks."
     ]
     async with text_analytics_client:
-        poller = await text_analytics_client.begin_analyze_actions(
-            document,
-            actions=[
-                AbstractSummaryAction(),
-            ],
-        )
-
-        document_results = await poller.result()
-        async for abstract_summary_results in document_results:
-            for result in abstract_summary_results:
-                if result.kind == "AbstractiveSummarization":
-                    print("Summaries abstracted:")
-                    [print(f"{summary.text}\n") for summary in result.summaries]
-                elif result.is_error is True:
-                    print("...Is an error with code '{}' and message '{}'".format(
-                        result.code, result.message
-                    ))
+        poller = await text_analytics_client.begin_abstractive_summary(document)
+        abstractive_summary_results = await poller.result()
+        async for result in abstractive_summary_results:
+            if result.kind == "AbstractiveSummarization":
+                print("Summaries abstracted:")
+                [print(f"{summary.text}\n") for summary in result.summaries]
+            elif result.is_error is True:
+                print("...Is an error with code '{}' and message '{}'".format(
+                    result.error.code, result.error.message
+                ))
+    # [END abstractive_summary_async]
 
 
 async def main():
     await sample_abstractive_summarization_async()
 
 
 if __name__ == '__main__':
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_model_version_async.py` & `azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_model_version_async.py`

 * *Files 3% similar despite different names*

```diff
@@ -20,20 +20,20 @@
     python sample_model_version_async.py
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
 """
 
-import os
 import asyncio
 
 
 async def sample_model_version_async() -> None:
     print("--------------Choosing model_version sample--------------")
+    import os
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics.aio import TextAnalyticsClient
     from azure.ai.textanalytics import RecognizeEntitiesAction
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
 
@@ -66,15 +66,15 @@
         async for action_results in document_results:
             action_result = action_results[0]
             if action_result.kind == "EntityRecognition":
                 for entity in action_result.entities:
                     print(f"......Entity '{entity.text}' has category '{entity.category}'")
             elif action_result.is_error is True:
                 print("......Is an error with code '{}' and message '{}'".format(
-                    action_result.code, action_result.message
+                    action_result.error.code, action_result.error.message
                 ))
 
 
 async def main():
     await sample_model_version_async()
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_alternative_document_input_async.py` & `azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_alternative_document_input_async.py`

 * *Files 1% similar despite different names*

```diff
@@ -15,19 +15,19 @@
     python sample_alternative_document_input_async.py
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
 """
 
-import os
 import asyncio
 
 
 async def sample_alternative_document_input() -> None:
+    import os
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics.aio import TextAnalyticsClient
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
 
     text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))
```

## Comparing `azure-ai-textanalytics-5.3.0b1/samples/async_samples/sample_analyze_healthcare_entities_with_cancellation_async.py` & `azure-ai-textanalytics-5.3.0b2/samples/async_samples/sample_analyze_healthcare_entities_with_cancellation_async.py`

 * *Files 1% similar despite different names*

```diff
@@ -14,22 +14,21 @@
     python sample_analyze_healthcare_entities_with_cancellation.py
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.
     2) AZURE_LANGUAGE_KEY - your Language subscription key
 """
 
-
-import os
 import asyncio
-from azure.core.exceptions import HttpResponseError
 
 
 async def sample_analyze_healthcare_entities_with_cancellation_async() -> None:
     # [START analyze_healthcare_entities_with_cancellation_async]
+    import os
+    from azure.core.exceptions import HttpResponseError
     from azure.core.credentials import AzureKeyCredential
     from azure.ai.textanalytics.aio import TextAnalyticsClient
 
     endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
     key = os.environ["AZURE_LANGUAGE_KEY"]
 
     text_analytics_client = TextAnalyticsClient(
```

